doi: 10.1111/isj.12072 Info Systems J (2016) 26, 585–611 585

Theorising about gender and computing interventions through an evaluation framework
Annemieke Craig*
Department of Information Systems and Business Analytics, Faculty of Business & Law, Deakin University, Geelong Waterfront Campus, Locked Bag 20001, Geelong 3220, VIC, Australia, email: acraig@deakin.edu.au

Abstract. Despite over 20 years of intervention programmes, the gender balance in the computing profession is not improving. It has been suggested that the problem with much of the research on gender and computing is that it is undertheorised. The contribution of this study is an evaluation framework, designed to evaluate gender and computing interventions, which will advance the theorisation of research through programme evaluation. This study was undertaken in three phases: The ﬁrst involved theory building through an extensive review of the literature resulting in a conceptual framework for intervention programme evaluation. The second phase consisted of a multiple-case study of 14 major intervention programmes in Australia. Subsequent modiﬁcations to the conceptual framework resulted in the gender and computing intervention evaluation framework. The value of the framework was conﬁrmed in phase three by intervention experts and showed that applying the framework will help programme champions to evaluate their programmes more thoroughly. The dissemination of sound evaluation results will then enable a deeper theorisation of the issues surrounding gender and computing interventions. Keywords: gender, computing, theory, diversity in computing, evaluation framework

INTRODUCTION

We need to know enough about our history not only to both protect ourselves from it and not be condemned to repeat it, but also to use it to our advantage. (Lee and Winkler, 1995, p. x) Computing is a relatively young profession compared with other disciplines such as medicine or law. Australia moved into the modern computing era in the late 1940s with the development of the CSIR Mk1 (Pearcey, 1994). It provided a computing service well into the 1960s, and by then, there were a total of 34 computers in the entire country. From these small beginnings,

© 2015 Wiley Publishing Ltd

586

A Craig

the industry developed rapidly, and by 2013, 83% of Australian households had access to an internet-enabled computer (Australian Bureau of Statistics, 2014). Over the same time period, there was strong growth in new employment opportunities in the emerging computer industry, which became known as the information and communication technology (ICT) sector. In 2012, approximately 544 000 people worked in ICT occupations with the industry contributing almost 8% to Australia’s gross domestic product and the revenue from the sector exceeding $130bn (IBSA, 2013, p. 1). ICT is seen as a key enabler for growth and productivity in the Australian economy. It is expected to continue to contribute signiﬁcantly to growth in productivity and business improvements in the future (Australian Workforce and Productivity Agency, 2013). By 2050, ICT in Australia, enhanced by an emerging national broadband network, is expected to deliver $131bn revenue (Ruthven, 2012). Within this same time frame, computers and computer work have become stereotyped as more appropriate for men (Game & Pringle, 1983; Mackinnon, 1995), and currently, women account for less than 20% of the total Australian ICT workforce (Australian Computer Society, 2013). Australia’s workforce is more gender-segregated than that of most other industrialised countries (Hausmann, Tyson & Zahidi, 2011). Over half of all female employees work as clerical and administrative workers, sales workers or community and personal service workers (Australian Bureau of Statistics, 2010). Women are under-represented in trades and in science, technology, engineering and mathematics positions. Likewise, men are under-represented as teachers and in the health and community services sectors (Australian Bureau of Statistics, 2010). A gender disparity between the number of men and women in leadership roles in Australia also perpetuates existing stereotypes about the role of women, both at work and in the wider society, and exacerbates gender pay inequity (Craig, Coldwell, Fisher & Lang, 2013). The Australian ICT workforce is predominantly male and young, with 67.8% of workers aged between 25 and 44 years, compared with 45.5% of the workforce as a whole (Australian Workforce and Productivity Agency, 2013). Women are highly represented in the less-technical areas of ICT, while men make up the majority of the higher level, higher status and higher-paid ICT workforce (IBSA, 2010, p. 12). The literature provides substantial arguments as to why a gender imbalance in computing is of concern. For example Trauth (2011) states that gender diversity is an economic necessity as well as essential for social justice: ‘women represent half the population and in many societies half the labour force … the “best brains” can be located in a variety of bodies, not just male’ (Trauth, 2011, p. 2). Klawe, Whitney & Simard (2009, p. 68) have suggested that there is a need for more women purely ‘out of self-interest’, and Schabel (2013) argues that there is a need for more women to ensure a sufﬁcient and diverse information technology workforce. Dubow (2013) explains that having diverse teams in the creative process is likely to provide an increase in innovation. The gender imbalance in the ICT workforce is not unique to Australia and is reﬂected in many Western cultures. For example 20% of computer scientists in the USA are women (Sydell, 2013); women represent 14.4% of ICT professionals in the UK (Kirkup, Zalevski, Maruyama & Batool, 2010), and Corneliussen (2010) indicates that in Norway where there is a high expectation of gender equality, there is also an under-representation of women in computing. Lagesen (2007) explains that girls perceived computing as an activity predominantly for
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

587

antisocial boys and men, and thus as incompatible with their image of themselves as young, socially active women. However, studies such as the ones conducted in Malaysia by Adams, Bauer & Baichoo (2003) and in Mauritius (Lagesen, 2008) suggest that the issue is not a universal phenomenon. An intervention programme is an activity conducted with the intention of bringing about change. Because the under-representation of women in computing education and the workforce was ﬁrst identiﬁed in the early 1980s (von Hellens, Trauth & Fisher, 2012), many intervention programmes aimed at the recruitment, retention and advancement of girls and women in computing have been conducted (Klawe, Whitney & Simard, 2009; Clayton, Beekhuyzen & Nielsen, 2012). However, statistics show little improvement in the rate of participation of women in this industry, and anecdotal evidence suggests that these intervention programmes have not been as effective as was anticipated (Craig, 2009). Corneliussen (2012, p. 85) suggests efforts to attract more women to computer education have shown temporary positive results, but have not yet proved to have long-lasting effects. The issues are complex, and hence, it is not clear how to change the gender imbalance (Hayes, 2010, p. 46). This may be associated with the problem of understanding what is meant by gender. There are numerous theories regarding the notion of gender and the reasons for similarities and differences between men and women. Trauth’s (2013) critical analysis of research papers in information systems journals, on the topic of gender and information systems, found that the gender theories evident between 1992 and 2012 ‘fall into three broad categories: gender essentialism, social shaping of gender, and gender intersectionality’ (p. 284). Gender essentialism refers to research where differences between men and women are attributed to underlying and ﬁxed biological or psychological differences. The second category – the shaping of gender and gender roles through social construction – assumes that ‘human actions are the product of the culture in which people are born and raised (Berger & Luckmann, 1966)’ as cited in Ridley & Young (2012). These two categories involve a gender binary where all men are a single group and all women are another single group. Trauth (2013) explains that this binary division makes any minority group invisible. Gender intersectionality, the third category, does not involve such binaries and acknowledges the variety of factors inﬂuencing gender relations such as gender identity, sexual orientation, race and ethnicity. To advance our understanding, Trauth (2013, p. 288) argues that ‘gender research’ should not be limited to ‘gender differences’ research and there is a ‘need to move beyond simple description (e.g. women behave this way; men behave that way) that leads to group level stereotypes (e.g. all men relate to technology in this way; all women relate to technology in that way) to include more nuanced analyses of the phenomena’. However, some studies show that societal factors such as race, gender and social class appear to have a greater impact on young peoples’ decisions than on the desires and aspirations of the individual (Bassot, 2012, p. 3). In providing insights from a social constructivist perspective, Bassot (2012) built on the application of collectivist interpretations of the zone of proximal development, of situated learning and of activity theory when looking at equality and social justice in the practice of career guidance. Bassot (2012, p. 3) argues that social constructivism asserts that individuals cannot be separated from their social contexts, and that social context is dynamic and constantly changing. To produce successful inclusion strategies, Corneliussen
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

588

A Craig

(2012, p. 83) argues that it is necessary to recognise variations and differences, between as well as within the gender categories. In looking at the relationship between gender and technoscience, Sørensen (2002) suggests that inclusion should be seen as a process where attractions are just as important as the lack of negative features (as cited in Lagesen, 2007, p. 68). With regard to the role of technology, Björkman (2005, p. 183) discusses the importance of knowledge and knowledge processes. She notes that the processes that produce knowledge are socially, culturally and historically situated and that technology is both created by and creates cultures. Björkman (2005, p. 186) suggests that feminist research in technologies should work on broadening the concepts and understanding of technology. Spencer (2003) and Björkman (2005) argue that there is nothing inevitable about how computing is constructed; thus, it can be revisioned and re-conceptualised. After 20 years of effort, there continues to be a need to address the under-representation of women in the computing profession (Camp, 2012; DuBow, 2013; Whitney, Gammal, Gee, Mahoney & Simard, 2013). The contribution of intervention strategies remains ambiguous: that their effect has been very limited or that without these intervention programs, the percentage of women in computing may have declined even further. Trauth (2012) argues that women will remain under-represented until there is equality in terms of the numbers of women workers, the pay they receive and in their representation in the higher levels of the power hierarchy. ‘Hence, there remains a need for intervention efforts. Some of these interventions should be directed at women. But others need to be aimed at men in the IT workplace and the larger society’ (Trauth, 2012, p. 52). The Australian Computer Society (2012, p. 26) suggests that without speciﬁc interventions, it is almost inevitable that the decline in women working in the ICT profession will continue. Numerous authors (Howcroft & Trauth, 2008; Quesenberry & Trauth, 2012) suggest that the problem with much of the gender and computing research relating to interventions is that it is currently undertheorised: Thus, an important challenge for ensuring the success of interventions aimed at addressing broader participation in computing is the need to act in accordance with a suitable theory of the problem. Too often well-intentioned individuals embark upon intervention programs without a clear understanding of what “the problem” is for which the intervention is the solution. At best, such endeavors could be ineffective; at worst such “interventions” could end up doing more harm than good if they are reinforcing damaging gender assumptions. Hence, broader participation efforts in the future will need more theorization of “the problem” rather than acting upon an intuitive sense of what is wrong. (Trauth 2012, p. 6) To address this problem, we need to identify, prior to the implementation of any programme, which speciﬁc problem the intervention programme is seeking to address how the programme is expected to work for whom and under what circumstances. In this way, we can build up an understanding of which programmes should be replicated and which should be abandoned, soundly based on a theory of the problem. The aim of the work presented in this article is to present an evaluation framework, based on theory and practice, which will support the evaluation of women and computing intervention
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

589

programmes. The gender and computing intervention evaluation framework will empower key stakeholders conducting these programmes with the knowledge required to conduct useful evaluations. This will make it more likely that sound judgements can be drawn and useful dissemination of results will occur. Adopting this evaluation framework will help advance the theorisation of gender and computing intervention research. Section on Literature Review presents a literature review on intervention programmes and their evaluation, looks at the type of evaluation that is possible and then focuses speciﬁcally on programme theory-driven evaluation. This is followed by the research design of this study in Section on Research Design. Findings from the empirical research, which led to the development of the gender and computing intervention evaluation framework, are presented in Section on Research Findings. The framework is described, and an example of how it could be used is presented. In the ﬁnal phase of the study, the framework and the way it worked were discussed with experts before being applied in another case. Section on Discussion presents the discussion of the framework and is followed by the conclusion, which includes the limitations of the framework.

L I T E R AT U R E R E V I E W

Intervention programmes and their evaluation In Australia, the majority of intervention programmes for women in computing are conducted by volunteers who do not have either the skills or the resources to evaluate the interventions effectively (Craig, 2009). Furthermore, programme champions frequently do not see the need for systematic evaluation as they ‘know’ their programmes will work and hence devote their limited resources to implementing the programmes (Craig, 2009). In evaluating programmes over time that encourage more women into computing, Corneliussen (2012, p. 156) notes that increased access to and use of computer technology has not so far raised girls’ interest in IT education. Indeed, ICTs presented in schools are seen as ‘less interesting and important’ by both girls and boys. The implications of this trend are not clear for gender equity or for the ICT industry overall. However, enjoyment and use of ICTs in leisure time activities indicates that this might be ‘a more important learning arena’ than school and may be where more boys than girls develop an interest in computer science education. Thus, Corneliussen (2012, p. 158) suggests that what is perceived as the boring use of computers at school may be reinforcing girls’ perception that computing is boring, whereas the tendency for computing to be seen as a male domain may be ‘fed by boys who discovered their fascination and motivation for computers in their leisure time activities’. Evaluation of intervention programmes enables programme stakeholders to determine the effectiveness of intervention programmes. For example the analyses of a Norwegian initiative (Lagesen, 2007) to recruit more women students into computer science suggested four main categories for effecting change: ﬁrst, educational reform including improving the learning environment; second, increasing the relative number of women; third, changing the masculine image of computer science; and lastly, changing the content of the ﬁeld. Lagesen’s (2007, p. 86) project was initially designed to pursue all four; however, the main efforts drew on the second
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

590

A Craig

and third components. These created a critical mass through a quota system, a ‘Women’s Day’ and advertising campaigns. The ﬁndings suggest that increasing the relative number of women was the most important result. A substantial increase in women enrolments meant that women felt less visible and received less unwanted attention, which improved their learning environment. Because a variety of programs and strategies have been tried, it would be useful to identify and replicate ‘successful’ programmes and best practice in other situations. For this to be possible, it is necessary to not only adopt the features and structure of a successful programme but also to recognise what interplay of culture and organisation might make it effective (Martin, Liff, Dutton & Light, 2004). It is necessary to evaluate why it was successful and in what context. However, the literature on interventions usually includes only limited evaluations such as exit surveys or participation rates at events (Parker, 2004; Lang, 2007). Unsuccessful programs are not reported apart from problems with programme implementation. For example programmes implemented in many Australian universities experience problems because of insecure funding, which results in intervention programmes being diluted or sporadic (Willis, Male & Korcyzynskyj, 2003).

Programme theory-driven evaluation ‘If your train’s on the wrong track, every station you come to is the wrong station’ (Bernard Malamud, as cited in Patton, 1997, p. 195). Chelimsky (1997) suggests that evaluation research is a vast and complex ﬁeld, which can be undertaken for accountability, development or knowledge creation. When intervention programmes are supported ﬁnancially by a government, the funding requirements generally specify the need for independent evaluation for accountability (see, e.g. the National Science Foundation in the USA or the Australian Research Council). To be inﬂuential in bringing about change, policymakers and practitioners will need to be provided with much more speciﬁc information based on evaluations conducted for knowledge creation: Which girls? Which attitudes? Which skills? Which ICT applications? And, in what context? (Parker, 2004). Black-box evaluations are those that measure what goes into a programme (the ‘black box’) and what comes out, without really considering what goes on inside the programme (Bush, Mullis & Mullis, 1995). An example would be to undertake a pretest before implementing a programme and then a post-test afterwards. This will demonstrate that something has happened but not how or why it happened. In the 1980s, evaluation theorists began to advocate the importance of evaluation directed at understanding and assessing a programme theory (Greene, 2000). A programme theory-driven evaluation is one where the evaluator creates knowledge by constructing a programme theory and then uses this theory as a guide in the evaluation process (Chen, 1990). Hansen (2005, p. 450) suggests that this allows us to ‘revise and further develop’ the programme theory, which in turn will help answer the questions of what works for whom, under which conditions. Funnell (1997) deﬁnes a programme theory as a ‘small’ theory of change about how an intervention will achieve outcomes to address an issue or solve a problem. An intervention programme needs to be ‘grounded in good theory’ (PERC, nd). This theory is the set of assumptions and expectations that represent the rationale for what is performed and why (Rossi, Lipsey & Freeman, 2004).
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

591

Wholey (1987, p. 78) suggests that programme theory is deﬁned as ‘program resources, program activities, and intended program outcomes, and speciﬁes a chain of causal assumptions linking program resources, activities, intermediate outcomes, and ultimate goals’. The logic model is one approach and provides a visual representation of how the change is expected to come about. Bickman (1987, p. 5) describes a logic model in the evaluation context as a ‘plausible, sensible model of how a program is supposed to work’. Owen & Rogers (1999) suggest that programme causality is central to this framework: that there is no basis for developing intervention programmes unless programme initiators use causal thinking to consider the expected outcomes and impacts of the intervention programme they are developing. The assumption of what will result from taking certain actions is based on research and underpins the logic model. Inductive thinking is used to make generalisations such as women are more likely to take up a career in ICT if there are a large number of role models in that career; the presence of a high number of role models is associated with higher participation in the industry, as women cannot be what they cannot see. Questions can then be asked such as which girls, how many role models do they need to see, in what capacity and over what period of time? There are many variations of logic models, and in practice, they can be created as a table, a matrix or using a range of shapes (WKKF, 2004) although they are typically created as one single image, which is simple and clear and aids communication (UWEX, 2005). Some diagrams consequently show only a section of the complete model, such as the outputs and outcomes/impacts. According to McLaughlin &Jordan (2004), the critical issue is to show all the logical relationships in the context of the problem so that all implicit understanding are clearly articulated. Then, the logic model can summarise a complex intervention programme and become a practical tool to facilitate communication with stakeholders. It will also facilitate evaluation as each step in the sequence can be appropriately evaluated at that point. Even before evaluation is conducted, the logic model will be a valuable tool as Weiss (1998, p. 66) explains: ‘the mere construction of a theory can expose naïve and simplistic expectations’.

RESEARCH DESIGN

This research study was divided into three phases. The ﬁrst phase involved theory building through an extensive review of the literature related to intervention programmes for women and computing and to the theory of evaluation (Craig, Fisher & Dawson, 2011). By combining the key elements of evaluation identiﬁed in the literature with a logic model approach, a conceptual framework to guide the evaluation of intervention programmes was created. The logic model on its own is insufﬁcient for comprehensive evaluation. A framework provides increased explanatory power, giving opportunity to examine the relationships between elements (Meredith, 1993). Thus, a framework can be used as a set of ideas to support the understanding of a domain, which in this instance is evaluation. A logic model (showing the inputs, activities and resulting outputs) is built on assumed causal links because its purpose is to evaluate a programme of action. However, the notion of causation in the social world is widely debated; therefore, the second phase of the research used an
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

592

A Craig

interpretive epistemology and social constructivism ontology to investigate Australian intervention programmes for girls and women in computing to better understand why certain activities may be associated with various outcomes. Qualitative methods were required to enable the researcher to gain a rich understanding of the events involved in intervention programmes through the perspective of the initiators of the programmes. They also enabled the collection of the detailed data required to understand how each of the elements of the conceptual framework was applicable for speciﬁc programmes and their evaluation. Williamson, Burstein & McKemmish (2002, p. 32) suggest that it is acceptable that a qualitative study should be ‘the intense study of an individual case’. However, a multiple-case study enables the particular phenomenon to be investigated in diverse settings (Darke, Shanks & Broadbent, 1998) and deepens understanding and ability to explain what has occurred (Miles & Huberman, 1994). Eisenhardt (1989) suggests at least four separate cases are required for multiple-case research, while Miles & Huberman (1994) warn that using more than 15 cases can make the study unwieldy. This study therefore adopted a multiple-case design as it was considered necessary to obtain a deeper understanding of the issues by looking at different cases in their speciﬁc organisational and political contexts. In 1990, the Australian Government had set a target of raising the number of women in engineering courses to 15% and other non-traditional courses, such as computing, to 40% by the mid-1990s. This inspired a ﬂurry of intervention programmes around the country (Craig, Fisher, Scollary & Singh, 1998). Since then, a total of 17 major intervention programmes have been conducted in Australia to encourage more girls and women into computing. From these, 14 programmes were chosen based on the following criteria: • • • • • One of the program’s objectives was to increase the number of women in computing. The intervention programme was a sustained activity of numerous intervention projects. The principal champion/instigator of the programme was prepared to participate. Programmes provided diversity in location and focus. The programme had been operating for at least a year.

Eight of the participating cases came from universities, three from government bodies and three from the industry sector. More cases were from the university sector because of the proliferation of programmes in this sector. At the time of data collection, some of the intervention programmes were still relatively new, some had been operating for a lengthy period of time and a few were completed. A single case was deﬁned as a concentrated inquiry into a cluster of intervention projects implemented by one group of people, referred to here as an intervention programme. Each of the 14 cases had a unique set of intervention projects, which made up their intervention programme. For example Uni1’s intervention programme consisted of the following projects: At the pre-tertiary level, they • created secondary school teacher awareness through a careers booklet; • created a website proﬁling a wide variety of computer careers; • had female undergraduate students speak at secondary school careers events;
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

593

• conducted a shadowing programme for girls in years 10 and 11; • provided a series of interviews in newspapers and radio to dispel the nerdy image of ICT; • undertook an audit of all publicity (e.g. websites and brochures) and made changes so that the photos in any publication incorporated 50% women and multiple students; and • conducted an annual Girls in Computing day for 50–60 girls for the past 3 years. At the tertiary level, • a compulsory orientation camp geared towards networking was conducted; • a scholarship programme was offered; and • female students were provided with professional development through industry partners. The time span of Uni1’s intervention programme was 5 years with only the annual girls in computing day taking place at the time of data collection for this research. The data collected for each case study included details about the intervention programme in context, the programme theory and the programme’s success from the perspective of the programme champion. Nineteen in-depth interviews with the programme champion(s) were conducted. Additional data came from the detailed study of 40 published and 10 unpublished papers, 17 reports, 32 surveys, 6 videos and 12 websites provided by the case studies. The analysis of these provided some validation for conclusions drawn from the interviews. The analysis of each individual case was followed by a cross-case comparison. All data were brought together in one NVivo project ﬁle (QSR International Pty Ltd, Doncaster, Victoria, Australia) enabling sorting, searching and linking. An initial set of categories for coding was created based on the themes identiﬁed in the gender literature such as awareness (Clarke, 1990; Lang, 2007), recruitment and retention (Clarke, 1990; Clayton & Lynch, 2002; Klawe, Whitney & Simard, 2009; Adya & Kaiser, 2005). Additional codes came from the evaluation literature: programme operation, success of programme, purpose and design of evaluation, stakeholder involvement, resources available and dissemination of ﬁndings (WKKF, 1998; Rossi, Lipsey & Freeman, 2004; Weiss, 1998). Concepts emerged from the source data, which enabled further reﬁnement of the categories (e.g. programme operation, prior research conducted, the way it was expected to work, difﬁculties with programme operation, follow-up with target audience, grant requirements, support from others, sustainability/burnout and stakeholder participation). Having all the data in one project ﬁle enabled the creation of a meta-matrix as described by Miles &Huberman (1994, p. 178) to facilitate an analysis for patterns in responses and opinions. The gender and computing intervention evaluation framework emerged as a result of phase two. Phase three of the research involved two tasks to conﬁrm the framework. Firstly, a 1-day workshop focusing on the evaluation framework was held with a group of 14 educators. As it had not been possible to obtain a core group of the programme champions from the case studies together, an alternative group was sought. During phase one, other educators who were involved in various initiatives that attempt to engage girls in thinking and working positively with computers had been identiﬁed. Fourteen of the 20 invitees accepted. These educators came from three different states, seven from the primary sector, ﬁve from the secondary and two from the tertiary sector. All conducted ‘Girls in Computing’ initiatives either for primary or for secondary school girls, or both. The workshop explored the elements of the framework and how they
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

594

A Craig

could be applied in their own situation. A particular focus was on the criteria practitioners could use to measure the outcomes from programmes: short-term outcomes, medium-term outcomes and longer-term outcomes/impacts. The second activity was the application of the gender and computing intervention evaluation framework with another major intervention programme.

RESEARCH FINDINGS

Effective evaluation is not an “event” that occurs at the end of a project, but is an ongoing process which helps decision makers better understand the project; how it is impacting participants, partner agencies and the community; and how it is being inﬂuenced/impacted by both internal and external factors. (WKKF, 1998, p. 3)

Phase one Following an in-depth review of the literature, the conceptual framework (Figure 1) was developed. A logic model is the core of the framework and is shown as the middle section of Figure 1 and labelled Context. A logic model is underpinned by the programme theory. To this, the key elements of evaluation are added (as shown in the rest of the diagram). The elements included identifying the purpose of the evaluation, the resources that are available, which stakeholders will carry out the evaluation, the context for the programme, the design of the evaluation instruments, the evidence that will be gathered and how the results will be used.

Phase two During phase two, the conceptual framework (Figure 1) was reﬁned and informed by practice through a multiple-case study of women in computing intervention programmes. The ﬁndings included that the majority (11 of the 14) of intervention programmes were conducted by volunteers with limited resources at their disposal. These women were passionate about the need for more women in computing and were prepared to devote large amounts of their own time to ‘make a difference’. Most reported that they were not qualiﬁed evaluators, but practitioners, and were more concerned about the implementation of an intervention programme than its evaluation. Not only was the purpose of the evaluation unclear but so was the goal of the intervention programme. Very few programme champions were able to indicate what the speciﬁc problem was that their programmes were addressing beyond obtaining more girls into computing. Almost all of the respondents described success by talking about individual instances of having made a difference in one person’s life and that this was enough to make the programme a success. There was disappointment expressed by many of the women that the incredible time and effort needed to sustain these programs (most of which were of a voluntary nature) had not brought about more change, with the overall state and national statistics showing little improvement. When evaluation was conducted, it was reported as a minor activity because of a lack of resources and the practitioner view point that their limited time and energy should be spent on implementing the programmes. They overwhelmingly reported the belief that their programmes
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

595

Figure 1. Conceptual evaluation framework. CS, Computer Science.

were likely to be successful anyway. The evaluations that were conducted included those to see if there was a need for a particular programme (needs assessment), and to see if the programme was working well or how it could be improved (process evaluation). Additional evaluation looked at the impact or outcomes that the programme had contributed (impact evaluation). Much of the impact evaluation conducted, however, focused on the immediate outcomes of the programmes with few programmes measuring medium-term or longer term-impact. This was mostly due to time restrictions on projects, lack of resources (including skills) and privacy issues especially when dealing with minors. The most frequent type of evaluation conducted was to measure participant satisfaction. Participant satisfaction can be considered as falling under the outputs component of the logic
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

596

A Craig

model (UWEX, 2005). However, it should be noted that while it is important that the participants are ‘satisﬁed’ with an intervention activity, this does not mean that outcomes such as the changing of knowledge, skills or attitudes have occurred. None of the cases examined here set speciﬁc measurable goals at the commencement of their intervention. None of the cases tackled the evaluation of programme theory possibly because, as Weiss (1998) suggests, this type of analysis is more difﬁcult to undertake. Programme champions had not investigated what elements would make for a useful evaluation and felt they needed help in ‘doing a better job on evaluation’. Evaluation was often an afterthought and not one that was considered a high priority.

The gender and computing intervention evaluation framework
The gender and computing intervention evaluation framework developed as an outcome of phase two is shown as a graphical representation in Figure 2 and described in Table 1, each of which needs to be read in conjunction with each other. The framework went through numerous iterations as it was reﬁned after each case study. Most importantly, the need for a thorough understanding of the problem the intervention programme is seeking to solve needed to be brought to the fore in the framework. Guidelines to support the framework were also developed. Once it had stabilised, the usefulness of the framework was explored with the latter case studies. The gender and computing intervention evaluation framework is composed of three stages. Firstly, the problem, its context and the development of the logic model and programme theory are understood. Secondly, the evaluation planning requires answers to the questions of why evaluation should be performed, who will undertake it and what resources are available to undertake the planned evaluation. Lastly, the framework will then guide the design of the evaluation and the type of evidence needed, as well as ensuring that the critical elements of using and sharing of the results are not overlooked.

Using the gender and computing intervention evaluation framework
To demonstrate how the framework can be operationalised, an example is drawn from one of the latter intervention projects conducted within Uni3’s intervention programme. Uni3 created a video to inform and encourage girls to consider computing. The reason for creating the video was the belief that girls would be interested in pursuing computing as a career after viewing the video. The interview with the programme champion indicated that the intervention sought to address the problem that girls do not choose computing because they do not know what careers it can lead to. Therefore, if their knowledge of the wide range of careers in ICT improves and they know what qualiﬁcations they need to enter those careers, then they will more likely add computing to their career horizon. A programme based on this theory would be one that tries to change knowledge of careers in ICT in order to change career choice behaviour. For the programme to be effective, it was assumed that girls would watch the video, girls would ﬁnd it engaging and girls would then choose to study computing. Making these assumptions explicit enables consideration of whether it is plausible that the planned activities will lead to the desired outcomes. Figure 3 shows an extract from the framework. It could be judged that more girls doing computing after watching one video is not very
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

597

Figure 2. The gender and computing intervention evaluation framework.

likely to happen, and hence, an opportunity is created to suggest other activities, which are ‘logically’ required to achieved the desired purpose as shown in Figure 4. It could lead to the programme champion recognising that to get to the ultimate outcome of more girls in computing courses further intervention is required than just the creation and distribution of the video. Each activity would require evaluation. The evaluation of the video for example should then be based on how many girls in the target age group actually watch it, of these how many found it engaging, and whether there is a subsequent increase in knowledge or a change in attitude towards ICT. Table 2 demonstrates how each of these desired outcomes can be evaluated. A scenario such as this also assumes that greater knowledge and/or changed attitudes lead to changed behaviour, or else, there is little point in implementing the intervention. Using the
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

598

A Craig

Table 1. Guidelines for the gender and computing intervention evaluation framework Phase Description

1. Understand the programme Context What is the problem the programme sets out to solve? What change is expected after the implementation of the programme? What is the programme theory that will lead from the programme activities to the outcomes/impacts? Are these assumptions realistic and grounded in the literature? Over what time period are changes expected to occur? What level of change is required to deﬁne success? Create a logic model showing the program inputs, activities and expected outcomes/impacts. 2. Evaluation planning Why Who Resources 3. Evaluation design Design Describe how the evaluation will be conducted and the evaluation activities in the context of why the intervention programme is needed. Design the evaluation activities. What is being measured, how and when? For example the participants who will be involved. • The results of the programme; the short-term and medium-term outcomes in participants’ knowledge, skills or behaviour. • The longer-term outputs/impacts. • How will the link between change and theory be evaluated? • How will the data be analysed? What evidence is needed and is it credible for assessing change? On what basis will conclusions be drawn and recommendations made? How will the results be used in the future? How will the lessons learned be shared? How will results contribute to the gender and IT literature and theory? To advance the theorising about gender and computing interventions. Who will be the evaluation team? Will all stakeholders including multiple team members be involved? What resources are available for the evaluation (e.g. volunteers, expertise, time, money and equipment)?

Evidence Learn Share

framework recommended ensures that the underlying programme theory is considered, moving from one step to the next realistic step in the journey and that it is performed at the planning stages of the program. The actual evaluation of each step does not need to be complex, for example using one question – ‘On a scale of 1 (no interest) to 10 (extremely interested), how interested are you in computing?’ – before and after an event, followed up a few weeks later with ‘Do you remember that event? Have you explored further any of options A, B or C that were suggested at the event?’ This type of evaluation will provide more stable and veriﬁable data concerning outcomes, and requires the articulation of the programme theory to direct the evaluation. It is not gender per se in the evaluation that is at issue. It is that ‘intervention’ by its nature implies that we will make an improvement to a situation. By not undertaking evaluation, which articulates the ‘problem’ and then checks that the ‘problem’ has been remediated, we are not getting any further with our understanding of what works for whom in what situations or advancing the theorising of gender and computing interventions.
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

599

Figure 3. Lack of knowledge – a framework extract.

Phase three This phase of the research sought conﬁrmation of the usefulness of the gender and computing intervention evaluation framework. A day-long workshop was followed by application of this framework to another major intervention programme. The educators attending the workshop were initially asked to discuss two major issues: The main problem they sought to address with their intervention project and what would success look like. The workshop then explored the critical elements of the evaluation (why, who, resources, design, evidence, learn and share) and how they could be applied in the intervention projects. Success was deﬁned as what unique contribution was made by the project. A particular focus was on the evidence practitioners could use to measure outcomes from programmes. The brainstorming sessions produced the following classiﬁcation of outcomes and impact: 1 short-term outcomes that could occur directly as a result of the programme, which could include that the girls have greater awareness of the multiple uses of computers, or developed an interest to learn new skills; 2 medium-term outcomes that could occur from the programme such as girls developing selfconﬁdence in computer use, feeling more comfortable in exploring technology, being more self-reliant or more willing to take risks with the technology; and
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

600

A Craig

Figure 4. Lack of knowledge – an alternative extract. IT, information technology.

3 longer-term outcomes/impact that may not occur until sometime in the future when the girls have developed an understanding, which enables them to make informed decisions about the use of technology in society, develops their full potential and may embrace a computing career. Based on this classiﬁcation and the framework, the workshop participants produced evaluation instruments to use with their own intervention programmes. At the end of the workshop, the participants were asked to complete an anonymous survey of working with the gender and computing intervention evaluation framework. Some indicative responses are presented in the following:
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions
Table 2. Lack of knowledge – assumptions and possible evaluation indicators Desired outcome Video created Video is engaging Possible indicators Number of videos distributed Any of the following; • Student distraction/focus throughout the presentation; • Girls listening to presentation and ﬁnding the information relevant to them; • ‘I let my mind wander while I was watching the video’ with Likert rating scale; or • What girls recall from viewing the video a few weeks later Are the girls motivated to learn/change? Number of girls? Age of girls? Girls are aware of what types of careers exist in computing and what people who have these jobs actually do. Girls are more likely to consider computing as part of their own future. Careers and/or ICT teacher receives more enquiries about computing.

601

Possible data sources Postal records Any of the following: Observation Anecdotal feedback from girls Survey Focus groups

Many appropriate girls in target age group view video Increased knowledge

Survey

Pre-survey and post-survey

Changed attitude

Pre-survey and post-survey Number of enquires at the pre-showing and post-showing of the video

ICT, information and communication technology.

I recognised that even a program that does not work out successfully can still provide you with much information if it is assessed correctly and this will ensure that future programs are more effective. (Workshop Participant _9) [The framework helps to] re-evaluate evaluation! In particular the purpose of the evaluation and, just as important, the intent of the project. (Workshop Participant _11) The framework helps to develop evaluations that identify real outcomes. Not just collecting data for the sake of it. It will assist me not only in the context of girls and ICT but across a range of activities. (Workshop Participant _1) An understanding of evaluation, the ability to write successful evaluation and the chance to use it. (Workshop Participant _2) All workshop participants reported that the gender and computing intervention evaluation framework was an appropriate tool for use with their intervention programmes and they felt conﬁdent that they could use it.

Application to a new intervention programme – Uni9
A new intervention programme was found, which would assist in further exploration/conﬁrmation of the evaluation framework. It was conducted by a university to be known as Uni9. This
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

602

A Craig

intervention programme was selected for detailed analysis because it met the requirements of case study entities as listed in the Section on Research Design, and the timing of the intervention was suitable for the researcher to attend. The primary focus of Uni9’s intervention programme was an international 5-day technical workshop for women only. This intervention had been conducted once before, and it was anticipated that it would be an annual event. The target audience for the workshop was female students from high school through to post-graduate level, academics and industry women. The workshop aimed to ‘increase the skills, self-image, participation and representation’ (Uni9_documents) of women in computing by conducting highly intensive seminars, lectures and hands-on workshops. The researcher observed the implementation of the 5-day workshop and its evaluation. Over a period of 6 days, 3-h interviews with the programme champion (who will be known as Trudy) were also conducted. The survey instruments, publicity ﬂyers and press release were also examined. The framework and guidelines (Figure 2 and Table 1) were applied to this intervention programme. While the ideal time to apply the framework is at the commencement of the planning for any intervention programme, it was considered valuable to use it for the programme evaluation. In an interview with Trudy (from whom the following quotes come) covering the background and context of the programme, similarities to the 14 case studies were immediately obvious. This included her passion for the programme, the enormous amount of voluntary work involved in conducting intervention programmes, success being focused on having an effect on one particular student and a lack of evaluation skills. I think overall it was really popular and successful … The funny thing is that I think a lot of my views are still on the organisation. There isn’t anything which says the thing in itself was a success. But yes I consider it a success. I have seen one very excited student and something is suddenly starting to happen. She is talking and questioning. These kinds of things. Yes it was good. If it was all just for this one student then that is good. These are the kinds of things that I am quite happy with … The previous year’s evaluation had taken the form of three questionnaires created by Trudy: … it is basically about are you happy with the information you got, would you come again? Would you recommend someone else to come along or was it too expensive? Were you happy with the content of the courses? What else would you have proposed? Results last year were that everyone was extremely happy. More evaluation had been planned for the previous year but had not occurred: We had planned more but we didn’t do it because of sheer lack of people …what we had planned is actually to have interviews on the phone with people directly afterwards and six months later, but it just didn’t happen. We didn’t even plan in particular what we wanted to ask. It was this idea of we wanted to ﬁnd out more what people actually wanted.
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

603

While the only thing learnt from the evaluations was that ‘everyone was happy’, Trudy intended to conduct the same evaluation again for this second workshop, primarily because of her perceived lack of skills: On one hand I want to repeat the evaluation we had last year no matter if it is really useful just to be able to compare it. But I also feel in addition to that so far up to now I did not have a hat for that to think about it and also it is not my ﬁeld of research, I do not know how to ask these questions. In another interviews with Trudy, her assumptions emerged regarding the establishment of the intervention programme. However, Trudy, an educator who had only been in the country for 2 years, had found these were not holding true (Table 3). From the interviews, the following preliminary framework began to emerge (Figure 5): Success for Trudy was obtaining more women into computing. It became apparent that the main problem, which Trudy wanted to address with her intervention programme, was for ‘females in computer science to feel less lonely and more powerful’. Through discussion of the framework and a series of question and answers (Table 4), the goal of the programme was clariﬁed to that of ‘increasing retention of females’ rather than ‘more girls do computing’. The underlying programme theory that should lead to the ﬁnal outcome aimed at the tertiary girls was slowly teased out. This process would need repeating for other types of participants in intervention programmes – industry women and academics. In relation to the tertiary students who were attending the workshop, Trudy believed that they would create a community and support one another, and hence, this would improve their retention. Accepting that the programme may do different things for different participants (and hence, there are multiple programme theories), the diagram in Figure 6 was developed, which Trudy acknowledged as logical.
Table 3. Assumptions and reality – Uni9 Assumptions made by Trudy Tertiary students do not pay fees. Students will attend technical workshops purely for the interest in the topic and to learn and have fun. Reality Students need to pay fees and therefore need to spend time over the holidays earning money. Students have less time to attend extra subjects just for interest’s sake. Students are restricted to a certain number of units and may not do more. Units have to ﬁt into course regulations so students cannot freely choose. Credit cannot be given for attendance. Students are continually asking: Is that of interest to industry? Will I get a job from this? Many did not. Many were unable or not prepared to. Transport was an issue. No time slot will suit everyone, but just before a deadline for the Masters students was problematic for these targeted participants. It was political, and there were concerns of discrimination against men by their non-inclusion.

Students are more interested in the learning aspect than what a course might do for their potential career. Students would access their email during the holidays. Students would give up a week of their holidays to come. Participants have access to transport. It is a good time of year to conduct the programme.

Having a function for women only would not be political.

© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

604

A Craig

Figure 5. An early framework – Uni9. IT, information technology. © 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions
Table 4. Clarifying the framework – Uni9 Questions and answers Q: What do we want to know? A: Whether the program has made a difference. Q: A difference to whom? A: The participants.

605

Q: Which participants? Are they all the same? A: No there are tertiary students, academic staff and a few women from industry. We didn’t get any high school girls. Q: What difference should the program make to the tertiary students? A: To enable the participants to network and/or see role-models. Q: Why? A: So they form bonds and all that sort of thing.

The evaluation areas for the intervention programme emerged from Figure 6: Did the tertiary students meet other girls in computer science? Experience new or innovative activities? Did they feel they could talk to the professors and industry women? Suggestions for follow-up evaluation emerged – so that in 6 months time, a brief follow-up telephone interview could check to see if the participants had maintained contact with each other. This would enable the testing of the theory of how the change was to come about: Yes, yes, yes…I also like it because it means… it is like a tool I can think about. I don’t need you, I don’t need to call you and say how do I evaluate? But thinking about it I can write my own questions. YES. (Trudy) In a follow-up interview, Trudy realised that there was a possibility that the evaluation might show that the students had not created a network of support: I just realised, what do we do if the evaluation says the program isn’t doing what I thought and I realise that they didn’t follow up and that they didn’t support each other. How do I ﬁnd out what to do then? Then we know that we have to come back to the program’s theory….. You go right back to the beginning. Did they form bonds while they were here? If they didn’t they aren’t going to meet up afterwards. (Researcher) OK. Yes! Yes! …I think it is a good way to think about it this way. Cool! Over time, a complete theory of how this programme works and for whom and in what circumstances could be developed. The need to not only learn from this type of evaluation but also to share the results with the women in computing community were also discussed. Most of the value in a logic model is in the process of creating, validating, and modifying the model … The clarity of thinking that occurs from building the model is critical to the overall success of the program. (WKKF, 1998, p. 43)
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

606

A Craig

Figure 6. Component of the framework Uni9. CS, Computer Science.

DISCUSSION

This study found that many of the intervention programmes conducted in Australia had not articulated explicitly the ‘problem’ they were setting out to solve nor a measurable goal for the programme. Was it a lack of awareness by secondary school girls about the computing profession? If these girls knew more about jobs in the industry, would they then ﬂock to such careers? Or was it that the role models currently available to girls were unappealing and that by showing them role models they could relate and aspire to, they would further their computing education?
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

607

This lack of explicitly articulating the ‘problem’ is not unique to interventions for women and computing. Bacchi (2009) explains that the problems a government policy or programme is attempting to ‘address and remedy’ are often not stated. Bacchi (2009, p. ix) argues that ‘this is implicit in the whole notion of policy – by their nature policies make changes’. This research has found that the lack of deﬁning the problem has also been implicit in intervention programmes for women and computing. The majority of programme champions of the intervention projects investigated in this research study often neglected evaluation because they ‘knew’ their programmes would be successful anyway and preferred to spend their limited resources on the actual intervention programme. To advance the gender and computing arena, it is time to make the ‘problem(s)’ that the intervention aims to resolve explicit and ‘to scrutinise them closely’ (Bacchi 2009, p. x). The study also found that much of the evaluation conducted was superﬁcial and involved mainly counting attendance at events or participant satisfaction with an event. It is suggested that the disparity between all the programme champions feeling their projects/programmes were successful and the despondency that most felt for not having made enough impact were due to the success of individual programmes being measured by participant satisfaction or the short-term outcomes of the programme, but the champions really hoped to inﬂuence some longer-term outcomes. While none of the evaluations measured achievement of the goal of more women in computing, the widely reported statistics of continued low participation led to this disillusionment. Ridley & Young (2012) argue that whether they are implicit or explicit, theoretical approaches shape how people understand the gender imbalance in computing. Many of the unarticulated assumptions are based on (unarticulated) ideas about gender and whether gender is socially constructed or there are essential differences between the genders. Evaluations for interventions on gender and computing need to be designed on an understanding of ‘the problem’ the programme is intended to solve. Most of the programme champions did not have this understanding and were attempting to address the larger issue of the lack of women in computing. In order to advance the understanding of interventions, which are successful or not, the evaluation needs to be designed to assess the validity of the programme theory on which the programme has been built. Making programme theory explicit means the theory itself can be evaluated. This requires an understanding and interrogation of what actual problem the intervention intends to address. Pawson &Tilley (1997, p. xvi) suggest that ‘progress emerges through a process of theory building and theory testing’. The gender and computing intervention evaluation framework developed as a result of this study has been speciﬁcally designed for programme stakeholders to consider programme theory and how change is expected to come about through their intervention programme. The usefulness of this framework in practice was conﬁrmed by workshop participants and the programme champion of a new intervention programme: Trudy indicated that while she was not skilled in evaluation, with the framework, she was conﬁdent in being able to conduct useful evaluations. This research has found that evaluation of interventions has been hampered by a lack of attention to underlying assumptions about what the programmes are expected to do (programme theory). In other words, the programme evaluations are atheoretical. This is made problematical by the difﬁculty of deﬁning gender in relation to technology with at least three differing views of
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

608

A Craig

how women differ from men in this regard. The development of the logic model and conceptual framework serves to illuminate the inconsistencies and unexamined assumptions underlying the programmes, and this contributes to the continuing development of the conceptualisation of gender and technology.

CONCLUSION

‘A program is a theory and an evaluation is its test’ (Rein, 1981, p. 141). The design of any intervention programme should have at its core a clear understanding of what speciﬁc problem it is seeking to address and what is known about the causes of this problem. Which cause(s) will be addressed by the intervention can then be decided as well as a clear goal set for the programme. By articulating at the commencement of the design of the programme speciﬁcally, the problem and the change expected after the intervention will result in more focused evaluation. For the last 20 years, there have been many attempts to improve the gender balance of the computing profession. A wide range of intervention programmes have been conducted to recruit women, retain women or advance women in computing. Yet, despite all this effort, there has been no marked improvement in the gender balance of the profession. Adopting the gender and computing intervention evaluation framework at the beginning of an intervention programme’s conception will encourage champions to articulate the speciﬁc problem that the programme aims to solve as well as the assumptions and the validity of the programme theory of how the programme is expected to work. The assumptions should be supported by the literature, strengthening the case for the plausibility of the theory and the likelihood that the stated outcomes will be achieved. This enables clariﬁcation on when and what needs to be evaluated, thereby improving the understanding of the programme so that it can inform social change. Using the gender and computing intervention evaluation will lead to a better understanding of which intervention programmes work for whom, information that is urgently required if gender equity in computing is to be achieved. There are limitations to the gender and computing intervention evaluation framework. Using the framework to understand what needs to be evaluated, and when, will still not guarantee the evaluation will be effective. Evaluation instruments developed and used, such as surveys or interviews, must be appropriate, and the actual questions asked must measure what they are intended to measure. As Rossi, Lipsey &Freeman (2004, p. 38) explain: … the methods selected must not only be capable of providing meaningful answers to the questions but also must be practical while still providing the degree of scientiﬁc rigor appropriate to the evaluation circumstance. The evaluation framework will also not alleviate the necessity to work within the constraints of the particular intervention programme, such as those of a lack of resources and time limitations. However, it does empower the programme members to be able to conduct useful evaluation of the underlying programme theory, when limited resources do not stretch to using the services of an evaluation expert. Through the use of the gender and computing intervention evaluation
© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

609

framework, the dissemination of sound evaluation results can be undertaken. Educators and researchers will be able to clearly articulate their assumptions, judge whether these are supported by the outcomes of the programmes and contribute to deﬁning gender and IT.

REFERENCES
Adya, M. & Kaiser, K. (2005) Early determinants of women in the IT workforce: a model of girls’ career choices. Information Technology & People, 18, 230–259. Australian Bureau of Statistics. (2010) Forms of employment. November. Cat 6359.0. Australian Bureau of Statistics. Retrieved Oct 2011, from http://www.ausstats. abs.gov.au/Ausstats/ subscriber.nsf/0/ED7010B1774DA 4D4CA2578800019CAFB/$File/63590_november% 202010.pdf Australian Bureau of Statistics (2014) Cat. 8146.0 - Household Use of Information Technology, Australia, 2012–13. Retrieved Mar 2014, http://www.abs.gov.au/ausstats/ abs@.nsf/mf/8146.0 Adams, J.C., Bauer, V. & Baichoo, S. (2003) An expanding pipeline: gender in Mauritius. In: Proceedings of the Thirty-Fourth SIGCSE Technical Symposium on Computer Science Education, Grissom, S. (ed.), pp. 59–63. ACM Press, Nevada. Australian Computer Society (2013) Australian ICT Statistical Compendium for 2013, Retrieved February 2014, www.acs.org.au/__…/Australian-ICT-Statistical-Compendium-2013.pdf Australian Computer Society (2012) 2012 Australian ICT Statistical Compendium, Retrieved Apr 2013, www.acs. org.au/__data/assets/pdf_ﬁle/0014/13541/ 2012_Statcompendium_ﬁnal_web.pdf. Australian Workforce and Productivity Agency (2013) Information and communications technology workforce study. Department of Industry, Innovation, Climate Change, Science, Research and Tertiary Education, Commonwealth of Australia 2013 Retrieved Feb 2014, from www.awpa.gov.au/publications/…/ICT-STUDY-FINAL-28-JUNE-2013.pdf Bacchi, C.L. (2009) Analysing Policy: What’s the Problem Represented To Be?Pearson Education, Frenchs Forest, N.S.W., 2009. Bassot, B. (2012) Upholding equality and social justice: a social constructivist perspective on emancipatory career guidance practice. Australian Journal of Career Development, 21, 3–13. Berger, P. & Luckmann, T. (1966) The Social Construction of Reality: A Treatise in the Sociology of Knowledge. Anchor Books, New York, USA. Bickman, L. (1987) The functions of program theory. In: New Directions for Program Evaluation No 33, Bickman, L. (ed.), pp. 5–17. Jossey Bass Publishers, San Francisco. Björkman, C. (2005) Feminist research and computer science: starting a dialogue. Journal of Information Communication and Ethics in Society, 3, 179–188. Bush, C., Mullis, R. & Mullis, A. (1995) Evaluation: an afterthought or an integral part of program development. Journal of Extension, 33. Retrieved May 3, 2013, from www.joe.org/joe/1995april/a4.html Camp, T. (2012) Computing, we have a problem …. ACM Inroads, 3, 34–40. Chelimsky, E. (1997) Thoughts for a new evaluation society. Evaluation, 3, 97–109. Chen, H. (1990) Theory Driven Evaluations: A Comprehensive Perspective. Sage, California. Clarke, V.A. (1990) Sex differences in computing participation: concerns, extent, reasons and strategies. Australian Journal of Education, 34, 52–66. Clayton, K., Beekhuyzen, J. & Nielsen, S. (2012) Now I know what ICT can do for me!. Information Systems Journal, 22, 375–390. Clayton, D. & Lynch, T. (2002) Ten years of strategies to increase participation of women in computing programs: the Central Queensland University experience, 1999–2001. SIGCSE Bulletin, 34, 89–93. Corneliussen, H.G. (2010) Cultural perceptions of computers in Norway 1980–2007. In: Gender Codes: Why Women Are Leaving Computing, Misa, T. (ed.), pp. 163–185. Wiley-IEEE Computer Society Press, Los Alamitos, CA. Corneliussen, H.G. (2012) Gender-Technology Relations. Palgrave Macmillan, Basingstoke, Retrieved Jan 2014, from 10.1057/9780230354623. Craig, A. (2009) Intervention programmes to recruit female computing students: why do the programme champions do it?, paper presented to the Eleventh Australasian Conference on Computing Education, Wellington, New Zealand, 20 to 23 January 2009. Craig, A., Coldwell, J., Fisher, J. & Lang, C. (2013) The silicon ceiling: women managers and leaders in ICT in Australia. In: Women and Management: Global Issues

© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

610

A Craig

and Promising Situations, Paludi, M.E. (ed.), pp. 177–200. Greenwood Publishing Group, Santa Barbara, CA, 2. Craig, A., Fisher, J. & Dawson, L. (2011) Women in ICT: Guidelines for evaluating intervention programmes. European Conference on Information Systems, Helsinki, Finland, Proceedings. Paper 52. http://aisel.aisnet.org/ ecis2011/52 Craig, A., Fisher, J., Scollary, A. & Singh, M. (1998) Closing the gap: women education and information technology courses in Australia. Journal of Systems Software, 40, 7–15. Darke, P., Shanks, G. & Broadbent, M. (1998) Successfully completing case study research: combining rigour, relevance and pragmatism. Information Systems Journal, 8, 273–289. DuBow, W.M. (2013) Diversity in computing: why it matters and how organizations can achieve it. Computer, 46, 24–29. Eisenhardt, K.M. (1989) Building theories from case study research. Academy of Management Review, 14, 532–550. Funnell, S. (1997) Program logic: an adaptable tool for designing and evaluating programs. Evaluation News and Comment, 6, 5–17. Game, A. & Pringle, R. (1983) Sex-typing in computerland. Australian Society, 3–8. Greene, J.C. (2000). Understanding social programs through evaluation. In: Handbook of Qualitative Research, Denzin, N. &Lincoln, Y. (Eds), pp. 981–999. Sage, Thousand Oaks, California. Hansen, H.F. (2005) Choosing evaluation models: a discussion on evaluation design. Evaluation, 11, 447–462. Hausmann, R., Tyson, L.D. & Zahidi, S. (2011) The Global Gender Gap Report 2011 World Economic Forum. Accessed November 2011 from http://reports.weforum. org/global-gender-gap-2011/ Hayes, C.C. (2010) Computer science: the incredible shrinking woman. In: Gender Codes: Why Women Are Leaving Computing, Misa, T. (ed.), pp. 25–50. Wiley-IEEE Computer Society Press, Los Alamitos, CA. Howcroft, D. & Trauth, E.M. (2008) The implications of a critical agenda in gender and IS research. Information Systems Journal, 18, 185–202. IBSA (2010) Environment scan 2010 - information and communication technology industries. Innovation and Business Skills Australia. Retrieved March 2012, from https://www.ibsa.org.au/sites/default/ﬁles/media/IBSA% 20Environment%20Scan%202010.pdf IBSA. (2013) Environment scan 2013 - information and communication technology industries. Innovation and Business Skills Australia. Retrieved March 2014, from https://www.ibsa.org.au/sites/default/ﬁles/media/Escan

%202013%20Information%20&%20Communication% 20Technology%20Industry.pdf Kirkup, G., Zalevski, A., Maruyama, T. & Batool, I. (2010) Women and Men in Science, Engineering and Technology: The UK Statistics Guide 2010. the UKRC, Bradford. Klawe, M., Whitney, T. & Simard, C. (2009) Women in computing - take 2. Communications of the ACM, 52, 68–76. Lagesen, V.A. (2007) The strength of numbers: strategies to include women into computer science. Social Studies of Science, 37, 67–92. Lagesen, V.A. (2008) A cyberfeminist utopia? Perceptions of gender and computer science among Malaysian women computer science students and faculty. ScienceTechnology & Human Values, 33, 5–27. Lang, C. (2007) Twenty-ﬁrst century Australian women and IT: exercising the power of choice. Computer Science Education, 17, 215–226. Lee, J.A.N. & Winkler, S. (1995) Key Events in the History of Computing, IEEE Computer Society, Washington, D.C., Retrieved May 6, 2004, from http://ei.cs.vt.edu/~history/ 50th/30.minute.show.html Mackinnon, A. (1995) Women and computing: an overview. In: Women, Computing and Culture 1994, Bishop, P., Dyer, M. & Grifﬁn, P. (Eds), pp. 15–22. Research Centre for Gender Studies & the School of Communication and Information Studies, University of South Australia, Adelaide. Martin, U., Liff, S., Dutton, W.H. & Light, A. (2004) Rocket Science or Social Science? Involving Women in the Creation of Computing’s Intellectual Property. Oxford Internet Institute, Oxford. McLaughlin, J.A. & Jordan, G.B. (2004) Using logic models. In: Handbook of Practical Program Evaluation, 2nd ed., Wholey, J., Hatry, H. & Newcomer, K. (Eds), pp. 7–32. Jossey-Bass, San Francisco. Miles, M.B. & Huberman, A.M. (1994) An Expanded Sourcebook; Qualitative Data Analysis (Second Edition). SAGE Publications, Thousand Oaks. Meredith, J. (1993) Theory building through conceptual methods. International Journal of Operations and Production Management, 13, 3–11. Owen, J. & Rogers, P. (1999) Program Evaluation: Forms and Approaches. Allen & Unwin, Sydney. Parker, L. (2004) Gender and technology in the information society: networking to inﬂuence ICT in education. GIST Gender Perspectives Opening Diversity for Information Society Technology. Bremen, Germany. Patton, M.Q. (1997) Utilization-focussed Evaluation. Sage Publications Ltd, Thousand Oaks, California. Pawson, R. & Tilley, N. (1997) Realistic Evaluation. Sage Publications Ltd, London.

© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

Theorising about gender and computing interventions

611

PERC (The Planning and Evaluation Resource Center) (nd) Programming Planning and Evaluation. Innovation Center for Community and Youth Development and the Institute for Applied Research in Youth Development at Tufts University. Retrieved December 12, 2007, from www.evaluationtools.org. Pearcey, T. (1994) Australia enters the computer age. In: Computing in Australia- The Development of a Profession, Bennett, J.M., Broomham, R., Murton, P.M., Pearcey, T. & Rutledge, R.W. (Eds), pp. 15–32. Hale and Iremonger and the ASC, Marrickville, NSW. Quesenberry, J.L. & Trauth, E.M. (2012) The (dis)placement of women in the IT workforce: an investigation of individual career values and organisational interventions. Information Systems Journal, 22, 457–473. Rein, M. (1981) Comprehensive program evaluation. In: Evaluation Research and Practice: Comparative and International Perspectives, Levine, R., Solomon, M., Hellstern, G.-M. & Wollmann H. (Eds), pp. 132–148. Sage, Beverly Hills, CA. Ridley, G. & Young, J. (2012) Theoretical approaches to gender and IT: examining some Australian evidence. Information Systems Journal, 22, 355–373. Rossi, P., Lipsey, M. & Freeman, H. (2004) Evaluation: A Systematic Approach, Seventh edn. Sage Publications Inc., California. Ruthven, P. (2012) A Snapshot of Australia’s Digital Future to 2050, IBM, Retrieved February 2014, from http://www07.ibm.com/ibm/au/digitalfuture/ Schabel, R.B. (2013) Personal reﬂections on gender diversity in computing. Computer, 46, 62–63. Spencer, S. (2003) Can you do addition? Questioning the domain of IT. In: Proceedings of AusWIT 2003 - Participation Progress and Potential, Spencer, S. (ed.), pp. 61–71. University of Tasmania, Hobart, Australia. Sørensen, K.H. (2002) Love, Duty and the S-curve: An Overview of Some Current Literature on Gender and ICT. In: Digital Divides and Inclusion Measures. A Review of Literature and Statistical Trends on Gender and ICT, STS Report 59, Sørensen, K.H. & Stewart, J. (Eds), pp. 35–36. NTNU, Centre for Technology and Society, Trondheim. Sydell, L. (2013) Blazing the trail for female programmers. National Public Radio (2013-04-29). Retrieved July 2013. Trauth, E.M. (2011) What can we learn from gender research? Seven lessons for business research methods.

The Electronic Journal of Business Research Methods, 9, 1–9. Trauth, E.M. (2012) Are there enough seats for women at the IT table. ACM Inroads, 3, 49–54. Trauth, E.M. (2013) The role of theory in gender and information systems research. Information and Organization, 23, 277–293. UWEX: University of Wisconsin (2005), Enhancing Program Performance with Logic Models Course 2005. Retrieved July 7, 2010, from www.uwex.edu/ces/pdande/evaluation von Hellens, L., Trauth, E.M. & Fisher, J. (2012) Editorial. Information Systems Journal, 22, 343–353. Weiss, C.H. (1998) Evaluation: Methods for Studying Prond grams and Policies, 2 edn. Prentice-Hall, Englewood Cliffs, NJ. Whitney, T., Gammal, D., Gee, B., Mahoney, J. & Simard, C. (2013) Priming the pipeline: addressing genderbased barriers in computing. Computer, 46, 30–36. Wholey, J.S. (1987) Evaluability assessment: developing program theory. New Directions for Program Evaluation, 33, 77–92. Williamson, K. Burstein, F. & McKemmish, S. (2002) The two major traditions of research’. In: Research Methods for Students, Academics and Professionals, Second Edition, Williamson, K. (ed.), pp. 25–47. Centre for Information Studies, Charles Stuart University, Wagga Wagga. Willis, A., Male, S. & Korcyzynskyj, Y. (2003) Equity issues in higher education - the Curtin initiative, In: Partners in Learning; Teaching and Learning Forum, Bunker, A. & O’Sullivan, M. (Eds). Edith Cowan, Perth. WKKF: The W. K. Kellogg Foundation (1998) Evaluation Handbook. K. Kellogg Foundation, Battle Creek, MI. WKKF: The W. K. Kellogg Foundation (2004) Logic Model Development Guide. W. K. Kellogg Foundation, Battle Creek, MI.

Biography
Associate Professor Annemieke Craig is an educator who teaches in the area of information systems. Her research interests are in the ﬁelds of access and equity with particular emphasis on trying to improve the proﬁle of women in computing. Annemieke has published and presented numerous articles in leading journals and conferences on her research activities related to gender and information technology. She is a chief investigator in the Australian Research Council Funded research project Digital Divas.

© 2015 Wiley Publishing Ltd, Information Systems Journal 26, 585–611

