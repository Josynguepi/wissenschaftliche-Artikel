Journal of Management Information Systems

ISSN: 0742-1222 (Print) 1557-928X (Online) Journal homepage: http://www.tandfonline.com/loi/mmis20

Not As Smart As We Think: A Study of Collective Intelligence in Virtual Groups
Jordan B. Barlow & Alan R. Dennis
To cite this article: Jordan B. Barlow & Alan R. Dennis (2016) Not As Smart As We Think: A Study of Collective Intelligence in Virtual Groups, Journal of Management Information Systems, 33:3, 684-712, DOI: 10.1080/07421222.2016.1243944 To link to this article: http://dx.doi.org/10.1080/07421222.2016.1243944

Published online: 07 Dec 2016.

Submit your article to this journal

Article views: 38

View related articles

View Crossmark data

Full Terms & Conditions of access and use can be found at http://www.tandfonline.com/action/journalInformation?journalCode=mmis20 Download by: [Bibliotheek TU Delft] Date: 14 February 2017, At: 09:21

Not As Smart As We Think: A Study of Collective Intelligence in Virtual Groups
JORDAN B. BARLOW AND ALAN R. DENNIS
JORDAN B. BARLOW (jobarlow@fullerton.edu) is an assistant professor in the Information Systems & Decision Sciences Department at California State University, Fullerton. He received his Ph.D. from the Kelley School of Business, Indiana University. His main research interests are collaboration (collective intelligence, computer-mediated communication, and virtual group work) and behavioral information systems security. He has published in MIS Quarterly, Journal of the AIS, Computers and Security, Group Decision and Negotiation, and Communications of the AIS. ALAN R. DENNIS (ardennis@indiana.edu; corresponding author) is a professor of information systems and holds the John T. Chambers Chair of Internet Systems in the Kelley School of Business, Indiana University. His research focuses on three main themes: team collaboration; information technology (IT) for the subconscious; and gamification. He has written more than 150 research papers and four books, and has won numerous awards for his theoretical and applied research. He is a fellow of the Association for Information Systems and editor in chief of AIS Transactions on Replication Research. He has also cofounded six IT start-up companies. ABSTRACT: Organizations increasingly use virtual groups for many types of work, yet little research has examined factors that make groups perform better across multiple different types of tasks. Previous research has proposed that groups, like individuals, have a general factor of collective intelligence, an ability to perform consistently across multiple types of tasks. We studied groups that used computer-mediated communication (CMC) to investigate whether collective intelligence is similar or different when groups work using CMC. A collective intelligence factor did not emerge among groups using CMC, suggesting that collective intelligence manifests itself differently depending on context. This is in contrast to previous findings. Our results surface a need for more research on boundary conditions of the construct of collective intelligence. Our findings also have practical implications: managers should take care when organizing virtual group work because groups that perform well on one type of task will not necessarily be the groups that do well on other tasks KEY WORDS AND PHRASES: collective intelligence, computer-mediated communication, group performance, intelligence, task types, virtual groups.

Groups increasingly use information and communication technologies (ICT) to enable virtual work [4, 61]. The effectiveness of virtual groups is a major issue for both research and practice [18, 34, 77]. Many factors affect group success [41], with recent reviews pointing to the need to better understand the effects of group characteristics [51, 57].
Journal of Management Information Systems / 2016, Vol. 33, No. 3, pp. 684–712. Copyright © Taylor & Francis Group, LLC ISSN 0742–1222 (print) / ISSN 1557–928X (online) DOI: 10.1080/07421222.2016.1243944

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

685

ICT research generally focuses on one type of task (e.g., decision making, idea generation), yet groups are often called on to complete multiple tasks [37]. Even when groups only work on one project, there are often subtasks, such as generating ideas and alternatives, making decisions, planning, and so on. Little research examines the characteristics of groups that lead to consistently high performance across the multiple types of tasks. That is, no research has examined whether some virtual groups are inherently “smarter” than others, being able to perform well across many tasks, or whether virtual group task performance is more dependent on the task such that there are no “smarter” groups that excel across all types of tasks. Individual intelligence is generally a good predictor of individual performance across a variety of tasks [22]. Woolley et al. [80] reported that groups, like individuals, have a certain level of “collective intelligence,” such that groups that perform well on one type of task will perform well on others. Collective intelligence is an ability akin to individual intelligence, but at the group level [80]. Psychology researchers define intelligence as a “very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience” [39, p. 13], and generally measure intelligence by examining performance across several tasks. Likewise, we define collective intelligence as an ability of groups to perform consistently well across a variety of group-based tasks and measure it through performance on various group tasks (see [80]). There are three notable aspects of this definition. First, collective intelligence is an ability. Like individual abilities, group-level abilities can be developed, but are also inherent in some groups more than others. Second, collective intelligence is a group’s ability to perform, so it is measured using performance. Thus many of the factors predicting task-specific performance might contribute to the development of collective intelligence (for reviews see [33, 38, 44, 65]). Third, collective intelligence includes consistency across tasks. While some characteristics or processes might enable groups to perform well on a given task, what differentiates collective intelligence from “performance” is that performance must be consistent across many different types of tasks, in much the way that individual intelligence is the ability of individuals to perform well on reasoning, problem solving, and so on. Engel et al. [30] even go so far as to define collective intelligence as an “ability of a group to work together, regardless of the type of task involved” [30, p. 3769]. In a pair of studies of face-to-face groups, Woolley et al. [80] found evidence of collective intelligence (i.e., consistent group performance across a series of tasks) that was highly predictive of performance on a more complex task. This collective intelligence factor was not related to the individual intelligence of group members, but was related to members’ social sensitivity—the ability to understand the emotions of others using visual facial cues [6]. Researchers have called for more studies to more fully understand these findings [23, 79]. One key question is whether the factor measured by Woolley et al. [80], which is highly correlated with visual processing (something not available in many ICT), transcends media or is instead a measure of a group’s capability to interact in one

686

BARLOW AND DENNIS

media where visual processing dominates, that is, face-to-face communication. Groups routinely use ICT [4], so it is important to understand collective intelligence: is collective intelligence like individual intelligence, an inherent factor that transcends media, or is it limited to media where facial cues dominate? We examined collective intelligence in an ICT environment in which groups used textbased computer-mediated communication (CMC) because this form of ICT is quite different from the face-to-face environment in which the original collective intelligence factor was developed. Woolley and her colleagues [31] suggest that the same collective intelligence factor, again highly related to social sensitivity, exists in CMC groups. However, our results show no evidence of a collective intelligence in CMC groups. Thus we question the generalizability of previous findings. The current state of research on collective intelligence concludes that such a factor exists in CMC groups. We believe it is critically important to demonstrate that this is not always the case. We do not propose that the research of Woolley et al. is invalid. Instead, we provide evidence that collective intelligence simply does not exist in certain CMC conditions, and we provide suggestions in regard to why this is the case. This will help researchers and practitioners to better understand the collective intelligence of CMC groups, and open up opportunities for more research to enable virtual groups to “work smarter.”

Prior Research and Theory
Woolley et al. [80] proposed that groups, like individuals, have a certain level of intelligence. Just as individual intelligence assesses cognitive ability by measuring performance on a variety of cognitive tasks, collective intelligence is an ability to perform consistently across a variety of tasks [43, 80]. This definition distinguishes the concept of collective intelligence from several other constructs that view groups as information processors [20], such as transactive memory systems and mental models [53], collaboration know-how [55], implicit coordination [67], distributed cognition [46], group memory [78], collective mindfulness [76], and so on. Although these previously studied constructs may correlate with or contribute to a group’s collective intelligence, the conceptualization of collective intelligence is fundamentally different. In much the way that creativity is a trait that is useful in brainstorming, but not necessarily decision making, these previously proposed grouplevel cognitive constructs have been shown to be predictors of performance for some types of tasks, but not across a range of task types (e.g., [55]). While these characteristics or skills can clearly benefit virtual groups, they do not represent the ability of a group to perform consistently well across a variety of different types of tasks.

Initial Findings on Collective Intelligence
Woolley et al. [80] carried out two studies with 192 groups of two to five people working face-to-face on a set of diverse tasks. Similar to measuring individual

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

687

intelligence, they used a factor analysis on the task performance scores to assess collective intelligence. They found that one dominant factor emerged from the factor analysis, which they labeled collective intelligence. They also found, as in the correlation between individual intelligence and performance, that collective intelligence was predictive of group performance on a separate, complex group task. What makes a group collectively intelligent— that is, able to perform well on multiple types of tasks? The collective intelligence factor was only slightly correlated with the individual intelligence of group members (both average and maximum), and individual intelligence, unlike the collective intelligence factor, was not related to group performance on the complex task. Variables that were significantly correlated with collective intelligence were social sensitivity of group members [6], variance of speaking turns, and percentage of females in the group. However, in a regression model with all three factors as independent variables, only social sensitivity was a significant predictor of collective intelligence. Social sensitivity is a measure of how well individuals can understand the emotions and feelings of others based on visual cues [6, 80]. It is measured by having participants look at photographs of individuals’ eyes and identify the emotion expressed [6, 8]. Thus, the collective intelligence factor of Woolley et al. [80] depends heavily on visual understanding of emotions between individuals as they work together, something that is absent when groups use some forms of ICT, such as text-based CMC.

Collective Intelligence in Virtual Work?
The key question in our study is whether virtual groups with limited visual cues have a trait (i.e., collective intelligence) that enables them to perform consistently across a diversity of tasks. In other words, is the factor found by Woolley et al. [80] an inherent characteristic that transcends media, much like individual intelligence, or does it apply only to face-to-face work, with collective intelligence emerging in a different manner (or not at all) when groups use ICTs that lack visual cues? If a general collective intelligence factor exists then, like individual intelligence, it should appear when groups use ICT. Virtual work could include the use of text tools (e.g., e-mail, instant messaging), audio tools (e.g., voice over IP), video tools (e.g., Skype), or a combination. For this study, we chose to focus on text-based CMC, as it is the most different from face-toface interaction because it lacks visual and audio cues. These barriers add problems and cognitive challenges in addition to those faced by groups working face-to-face. Communication failures and misunderstandings lead to problems in maintaining mutual knowledge in virtual work [17]. CMC has distinct properties as compared to face-to-face discourse, including different levels of interaction, organization, and language use [1]. For example, working virtually allows group members to work more anonymously and/or potentially ignore information that other group members provide [25]. Groups using CMC

688

BARLOW AND DENNIS

refer less to personality cues and physical appearance, and have increased opportunities to self-censor [74]. The collective intelligence factor of Woolley et al. [80] depends heavily on social sensitivity, which is measured by having participants identify the emotions expressed visually [6]. This visual understanding of emotions is impossible with CMC that does not provide visual cues. Such lack of visual cues could also affect responsibility, involvement, commitment, and expectations of posttask self-recrimination, which may play roles similar to that of social sensitivity in groups. Therefore, previously reported results of collective intelligence may not be directly relevant to some virtual groups. Previous research on virtual work suggests that performance depends on what type of task the group is performing [32, 56], and how well the technology fits with, or is appropriated for, the given task [26, 35]. Particularly, factors such as task complexity, media capability, and coordination structures affect the performance of virtual groups in different ways for different types of tasks. Virtual teams must adopt different types of work structures to be successful in tasks of varying complexity [7]. Groups working virtually can choose from a variety of media with differing capabilities, and the capabilities of these media support tasks to various extents, depending on the support they provide for conveyance and convergence requirements, which differ by task [24, 26]. In sum, prior research on group performance shows that task type is an important factor influencing virtual group performance, and suggests that consistent performance across different types of tasks is difficult to achieve when working virtually. Little research has examined whether groups perform consistently across tasks when they use CMC (i.e., with some “intelligent” groups generally outperforming others). We are aware of a prior study that found that collective intelligence was manifested in an online environment and that social sensitivity was also linked to it, despite the lack of visual cues [31]. This study was also replicated in multiple countries [30]. One theoretical explanation for this outcome comes from the theory of mind [5], which argues that individuals with greater “mentalizing” capabilities are more competent at social interaction. The visual social sensitivity test is one of many possible ways to measure theory of mind abilities, and the underlying ability measured by the test is not specific to visual cues [64]. However, the various nonvisual tests are not as widely accepted [64], and the method of administering the social sensitivity test relies fully on an individual’s abilities to interpret visual social cues. There is also a methodological explanation for this outcome. The Engel et al. [30, 31] studies used a purpose-built online tool designed to “administer the test battery” [31, p. 5]. This was unlike CMC tools used in past research or tools that are commercially available. The tasks were very short in duration (fourteen tasks completed in sixty-four minutes, averaging about four minutes each, including instructions) and were time limited (like test questions) such that the software simply moved on if the group did not answer the question before time expired. The tasks included those that were additive (e.g., brainstorming), executing (e.g., typing), memory (e.g., recall words), choice (e.g., Sudoku, IQ test questions), judgment (e.g., rating images), and detecting (e.g., finding frequent words). These tasks are quite different from those in Woolley et al. [80], for which we

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

689

commend the authors because this ensures that their results are not due to idiosyncrasies in prior tasks. However, these tasks are more like “tests” [31, p. 5] than tasks traditionally found in prior group research or organizational practice [40, 58, 59]. Specifically, these tasks do not require much convergence, an important type of group interaction that is both important in group work [26] and more difficult for virtual groups to achieve than face-toface groups [62]. In other words, the tasks used do not represent a wide variety of groupbased tasks. Based on our definition of collective intelligence, which fits closely with the original Woolley et al. study, it should be measured based on a wide variety of tasks—this is what differentiates the very concept of “collective intelligence” from “group performance.” Woolley et al.’s original study focused on the need to measure tasks from all types of task categorizations, such as the McGrath circumplex. Yet the studies by Engel et al. use a more limited set of tasks. Therefore, we believe that more research is needed with traditional group tools and tasks.

Hypothesis Development
A study of collective intelligence in groups using CMC with limited or no visual cues should result in one of three outcomes, each of which would provide an important contribution to collective intelligence research and understanding of virtual group performance across tasks. First, if a collective intelligence factor emerges and is highly correlated with social sensitivity and the other variables found by Woolley’s research group (i.e., [31, 80]), as we hypothesize, then our research would validate the factor proposed as a general form of collective intelligence that transcends media. Thus, regardless of the media chosen, truly intelligent groups would be high performers across multiple tasks whether they met face-to-face or via ICT. This outcome would also suggest that visual social sensitivity is an important factor that can predict success even in settings with limited visual indications of emotion. As explained in previous collective intelligence research, this hypothesis is based on the theory of mind and proposes that some groups perform consistently well across tasks and contexts because of their group-level abilities such as interpreting social cues among group members. Second, if a collective intelligence factor emerges but is not correlated with social sensitivity or the other variables found by Woolley et al. [80], the results would limit the generalizability of their findings by presenting some boundary conditions. This would suggest that there is a general collective intelligence factor inherent to groups but that other factors (such as a nonvisual form of social sensitivity) become more important for groups when they use CMC with limited visual cues. In other words, the collective intelligence factor proposed by Woolley et al. [80] is also important for virtual work, but it is related to different group characteristics when groups use CMC. This outcome is also supported by the theory of mind, but would indicate that another, nonvisual, form of social sensitivity plays out in groups with no visual cues and that the visual social sensitivity test is not robust enough to uncover it.

690

BARLOW AND DENNIS

Third, if a collective intelligence factor does not emerge when groups use CMC, we would conclude that this factor, validated in face-to-face group work, cannot be considered general collective intelligence in the same sense as an individual intelligence factor. Rather, this outcome would suggest that collective intelligence (i.e., the consistency of performance across different types of tasks) may be inherent when groups work face-toface, but fails to appear as an inherent trait when they work virtually because having many social cues is important to establish the social norms and routines necessary to succeed [50]. In other words, it may be that collective intelligence can only be developed under certain circumstances. Some groups may be more “intelligent” (i.e., high performing) than others, but only when the conditions are right. Such an outcome would demonstrate that for CMC, an inherent disconnect exists between task performance and inherent group characteristics, which makes performance more dependent on task type than on any inherent group characteristic. This would lead us to conclude that different types of tasks have different requirements, and thus the processes groups use to perform them are likely more important in this setting than underlying group characteristics. None of these three outcomes depends on perfect replication of the Woolley et al. [80] or Engel et al. [30, 31] studies. In fact, if we were to conduct a perfect replication of either of their studies, we would not know whether the collective intelligence factor we studied was somehow unique to the tools and tasks they used [27]. Thus to investigate whether the collective intelligence factor found by Woolley et al. [80] and Engel et al. [30, 31] is generalizable to groups using traditional CMC tools on tasks including those high in convergence requirements, we need to show that when groups use CMC, performance on any set of diverse tasks is either (1) consistent, with highly intelligent groups performing better; or (2) not consistent, with no group characteristic emerging that is linked to higher performance across multiple tasks. In summary, theoretical arguments can be made as to why collective intelligence will or will not emerge in contexts with few visual cues, indicating the importance of further research examining such a factor in this context. Prior empirical research finds that collective intelligence does emerge. Therefore, based on our assessment of this set of conflicting prior theory and limited empirical findings, we hypothesize: Hypothesis 1: A collective intelligence factor will emerge in groups using CMC with limited visual cues. In addition to understanding collective intelligence as a construct, we examine how various constructs are related to collective intelligence for groups using CMC. If a collective intelligence factor does not emerge when groups use CMC, it may be important to understand which constructs are related to performance for specific task types to better understand why across-task performance (i.e., collective intelligence) is not consistent. We used four factors from Woolley et al. [80] as potential predictors of collective intelligence: social sensitivity, individual intelligence, equality of participation, and gender. These four factors have been studied in prior research, both face-to-face and CMC, but no past research has examined their impact on collective intelligence (with the

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

691

exception of the studies heavily cited above [30, 31, 80]). Most CMC research examining these four factors has considered their impact on group performance for one specific task, such as brainstorming or decision making. Collective intelligence is the ability of a group to perform well across a variety of tasks [80], so collective intelligence is related to, but not identical to, performance on one task. Given the paucity of prior research on collective intelligence, we use this research examining performance on one task to build our arguments for the effects of these four factors on collective intelligence (i.e., a group’s ability to perform consistently across multiple tasks). Social sensitivity is the ability to understand the feelings of others [8] and is often measured using the Reading the Mind in the Eyes test [6], which tests participants’ ability to read emotion from visual cues. It is related to group performance in faceto-face settings [8, 52]. Although members of groups working virtually may be able to detect other group members’ emotions without visual cues (e.g., through interpretation of textual cues), the social sensitivity construct is measured by interpreting visual cues [6, 80]. Thus, the construct measured using this test may not be as salient when groups use CMC that lacks visual cues, because CMC that removes visual cues reduces social sensitivity [10]. The lack of visual (and vocal) cues is one important reason why groups using CMC often have problems converging on a decision [26]. Thus, social sensitivity (the ability to read emotion and understand others) may take on increased importance in CMC contexts where the medium inherently reduces visual and vocal cues. Social sensitivity based on visual cues is not ideal for CMC contexts, but we know of no analogous test of an individual’s ability to interpret complex social or emotional cues in text-based CMC. One study measured the “interpersonal sensitivity” of CMC groups by subjectively rating the text of group transcripts on a single-item scale. This study did not measure the relationship between social sensitivity and group performance, but did find that social sensitivity behaviors were not different between virtual and face-to-face groups [72]. Prior empirical research surprisingly shows that collective intelligence in CMC environments is related to the average social sensitivity of the group’s members, as measured by the visual social sensitivity test [31]. One explanation is that the visual test is only one of multiple ways to measure a construct that is not unique to visual cues [64], although the other nonvisual tests it has been compared to are not as widely accepted [64]. There is, therefore, a need to further empirically test the relationship between social sensitivity and group outcomes for groups using nonvisual CMC. We hypothesize: Hypothesis 2: Social sensitivity is positively related to collective intelligence in groups using CMC. Few published studies have examined individual intelligence in groups working virtually [48], and these have only examined it in the context of brainstorming tasks [73], finding a significant effect of group member intelligence on group

692

BARLOW AND DENNIS

performance. Woolley et al. [80] found mixed results with the relationship between individual and collective intelligence, with one study showing significant effects and the other not. Engel et al. [31] did not report the correlation of individual and collective intelligence. Thus, it remains unclear how individual intelligence is related to collective intelligence in CMC groups. By definition, individual intelligence is a trait that enables individuals to perform consistently well across a range of cognitive tasks [39]; on average, individuals with higher intelligence perform better than individuals with lower intelligence [39]. As individuals work together in groups, their individual intelligence should affect performance on cognitive tasks in a manner similar to the effect when they work separately; a group of individuals with higher intelligence should perform better than a group of individuals with lower intelligence. Therefore, the average intelligence of group members should be related to collective intelligence. A meta-analysis showed that individual intelligence has a small to moderate effect on group performance in face-to-face work, with the strongest effects for additive tasks where group performance is the sum of individual performance, such as brainstorming work [28]. Another study found that the correlation between individual intelligence and group performance was also high for conjunctive and disjunctive tasks (where group performance is a function of the highest- or lowest-performing member of the group, respectively) [19, 71]. Individual intelligence matters in many face-to-face group tasks, so we conclude that individual intelligence should influence the collective intelligence of groups working virtually. We hypothesize: Hypothesis 3: Individual intelligence is positively related to collective intelligence in groups using CMC. Equality of participation considers the distribution of communication within a group [33, 34]. Groups with low equality of participation (i.e., a high variance of speaking turns) are those in which some group members dominate discussion while other members of the group contribute proportionately less. More equal participation has been found to improve group performance because it enables a greater sharing and integration of knowledge [33, 34, 66]. Equality of participation is important to group performance for tasks in which there is a need to integrate knowledge from group members because if some members do not contribute, the group loses this knowledge [25]; when a subset of the group acting alone is able to complete the task, equality of participation is unimportant [12]. Thus, because equality of participation seems to matter only for performance on some types of tasks, it is likely not a strong predictor of collective intelligence. Woolley et al. [80] found participation equality to be correlated with collective intelligence, but not a significant predictor of the collective intelligence factor in a regression model. Use of text-based CMC usually results in more equal participation [32, 45], so this factor may have less effect when groups work virtually. Engel et al. [31] found no effects due to participation equality. Therefore, we hypothesize: Hypothesis 4: Equality of participation is not related to collective intelligence in groups using CMC.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

693

Gender diversity can be a double-edged sword. Greater diversity offers the group more unique knowledge and perspectives, which should improve group performance [36, 42]. Conversely, individuals are often more willing to share knowledge with and use knowledge from people who are more like them, so increased diversity reduces the amount of knowledge shared and used, which reduces group performance [42, 47, 49]. Both processes may coexist, or one may dominate [14, 36, 42]. Gender was correlated with collective intelligence in the Woolley et al. [80] face-toface studies, but women also scored higher on social sensitivity. CMC may reduce some of the negative effects of diversity by making differences less visible, but prior research suggests that the effects have been mixed. For example, the use of CMC has helped diverse teams improve performance [9, 81], has had no effect [69], has lowered performance [3, 63], or has both helped and hurt [36]. These conflicting results suggest a need to better understand the effect of gender on performance for groups using CMC. Because there are differing effects on performance, the relationship of gender with collective intelligence, seems tenuous at best. Research on gender often concludes that group success is less dependent on the presence of a particular gender than on the mix of both genders [2]. Given the competing theoretical and empirical research, we hypothesize: Hypothesis 5: Gender is not related to collective intelligence in groups using CMC.

Method Participants
Participants were 324 undergraduate and graduate students (43 percent female, average age twenty-one years1) at a large Midwestern university business school, randomly organized into 86 groups of 3–5 members. Thirty groups (44.1 percent) were mixed gender with at least 50 percent female and 37 groups (54.4 percent) were mixed gender with a female minority, while 12 groups (17.6 percent) were all male and 7 groups (10.3 percent) were all female. Participants received an incentive to participate, either extra credit or entry in a drawing for a cash prize. Student samples are appropriate for testing theories about phenomena that are theorized to hold true across the population [16]. The emergence or nonemergence of this factor in any subset of the population will either confirm the generalizability of collective intelligence or show the need to better understand how it differs by context.

Tasks and Performance Measures
Groups used Gmail chat software, a commonly used text-based CMC tool that is similar to other text-based CMC tools. Following Woolley et al. [80], groups completed three initial tasks and one complex task. The initial tasks were used in the factor analysis to find a collective intelligence factor, while the complex task was used to

694

BARLOW AND DENNIS

have a single measure of group performance separate from the collective intelligence tasks, and which would evaluate the performance-predicting capabilities of the collective intelligence factor. Like Woolley et al. [80], we selected the three initial tasks from different quadrants of the McGrath group task circumplex [58], which classifies and explains four different types of tasks that groups perform—brainstorming, decision, negotiation, and execution—each requiring different cognitive processes. We followed the strategy of Engel et al. [30, 31] in that we did not use the same tasks as Woolley et al. [80]; as Engel et al. [30, 31] note in their extension of the Woolley et al. [80] study, using the same tasks as the original study is not desirable because it limits the generalizability to just that set of tasks (see also [27]). We chose three tasks—brainstorming, decision, and negotiation—due to time constraints. The fourth task type—an execution task—was not used, as all tasks require some form of execution. Tasks used in previous research classified as strictly execution tasks include typing a document or replicating a set of patterns. Further, we also aimed to include tasks that required both conveyance and convergence, two distinct and important types of group processes [26]. Convergence is more difficult for groups using CMC [26], and the previous study of collective intelligence in CMC groups did not include tasks with heavy convergence requirements [30, 31].

Brainstorming Participants were given seven minutes to brainstorm ideas to increase tourism to the city in which the university was located. The tourism task is a classic brainstorming task that has been used in many studies [21]. As in the Woolley et al. [80] studies, performance was determined as the number of unique ideas produced by each group. Three coders independently counted the ideas from the transcripts. Inter-rater reliability calculations indicated adequate agreement among raters (Fleiss’s kappa = 0.92).

Decision The college admissions task, a hidden profile task, was used for the decision portion. A hidden profile task is a task in which each group member first considers a set of incomplete information, and then the group comes together to make a final decision. Incomplete information includes both common information, known to every group member, and unique information, known to only a subset of group members [70]. The college admissions task has been used in several studies [35, 61, 82]. Individuals were given information about four hypothetical candidates applying to a university. Participants were then asked to decide, as a group, which candidates to admit and which to deny. Groups were allowed to admit up to two of the four candidates after discussing them for twelve minutes. Groups received one point for each correct decision (admit or deny), resulting in a performance score ranging from zero to four. The correct decision, which is based on certain criteria given in instructions to the participants, was validated by the university admissions officers.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

695

Negotiation For the negotiation task, we used a slightly simplified version of the shopping plan task [80]. Groups had twenty minutes to create a shopping plan where they decided which stores to visit to complete their individual shopping lists. Plans were scored using the criteria of Woolley et al. [80]: the number of items purchased, the time taken to complete shopping, the quality and price of items, and whether or not items spoiled.

Complex Task Complex tasks often require some aspect of each of the four types or modes of activity [59], as groups need to brainstorm, compare, choose, and execute strategies to complete the task. For the complex task in our study, groups were asked to make a distribution decision about a hypothetical candy production firm. Each group member was assigned as a division manager with specific resource needs, and groups were given information about the profit margins of each division. The groups were asked to decide how to distribute two key ingredients to the various divisions to maximize profits for the company [60]. Groups were given twenty-five minutes to complete this task. The performance score was the standardized maximum profit per division achieved by the company based on the distribution of the ingredients.

Measures
Social sensitivity was measured using the Reading the Mind in the Eyes test [6]. The task consists of thirty-six consecutive photographs of eyes, where participants select one of four possible emotions that the photographed person is feeling. Although it measures visual cues, this test has been correlated with performance of groups working with nonvisual CMC [31]. We are aware of no text-based test of social sensitivity. One study measured “interpersonal sensitivity” using subjective ratings by two coders on a single scale based on the coders’ perceptions of CMC comments [72]. However, the inter-rater reliability of this method was lower than the reliability for other constructs in the same study; further, we believe that it is difficult for coders to decipher a person’s ability to interpret social cues based solely on a short transcript of CMC behavior (indeed, this would require great social sensitivity of the coders themselves). Individual intelligence. Participants completed the Wonderlic Personnel Test, a cognitive ability exercise that has been validated by research [29] and used in numerous studies [11, 68, 80]. The test consists of fifty questions to answer in twelve minutes. Equality of participation was measured as variation in speaking turns. Groups with a high variance are those with low equality of participation. There are several approaches to measuring equality of participation, but most measures of observed participation equality in CMC groups rely on a ratio of group members’ typed

696

BARLOW AND DENNIS

contributions [66]. We wanted to remain consistent with Woolley et al. [80], so we calculated the percentage of messages contributed by each participant from the chat transcript, and used the variance of these percentages as the measure. Gender. Participants were asked to self-report their gender. For each group, percent female was calculated, as done by Woolley et al. [80].

Procedures
Participants first completed an online survey before signing up for a lab session. The survey contained (a) an informed consent statement; (b) the Reading the Mind in the Eyes test [6], measuring social sensitivity; and (c) a set of questions designed to enable robustness and validity checks (described below). The remainder of the data were collected in the research lab. Participants sat at individual workstations where they could not see the screens of other participants. They first completed the Wonderlic intelligence test, and then completed the group tasks in groups of three to five members. The three simple tasks—brainstorming, negotiation, and decision-making—were completed in random order. Following these tasks, the groups performed the complex problem-solving task. After the group tasks, participants completed a survey soliciting demographic information. Procedures were refined during five pilot tests. Minor adjustments were made to timing and task requirements to allow participants to finish within two hours to prevent fatigue.

Comparison of Individual Intelligence and Performance
In addition to the main experiment, we recruited ninety-one additional participants from the same subject pool to complete the same tasks using the same measures and procedures but working as individuals rather than in groups. We analyzed these individual-level data using the procedures described in the Analysis and Results section below. We found that an individual intelligence factor emerged from the data and that this factor was significantly correlated with scores on the Wonderlic intelligence test (r = 0.318; p < 0.01). We conclude that because individual intelligence influenced individual performance for these subjects, tasks, measures, and procedures, they constitute an appropriate context in which to investigate collective intelligence when groups use CMC. These tests showed that our procedures were able to detect the emergence of an intelligence factor with a similar sample size as used in the group study.

Analysis and Results Collective Intelligence Factor
Following the procedures of Woolley et al. [80], we used group performance scores on the set of simple tasks to determine whether a collective intelligence factor emerged

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

697

from the data. The first criterion is that the average correlation between task scores should be positive [80]. The correlations were either not statistically significant or significantly negative, as shown in Table 1. The average correlation was –0.12, indicating that performance on one task was not correlated with performance on other tasks. The method to identify an intelligence factor, whether individual or collective, consists of a factor analysis of performance scores on a variety of tasks [22, 31, 80]. The intelligence factor in this factor analysis should (a) account for 30–50 percent of the variance, with the next factor accounting for significantly less, (b) have an eigenvalue greater than 1.38, and (c) demonstrate an obvious elbow in the scree plot [15]. None of these criteria were met. In our principal components factor analysis, the first factor accounted for 42 percent of the variance, but the second factor accounted for 36 percent, suggesting two dominant factors, rather than an emerging intelligence factor. The first factor ’s eigenvalue was 1.26, and no drop-off elbow appeared in the scree plot, which is shown in Figure 1. Taken together, the procedures of Woolley et al. [80] indicate that a general collective intelligence factor does not emerge when groups use CMC with limited visual cues. Thus, we conclude that H1 is not supported. H2–H5 hypothesized the effects of several individual factors on collective intelligence. Because no collective intelligence trait emerged, we cannot directly test these hypotheses. However, in the Discussion section below, we examine the relationship between these factors and performance on each task to explain why these factors did not have consistent effects on group performance across multiple types of tasks.

Robustness and Validity Checks
Robustness and Validity Checks on Task Selection As we noted in the Methods section, our analysis of individual performance of our tasks found that an individual intelligence factor emerged that was correlated to the Table 1. Correlations Between Group Tasks and Traits
1 1. Brainstorming task 2. Decision task 3. Negotiation task 4. Complex task 5. Average intelligence 6. Max. intelligence 7. Gender (% female) 8. Average social sensitivity 9. Max. social sensitivity 10. Equality of participation 2 3 4 5 6 7 8 9

–0.14 –0.25* 0.02 0.51** 0.47** –0.25* 0.34** 0.26* –0.19

–0.09 0.01 –0.06 –0.11 –0.11 –0.26* –0.23* 0.04

0.15 –0.02 0.07 –0.05 –0.08 0.75** 0.08 –0.18 –0.37** –0.22* –0.09 0.04 0.44** 0.42** 0.01 –0.07 –0.09 0.22* 0.32** 0.19 0.63** –0.06 0.05 –0.15 –0.04 0.00 0.08 0.07

*p < .05; ** p < .01.

698

BARLOW AND DENNIS

Figure 1. Scree Plots Comparing Results to Woolley et al. [80]

individual’s Wonderlic score, suggesting that our tasks were appropriate. The tasks were drawn from prior group research and were similar to tasks used by Woolley et al. [80]. Exact replication is not desirable [31], as the goal was to examine performance across different tasks, not to perfectly replicate their study, which would raise concerns about generalizability to other tasks [27]. If the Woolley et al. [80] or Engel et al. [30, 31] findings are dependent on specific tasks, this would further strengthen our argument that the factor they found was not truly collective intelligence. We performed a task robustness check and two validity checks to ensure that the differences between our findings and those of Woolley et al. [80] and Engel et al. [31] were not dependent on our task selection or on the number of tasks used. First, we used several combinations of tasks for our analysis. Our findings above use the set of three simpler tasks (brainstorming, decision, and negotiation). We ran a factor analysis on all other combinations of tasks (see Figure 2) and found that no matter what tasks we included in the factor analysis, a single collective intelligence factor did not emerge. To ensure that the findings were not based on using too few tasks, we ran the procedures on the combination of all four tasks, with the same results (see Figure 2 and Table 2). Second, we compared the variation in our task performance scores to the Woolley et al. [80] studies. The mean coefficient of variation across our tasks (0.308) was not significantly different from the mean coefficient of variation for their tasks (COV = 0.345; t(17) = .36, p = .738), suggesting that lack of variation in performance is not an issue.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

699

Figure 2. Scree Plots for Factor Analyses of All Combinations of Tasks

Third, we checked for ceiling effects. Our mean performance across the tasks was 69 percent of the optimal, which was not significantly different from the mean performance in the their studies (58 percent of optimal; t(17) = 0.80, p = .468), suggesting that ceiling effects were not an issue.

Validity Checks for Other Constructs Although only task performance scores are used in the factor analysis to detect an intelligence factor, we also compared the descriptive statistics of our other measures to those of Woolley et al. [80] to further ensure that our groups were not significantly different from the groups in their studies. These statistics are shown in Table 3. Like Woolley et al. [80], who measured cohesion, motivation, and satisfaction (and found none of them to be correlated with collective intelligence or performance on any task), we also collected measures of additional group-related constructs including social and task motivation and disposition to cooperate and trust, and also found none of them to be significantly correlated with performance on our tasks, with the sole exception of social motivation [20], which was correlated with performance on the brainstorming task (r = 0.26, p = 0.02).

Robustness Check for Statistical Methods To ensure that our results were not dependent on the type of extraction used in our factor analysis, we repeated our analysis using four methods of extraction: maximum likelihood, unweighted least squares, generalized least squares, and principal axis factoring. In each case, no intelligence factor emerged. In all cases, multiple factors

700

BARLOW AND DENNIS

Table 2. Results of Task Combination Robustness Check Brainstorm + Decision + Complex
–0.04 38.15

Analysis
Average correlation Variance explained by first factor Variance explained by second factor First eigenvalue

Brainstorm + Negotiation + Complex
–0.03 42.99

Decision + Negotiation + Complex
0.02 39.14

All four tasks
–0.05 32.64

33.25

33.81

34.12

27.69

1.14

1.29

1.17

1.31

explained over 30 percent of variance, no factor had an eigenvalue of 1.3 or higher, and no scree plots showed any obvious elbow to indicate one dominant factor. Use of these statistical methods to analyze the separate individual level data described in the Methods section showed that when individuals completed the same tasks, performance was consistent across tasks and an intelligence factor emerged. The average pairwise correlation of performance scores was 0.17; the first factor in the factor analysis accounted for 45 percent of variance, with the second factor being much lower (31 percent), creating an obvious elbow in the scree plot. These results demonstrate the utility of our tasks and factor analysis procedures for detecting a general intelligence factor. It was only when the tasks were performed by groups using CMC that performance became inconsistent with no general intelligence factor emerging.

Robustness Check for Motivation We completed an additional robustness check to ensure that random noise in the data resulting from careless participants did not affect our results. As part of the presurvey, participants completed a set of survey questions designed to determine whether the participant was paying attention and would earnestly complete the study. The set of questions was generally irrelevant to the study but contained one question with an objective answer (i.e., “Please select Disagree as the answer to this question”). In addition, the survey was designed to be completed in fifteen to thirty minutes. Participants who took less than ten minutes on the study and/or answered the objective question incorrectly were flagged. Analyses excluding groups with flagged participants achieved the same results (i.e., no intelligence factor emerged, and individual intelligence was related only to the brainstorming task). In addition, a research assistant read the chat transcripts to verify that the groups were earnestly trying to perform well on the tasks. No group was identified as failing to take the task seriously.

Table 3. Comparison of Group Level Measures to Woolley et al. [80] Group level Average Wonderlic score % Female
42.6 50.2 28.0 37.2 0.0 0.0 100.0 100.0 27.3 3.4 20.3 35.7 23.9 6.0 8.0 39.0 24.8 3.1 14.7 31.0 25.9 2.8 18.5 35.0

Individual level Average social sensitivity

Wonderlic score
n/a n/a n/a n/a

Social sensitivity

Variance in speaking turns
138.8 147.7 5.2 858.3 112.4 97.2 0.7 355.0

Group size
3.7 0.7 3.0 5.0 n/a n/a 2.0 5.0

Mean St. dev. Min. Max.

27.4 6.1 8.0 43.0

* * * 39.0

24.8 5.4 7.0 35.0

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

*Woolley et al. [80] did not directly report these values in their study. However, information in Woolley et al.’s published supplementary materials indicate that for Study 2, the mean Wonderlic score was between 22.9 and 24.4; the standard deviation was between 6.81 and 7.07; and the minimum score was less than or equal to 8. Notes: Values in the left columns are the current study; values in the right columns refer to Woolley et al. Study 2.

701

702

BARLOW AND DENNIS

Discussion Interpretation of Results
The status quo on collective intelligence is that it is present in both face-to-face and CMC groups and that it is related to the average social sensitivity of group members [30, 31, 80]. Our findings challenge this conclusion. Our study shows that no collective intelligence factor emerged when groups used a commercial text-based CMC to perform tasks similar to those in prior research. That is, when using CMC tools with limited social cues, groups that performed well on one task were no more likely to perform well on a different task. Yet when we used these same procedures and measures to examine the performance of individuals drawn from the same subject pool performing the same tasks separately, we found that an individual intelligence factor emerged (i.e., individuals who performed well on one task were more likely to perform well on the other tasks), and that this individual intelligence factor was correlated with the individual’s Wonderlic score. Since these subjects, tasks, measures, and procedures resulted in an individual intelligence factor, we are confident that they would have resulted in a collective intelligence factor in groups if it existed in our groups. Yet it did not. Intelligence mattered to individual performance on these tasks, but was unrelated to group performance on these same tasks. Groups of more intelligent people did not perform any better than groups of less intelligent people, and social sensitivity was only related to performance on one task (and negatively related). Thus the status quo must be revised; there are important boundary conditions to collective intelligence in groups using CMC. Collective intelligence is present in some CMC groups but not others; collective intelligence is related to social sensitivity for some CMC groups but not others. Our findings are counter to the findings of Engel and colleagues [30, 31]. This is not to say that their findings are incorrect, only that they do not generalize beyond some boundary conditions. So what are the boundary conditions? There are numerous differences between their studies and ours that could have led to the differences. We cannot draw definitive conclusions, but our speculations center on two theoretical factors central to prior research: task and technology. First, they used fourteen tasks completed in sixty-four minutes, tasks that seem closer to intelligence test questions that have demonstrably correct answers, sometimes called “Eureka tasks” (e.g., Sudoku puzzles, word searches, IQ test questions) [30] than the tasks we used, which were drawn from prior group research and required more complex convergence activities, which is often more difficult in virtual settings [26, 62]. That is, while the tasks used in their studies were of various types, the tasks were similar in that they were short with correct solutions and did not require much conveyance and convergence, while our study includes tasks with a variety of different coordination requirements. Therefore, the collective intelligence factor in the Engel et al. studies might only apply to the type of task used in their research, rather than being a general intelligence factor that would emerge in all types of tasks including more complex ones. Second, they used custom-built “test battery” software, which presented the tasks in tightly controlled time sequence; if groups were unable to complete the task in the

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

703

specified time, the software skipped the task and presented the next in the sequence [30]. The software presented task instructions on the left 35 percent of the screen, the task in the center (e.g., a Sudoku puzzle) and a text chat on the right 10 percent of the screen [31]. Thus, the technology was very different from prior research or commercial software for group collaboration. In contrast, we used commercial software (Google Chat) that has been used in many prior studies. This is not to discount their findings. Our point is that there are sharp differences in how participants in their study would have experienced CMC compared to how those in ours did. Researchers are free to operationalize constructs, tasks, and technologies in ways that best fit their research goals. We believe that our approach fits well with prior CMC research and has ecological validity. While groups may have a level of collective intelligence when performing many short tasks requiring little convergence using carefully created test battery software, our results show that for tasks requiring both conveyance and convergence using a commercial CMC tool, a collective intelligence factor does not emerge. We contend that our tasks and software are closer to what groups in organizations experience, so we conclude that when groups work virtually, organizations cannot expect groups to perform well across all types of tasks. Therefore, we conclude that the factor found by Woolley et al. [80] is not a general factor of collective intelligence inherent to groups under all conditions. Instead, it is a measure of a group’s ability to work well in some settings (e.g., face-to-face, or online using “test battery” software, or perhaps only with a certain range of tasks not requiring as much conveyance and convergence as the tasks in our study). Collective intelligence manifests differently in some virtual settings; that is, groups using CMC do not always have an inherent factor that makes some groups more intelligent than others across many tasks. We conclude that collective intelligence is a concept that is unlike individual intelligence in that it emerges differently under different conditions. For some forms of CMC (e.g., the text chat we used here), task characteristics have a more powerful effect on group performance than any inherent group factor. Our results show that on average, groups that did well in one task did not do well in others (i.e., regression to the mean). Truly intelligent groups should perform well relative to other groups, across tasks and media, even (and especially) in conditions where the task and the technology do not fit well together. Therefore, the concept of collective intelligence is more complex than past research would suggest, manifesting under some conditions but not others. Organizations cannot expect a group that performs well on one task to be high-performing in other tasks, particularly if they work with CMC with limited visual cues. This finding matches previous work on virtual project teams, which found that groups that did well on some tasks often did poorly on others [63].

Effects of Individual Factors Across Tasks
Since no collective intelligence factor was found when groups used CMC, it was not possible to directly test H2 through H5. However, it is possible to examine how the various individual factors influenced task performance across the tasks separately.

704

BARLOW AND DENNIS

Previous research on collective intelligence has claimed the factors in H2–H5 to be related to the collective intelligence factor, and thus important for groups to perform well across tasks. While each of these factors has been examined in prior research, no research has compared their combined effects across different task types, resulting in an additional contribution of our study. First, similar to the findings of Woolley et al. [80], individual intelligence was not correlated with performance on the decision task, the negotiation task, or the complex task, but was correlated with performance on the brainstorming task (see Table 1). Brainstorming performance was also correlated with gender and social sensitivity (likely due to the correlation between individual intelligence and social sensitivity). Groups with a smaller percentage of females generated more ideas; this result contrasts with the positive correlation between percentage of females and group performance in the Woolley et al. [80] and Engel et al. [31] studies. Unlike [80] and [30, 31], social sensitivity and percentage of females in the group were not significantly positively correlated with task performance for any tasks beyond brainstorming. Social sensitivity was negatively correlated with the performance of decision tasks, meaning that groups with a higher average social sensitivity made worse decisions. Equality of participation was not correlated with performance, indicating variance in turns may not be as relevant when groups use CMC. To examine the effects of these variables on task performance beyond correlations, we ran four separate regression models with performance on each of the four tasks as the dependent variables. Table 4 summarizes these results. For the brainstorming task, both individual intelligence and social sensitivity were significant predictors of performance. Although percentage of females was correlated with task performance, it was not significant in the regression model, suggesting that the correlation was a statistical artifact due to correlations with other variables. For the decision task,

Table 4. Results of Separate Regression Models on Task Performance Variables Brainstorming task Parameter estimate (and p-value) Negotiation task

Task

Decision task

Complex task

Intercept –21.41 (< 0.01) 6.03 (< 0.01) 70.64 (< 0.01) 19.00 (< 0.01) # members 1.24 (0.17) –0.29 (0.12) 1.26 (0.55) –0.27 (0.51) Task order 0.77 (0.28) –0.01 (0.97) –0.35 (0.84) N/A Average social 0.49* (0.03) –0.10* (0.03) –0.48 (0.35) 0.03 (0.75) sensitivity Average 0.61** (0.01) 0.02 (0.68) 0.22 (0.66) –0.01 (0.95) intelligence Gender (% female) –0.89 (0.14) –0.08 (0.53) 1.07 (0.45) –0.43 (0.11) Speaking variance –0.01 (0.14) 0.00 (0.88) –0.00 (0.89) 0.00 (0.96) *p < .05; ** p < .01.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

705

social sensitivity was significantly negatively related to performance. For the negotiation task and complex task, no variables were significantly related to performance. A power analysis showed a power of 0.98 to detect a medium-size effect. In our study, social sensitivity had a significant positive effect on brainstorming performance, a significant negative effect on decision-making performance, and no significant effect on the performance for the negotiation task or the complex task. We conclude that researchers should take caution in claiming that social sensitivity is an important factor influencing group performance without considering the media groups use and the tasks they perform. Although social sensitivity influences performance when using groups work face-to-face, it has no consistent effects when groups use CMC that lack visual cues. The ability to understand nonvisual social cues may be linked to performance for some types of tasks [75], but this skill is likely separate from visual social sensitivity. One of the most surprising results of our study and the Woolley et al. [80] studies is the general lack of impact of individual intelligence on overall group performance. Individual intelligence is generally a good predictor of performance on individual tasks [22, 23, 80], and was significantly correlated with individual performance for the tasks we studied. However, something happens in group work that disconnects individual intelligence from group performance. Our study suggests that group member intelligence may be relevant only for tasks involving a sum of individual performance, such as brainstorming tasks. Brainstorming is an additive task in which group performance is largely a function of the sum of individual performance (i.e., the number of ideas produced by a group is the sum of ideas produced by individuals) [71]. As such, brainstorming is more akin to an individual task than other types of group tasks that require group members to come to consensus on the outcome(s). Individual intelligence is not significant for interactive tasks that require consensus. Also contrary to Woolley et al. [80] and Engel et al. [31], our results show that social sensitivity was negatively related to the performance of decision tasks. This result shows that visual social sensitivity does not consistently predict good performance when groups use media other than face-to-face communication. Instead, those higher in visual social sensitivity were more likely to be negatively impacted by the lack of visual processing in text-based CMC, with their performance suffering when they could not use the visual processing that helps in face-to-face collaboration. Further, social sensitivity was highly correlated with individual intelligence, indicating a possibility that correlations of social sensitivity with group performance in CMC [31] may be a result of correlations with other variables.

Limitations
One limitation is that we studied students. Participants in our study averaged a slightly higher individual intelligence score than participants in the Woolley et al. [80] studies and were on average two years younger. Student samples are appropriate for testing

706

BARLOW AND DENNIS

theories about phenomena that are theorized to hold true across the population [16]. If a collective intelligence factor does not emerge in groups of students, then we can conclude that the collective intelligence factor does not always emerge. Although we cannot know the motivation of students in lab experiments, even group members in practice often have varying degrees of motivation. Another limitation of our study is that groups had not worked together previously. As groups develop relationships and routines over time, they may be able to better perform on a wider variety of tasks [59]. However, we note that Engel et al. [31] also studied groups with no previous interaction, suggesting that the lack of interaction does not explain why no single collective intelligence factor emerged for the groups in our study. For studies of collective intelligence, new groups are more appropriate than established groups. Collective intelligence should matter most for new groups because intelligence has the highest effect on performance when individuals first start working on tasks [28]. As individuals develop experience with a task, routines set in and the effects of intelligence are not as strong [28]. However, Engel et al.’s second study [30] showed results similar to their first when examining teams with previous experience working together.

Implications for Future Research
Despite these limitations, we believe that the current study opens up five interesting opportunities for future research. First, more research is needed to understand the meaning of the collective intelligence factor found in previous studies. Our study suggests some boundary conditions of the collective intelligence factor that emerges in face-to-face work and some CMC groups. More studies are needed to examine collective intelligence and the conditions under which it emerges and influences group performance. Future research should continue to assess why groups can develop a general ability to work well across tasks when visual cues are present, and under which circumstances groups can develop collective intelligence when using CMC lacking such visual cues. Even after a century of research on intelligence, psychologists are still trying to understand the underlying brain processes behind individual intelligence [39]; likewise, a new research stream needs to examine the underlying processes of how to make groups perform consistently well; that is, how to make groups more intelligent. Research is also needed to determine whether collective intelligence should be conceptualized and measured as an inherent factor within certain task categories. It may be that, like individual intelligence, groups perform consistently across all convergence tasks, even when using CMC, but this type of collective intelligence does not extend to all types of group tasks, such as those heavily dependent on convergence. Research should examine whether the collective intelligence factor is important only for conveyance processes, but not for convergence processes, such as conflict resolution.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

707

Second, an interesting avenue for future research would be to explore whether collective intelligence has multiple subfactors associated with it. Some research on individual intelligence suggests that individual IQ is more than a single factor [54]. Using the standard procedures for measuring an intelligence factor as described by Woolley et al. [80] and Engel et al. [31], we are not able to conclude that there was a single collective intelligence factor in our groups. When we attempted to measure collective intelligence, we found two dominant factors, leading us to the idea that in settings where collective intelligence emerges (unlike in our study), it could exhibit different subfactors. However, our robustness checks with different variations of tasks found neither one nor two factors. Further, the component matrix of our factor analysis indicated that for each of the two factors, some task performance scores loaded negatively. Thus, neither of the two factors reflected collective intelligence as we define it—otherwise, all task performance scores would have loaded positively on at least one of the factors. Thus, we can conclude that no collective intelligence factor emerged in our data, whether as a single factor or as one of multiple factors. However, more research is needed to better understand what a multiple-factor collective intelligence would mean (e.g., if different factors could potentially exist for different tasks or settings) and how it could be measured, or alternatively, whether a factor analysis with multiple dominant factors could be interpreted as collective intelligence alongside another factor. Third, a key avenue for future research is the development of technology and process interventions to improve collective intelligence and, ultimately, group performance. Our study, which used commercial tools, found no collective intelligence, yet the work by Engel et al. [31] did. We need more design research to understand how the “test battery” software they used led to collective intelligence whereas the commercial software we used did not. Fourth, more research is needed to better understand the lack of a relationship between individual group members’ intelligence and group performance. While previous research has produced mixed findings on this relationship [28], our study suggests that group member intelligence may be relevant only for tasks involving a sum of individual performance, such as brainstorming tasks; intelligence is not as important for group tasks that require converging on a consensus. What is it about the need to reach consensus that weakens the relationship between individual intelligence and group performance even though the relationship between individual intelligence and individual performance on the same tasks is strong? Could it be that “intelligent people are full of doubts, while the stupid ones are full of confidence” [13], so that the effects of intelligence are drowned out in convergence processes? Fifth, more research is needed on the role of social sensitivity in group performance. Although it may be the key factor related to across-task performance of groups working face-to-face, we found it to have varying effects, depending on the task, on groups working virtually. It makes sense that a factor that is dependent on visual cues would have little effect when using CMC lacking visual cues. Nonetheless, we need more research to understand its effects on groups working with other forms of CMC that offer audio or visual cues. Research is needed to

708

BARLOW AND DENNIS

develop measures of social sensitivity for groups using nonvisual CMC. The ability to interpret social cues via text may be a distinct ability from that of interpreting visual nonverbal cues.

Implications for Practice
First, the findings of our study suggest that managers may not necessarily be able to rely on virtual groups that have previously done well on one initial task, to perform well on a new type of task (at least when considering short-term teams with little experience together). There may be some consistency within a type of task (creativity, decision making, negotiation), but groups that are highly creative are not necessarily also good at making decisions or negotiating, at least when using CMC. Second, individual intelligence was not related to virtual group performance. Although there is an intuitive appeal to selecting highly intelligent group members in the hopes of getting better performance, our research suggests that this approach is no more likely to lead to good group performance than selecting group members based on other characteristics. Group member intelligence may or may not be necessary for good group performance, but it is not sufficient, except for tasks that do not require consensus such as brainstorming. As a result, we caution managers that when putting together a group of individuals to complete a task other than brainstorming, simply choosing the brightest individuals will not necessarily result in the best group performance. Group performance depends more on the requirements of the task and the nature of the ICT used (e.g., face-to-face, CMC) more than on the traits of group members. Third, our research suggests that groups whose members are high in social sensitivity do not perform well when they use text-based CMC that lacks visual cues. It may be that the lack of visual social cues impairs the ability of these individuals to work. We suggest that groups of this type when performing a decision-making task should elect to use a CMC tool that supports visual cues, such as video conferencing, rather than a text-based CMC such as e-mail or instant messaging (IM).

Conclusion
Our research suggests that collective intelligence is not inherent to all groups. While previous research indicated that groups working face-to-face and some groups using CMC have a certain level of collective intelligence in that they perform consistently across tasks, group performance across broad tasks is more complex for groups using CMC that lacks visual cues. Additional research is needed to more fully understand collective intelligence and the circumstances under which it emerges. Our findings suggest that managers should be cautious when creating groups—groups that perform well on certain tasks will not necessarily be the best-performing groups on other tasks, and the best groups are not necessarily those made up of group members with the highest intelligence.

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

709

NOTE
1. As a comparison, the average age in the Woolley et al. [80] studies was in the twenties. Woolley et al.’s Study 1 had 120 participants, whose average age was 32. Woolley et al.’s Study 2 had 579 participants; the average age of the Boston participants was 29 and the average age of the Pittsburgh participants was 23. Woolley et al. do not list how many participants were in Boston and how many were in Pittsburgh. Engel et al. did not report age of participants in their studies. For comparison of gender distribution, see Table 3.

REFERENCES
1. Abbasi, A., and Chen, H. Cybergate: A design framework and system for text analysis of computer-mediated communication. MIS Quarterly, 32, 4 (2008), 811–837. 2. Adams, R.B., and Funk, P. Beyond the glass ceiling: Does gender matter? Management Science, 58, 2 (2012), 219–235. 3. Adrianson, L. Gender and computer-mediated communication: Group processes in problem solving. Computers in Human Behavior, 7, 1 (2001), 71–94. 4. American Management Association. Developing successful global leaders. 2012. www. trainingindustry.com/media/13267033/ama_developing_global_leaders.pdf. 5. Apperly, I.A. What is “theory of mind?” Concepts, cognitive processes and individual differences. Quarterly Journal of Experimental Psychology, 65, 5 (2012), 825–839. 6. Baron-Cohen, S.; Wheelwright, S.; Hill, J.; Raste, Y.; and Plumb, I. The “Reading the Mind in the Eyes” test revised version: A study with normal adults, and adults with Asperger Syndrome or high-functioning autism. Journal of Child Psychology and Psychiatry, 42, 2 (2001), 241–251. 7. Bell, B.S., and Kozlowski, S.W.J. A typology of virtual teams: Implications for effective leadership. Group and Organization Management, 27, 1 (2002), 14–49. 8. Bender, L.; Walia, G.; Kambhampaty, K.; Nygard, K.E.; and Nygard, T.E. Social sensitivity and classroom team projects: An empirical investigation. SIGCSE 2012 Conference, Raleigh, NC, 2012. 9. Bhappu, A.D.; Griffith, T.L.; and Northcraft, G.B. Media effects and communication bias in diverse groups. Organizational Behavior and Human Decision Processes, 70, 3 (1997), 199–205. 10. Billings, M., and Watts, L. A safe space to vent: Conciliation and conflict in distributed teams. Proceedings of the 2007 Tenth European Conference on Computer-Supported Cooperative Work. Limerick, Ireland: IEEE, 2007, pp. 139–158. 11. Blickle, G.; Kramer, J.; and Mierke, J. Telephone-administered intelligence testing for research in work and organizational psychology: A comparative assessment study. European Journal of Psychological Assessment, 26, 3 (2010), 154–161. 12. Borge, M.; Ganoe, C.H.; Shih, S.-I.; and Carroll, J.M. Patterns of team processes and breakdowns in information analysis tasks. Fifteenth ACM Conference on Computer Supported Cooperative Work. Seattle, WA: ACM, 2012, pp. 1105–1114. 13. Bukowski, C. Charles Bukowski Quotes. 2016. http://www.goodreads.com/quotes/ 428040-the-problem-with-the-world-is-that-the-intelligent-people. 14. Carte, T.A., and Chidambaram, L. A capabilities-based theory of technology deployment in diverse teams: Leapfrogging the pitfalls of diversity and leveraging its potential with collaborative technology. Journal of the AIS, 5, 11 (2004), 448–471. 15. Chabris, C.F. Cognitive and neurobiological mechanisms of the law of general intelligence. In M.J. Roberts (ed.), Integrating the Mind: Domain General Versus Domain Specific Processes in Higher Cognition. Hove, UK: Psychology Press, 2007, pp. 449–491. 16. Compeau, D.; Marcolin, B.; Kelley, H.; and Higgins, C. Generalizability of information systems research using student subjects: A reflection on our practices and recommendations for future research. Information Systems Research, 23, 4 (2012), 1093–1109. 17. Cramton, C.D. The mutual knowledge problem and its consequences for dispersed collaboration. Organization Science, 12, 3 (2001), 346–371.

710

BARLOW AND DENNIS

18. Cummings, J.N., and Haas, M.R. So many teams, so little time: Time allocation matters in geographically dispersed teams. Journal of Organizational Behavior, 33 (2012), 316–341. 19. Day, E.A.; Arthur, W.; Miyashiro, B.; Edwards, B.D.; Tubre, T.C.; and Tubre, A.H. Criterion-related validity of statistical operationalizations of group general cognitive ability as a function. Journal of Applied Social Psychology, 34, 7 (2004), 1521–1549. 20. De Dreu, C.K.W.; Nijstad, B.A.; and van Knippenberg, D. Motivated information processing in group judgment and decision making. Personality and Social Psychology Review, 12, 1 (2008), 22–49. 21. Dean, D.L.; Hender, J.M.; Rodgers, T.L.; and Santanen, E.L. Identifying quality, novel, and creative ideas: Constructs and scales for idea evaluation. Journal of the Association for Information Systems, 7, 10 (2006), 646–698. 22. Deary, I.J. Looking Down on Human Intelligence: From Psychometrics to the Brain. New York: Oxford University Press, 2000. 23. Deary, I.J. Intelligence. Annual Review of Psychology, 63, 1 (2012), 453–482. 24. DeLuca, D., and Valacich, J.S. Virtual teams in and out of synchronicity. Information Technology and People, 19, 4 (2006), 323–344. 25. Dennis, A.R. Information exchange and use in group decision making: You can lead a group to information, but you can’t make it think. MIS Quarterly, 20, 4 (1996), 433–457. 26. Dennis, A.R.; Fuller, R.M.; and Valacich, J.S. Media, tasks, and communication processes: A theory of media synchronicity. MIS Quarterly, 32, 3 (2008), 575–600. 27. Dennis, A.R., and Valacich, J.S. A replication manifesto. AIS Transactions on Replication Research, 1, 1 (2014), 1–5. 28. Devine, D.J., and Philips, J.L. Do smarter teams do better: A meta-analysis of cognitive ability and team performance. Small Group Research, 44, 3 (2001), 507–532. 29. Dodrill, C.B., and Warner, M.H. Further studies of the Wonderlic Personnel Test as a brief measure of intelligence. Journal of Consulting and Clinical Psychology, 56, 1 (1988), 145–147. 30. Engel, D.; Woolley, A.W.; Chabris, C.F.; Takahashi, M.; Nemoto, K.; Kaiser, C.; Kim, Y.J.; and Malone, T.W. Collective intelligence in computer-mediated collaboration emerges in different contexts and cultures. Thirty-Third Annual ACM Conference on Human Factors in Computing Systems (CHI ‘15). Seoul, Korea, 2015, pp. 3769–3778. 31. Engel, D.; Woolley, A.W.; Jing, L.X.; Chabris, C.F.; and Malone, T.W. Reading the mind in the eyes or reading between the lines? Theory of mind predicts collective intelligence equally well online and face-to-face. PLOS One, 9, 12 (2014), 1–16. 32. Fjermestad, J., and Hiltz, S.R. An analysis of the effects of mode of communication on group decision making. Thirty-First Hawaii International Conference on System Sciences. Kohala Coast, Hawaii, 1998, pp. 17–26. 33. Fjermestad, J., and Hiltz, S.R. An assessment of group support systems experimental research: Methodology and results. Journal of Management Information Systems, 15, 3 (1998–99), 7–149. 34. Fjermestad, J., and Hiltz, S.R. Group support systems: A descriptive evaluation of case and field studies. Journal of Management Information Systems, 17, 3 (2000–2001), 115–161. 35. Fuller, R.M., and Dennis, A.R. Does fit matter? The impact of task-technology fit and appropriation on team performance in repeated tasks. Information Systems Research, 20, 1 (2009), 2–27. 36. Giambatista, R.C., and Bhappu, A.D. Diversity’s harvest: Interactions of diversity sources and communication technology on creative group performance. Organizational Behavior and Human Decision Processes, 111, 2 (2010), 116–126. 37. Gibson, C.B. Do they do what they believe they can? Group efficacy and group effectiveness across tasks and cultures. Academy of Management Journal, 42, 2 (1999), 138–152. 38. Gilson, L.L.; Maynard, M.T.; Young, N.C.J.; Vartiainen, M.; and Hakonen, M. Virtual teams research: 10 years, 10 themes, and 10 opportunities. Journal of Management, 41, 5 (2015), 1313–1337. 39. Gottfredson, L.J. Mainstream science on intelligence. Intelligence, 24 (1997), 13–23. 40. Grudin, J., and Poltrock, S. CSCW: Computer supported cooperative work. In R.F. Soegaard, (ed.), Encyclopedia of Human–Computer Interaction. Aarhus, Denmark: The Interaction-Design.org Foundation, 2012. https://www.interaction-design.org/literature/book/ the-encyclopedia-of-human-computer-interaction-2nd-ed

COLLECTIVE INTELLIGENCE IN VIRTUAL GROUPS

711

41. Guinan, P.J.; Cooprider, J.G.; and Faraj, S. Enabling software development team performance during requirements definition: A behavioral versus technical approach. Information Systems Research, 9, 2 (1998), 101–125. 42. Harrison, D.A., and Klein, K.J. What’s the difference? Diversity constructs as separation, variety, or disparity in organizations. Academy of Management Review, 32, 4 (2007), 1199–1228. 43. Hauser, R.M. Causes and consequences of cognitive functioning across the life course. Educational Researcher, 9 (2010), 95–109. 44. Hertel, G.; Geister, S.; and Konradt, O. Managing virtual teams: A review of current empirical research. Human Resource Management Review, 15, 1 (2005), 69–95. 45. Hiltz, S.R.; Johnson, K.; and Turoff, M. Experiments in group decision making: communication processes and outcome in face-to-face versus computerized conferences. Human Communication Research, 13, 2 (1986), 225–252. 46. Hollan, J.; Hutchins, E.; and Kirsh, D. Distributed cognition: Toward a new foundation for human–computer interaction research. ACM Transactions on Computer-Human Interaction (TOCHI), 7, 2 (2000), 174–196. 47. Homan, A.C.; Hollenbeck, J.R.; Humphrey, S.E.; Van Knippenberg, D.; Ilgen, D.R.; and Van Kleef, G.A. Facing differences with an open mind: Openness to experience, salience of intragroup differences, and performance of diverse work groups. Academy of Management Journal, 51, 6 (2008), 1204–1222. 48. Jung, J.H.; Schneider, C.; and Valacich, J. Enhancing the motivational affordance of information systems: The effects of real-time performance feedback and goal setting in group collaboration environments. Management Science, 56, 4 (2010), 724–742. 49. Kankanhalli, A.; Tan, B.C.Y.; and Wei, K.K. Conflict and performance in global virtual teams. Journal of Management Information Systems, 23, 3 (2007), 237–274. 50. Kennedy, D.M.; Vozdolska, R.R.; and McComb, S.A. Team decision making in computer-supported cooperative work: How initial computer-mediated or face-to-face meetings set the stage for later outcomes. Decision Sciences, 41, 4 (2010), 933–954. 51. Kirkman, B.L.; Gibson, C.B.; and Kim, K. Across borders and technologies: Advancements in virtual team research. In S.W.J. Kozlowski (ed.), The Oxford Handbook of Organizational Psychology. New York: Oxford University Press, 2012, pp. 789–858. 52. Kress, G.L., and Schar, M. Teamology: The art and science of design team formation. In H. Plattner et al., (eds.), Design Thinking Research. Berlin: Springer-Verlag, 2012, pp. 189–209. 53. Lewis, K., and Herndon, B. Transactive memory systems: Current issues and future research directions. Organization Science, 22, 5 (2011), 1254–1265. 54. MacCann, C.; Joseph, D.L.; Newman, D.A.; and Roberts, R.D. Emotional intelligence is a second-stratum factor of intelligence: Evidence from hierarchical and bifactor models. Emotion, 14, 2 (2014), 358–374. 55. Majchrzak, A.; Malhotra, A.; and John, R. Perceived individual collaboration knowhow development through information technology-enabled contextualization: Evidence from distributed teams. Information Systems Research, 16, 1 (2005), 9–27. 56. Maruping, L.M., and Agarwal, R. Managing team interpersonal processes through technology: A task-technology fit perspective. Journal of Applied Psychology, 89, 6 (2004), 975–990. 57. Maynard, M.T.; Mathieu, J.E.; Rapp, T.L.; and Gilson, L.L. Something(s) old and something(s) new: Modeling drivers of global virtual team effectiveness. Journal of Organizational Behavior, 33, 3 (2012), 342–365. 58. McGrath, J.E. Groups: Interaction and Performance. Englewood Cliffs, NJ: Prentice Hall, 1984. 59. McGrath, J.E. Time, interaction, and performance (TIP): A theory of groups. Small Group Research, 22, 2 (1991), 147–174. 60. Mennecke, B.E., and Wheeler, B.C. ISWorld Task Repository Index. 2012. https://scholar works.iu.edu/dspace/bitstream/handle/2022/14355/ISWorld%20Task%20Repository.pdf. 61. Minas, R.K.; Potter, R.F.; Dennis, A.R.; Bartelt, V.; and Bae, S. Putting on the thinking cap: Using NeuroIS to understand information processing biases in virtual teams. Journal of Management Information Systems, 30, 4 (2014), 49–82.

712

BARLOW AND DENNIS

62. Murthy, U.S., and Kerr, D.S. Decision making performance of interacting groups: An experimental investigation of the effects of task type and communication mode. Information and Management, 40, 5 (2003), 351–360. 63. Ocker, R.J., and Fjermestad, J. Communication differences in virtual design teams: Findings from a multi-method analysis of high and low performing experimental teams. Data Base for the Advances in Information Systems, 39, 1 (2008), 51–67. 64. Pinkham, A.E.; Penn, D.L.; Green, M.F.; Buck, B.; Healey, K.; and Harvey, P.D. The social cognition psychometric evaluation study: Results of the expert survey and RAND panel. Schizophrenia Bulletin, 40, 4 (2013), 813–823. 65. Powell, A.; Piccoli, G.; and Ives, B. Virtual teams: A review of current literature and directions for future research. Data Base for Advances in Information Systems, 35, 1 (2004), 6–36. 66. Reinig, B.A., and Mejias, R.J. On the measurement of participation equality. International Journal of E-Collaboration, 10, 4 (2014), 32–48. 67. Rico, R.; Sanchez-Manzanares, M.; Gil, F.; and Gibson, C.B. Team implicit coordination processes: A team knowledge-based approach. Academy of Management Review, 33, 1 (2008), 163–184. 68. Simon, S.J.; Grover, V.; Teng, J.T.C.; and Whitcomb, K. The relationship of information system training methods and cognitive ability to end-user satisfaction, comprehension, and skill transfer: A longitudinal field study. Information Systems Research, 7, 4 (1996), 466–490. 69. Staples, D.S.; and Zhao, L.; and Adrianson, L. The effects of cultural diversity in virtual teams versus face-to-face teams. Group Decision and Negotiation, 15, 4 (2006), 389–406. 70. Stasser, G. Information salience and the discovery of hidden profiles by decisionmaking groups: A thought experiment. Organizational Behavior and Human Decision Processes, 52, 1 (1992), 156–181. 71. Steiner, I.D. Group Process and Productivity. New York: Academic Press, 1972. 72. Thatcher, A., and De La Cour, A. Small group decision-making in face-to-face and computer-mediated environments: The role of personality. Behaviour and Information Technology, 22, 3 (2003), 203–218. 73. Valacich, J.S.; Jung, J.H.; and Looney, C.A. The effects of individual cognitive ability and idea stimulation on idea-generation performance. Group Dynamics: Theory, Research, and Practice, 10, 1 (2006), 1–15. 74. Walther, J.B. Computer-mediated communication: Impersonal, interpersonal, and hyperpersonal interaction. Communication Research, 23, 1 (1996), 3–43. 75. Walther, J.B., and Tidwell, L.C. Nonverbal cues in computer-mediated communication, and the effects of chronemics on relational communication. Journal of Organizational Computing, 5, 4 (1995), 355–378. 76. Weick, K.E.; Sutcliffe, K.M.; and Obstfeld, D. Organizing for high reliability: Processes of collective mindfulness. In R.S. Sutton and B.M. Staw (eds.), Research in Organizational Behavior. Stanford, CA: JAI Press, 1999, pp. 81–123. 77. Wilson, J.; Crisp, C.B.; and Mortensen, M. Extending construal-level theory to distributed groups: Understanding the effects of virtuality. Organization Science, 24, 2 (2013), 629–644. 78. Wittenbaum, G.M. Putting communication into the study of group memory. Human Communication Research, 29 (2003), 616–623. 79. Woodley, M.A., and Bell, E. Is collective intelligence (mostly) the general factor of personality? A comment on Woolley, Chabris, Pentland, Hashmi and Malone. Intelligence, 29, 2–3 (2010), 79–81. 80. Woolley, A.W.; Chabris, C.F.; Pentland, A.; Hashmi, N.; and Malone, T.W. Evidence for a collective intelligence factor in the performance of human groups. Science, 330 (2010), 686–688. 81. Zhang, D.; Lowry, P.B.; Zhou, L.; and Fu, X.; Staples, D. S.; and Zhao, L. The impact of individualism-collectivism, social presence, and group diversity on group decision making under majority influence. Journal of Management Information Systems, 23, 4 (2007), 53–80. 82. Zigurs, I.; Poole, M.S.; and DeSanctis, G.L. A study of influence in computer-mediated group decision making. MIS Quarterly, 12, 4 (1988), 625–644.

