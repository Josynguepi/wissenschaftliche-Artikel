{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Implementierung BA 2019 gut",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josynguepi/wissenschaftliche-Artikel/blob/master/Implementierung_BA_2019_gut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Bsdh1kS03zET",
        "colab_type": "code",
        "outputId": "5d92d4b7-4850-4df3-904b-691dd2906450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Collecting numpy>=1.15.0 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.13.0,>=6.12.1->spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
            "Successfully built wrapt\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, wrapt\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: wrapt 1.11.1\n",
            "    Uninstalling wrapt-1.11.1:\n",
            "      Successfully uninstalled wrapt-1.11.1\n",
            "Successfully installed numpy-1.16.1 wrapt-1.10.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RreipcN0ASQT",
        "colab_type": "code",
        "outputId": "e796b3df-02b5-4420-d66f-93687f3f5cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBENLS_e96zP",
        "colab_type": "code",
        "outputId": "eb3aa261-f904-43d4-c39a-b6961de3693e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "cell_type": "code",
      "source": [
        "spacy.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    \u001b[93mInfo about spaCy\u001b[0m\n",
            "\n",
            "    spaCy version      2.0.18         \n",
            "    Location           /usr/local/lib/python3.6/dist-packages/spacy\n",
            "    Platform           Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "    Python version     3.6.7          \n",
            "    Models             en             \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Location': '/usr/local/lib/python3.6/dist-packages/spacy',\n",
              " 'Models': 'en',\n",
              " 'Platform': 'Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic',\n",
              " 'Python version': '3.6.7',\n",
              " 'spaCy version': '2.0.18'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "TjjcoCfk-iAv",
        "colab_type": "code",
        "outputId": "a50ca392-a2cf-4026-cbde-6514efcfc532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Josynguepi/wissenschaftliche-Artikel.git\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wissenschaftliche-Artikel'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Total 102 (delta 0), reused 0 (delta 0), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (102/102), 2.47 MiB | 12.51 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S7RFsRKSXpte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2de6c043-a4be-4310-ed1d-83f9c5994513"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  wissenschaftliche-Artikel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-lHXsxBhTZif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnJYhuy2Trew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc=nlp(open('AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt').read())\n",
        "doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Lhsj1Uatlfq",
        "colab_type": "code",
        "outputId": "7b026364-e7c0-4f93-bf05-96e844ef715d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 41
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a74bdcd-6e84-4c75-b957-8f18e8143553\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2a74bdcd-6e84-4c75-b957-8f18e8143553\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oxWL_u6nsJrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5335
        },
        "outputId": "a97bcad1-f1e5-4d16-8dbf-f01b1b4eccc5"
      },
      "cell_type": "code",
      "source": [
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User uploaded file \"_2016_Editorial Board_Information Systems Research_1.txt\" with length 4539 bytes\n",
            "User uploaded file \"_2016_Editorial Board_Information Systems Research_2.txt\" with length 4537 bytes\n",
            "User uploaded file \"_2016_Editorial Board_Information Systems Research_4.txt\" with length 4431 bytes\n",
            "User uploaded file \"_2016_Editorial_The Journal of Strategic Information Systems_3.txt\" with length 8961 bytes\n",
            "User uploaded file \"_2016_Research Spotlights_Information Systems Research_1.txt\" with length 18813 bytes\n",
            "User uploaded file \"_2016_Research Spotlights_Information Systems Research_2.txt\" with length 22066 bytes\n",
            "User uploaded file \"_2016_Research Spotlights_Information Systems Research_3.txt\" with length 17865 bytes\n",
            "User uploaded file \"_2016_Research Spotlights_Information Systems Research_4.txt\" with length 23405 bytes\n",
            "User uploaded file \"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\" with length 135239 bytes\n",
            "User uploaded file \"AgarwalA#MukhopadhyayT_2016_The Impact of Competing Ads on Click Performance in Sponsored Search_Information Systems Research_3.txt\" with length 107301 bytes\n",
            "User uploaded file \"AgarwalR_2016_Editorial Notes_Information Systems Research_4.txt\" with length 16969 bytes\n",
            "User uploaded file \"AgarwalR_2016_Editorial—On the Intellectual Structure and Evolution of ISR_Information Systems Research_3.txt\" with length 28181 bytes\n",
            "User uploaded file \"AlthuizenN#ReichelA_2016_The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving - A Dual Pathway to Creativity_Journal of Management Information Systems_1.txt\" with length 88986 bytes\n",
            "User uploaded file \"AndersonB#VanceA#KirwanC#EargleD#JenkinsJ_2016_How users perceive and respond to security messages - a NeuroIS research agenda and empirical study_European Journal of Information Systems_4.txt\" with length 144419 bytes\n",
            "User uploaded file \"AndersonB#VanceA#KirwanC#JenkinsJ#EargleD_2016_From Warning to Wallpaper - Why the Brain Habituates to Security Warnings and What Can Be Done About It_Journal of Management Information Systems_3.txt\" with length 91372 bytes\n",
            "User uploaded file \"ArazyO#DaxenbergerJ#Lifshitz-AssafH#NovO#GurevychI_2016_Turbulent Stability of Emergent Roles - The Dualistic Nature of Self-Organizing Knowledge Coproduction_Information Systems Research_4.txt\" with length 122444 bytes\n",
            "User uploaded file \"ArgyrisY#RansbothamS_2016_Knowledge entrepreneurship - institutionalising wiki-based knowledge-management processes in competitive and hierarchical organisations_Journal of Information Technology_2.txt\" with length 80349 bytes\n",
            "User uploaded file \"AtasoyH#BankerR#PavlouP_2016_On the Longitudinal Effects of IT Use on Firm-Level Employment_Information Systems Research_1.txt\" with length 117414 bytes\n",
            "User uploaded file \"AUBERT~1.TXT\" with length 21011 bytes\n",
            "User uploaded file \"AUDZEY~1.TXT\" with length 106503 bytes\n",
            "User uploaded file \"AvgerouC#HayesN#RovereR_2016_Growth in ICT uptake in developing countries - new users, new uses, new challenges_Journal of Information Technology_4.txt\" with length 28218 bytes\n",
            "User uploaded file \"BaesensB#BapnaR#MarsdenJ#VanthienenJ#ZhaoJ_2016_Transformational Issues of Big Data and Analytics in Networked Business_MIS Quarterly_4.txt\" with length 61655 bytes\n",
            "User uploaded file \"BairdA#MillerC#RaghuT#SinhaR_2016_Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives_Information Systems Research_2.txt\" with length 106123 bytes\n",
            "User uploaded file \"BapnaR#GuptaA#RayG#SinghS_2016_IT Outsourcing and the Impact of Advisors on Clients and Vendors_Information Systems Research_3.txt\" with length 64075 bytes\n",
            "User uploaded file \"BarlowJ#DennisA_2016_Not As Smart As We Think - A Study of Collective Intelligence in Virtual Groups_Journal of Management Information Systems_3.txt\" with length 92140 bytes\n",
            "User uploaded file \"BarrettM#ObornE#OrlikowskiW_2016_Creating Value in Online Communities - The Sociomaterial Configuring of Strategy, Platform, and Stakeholder Engagement_Information Systems Research_4.txt\" with length 111245 bytes\n",
            "User uploaded file \"BattlesonD#WestB#KimJ#RameshB#RobinsonP_2016_Achieving dynamic capabilities with cloud computing - an empirical investigation_European Journal of Information Systems_3.txt\" with length 133330 bytes\n",
            "User uploaded file \"BauerJ#FrankeN#TuertscherP_2016_Intellectual Property Norms in Online Communities - How User-Organized Intellectual Property Regulation Supports Innovation_Information Systems Research_4.txt\" with length 146180 bytes\n",
            "User uploaded file \"BenarochM#LichtensteinY#FinkL_2016_Contract Design Choices and the Balance of Ex Ante and Ex Post Transaction Costs in Software Development Outsourcing1_MIS Quarterly_1.txt\" with length 126272 bytes\n",
            "User uploaded file \"BenjaminV#ZhangB#JrJ#ChenH_2016_Examining Hacker Participation Length in Cybercriminal Internet-Relay-Chat Communities_Journal of Management Information Systems_2.txt\" with length 74847 bytes\n",
            "User uploaded file \"BenlianA#HaffkeI_2016_Does mutuality matter - Examining the bilateral nature and effects of CEO–CIO mutual understanding_The Journal of Strategic Information Systems_2.txt\" with length 113954 bytes\n",
            "User uploaded file \"BenthausJ#RisiusM#BeckR_2016_Social media management strategies for organizational impression management and their effect on public perception_The Journal of Strategic Information Systems_2.txt\" with length 75804 bytes\n",
            "User uploaded file \"BlohmI#RiedlC#FüllerJ#LeimeisterJ_2016_Rate or Trade - Identifying Winning Ideas in Open Idea Sourcing_Information Systems Research_1.txt\" with length 123026 bytes\n",
            "User uploaded file \"BreukerD#MatznerM#DelfmannP#BeckerJ_2016_Comprehensible Predictive Models for Business Processes_MIS Quarterly_4.txt\" with length 131783 bytes\n",
            "User uploaded file \"BrownS#FullerR#ThatcherS_2016_Impression Formation and Durability in Mediated Communication_Journal of the Association for Information Systems_9.txt\" with length 126805 bytes\n",
            "User uploaded file \"BrownS#MasseyA#WardK_2016_Handle mergers and acquisitions with care - the fragility of trust between the IT-service provider and end-users_European Journal of Information Systems_2.txt\" with length 91766 bytes\n",
            "User uploaded file \"BrynjolfssonE#GevaT#ReichmanS_2016_Crowd-Squared - Amplifying the Predictive Power of Search Trend Data_MIS Quarterly_4.txt\" with length 103225 bytes\n",
            "User uploaded file \"BurgessS_2016_Representing small business web presence content - the web presence pyramid model_European Journal of Information Systems_2.txt\" with length 89983 bytes\n",
            "User uploaded file \"BurtchG#GhoseA#WattalS_2016_Secret Admirers - An Empirical Examination of Information Hiding and Contribution Dynamics in Online Crowdfunding_Information Systems Research_3.txt\" with length 104565 bytes\n",
            "User uploaded file \"BygstadB#MunkvoldB#VolkoffO_2016_Identifying generative mechanisms through affordances - a framework for critical realist data analysis_Journal of Information Technology_1.txt\" with length 80266 bytes\n",
            "User uploaded file \"CameronA#WebsterJ#BarkiH#GuineaA_2016_Four common multicommunicating misconceptions_European Journal of Information Systems_5.txt\" with length 35333 bytes\n",
            "User uploaded file \"CavusogluH#PhanT#CavusogluH#AiroldiE_2016_Assessing the Impact of Granular Privacy Controls on Content Sharing and Disclosure on Facebook_Information Systems Research_4.txt\" with length 152858 bytes\n",
            "User uploaded file \"ChanJ#GhoseA#SeamansR_2016_The Internet and Racial Hate Crime - Offline Spillovers from Online Access_MIS Quarterly_2.txt\" with length 106234 bytes\n",
            "User uploaded file \"ChenD#HortonJ_2016_Are Online Labor Markets Spot Markets for Tasks - A Field Experiment on the Behavioral Response to Wage Cuts_Information Systems Research_2.txt\" with length 97234 bytes\n",
            "User uploaded file \"ChengH#LiZ#NaranjoA_2016_Cloud Computing Spot Pricing Dynamics - Latency and Limits to Arbitrage_Information Systems Research_1.txt\" with length 102173 bytes\n",
            "User uploaded file \"ChengX#FuS#DruckenmillerD_2016_Trust Development in Globally Distributed Collaboration - A Case of U.S. and Chinese Mixed Teams_Journal of Management Information Systems_4.txt\" with length 87876 bytes\n",
            "User uploaded file \"ChengZ#DimokaA#PavlouP_2016_Context may be King, but generalizability is the Emperor!_Journal of Information Technology_3.txt\" with length 51835 bytes\n",
            "User uploaded file \"ChenJ#FanM#LiM_2016_Advertising Versus Brokerage Model for Online Trading Platforms_MIS Quarterly_3.txt\" with length 120239 bytes\n",
            "User uploaded file \"ChenY#HuangK_2016_Pricing Data Services - Pricing by Minutes, by Gigs, or by Megabytes per Second_Information Systems Research_3.txt\" with length 113568 bytes\n",
            "User uploaded file \"ChenY#ZahediF_2016_Individuals’ Internet Security Perceptions and Behaviors - Polycontextual Contrasts Between the United States and China_MIS Quarterly_1.txt\" with length 115059 bytes\n",
            "User uploaded file \"ChoiB#KimS#JiangZ_2016_Influence of Firm’s Recovery Endeavors upon Privacy Breach on Online Customer Behavior_Journal of Management Information Systems_3.txt\" with length 95323 bytes\n",
            "User uploaded file \"ChoS#QiuL#BandyopadhyayS_2016_Should Online Content Providers Be Allowed To Subsidize Content -—An Economic Analysis_Information Systems Research_3.txt\" with length 79872 bytes\n",
            "User uploaded file \"ChoudrieJ#ZamaniE_2016_Understanding individual user resistance and workarounds of enterprise social networks - the case of Service Ltd_Journal of Information Technology_2.txt\" with length 124100 bytes\n",
            "User uploaded file \"ClarkeR#Burton-JonesA#WeberR_2016_On the Ontological Quality and Logical Quality of Conceptual-Modeling Grammars - The Need for a Dual Perspective_Information Systems Research_2.txt\" with length 94008 bytes\n",
            "User uploaded file \"ClarkeR_2016_Big data, big risks_Information Systems Journal_1.txt\" with length 50906 bytes\n",
            "User uploaded file \"ClemonsE#DewanR#KauffmanR#WeberT_2016_Special Section - When Machine Meets Society - Social Impacts of Information and Information Economics_Journal of Management Information Systems_2.txt\" with length 12654 bytes\n",
            "User uploaded file \"ClemonsE#WilsonJ#MattC#HessT#RenF#JinF#KohN_2016_Global Differences in Online Shopping Behavior - Understanding Factors Leading to Trust_Journal of Management Information Systems_4.txt\" with length 97544 bytes\n",
            "User uploaded file \"CraigA_2016_Theorising about gender and computing interventions through an evaluation framework_Information Systems Journal_6.txt\" with length 80181 bytes\n",
            "User uploaded file \"CramW#BrohmanK#GallupeR_2016_Information Systems Control - A Review and Framework for Emerging Information Systems Processes_Journal of the Association for Information Systems_4.txt\" with length 169931 bytes\n",
            "User uploaded file \"CramW#BrohmanM#GallupeR_2016_Hitting a moving target - a process model of information systems control change_Information Systems Journal_3.txt\" with length 109075 bytes\n",
            "User uploaded file \"CramW#NewellS_2016_Mindful revolution or mindless trend - Examining agile development as a management fashion_European Journal of Information Systems_2.txt\" with length 92515 bytes\n",
            "User uploaded file \"CrowstonK_2016_Response to Ideational Influence, Connectedness, and Venue Representation - Making an Assessment of Scholarly Capital_Journal of the Association for Information Systems_1.txt\" with length 12066 bytes\n",
            "User uploaded file \"CUELLA~2.TXT\" with length 105039 bytes\n",
            "User uploaded file \"CuellarM#VidgenR#Hirotoshi Takeda#TruexD_2016_Rejoinder to the Response to The Scholarly Capital Model_Journal of the Association for Information Systems_1.txt\" with length 16502 bytes\n",
            "User uploaded file \"ĆwiakowskiP#GiergicznyM#KrawczkM_2016_Pirates in the Lab - Using Incentivized Choice Experiments to Explore Preference for (un)authorized Content_MIS Quarterly_3.txt\" with length 43341 bytes\n",
            "User uploaded file \"DanielS#StewartK_2016_Open source project success - Resource access, flow, and integration_The Journal of Strategic Information Systems_3.txt\" with length 95867 bytes\n",
            "User uploaded file \"DavisonR#MartinsonsM_2016_Context is king! Considering particularism in research design and reporting_Journal of Information Technology_3.txt\" with length 61528 bytes\n",
            "User uploaded file \"DavisonR_2016_The art of storytelling_Information Systems Journal_3.txt\" with length 14110 bytes\n",
            "User uploaded file \"DawsonG#DenfordJ#DesouzaK_2016_Governing innovation in U.S. state government - An ecosystem perspective_The Journal of Strategic Information Systems_4.txt\" with length 114062 bytes\n",
            "User uploaded file \"DawsonG#DenfordJ#WilliamsC#PrestonD#DesouzaK_2016_An Examination of Effective IT Governance in the Public Sector Using the Legal View of Agency Theory_Journal of Management Information Systems_4.txt\" with length 93676 bytes\n",
            "User uploaded file \"DAWSON~3.TXT\" with length 110427 bytes\n",
            "User uploaded file \"DemirezenE#KumarS#SenA_2016_Sustainability of Healthcare Information Exchanges - A Game-Theoretic Approach_Information Systems Research_2.txt\" with length 103580 bytes\n",
            "User uploaded file \"DemirezenE#KumarS#ShettyB_2016_Managing Co-Creation in Information Technology Projects - A Differential Games Approach_Information Systems Research_3.txt\" with length 108468 bytes\n",
            "User uploaded file \"DengX#JoshiK#GalliersR_2016_The Duality of Empowerment and Marginalization in Microtask Crowdsourcing - Giving Voice to the Less Powerful Through Value Sensitive Design_MIS Quarterly_2.txt\" with length 181604 bytes\n",
            "User uploaded file \"DennisA#MinasR#LockwoodN_2016_Mapping the Corporate Blogosphere - Linking Audience, Content, and Management to Blog Visibility_Journal of the Association for Information Systems_3.txt\" with length 115136 bytes\n",
            "User uploaded file \"DeyD#LahiriA_2016_Versioning - Go Vertical in a Horizontal Market_Journal of Management Information Systems_2.txt\" with length 70545 bytes\n",
            "User uploaded file \"Díaz AndradeA#DoolinB_2016_Information and Communication Technology and the Social Inclusion of Refugees_MIS Quarterly_2.txt\" with length 56368 bytes\n",
            "User uploaded file \"DuclosV_2016_The map and the territory - an ethnographic study of the low utilisation of a global eHealth network_Journal of Information Technology_4.txt\" with length 88772 bytes\n",
            "User uploaded file \"EbelP#BretschneiderU#LeimeisterJ_2016_Leveraging virtual business model innovation - a framework for designing business model development tools_Information Systems Journal_5.txt\" with length 105340 bytes\n",
            "User uploaded file \"ECKHAR~1.TXT\" with length 106608 bytes\n",
            "User uploaded file \"Editor-in-ChiefV_2016_Editorial Introduction_Journal of Management Information Systems_4.txt\" with length 8123 bytes\n",
            "User uploaded file \"EvangelopoulosN_2016_Thematic orientation of the ISJ within a semantic space of IS research_Information Systems Journal_1.txt\" with length 22002 bytes\n",
            "User uploaded file \"FarajS#von KroghG#MonteiroE#LakhaniK_2016_Special Section Introduction—Online Community as Space for Knowledge Flows_Information Systems Research_4.txt\" with length 98428 bytes\n",
            "User uploaded file \"FayardA#GkeredakisE#LevinaN_2016_Framing Innovation Opportunities While Staying Committed to an Organizational Epistemic Stance_Information Systems Research_2.txt\" with length 132628 bytes\n",
            "User uploaded file \"FergusonJ#SoekijadM_2016_Multiple interests or unified voice - Online communities as intermediary spaces for development_Journal of Information Technology_4.txt\" with length 106100 bytes\n",
            "User uploaded file \"FernándezW_2016_Commentary on Davison and Martinsons’ ‘Context is King! Considering particularism in research design and reporting’_Journal of Information Technology_3.txt\" with length 8745 bytes\n",
            "User uploaded file \"FindikogluM#Watson-ManheimM_2016_Linking macro-level goals to micro-level routines - EHR-enabled transformation of primary care services_Journal of Information Technology_4.txt\" with length 106452 bytes\n",
            "User uploaded file \"FothM_2016_Factors influencing the intention to comply with data protection regulations in hospitals - based on gender differences in behaviour and deterrence_European Journal of Information Systems_2.txt\" with length 97278 bytes\n",
            "User uploaded file \"FridgenG#HäfnerL#KönigC#SachsT_2016_Providing Utility to Utilities - The Value of Information Systems Enabled Flexibility in Electricity Consumption_Journal of the Association for Information Systems_8.txt\" with length 107185 bytes\n",
            "User uploaded file \"GalliersB#JarvenpaaS_2016_Editorial_The Journal of Strategic Information Systems_2.txt\" with length 11743 bytes\n",
            "User uploaded file \"GalliersR#JarvenpaaS_2016_Editorial_The Journal of Strategic Information Systems_1.txt\" with length 15536 bytes\n",
            "User uploaded file \"GalliersR#JarvenpaaS_2016_Editorial_The Journal of Strategic Information Systems_4.txt\" with length 9663 bytes\n",
            "User uploaded file \"GanjuK#PavlouP#BankerR_2016_Does Information and Communication Technology Lead to the Well-Being of Nations - A Country-Level Empirical Investigation_MIS Quarterly_2.txt\" with length 95984 bytes\n",
            "User uploaded file \"GASKIN~1.TXT\" with length 87583 bytes\n",
            "User uploaded file \"GHIASS~1.TXT\" with length 77570 bytes\n",
            "User uploaded file \"GhobadiS#MathiassenL_2016_Perceived barriers to effective knowledge sharing in agile software teams_Information Systems Journal_2.txt\" with length 95089 bytes\n",
            "User uploaded file \"GholamiR#WatsonR#HasanH#MollaA#Bjørn-AndersenN_2016_Information Systems Solutions for Environmental Sustainability - How Can We Do More_Journal of the Association for Information Systems_8.txt\" with length 66865 bytes\n",
            "User uploaded file \"GhoseA#Todri-AdamopoulosV_2016_Toward a Digital Attribution Model - Measuring the Impact of Display Advertising on Online Consumer Behavior_MIS Quarterly_4.txt\" with length 129993 bytes\n",
            "User uploaded file \"GiboneyJ#BriggsR#JRJ_2016_Special Issue - Designing Tools to Answer Great Information Systems Research Questions_Journal of Management Information Systems_4.txt\" with length 11548 bytes\n",
            "User uploaded file \"GiessmannA#LegnerC_2016_Designing business models for cloud platforms_Information Systems Journal_5.txt\" with length 90114 bytes\n",
            "User uploaded file \"GLEASU~1.TXT\" with length 100467 bytes\n",
            "User uploaded file \"GoesP#GuoC#LinM_2016_Do Incentive Hierarchies Induce User Effort - Evidence from an Online Knowledge Exchange_Information Systems Research_3.txt\" with length 100723 bytes\n",
            "User uploaded file \"GómezJ#SalazarI#VargasP_2016_Firm Boundaries, Information Processing Capacity, and Performance in Manufacturing Firms_Journal of Management Information Systems_3.txt\" with length 107546 bytes\n",
            "User uploaded file \"GongF#NaultB#RahmanM_2016_An Internet-Enabled Move to the Market in Logistics_Information Systems Research_2.txt\" with length 73710 bytes\n",
            "User uploaded file \"GreensteinS#ZhuF_2016_Open Content, Linus’ Law, and Neutral Point of View_Information Systems Research_3.txt\" with length 96921 bytes\n",
            "User uploaded file \"GuoH#ChengH#KelleyK_2016_Impact of Network Structure on Malware Propagation - A Growth Curve Perspective_Journal of Management Information Systems_1.txt\" with length 83545 bytes\n",
            "User uploaded file \"HannI#KohB#NiculescuM_2016_The Double-Edged Sword of Backward Compatibility - The Adoption of Multigenerational Platforms in the Presence of Intergenerational Services_Information Systems Research_1.txt\" with length 102142 bytes\n",
            "User uploaded file \"HedmanJ#HenningssonS_2016_Developing ecological sustainability - a green IS response model_Information Systems Journal_3.txt\" with length 94490 bytes\n",
            "User uploaded file \"HedmanJ#SarkerS#VeitD_2016_Digitization in business models and entrepreneurship_Information Systems Journal_5.txt\" with length 6229 bytes\n",
            "User uploaded file \"HenningssonS#KettingerW_2016_Understanding Information Systems Integration Deficiencies in Mergers and Acquisitions - A Configurational Perspective_Journal of Management Information Systems_4.txt\" with length 87124 bytes\n",
            "User uploaded file \"Heshan Sun#Yulin Fang#Haiyun (Melody) Zou_2016_Choosing a Fit Technology - Understanding Mindfulness in Technology Adoption and Continuance_Journal of the Association for Information Systems_6.txt\" with length 127733 bytes\n",
            "User uploaded file \"HinzO#HillS#KimJ_2016_Tv’s Dirty Little Secret - The Negative Effect of Popular Tv on Online Auction Sales_MIS Quarterly_3.txt\" with length 113333 bytes\n",
            "User uploaded file \"HongY#WangC#PavlouP_2016_Comparing Open and Sealed Bid Auctions - Evidence from Online Labor Markets_Information Systems Research_1.txt\" with length 118245 bytes\n",
            "User uploaded file \"HoS#HancockJ#BoothC#LiuX_2016_Computer-Mediated Deception - Strategies Revealed by Language-Action Cues in Spontaneous Communication_Journal of Management Information Systems_2.txt\" with length 79464 bytes\n",
            "User uploaded file \"HuangP#ZhangZ_2016_Participation in Open Knowledge Communities and Job-Hopping - Evidence from Enterprise Software_MIS Quarterly_3.txt\" with length 101051 bytes\n",
            "User uploaded file \"HuP#HuH#WeiC#HsuP_2016_Examining Firms’ Green Information Technology Practices - A Hierarchical View of Key Drivers and Their Effects_Journal of Management Information Systems_4.txt\" with length 97856 bytes\n",
            "User uploaded file \"Hyun KimS#MukhopadhyayT#KrautR_2016_When Does Repository Kms Use Lift Performance - The Role of Alternative Knowledge Sources and Task Environments_MIS Quarterly_1.txt\" with length 136519 bytes\n",
            "User uploaded file \"ImI#JunJ#OhW#JeongS_2016_Deal-Seeking Versus Brand-Seeking - Search Behaviors and Purchase Propensities in Sponsored Search Platforms_MIS Quarterly_1.txt\" with length 72949 bytes\n",
            "User uploaded file \"JenkinsJ#AndersonB#VanceA#KirwanC#EargleD_2016_More Harm Than Good - How Messages That Interrupt Can Make Us Vulnerable_Information Systems Research_4.txt\" with length 88221 bytes\n",
            "User uploaded file \"JhaS#PinsonneaultA#DubéL_2016_The Evolution of an Ict Platform-Enabled Ecosystem for Poverty Alleviation - The Case of Ekutir_MIS Quarterly_2.txt\" with length 65232 bytes\n",
            "User uploaded file \"JiangZ#WangW#TanB#YuJ_2016_The Determinants and Impacts of Aesthetics in Users’ First Interaction with Websites_Journal of Management Information Systems_1.txt\" with length 94836 bytes\n",
            "User uploaded file \"JiaR#ReichB#JiaH_2016_A commentary on - “Creating agile organizations through IT - The influence of IT service climate on IT service quality and IT agility”_The Journal of Strategic Information Systems_3.txt\" with length 25866 bytes\n",
            "User uploaded file \"Jifeng Luo#Ming Fan#Han Zhang_2016_Information Technology, Cross-Channel Capabilities, and Managerial Actions - Evidence from the Apparel Industry_Journal of the Association for Information Systems_5.txt\" with length 78664 bytes\n",
            "User uploaded file \"Jingguo Wang#Yuan Li#RaoH_2016_Overconfidence in Phishing Email Detection_Journal of the Association for Information Systems_11.txt\" with length 97198 bytes\n",
            "User uploaded file \"JiY#KumarS#MookerjeeV_2016_When Being Hot Is Not Cool - Monitoring Hot Lists for Information Security_Information Systems Research_4.txt\" with length 109534 bytes\n",
            "User uploaded file \"JohnB#GohD#ChuaA#WickramasingheN_2016_Graph-based Cluster Analysis to Identify Similar Questions - A Design Science Approach_Journal of the Association for Information Systems_9.txt\" with length 83260 bytes\n",
            "User uploaded file \"JOHNSO~1.TXT\" with length 111741 bytes\n",
            "User uploaded file \"JohnstonA#WarkentinM#McBrideM#CarterL_2016_Dispositional and situational factors - influences on information security policy violations_European Journal of Information Systems_3.txt\" with length 107088 bytes\n",
            "User uploaded file \"JrJ#BurgoonJ#EditorsJ_2016_Special Issue - Information Systems for Deception Detection_Journal of Management Information Systems_2.txt\" with length 16925 bytes\n",
            "User uploaded file \"KaneG#RansbothamS_2016_Content and Collaboration - An Affiliation Network Approach to Information Quality in Online Peer Production Communities_Information Systems Research_2.txt\" with length 88674 bytes\n",
            "User uploaded file \"KaschigA#MaierR#SandowA_2016_The effects of collecting and connecting activities on knowledge creation in organizations_The Journal of Strategic Information Systems_4.txt\" with length 85798 bytes\n",
            "User uploaded file \"KatzyB#SungG#CrowstonK_2016_Alignment in an inter-organisational network - the case of ARC transistance_European Journal of Information Systems_6.txt\" with length 86384 bytes\n",
            "User uploaded file \"KetterW#PetersM#CollinsJ#GuptaA_2016_A Multiagent Competitive Gaming Platform to Address Societal Challenges_MIS Quarterly_2.txt\" with length 58449 bytes\n",
            "User uploaded file \"KetterW#PetersM#CollinsJ#GuptaA_2016_Competitive Benchmarking - An Is Research Approach to Address Wicked Problems with Big Data and Analytics_MIS Quarterly_4.txt\" with length 143791 bytes\n",
            "User uploaded file \"KhatriV#VesseyI_2016_Understanding the Role of IS and Application Domain Knowledge on Conceptual Schema Problem Solving - A Verbal Protocol Study_Journal of the Association for Information Systems_12.txt\" with length 130015 bytes\n",
            "User uploaded file \"KIMD#Y~1.TXT\" with length 107623 bytes\n",
            "User uploaded file \"KimK#GopalA#HobergG_2016_Does Product Market Competition Drive CVC Investment - Evidence from the U.S. IT Industry_Information Systems Research_2.txt\" with length 126629 bytes\n",
            "User uploaded file \"KlecunE_2016_Transforming healthcare - policy discourses of IT and patient-centred care_European Journal of Information Systems_1.txt\" with length 75217 bytes\n",
            "User uploaded file \"KohliR#TanS_2016_Electronic Health Records - How Can Is Researchers Contribute to Transforming Healthcare_MIS Quarterly_3.txt\" with length 110562 bytes\n",
            "User uploaded file \"KranzJ#HaneltA#KolbeL_2016_Understanding the influence of absorptive capacity and ambidexterity on the process of business model change – the case of on-premise and_Information Systems Journal_5.txt\" with length 120310 bytes\n",
            "User uploaded file \"KumarV#LoonamJ#AllenJ#SawyerS_2016_Exploring enterprise social systems &amp; organisational change - implementation in a digital age_Journal of Information Technology_2.txt\" with length 27597 bytes\n",
            "User uploaded file \"KwonH#SoH#HanS#OhW_2016_Excessive Dependence on Mobile Social Apps - A Rational Addiction Perspective_Information Systems Research_4.txt\" with length 116304 bytes\n",
            "User uploaded file \"LacityM#KhanS#YanA_2016_Review of the empirical business services sourcing literature - an update and future directions_Journal of Information Technology_3.txt\" with length 284814 bytes\n",
            "User uploaded file \"LACITY~2.TXT\" with length 40054 bytes\n",
            "User uploaded file \"LAEREJ~1.TXT\" with length 77911 bytes\n",
            "User uploaded file \"LaiV#LaiF#LowryP_2016_Technology Evaluation and Imitation - Do They Have Differential or Dichotomous Effects on ERP Adoption and Assimilation in China_Journal of Management Information Systems_4.txt\" with length 123541 bytes\n",
            "User uploaded file \"LangeM#MendlingJ#ReckerJ_2016_An empirical analysis of the factors and measures of Enterprise Architecture Management success_European Journal of Information Systems_5.txt\" with length 112042 bytes\n",
            "User uploaded file \"LanktonN#McKnightD#WrightR#ThatcherJ_2016_Using Expectation Disconfirmation Theory and Polynomial Modeling to Understand Trust in Technology_Information Systems Research_1.txt\" with length 94002 bytes\n",
            "User uploaded file \"LappasT#SabnisG#ValkanasG_2016_The Impact of Fake Reviews on Online Visibility - A Vulnerability Assessment of the Hotel Industry_Information Systems Research_4.txt\" with length 119996 bytes\n",
            "User uploaded file \"LarsenK#How BongC_2016_A Tool for Addressing Construct Identity in Literature Reviews and Meta-Analyses_MIS Quarterly_3.txt\" with length 163067 bytes\n",
            "User uploaded file \"LashM#ZhaoK_2016_Early Predictions of Movie Success - The Who, What, and When of Profitability_Journal of Management Information Systems_3.txt\" with length 83956 bytes\n",
            "User uploaded file \"LAUMER~2.TXT\" with length 89254 bytes\n",
            "User uploaded file \"LAUMER~1.TXT\" with length 122168 bytes\n",
            "User uploaded file \"LeeA_2016_A Commentary - Theory appropriation and the growth of knowledge_The Journal of Strategic Information Systems_1.txt\" with length 20459 bytes\n",
            "User uploaded file \"LeeC#GengX#RaghunathanS_2016_Mandatory Standards and Organizational Information Security_Information Systems Research_1.txt\" with length 87749 bytes\n",
            "User uploaded file \"LeeG#QiuL#WhinstonA_2016_A Friend Like Me - Modeling Network Formation in a Location-Based Social Network_Journal of Management Information Systems_4.txt\" with length 69548 bytes\n",
            "User uploaded file \"LeeJ_2016_Invited Commentary—Reflections on ICT-enabled Bright Society Research_Information Systems Research_1.txt\" with length 24446 bytes\n",
            "User uploaded file \"LeonardiP#BaileyD#DinizE#ShollerD#NardiB_2016_Multiplex Appropriation in Complex Systems Implementation - The Case of Brazil’s Correspondent Banking System_MIS Quarterly_2.txt\" with length 62842 bytes\n",
            "User uploaded file \"LeongC#PanS#NewellS#CuiL_2016_The Emergence of Self-Organizing E-Commerce Ecosystems in Remote Villages of China - A Tale of Digital Empowerment for Rural Development_MIS Quarterly_2.txt\" with length 75544 bytes\n",
            "User uploaded file \"LiangN#BirosD#LuseA_2016_An Empirical Validation of Malicious Insider Characteristics_Journal of Management Information Systems_2.txt\" with length 88032 bytes\n",
            "User uploaded file \"LindbergA#BerenteN#GaskinJ#LyytinenK_2016_Coordinating Interdependencies in Online Communities - A Study of an Open Source Software Project_Information Systems Research_4.txt\" with length 111412 bytes\n",
            "User uploaded file \"LiuY#LiH#GoncalvesJ#KostakosV#XiaoB_2016_Fragmentation or cohesion - Visualizing the process and consequences of information system diversity, 1993–2012_European Journal of Information Systems_6.txt\" with length 117584 bytes\n",
            "User uploaded file \"LiW#ChenH#JrJ_2016_Identifying and Profiling Key Sellers in Cyber Carding Community - AZSecure Text Mining System_Journal of Management Information Systems_4.txt\" with length 77269 bytes\n",
            "User uploaded file \"LiW#LiuK#BelitskiM#GhobadianA#O'ReganN_2016_e-Leadership through strategic alignment - an empirical study of small- and medium-sized enterprises in the digital age_Journal of Information Technology_2.txt\" with length 122811 bytes\n",
            "User uploaded file \"LiX_2016_Could Deal Promotion Improve Merchants’ Online Reputations - The Moderating Role of Prior Reviews_Journal of Management Information Systems_1.txt\" with length 83919 bytes\n",
            "User uploaded file \"LoebbeckeC#van FenemaP#PowellP_2016_Managing inter-organizational knowledge sharing_The Journal of Strategic Information Systems_1.txt\" with length 63851 bytes\n",
            "User uploaded file \"LoveJ#HirschheimR_2016_Reflections on Information Systems Journal's thematic composition_Information Systems Journal_1.txt\" with length 59063 bytes\n",
            "User uploaded file \"LowryP#WilsonD_2016_Creating agile organizations through IT - The influence of internal IT service perceptions on IT service quality and IT agility_The Journal of Strategic Information Systems_3.txt\" with length 90907 bytes\n",
            "User uploaded file \"LOWRYP~2.TXT\" with length 149006 bytes\n",
            "User uploaded file \"LudwigS#LaerT#RuyterK#FriedmanM_2016_Untangling a Web of Lies - Exploring Automated Detection of Deception in Computer-Mediated Communication_Journal of Management Information Systems_2.txt\" with length 85200 bytes\n",
            "User uploaded file \"LuY#GuptaA#KetterW#van HeckE_2016_Exploring Bidder Heterogeneity in Multichannel Sequential B2b Auctions_MIS Quarterly_3.txt\" with length 77955 bytes\n",
            "User uploaded file \"LyytinenK#YooY#Boland Jr.R_2016_Digital product innovation within four classes of innovation networks_Information Systems Journal_1.txt\" with length 102447 bytes\n",
            "User uploaded file \"MacCroryF#ChoudharyV#PinsonneaultA_2016_Designing Promotion Ladders to Mitigate Turnover of IT Professionals_Information Systems Research_3.txt\" with length 64501 bytes\n",
            "User uploaded file \"MajchrzakA#MalhotraA_2016_Effect of Knowledge-Sharing Trajectories on Innovative Outcomes in Temporary Online Crowds_Information Systems Research_4.txt\" with length 103241 bytes\n",
            "User uploaded file \"MajchrzakA#MarkusM#WarehamJ_2016_Designing for Digital Transformation - Lessons for Information Systems Research from the Study of Ict and Societal Challenges_MIS Quarterly_2.txt\" with length 56391 bytes\n",
            "User uploaded file \"MalaurentJ#AvisonD_2016_Reconciling global and local needs - a canonical action research project to deal with workarounds_Information Systems Journal_3.txt\" with length 97725 bytes\n",
            "User uploaded file \"MarkopoulosP#AronR#UngarL_2016_Product Information Websites - Are They Good for Consumers_Journal of Management Information Systems_3.txt\" with length 82639 bytes\n",
            "User uploaded file \"MartensD#ProvostF#ClarkJ#de FortunyE_2016_Mining Massive Fine-Grained Behavior Data to Improve Predictive Analytics_MIS Quarterly_4.txt\" with length 75438 bytes\n",
            "User uploaded file \"MartinsonsM#DavisonR_2016_People, places and time in research design and reporting - responding to commentaries on particularism_Journal of Information Technology_3.txt\" with length 10270 bytes\n",
            "User uploaded file \"MartinsonsM_2016_Research of information systems - from parochial to international, towards global or glocal_Information Systems Journal_1.txt\" with length 57194 bytes\n",
            "User uploaded file \"MasliA#RichardsonV#Weidenmier WatsonM#ZmudR_2016_Senior Executives’ It Management Responsibilities - Serious It-Related Deficiencies and Ceo-Cfo Turnover_MIS Quarterly_3.txt\" with length 132571 bytes\n",
            "User uploaded file \"McGrathK_2016_Identity Verification and Societal Challenges - Explaining the Gap Between Service Provision and Development Outcomes_MIS Quarterly_2.txt\" with length 83023 bytes\n",
            "User uploaded file \"Mein GohJ#GaoG#AgarwalR_2016_The Creation of Social Value - Can an Online Health Community Reduce Rural–Urban Health Disparities_MIS Quarterly_1.txt\" with length 78283 bytes\n",
            "User uploaded file \"MenonS#SarkarS_2016_Privacy and Big Data - Scalable Approaches to Sanitize Large Transactional Databases for Sharing_MIS Quarterly_4.txt\" with length 88912 bytes\n",
            "User uploaded file \"MettlerT#WinterR_2016_Are business users social - A design experiment exploring information sharing in enterprise social systems_Journal of Information Technology_2.txt\" with length 73407 bytes\n",
            "User uploaded file \"MirandaS#YoungA#YetginE_2016_Are Social Media Emancipatory or Hegemonic - Societal Effects of Mass Media Digitization in the Case of the Sopa Discourse_MIS Quarterly_2.txt\" with length 225767 bytes\n",
            "User uploaded file \"MithasS#KrishnanM#FornellC_2016_Information Technology, Customer Satisfaction, and Profit - Theory and Evidence_Information Systems Research_1.txt\" with length 98978 bytes\n",
            "User uploaded file \"MithasS#RustR_2016_How Information Technology Strategy and Investments Influence Firm Performance - Conjecture and Empirical Evidence1_MIS Quarterly_1.txt\" with length 107690 bytes\n",
            "User uploaded file \"MoodyG#KirschL#SlaughterS#DunnB#WengQ_2016_Facilitating the Transformational - An Exploration of Control in Cyberinfrastructure Projects and the Discovery of Field Control_Information Systems Research_2.txt\" with length 129076 bytes\n",
            "User uploaded file \"MüllerO#JunglasI#BrockeJ#DebortoliS_2016_Utilizing big data analytics for information systems research - challenges, promises and guidelines_European Journal of Information Systems_4.txt\" with length 78532 bytes\n",
            "User uploaded file \"NaidooR_2016_A communicative-tension model of change-induced collective voluntary turnover in IT_The Journal of Strategic Information Systems_4.txt\" with length 128852 bytes\n",
            "User uploaded file \"NaldiM#MastroeniL_2016_Economic decision criteria for the migration to cloud storage_European Journal of Information Systems_1.txt\" with length 63540 bytes\n",
            "User uploaded file \"NevoS#NevoD#PinsonneaultA_2016_A Temporally Situated Self-Agency Theory of Information Technology Reinvention_MIS Quarterly_1.txt\" with length 167699 bytes\n",
            "User uploaded file \"NiehavesB#OrtbachK_2016_The inner and the outer model in explanatory design theory - the case of designing electronic feedback systems_European Journal of Information Systems_4.txt\" with length 78683 bytes\n",
            "User uploaded file \"NuijtenA#KeilM#CommandeurH_2016_Collaborative partner or opponent - How the messenger influences the deaf effect in IT projects_European Journal of Information Systems_6.txt\" with length 90950 bytes\n",
            "User uploaded file \"OhH#AnimeshA#PinsonneaultA_2016_Free Versus for-a-Fee - The Impact of a Paywall on the Pattern and Effectiveness of Word-of-Mouth Via Social Media_MIS Quarterly_1.txt\" with length 128963 bytes\n",
            "User uploaded file \"OhW#MoonJ#HahnJ#KimT_2016_Leader Influence on Sustained Participation in Online Collaborative Work Communities - A Simulation-Based Approach_Information Systems Research_2.txt\" with length 115357 bytes\n",
            "User uploaded file \"OjalaA_2016_Business models and opportunity creation - How IT entrepreneurs create and develop business models under uncertainty_Information Systems Journal_5.txt\" with length 83276 bytes\n",
            "User uploaded file \"OregliaE#SrinivasanJ_2016_Ict, Intermediaries, and the Transformation of Gendered Power Structures_MIS Quarterly_2.txt\" with length 52192 bytes\n",
            "User uploaded file \"OschW#SteinfieldC_2016_Team boundary spanning - strategic implications for the implementation and use of enterprise social media_Journal of Information Technology_2.txt\" with length 111904 bytes\n",
            "User uploaded file \"PANNIE~1.TXT\" with length 81885 bytes\n",
            "User uploaded file \"ParéG#TateM#JohnstoneD#KitsiouS_2016_Contextualizing the twin concepts of systematicity and transparency in information systems literature reviews_European Journal of Information Systems_6.txt\" with length 92984 bytes\n",
            "User uploaded file \"ParkE#RameshB#CaoL_2016_Emotion in IT Investment Decision Making with A Real Options Perspective - The Intertwining of Cognition and Regret_Journal of Management Information Systems_3.txt\" with length 96174 bytes\n",
            "User uploaded file \"ParkS#KeilM#BockG#KimJ_2016_Winner's regret in online C2C Auctions - an automatic thinking perspective_Information Systems Journal_6.txt\" with length 90011 bytes\n",
            "User uploaded file \"PaytonF_2016_Cultures of participation—for students, by students_Information Systems Journal_4.txt\" with length 60344 bytes\n",
            "User uploaded file \"PiccoliG_2016_Triggered essential reviewing - the effect of technology affordances on service experience evaluations_European Journal of Information Systems_6.txt\" with length 80877 bytes\n",
            "User uploaded file \"Pil HanS#ParkS#OhW_2016_Mobile App Analytics - A Multiple Discrete-Continuous Choice Framework_MIS Quarterly_4.txt\" with length 121804 bytes\n",
            "User uploaded file \"POULOU~1.TXT\" with length 216294 bytes\n",
            "User uploaded file \"PowellP_2016_Editorial_Information Systems Journal_1.txt\" with length 7495 bytes\n",
            "User uploaded file \"PowellP_2016_Editorial_Information Systems Journal_4.txt\" with length 7243 bytes\n",
            "User uploaded file \"ProudfootJ#JenkinsJ#BurgoonJ#JrJ_2016_More Than Meets the Eye - How Oculometric Behaviors Evolve Over the Course of Automated Deception Detection Interactions_Journal of Management Information Systems_2.txt\" with length 90879 bytes\n",
            "User uploaded file \"RaghunathanS#SarkarS_2016_Competitive Bundling in Information Markets - A Seller-Side Analysis_MIS Quarterly_1.txt\" with length 217659 bytes\n",
            "User uploaded file \"RaiA_2016_Editor’s Comments_MIS Quarterly_1.txt\" with length 30921 bytes\n",
            "User uploaded file \"RaiA_2016_Editor’s Comments_MIS Quarterly_2.txt\" with length 31568 bytes\n",
            "User uploaded file \"RaiA_2016_Editor’s Comments_MIS Quarterly_3.txt\" with length 32601 bytes\n",
            "User uploaded file \"RaiA_2016_Editor’s Comments_MIS Quarterly_4.txt\" with length 47628 bytes\n",
            "User uploaded file \"RAJOR#~1.TXT\" with length 71535 bytes\n",
            "User uploaded file \"RansbothamS#FichmanR#GopalR#GuptaA_2016_Special Section Introduction—Ubiquitous IT and Digital Vulnerabilities_Information Systems Research_4.txt\" with length 83515 bytes\n",
            "User uploaded file \"ReckerJ#LekseD_2016_A field study of spatial preferences in enterprise microblogging_Journal of Information Technology_2.txt\" with length 84022 bytes\n",
            "User uploaded file \"RenaudA#WalshI#KalikaM_2016_Is SAM still alive - A bibliometric and interpretive mapping of the strategic alignment research field_The Journal of Strategic Information Systems_2.txt\" with length 141647 bytes\n",
            "User uploaded file \"RodeH_2016_To share or not to share - the effects of extrinsic and intrinsic motivations on knowledge-sharing in enterprise social media platforms_Journal of Information Technology_2.txt\" with length 88305 bytes\n",
            "User uploaded file \"SabooA#KumarV#ParkI_2016_Using Big Data to Model Time-Varying Effects for Marketing Resource (re)allocation_MIS Quarterly_4.txt\" with length 133998 bytes\n",
            "User uploaded file \"SandeepM#RavishankarM_2016_Impact sourcing ventures and local communities - a frame alignment perspective_Information Systems Journal_2.txt\" with length 93796 bytes\n",
            "User uploaded file \"SanthanamR#LiuD#ShenW_2016_Gamification of Technology-Mediated Training - Not All Competitions Are the Same_Information Systems Research_2.txt\" with length 69702 bytes\n",
            "User uploaded file \"SanyalP_2016_Characteristics and Economic Consequences of Jump Bids in Combinatorial Auctions_Information Systems Research_2.txt\" with length 97985 bytes\n",
            "User uploaded file \"SarkerS_2016_Building on Davison and Martinsons’ concerns - a call for balance between contextual specificity and generality in IS research_Journal of Information Technology_3.txt\" with length 15959 bytes\n",
            "User uploaded file \"Saskia BayerlP#LaucheK#AxtellC_2016_Revisiting Group-Based Technology Adoption as a Dynamic Process - The Role of Changing Attitude-Rationale Configurations_MIS Quarterly_3.txt\" with length 72056 bytes\n",
            "User uploaded file \"SaundersA#BrynjolfssonE_2016_Valuing Information Technology Related Intangible Assets_MIS Quarterly_1.txt\" with length 118888 bytes\n",
            "User uploaded file \"SCHERM~2.TXT\" with length 85880 bytes\n",
            "User uploaded file \"SchermannM#YettonP#KrcmarH_2016_A response to “Transaction Cost Economics on Trial Again”_The Journal of Strategic Information Systems_1.txt\" with length 36460 bytes\n",
            "User uploaded file \"SchmitzK#TengJ#WebbK_2016_Capturing the Complexity of Malleable It Use - Adaptive Structuration Theory for Individuals_MIS Quarterly_3.txt\" with length 182525 bytes\n",
            "User uploaded file \"SchneiderS#SunyaevA_2016_Determinant factors of cloud-sourcing decisions - reflecting on the IT outsourcing literature in the era of cloud computing_Journal of Information Technology_1.txt\" with length 172493 bytes\n",
            "User uploaded file \"ScottM#DeLoneW#GoldenW_2016_Measuring eGovernment success - a public value approach_European Journal of Information Systems_3.txt\" with length 114971 bytes\n",
            "User uploaded file \"SelanderL#JarvenpaaS_2016_Digital Action Repertoires and Transforming a Social Movement Organization_MIS Quarterly_2.txt\" with length 95971 bytes\n",
            "User uploaded file \"SerranoC#KarahannaE_2016_The Compensatory Interaction Between User Capabilities and Technology Capabilities in Influencing Task Performance - An Empirical Assessment in_MIS Quarterly_3.txt\" with length 176083 bytes\n",
            "User uploaded file \"ShaikhM#VaastE_2016_Folding and Unfolding - Balancing Openness and Transparency in Open Source Communities_Information Systems Research_4.txt\" with length 122828 bytes\n",
            "User uploaded file \"SHAOZ#~1.TXT\" with length 124875 bytes\n",
            "User uploaded file \"ShiZ#LeeG#WhinstonA_2016_Toward a Better Measure of Business Proximity - Topic Modeling for Industry Intelligence_MIS Quarterly_4.txt\" with length 97337 bytes\n",
            "User uploaded file \"ShmueliO#PliskinN#FinkL_2016_Can the outside-view approach improve planning decisions in software development projects_Information Systems Journal_4.txt\" with length 86131 bytes\n",
            "User uploaded file \"SholloA#GalliersR_2016_Towards an understanding of the role of business intelligence systems in organisational knowing_Information Systems Journal_4.txt\" with length 95795 bytes\n",
            "User uploaded file \"SIERIN~1.TXT\" with length 92036 bytes\n",
            "User uploaded file \"SöllnerM#HoffmannA#LeimeisterJ_2016_Why different trust relationships matter for information systems users_European Journal of Information Systems_3.txt\" with length 80589 bytes\n",
            "User uploaded file \"SPIEGE~1.TXT\" with length 102042 bytes\n",
            "User uploaded file \"SrivastavaS#TeoT#DevarajS_2016_You Can’t Bribe a Computer - Dealing with the Societal Challenge of Corruption Through Ict_MIS Quarterly_2.txt\" with length 104887 bytes\n",
            "User uploaded file \"StankoM_2016_Toward a Theory of Remixing in Online Innovation Communities_Information Systems Research_4.txt\" with length 108176 bytes\n",
            "User uploaded file \"SteinbartP#KeithM#BabbJ_2016_Examining the Continuance of Secure Behavior - A Longitudinal Field Study of Mobile Device Authentication_Information Systems Research_2.txt\" with length 110383 bytes\n",
            "User uploaded file \"SteinM#GalliersR#WhitleyE_2016_Twenty years of the European information systems academy at ECIS - emergent trends and research topics_European Journal of Information Systems_1.txt\" with length 86363 bytes\n",
            "User uploaded file \"SunZ#DawandeM#JanakiramanG#MookerjeeV_2016_The Making of a Good Impression - Information Hiding in Ad Exchanges_MIS Quarterly_3.txt\" with length 132193 bytes\n",
            "User uploaded file \"SusarlaA#OhJ#TanY_2016_Influentials, Imitables, or Susceptibles - Virality and Word-of-Mouth Conversations in Online Social Networks_Journal of Management Information Systems_1.txt\" with length 85793 bytes\n",
            "User uploaded file \"TanC#BenbasatI#CenfetelliR_2016_An Exploratory Study of the Formation and Impact of Electronic Service Failures_MIS Quarterly_1.txt\" with length 237586 bytes\n",
            "User uploaded file \"Te’eniD_2016_Contextualization and problematization, gamification and affordance - a traveler’s reflections on EJIS_European Journal of Information Systems_6.txt\" with length 15850 bytes\n",
            "User uploaded file \"TebbouneS#UrquhartC_2016_Netsourcing strategies for vendors - a resource-based and transaction cost economics perspective_Journal of Information Technology_1.txt\" with length 89061 bytes\n",
            "User uploaded file \"TeoT#NishantR#KohP_2016_Do shareholders favor business analytics announcements_Journal of Information Technology_4.txt\" with length 88839 bytes\n",
            "User uploaded file \"ThiesF#WesselM#BenlianA_2016_Effects of Social Interaction Dynamics on Platforms_Journal of Management Information Systems_3.txt\" with length 95749 bytes\n",
            "User uploaded file \"TiwanaA#KimS_2016_Concurrent IT Sourcing - Mechanisms and Contingent Advantages_Journal of Management Information Systems_1.txt\" with length 103020 bytes\n",
            "User uploaded file \"TrauthE_2016_Editorial 26 -2_Information Systems Journal_2.txt\" with length 7717 bytes\n",
            "User uploaded file \"TrauthE_2016_Editorial_Information Systems Journal_6.txt\" with length 7806 bytes\n",
            "User uploaded file \"TrippJ#RiemenschneiderC#ThatcherJ_2016_Job Satisfaction in Agile Development Teams - Agile Development as Work Redesign_Journal of the Association for Information Systems_4.txt\" with length 143817 bytes\n",
            "User uploaded file \"TurelO#Qahri-SaremiH_2016_Problematic Use of Social Networking Sites - Antecedents and Consequence from a Dual-System Theory Perspective_Journal of Management Information Systems_4.txt\" with length 90889 bytes\n",
            "User uploaded file \"TurelO_2016_Untangling the complex role of guilt in rational decisions to discontinue the use of a hedonic Information System_European Journal of Information Systems_5.txt\" with length 88316 bytes\n",
            "User uploaded file \"UrquhartC_2016_Response to Davison and Martinsons - context is king! Yes and no - it’s still all about theory (building)_Journal of Information Technology_3.txt\" with length 12382 bytes\n",
            "User uploaded file \"UteshevaA#SimpsonJ#Cecez-KecmanovicD_2016_Identity metamorphoses in digital disruption - a relational theory of identity_European Journal of Information Systems_4.txt\" with length 114784 bytes\n",
            "User uploaded file \"van BeijsterveldJ#van GroenendaalW_2016_Solving misfits in ERP implementations by SMEs_Information Systems Journal_4.txt\" with length 77127 bytes\n",
            "User uploaded file \"VenableJ#Pries-HejeJ#BaskervilleR_2016_FEDS - a Framework for Evaluation in Design Science Research_European Journal of Information Systems_1.txt\" with length 75362 bytes\n",
            "User uploaded file \"VENKAT~4.TXT\" with length 115082 bytes\n",
            "User uploaded file \"VenkateshV#BrownS#SullivanY_2016_Guidelines for Conducting Mixed-methods Research - An Extension and Illustration_Journal of the Association for Information Systems_7.txt\" with length 209492 bytes\n",
            "User uploaded file \"VenkateshV#RaiA#SykesT#AljafariR_2016_Combating Infant Mortality in Rural India - Evidence from a Field Study of Ehealth Kiosk Implementations_MIS Quarterly_2.txt\" with length 130782 bytes\n",
            "User uploaded file \"VenkateshV#ThongJ#ChanF#HuP_2016_Managing Citizens’ Uncertainty in E-Government Services - The Mediating and Moderating Roles of Transparency and Trust_Information Systems Research_1.txt\" with length 137011 bytes\n",
            "User uploaded file \"VenkateshV#ThongJ#Xin Xu_2016_Unified Theory of Acceptance and Use of Technology - A Synthesis and the Road Ahead_Journal of the Association for Information Systems_5.txt\" with length 162339 bytes\n",
            "User uploaded file \"VialG#RivardS_2016_A process explanation of the effects of institutional distance between parties in outsourced information systems development projects_European Journal of Information Systems_5.txt\" with length 98766 bytes\n",
            "User uploaded file \"VijayasarathyL#CasterellaG_2016_The Effects of Information Request Language and Template Usage on Query Formulation_Journal of the Association for Information Systems_10.txt\" with length 109237 bytes\n",
            "User uploaded file \"VITHAR~1.TXT\" with length 125666 bytes\n",
            "User uploaded file \"WakefieldR#WakefieldK_2016_Social media network behavior - A study of user passion and affect_The Journal of Strategic Information Systems_2.txt\" with length 90582 bytes\n",
            "User uploaded file \"WALLJ#~1.TXT\" with length 166588 bytes\n",
            "User uploaded file \"WalshI#Gettler-SummaM#KalikaM_2016_Expectable use - An important facet of IT usage_The Journal of Strategic Information Systems_3.txt\" with length 137596 bytes\n",
            "User uploaded file \"WangW#BenbasatI_2016_Empirical Assessment of Alternative Designs for Enhancing Different Types of Trusting Beliefs in Online Recommendation Agents_Journal of Management Information Systems_3.txt\" with length 94743 bytes\n",
            "User uploaded file \"WarkentinM#WaldenE#JohnstonA#StraubD_2016_Neural Correlates of Protection Motivation for Secure IT Behaviors - An fMRI Examination_Journal of the Association for Information Systems_3.txt\" with length 81683 bytes\n",
            "User uploaded file \"WeberT_2016_Product Pricing in a Peer-to-Peer Economy_Journal of Management Information Systems_2.txt\" with length 62605 bytes\n",
            "User uploaded file \"WienerM#MähringM#RemusU#SaundersC_2016_Control Configuration and Control Enactment in Information Systems Projects - Review and Expanded Theoretical Framework_MIS Quarterly_3.txt\" with length 225554 bytes\n",
            "User uploaded file \"WindelerJ#RiemenschneiderC_2016_The influence of ethnicity on organizational commitment and merit pay of IT workers - the role of leader support_Information Systems Journal_2.txt\" with length 123498 bytes\n",
            "User uploaded file \"WolffJ_2016_Perverse Effects in Defense of Computer Systems - When More Is Less_Journal of Management Information Systems_2.txt\" with length 79163 bytes\n",
            "User uploaded file \"XuB#XuZ#LiD_2016_Internet aggression in online communities - a contemporary deterrence perspective_Information Systems Journal_6.txt\" with length 82276 bytes\n",
            "User uploaded file \"Xuefei (Nancy) Deng#JoshiK_2016_Why Individuals Participate in Micro-task Crowdsourcing Work Environment - Revealing Crowdworkers' Perceptions_Journal of the Association for Information Systems_10.txt\" with length 104878 bytes\n",
            "User uploaded file \"XUJ#CE~1.TXT\" with length 97466 bytes\n",
            "User uploaded file \"YahavI#ShmueliG#ManiD_2016_A Tree-Based Approach for Addressing Self-Selection in Impact Studies with Big Data_MIS Quarterly_4.txt\" with length 146585 bytes\n",
            "User uploaded file \"Yili Hong#Ni Huang#BurtchG#Chunxiao Li_2016_Culture, Conformity, and Emotional Suppression in Online Reviews_Journal of the Association for Information Systems_11.txt\" with length 81259 bytes\n",
            "User uploaded file \"YinD#MitraS#ZhangH_2016_When Do Consumers Value Positive vs. Negative Reviews - An Empirical Investigation of Confirmation Bias in Online Word of Mouth_Information Systems Research_1.txt\" with length 71845 bytes\n",
            "User uploaded file \"YoungB#MathiassenL#DavidsonE_2016_Inconsistent and Incongruent Frames During ITenabled Change - An Action Research Study into Sales Process Innovation_Journal of the Association for Information Systems_7.txt\" with length 106083 bytes\n",
            "User uploaded file \"ZahediF#WaliaN#JainH_2016_Augmented Virtual Doctor Office - Theory-based Design and Assessment_Journal of Management Information Systems_3.txt\" with length 98936 bytes\n",
            "User uploaded file \"ZhangD#ZhouL#KehoeJ#KilicI_2016_What Online Reviewer Behaviors Really Matter - Effects of Verbal and Nonverbal Behaviors on Detection of Fake Online Reviews_Journal of Management Information Systems_2.txt\" with length 75805 bytes\n",
            "User uploaded file \"ZhangK#BhattacharyyaS#RamS_2016_Large-Scale Network Analysis for Online Social Brand Advertising_MIS Quarterly_4.txt\" with length 78194 bytes\n",
            "User uploaded file \"ZhangZ#NanG#LiM#TanY_2016_Duopoly Pricing Strategy for Information Products with Premium Service - Free Product or Bundling_Journal of Management Information Systems_1.txt\" with length 101562 bytes\n",
            "User uploaded file \"ZhaoL#DetlorB#ConnellyC_2016_Sharing Knowledge in Social Q&A Sites - The Unintended Consequences of Extrinsic Motivation_Journal of Management Information Systems_1.txt\" with length 87574 bytes\n",
            "User uploaded file \"ZhengY#YuA_2016_Affordances of social media in collective action - the case of Free Lunch for Children in China_Information Systems Journal_3.txt\" with length 88415 bytes\n",
            "User uploaded file \"ZhouW#DuanW_2016_Do Professional Reviews Affect Online User Choices Through User Reviews - An Empirical Study_Journal of Management Information Systems_1.txt\" with length 79598 bytes\n",
            "User uploaded file \"ZuchowskiO#PoseggaO#SchlagweinD#FischbachK_2016_Internal crowdsourcing - conceptual framework, structured review, and research agenda_Journal of Information Technology_2.txt\" with length 107668 bytes\n",
            "User uploaded file \"ZwassV_2016_Editorial Introduction_Journal of Management Information Systems_1.txt\" with length 18725 bytes\n",
            "User uploaded file \"ZwassV_2016_Editorial Introduction_Journal of Management Information Systems_3.txt\" with length 9679 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72fqtHzXrja3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEdJGVkO-ZbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvTvZUbSMPZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqr77dVQ34Na",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc1=nlp(open(\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\").read())\n",
        "for token in doc1:\n",
        "  print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_-1ZppOLBMm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10917
        },
        "outputId": "e2dd04ad-be3e-40a1-901f-fee130bcdbcb"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc1=nlp(open(u\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\").read())\n",
        "doc1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "J\n",
              "Editorial\n",
              "\n",
              "ournal of the\n",
              "\n",
              "A\n",
              "\n",
              "ssociation for\n",
              "\n",
              "I\n",
              "\n",
              "nformation\n",
              "\n",
              "S\n",
              "\n",
              "ystems\n",
              "ISSN: 1536-9323\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "Ahmed Abbasi\n",
              "McIntire School of Commerce, University of Virginia abbasi@comm.virginia.edu\n",
              "\n",
              "Suprateek Sarker\n",
              "McIntire School of Commerce, University of Virginia School of Business, Aalto University suprateek.sarker@comm.virginia.edu\n",
              "\n",
              "Roger H. L. Chiang\n",
              "Carl H. Lindner College of Business, University of Cincinnati\n",
              "\n",
              "roger.chiang@uc.edu\n",
              "\n",
              "Abstract:\n",
              "Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data’s characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data’s disruptive effects. Keywords: Big Data, Behavioral, Business Analytics, Business Intelligence, Design Science, Economics of IS, Information Value Chain, Research Directions.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "pp. i – xxxii\n",
              "\n",
              "February\n",
              "\n",
              "2016\n",
              "\n",
              "\n",
              "ii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "1\n",
              "\n",
              "Introduction\n",
              "\n",
              "As business processes become major differentiators for organizations in many industries, organizations are increasingly using analytics to “wring every last drop” of value from those processes (Davenport, 2006). Consequently, companies now view their data as a primary business asset (Redman, 2008). In organizational settings, the information technology (IT) function is tasked with managing and integrating data as an “enabler” of data-driven business processes and decision making (Chandler, Hostmann, Rayner, & Herschel, 2011; Lycett, 2013). Big data’s rise has further amplified the importance of IT in this role (Horan, 2011), resulting in important implications for IT managers and scholars within and beyond the information systems (IS) discipline. Recent editorials and special issues in top IS journals have discussed big data analytics from different vantage points. For example, Chen, Chiang, and Storey (2012) highlight the various domains and information sources associated with big data. They also touch on their vision for evolving analytics toward big data -- from the traditional structured relational database-driven paradigm to “business intelligence and analytics 2.0”, which leverages Web and unstructured content, and to “business intelligence and analytics 3.0”, which, in addition, encompasses mobile and sensor-based data. Goes (2014) presents valuable taxonomies for big data infrastructure and big data analytics. Agarwal and Dhar (2014) discuss challenges and opportunities pertaining to big data in information systems research and note that IS researchers are well positioned to take advantage of opportunities in this area. Sharma, Mithas, and Kankanhalli (2014) offer several research questions that IS researchers need to pursue. Adding to this conversation is the call for papers for MIS Quarterly’s upcoming special issue that emphasizes the need for IS researchers to examine and exploit big data’s disruptive nature (Baesens, Bapna, Marsden, Vanthienen, & Zhao, 2014). All of these commentaries, editorials, and/or special issue introductions highlight important facets of big data research in IS. In this editorial, we synthesize takeaways from prior expositions and expand on them by focusing on the interplay between big data’s unique characteristics, these characteristics’ implications for the information value chain, and potential areas of inquiry for the three major IS research traditions. We present a framework (Figure 1) that highlights this interplay and helps one to generate (and potentially refine) a set of meaningful research questions on this topic for the IS discipline. There has been a fair amount of IS research on the information value chain, which is the cycle of converting data to information to knowledge to decisions to actions and, thereby, generate additional data. Major bodies of IS research pertaining to the information value chain have examined the derivation and management of knowledge and decision making and actions; however, the effects of big data on the value chain remain relatively unexplored. Scholars and others often define big data by four key characteristics: volume, velocity, variety, and veracity; however, big data is not simply a matter of injecting additional scale, variation, speed, or noise to research data sets. As Chris Anderson of Wired famously said (2008, emphasis added), “Because in the era of big data, more isn’t just more. More is different”. In other words, these characteristics have the propensity to disrupt the traditional information value chain and result in a new “big data information value chain”. Such disruption not only presents opportunities for novel research from within or across the different IS research traditions (e.g., positivist, interpretive, and critical behavioral research, design research, and economics of IS-based research) but also raises epistemological questions and challenges for some of the IS research traditions. This editorial proceeds as follows. First, in Sections 2, 3, and 4, we provide an overview of the traditional information value chain and related IS research, big data’s disruptive characteristics, and the resulting big data information value chain. With these sections, we highlight the profound impact of big data on people, processes, technologies, and, consequently, on organizations, industries, and virtually every facet of the world we live in 1. Second, in Sections 5 to 11, we discuss possible research directions and implications for the three traditions of IS research alluded to earlier. In a way, we highlight the fact that we have much to learn about the big data phenomena and scholars in each tradition can and need to play a role in the research endeavor. We emphasize that that, by highlighting the three traditions and certain questions in our editorial, we do not wish to exclude other forms or areas of inquiry. In fact, we believe that we will see new traditions, genres, and areas of inquiry spring out of the IS community’s engagement with phenomena related to big data.\n",
              "\n",
              "1\n",
              "\n",
              "Those already familiar with the notion of “big data” and its impacts on the value chain may prefer to skim through this discussion or perhaps proceed directly to the discussion on research directions (starting in Section 5).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "iii\n",
              "\n",
              "Figure 1. Overview of the Big Data Information Value Chain Perspective 2\n",
              "\n",
              "2\n",
              "\n",
              "The Information Value Chain\n",
              "\n",
              "The information value chain is the cyclical set of activities necessary to convert data into information and, subsequently, to transform information into knowledge (Fayyad, Piatetsky-Shapiro, & Smyth, 1996a, 1996b; Han, Kamber, & Pei, 2006), which individuals use to make decisions and take action. The decisions and actions then result in outcomes such as business value and additional data (Sharma et al., 2014). Each stage of the value chain encompasses people, processes, and technologies (Chandler et al., 2011). Information value chains operate in a given context; for instance, at the enterprise level, in a centralized IT unit, or for a specific functional or business unit. Figure 2 illustrates the traditional information value chain prior to the era of big data. The list of people, processes, and technologies shown in Figure 2 are illustrative, not exhaustive. As a further caveat, one could place certain processes and technologies differently depending on how one interprets data, information, and knowledge and how one delineates between decisions and actions. Here, we present the examples to illustrate the interplay between people, processes, and technologies across the stages of the traditional information value chain (i.e., a pre-big data era baseline). In Section 3, we contrast this traditional value chain with the new information value chain resulting from big data’s disruption. One can broadly categorize the set of activities in the information value chain into two groups: knowledge derivation and decision making (Chandler et al., 2011; Goes, 2014; Sharma et al., 2014). In the traditional information value chain, structured data is predominantly stored on premises in organizations’ data centers that use relational database management systems (RDBMS). Many organizations integrate various structured data sources into data warehouses and/or data marts using extract, transform, and load (ETL) technologies. Database administrators, database managers, and ETL developers typically perform these tasks. Programmers or data analysts then process and analyze the data stored in these systems using structured query language (SQL) and use the resulting data as input for report generators, business intelligence (BI) tools, and/or analytical models incorporated in predictive technologies. In the value chain’s knowledge stage, the people tasked with deriving knowledge from the data intersect with the decision makers; in other words, in the knowledge stage, the enablers and producers interact with the consumers of information (Chandler et al., 2011). In this stage, technologies such as knowledge management systems, corporate wikis, reporting tools, BI dashboards, and expert systems preserve or present existing knowledge or facilitate the creation of new knowledge through processes such as forecasting or reporting. Technologies such as decision support systems (DSS), recommender systems, and collaboration tools support managers and analysts’ decision -making processes. The people, process, and technologies at various stages are also influenced by contextual factors such as the organizational/department/unit culture and IT governance. Traditionally, IS research on the information value chain has also focused on the two aforementioned areas: 1) deriving knowledge and 2) decision making (Sharma et al. 2014). Scholarship in the area of deriving knowledge encompasses areas such as large bodies of work on knowledge management, structured data mining, and database design (e.g., Alavi & Leidner 2001; Chiang, Barron, & Storey, 1994; Storey, Chiang, Dey, Goldstein, & Sudaresan, 1997). The IS body of literature on decision making\n",
              "2\n",
              "\n",
              "The arrow from the big data information value chain to the IS research traditions indicates the epistemological/paradigmatic considerations and challenges big data may bring to the way we conduct research and the types of criteria we might privilege in examining the big data phenomenon. The arrow between big data’s characteristics and the information value signifies the disruptive impact of the four Vs on the traditional value chain.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "iv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "is rich and extensive. Major areas of emphasis include research on designing decision support systems (DSS) and behavioral research on the effectiveness of IT artifacts or other decision aids for supporting decision making (Nunamaker, Chen, & Purdin, 1991; Wixom & Watson, 2001; Shim et al., 2002; Arnott & Pervan, 2008).\n",
              "\n",
              "Figure 2. The Traditional Information Value Chain and Examples of the Accompanying People, Processes, and Technologies\n",
              "\n",
              "In this era, academic scholars and practitioners have tended to use relatively scarce, largely static, and deliberately sampled and collected data (Kitchin, 2014a, 2014b). Having described the traditional information value chain, we now move on to describing the key characteristics of big data and elaborating on how these characteristics might disrupt the information value chain.\n",
              "\n",
              "3\n",
              "\n",
              "Enter Big Data: The Four Vs\n",
              "\n",
              "One can separate big data and “regular-sized” data based on the presence of a set of characteristics commonly referred to as the four Vs: volume, variety, velocity, and veracity (Schroeck, Shockley, Smart, Romero-Morales, & Tufano, 2012; Goes, 2014).\n",
              "\n",
              "3.1\n",
              "\n",
              "Volume\n",
              "\n",
              "The U.S. Library of Congress, which archives both digital and offline content, has collected hundreds of terabytes of data (Manyika et al., 2011). Interestingly, the average company in 15 of 17 industry sectors in the United States has more data stored than the Library of Congress (Manyika et al., 2011), which underscores the fact that big data is pervasive across industries including finance, manufacturing, retail, health, security, technology, and sports. For a detailed discussion of various applications domains for big data, see Chen et al. (2012). Furthermore, in the vocabulary of big data, petabytes and exabytes have now replaced terabytes. For instance, large retailers each collect tens of exabytes of transactional data every year (McAfee & Brynjolfsson, 2012). To put these volumes into perspective using the classic grains of sand\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "v\n",
              "\n",
              "analogy, if a megabyte is a tablespoon of sand, a terabyte is a sandbox two-feet wide and one-inch deep, a petabyte is a mile-long beach, and an exabyte is a beach extending from Maine to North Carolina.\n",
              "\n",
              "3.2\n",
              "\n",
              "Variety\n",
              "\n",
              "Organizations are now dealing with structured, semi-structured, and unstructured data from in and outside the enterprise (Schroeck et al., 2012). The variety includes traditional transactional data, user-generated text, images, and videos, social network data, sensor-based data, Web and mobile clickstreams, and spatial-temporal data (Chen et al., 2012; McAfee & Brynjolfsson, 2012). Effectively leveraging the variety of available data presents both opportunities and challenges.\n",
              "\n",
              "3.3\n",
              "\n",
              "Velocity\n",
              "\n",
              "The speed of data creation is a hallmark of big data. For instance, Wal-Mart collects over 2.5 petabytes of customer transaction data every hour (McAfee & Brynjolfsson, 2012). With respect to unstructured data, over one billion new tweets occur every three days, and five billion search queries occur daily (Abbasi & Adjeroh, 2014). Such information has important implications for “real-time” predictive analytics in various application areas, ranging from finance to health (Bollen, Mao, & Zeng, 2011; Broniatowski, Paul, & Dredze, 2014). Simply put, analyzing “data in motion” presents new challenges because the desired patterns and insights are moving targets, which is not the case for static data.\n",
              "\n",
              "3.4\n",
              "\n",
              "Veracity\n",
              "\n",
              "The credibility and reliability of different data sources vary. For instance, social media is plagued with spam, and Web spam accounts for over 20 percent of all content on the World Wide Web (Abbasi & Adjeroh, 2014). Similarly, clickstreams from website and mobile traffic are highly susceptible to noise (Kaushik, 2011). Furthermore, deriving deep semantic knowledge from text remains challenging in many situations despite significant advances in natural language processing.\n",
              "\n",
              "4\n",
              "\n",
              "The Big Data Information Value Chain\n",
              "\n",
              "In his seminal book The Innovator’s Dilemma, Christensen (1997) introduced and elaborated on the idea of disruption as relevant here. In organizational settings, disruptive phenomena significantly alter value chains. Indeed, big data and its four “V” characteristics have had a profound impact on the people, processes, and technologies related to the information value chain. As Mayer-Schönberger & Cukier (2013, p. 19) note, “The era of big data changes how we live and interact with the world”. Simply put, big data means big disruption (Newman, 2014). Figure 3 illustrates this disruption in three ways. First, the new value chain involves a different set of people, processes, and technologies. While IT is known to exist in a constantly changing landscape, we can clearly see the accompanying changes to the people and processes attributable to big data as a disruptor. Second, there is greater amalgamation of technologies into “platforms” and processes into “pipelines” in the value chain’s knowledge-derivation phase. Third, we see greater reliance on data scientists and analysts across all stages of the value chain to support self-service and realtime decision making. Big data’s four Vs clearly change how one stores and manages data. In terms of technical considerations, data’s volume, velocity, and variety in organizations have caused IT departments to consider distributed storage architectures capable of handling large quantities of unstructured data. In terms of the technology, NoSQL (“not only SQL”) systems, such as those leveraging Hadoop (Dean & Ghemawat, 2008) and/or Spark, have emerged as being better suited for the larger volumes and variety of unstructured data, while organizations commonly use in-memory databases to exploit velocity in real-time applications (Heudecker, 2013). In addition, firms are increasingly interested in collecting social media and sensor-based data (Chen et al., 2012) to supplement the internal data sources they have traditionally relied on, which contributes to the data’s variety. However, using such data sources comes with an array of data-quality and credibility concerns, which require appropriate data-management, data-preparation, and knowledge-management activities. Data’s volume and velocity have also caused IT departments to shift physical on-premises data centers to cloud-based infrastructure-as-a-service (IaaS), platform-as-a-service (PaaS), and database-asa-service (DBaaS) offerings better suited to meet organizations’ elastic computing and storage needs (Buytendijk, 2014). The interplay between traditional schema-based structured data storage and management, new schema-less big data technologies, and organizations’ increasing reliance on cloud\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "vi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "computing can create complex big data architectures. In some respects, the key data-management and storage questions that practitioners pose have shifted to “what other internal/external data sources can we leverage” and “what kind of enterprise data infrastructure do we need to support our growing needs?”.\n",
              "\n",
              "Figure 3. The Big Data Information Value Chain and Examples of Related People, Processes, and Technologies\n",
              "\n",
              "The shift towards schema-less data storage and management coupled with organizations’ increasing desire to leverage big data as a source of competitive advantage has brought about the rise of data scientists (Davenport & Patil, 2012; McAffe & Brynjolfsson, 2012). In the words of Davenport and Patil (2012, p. 73), “data scientists are the people who understand how to fish out answers to important business questions from today’s tsunami of unstructured information”. Data scientists and scriptingoriented programmers now perform (or at least complement those who do perform) many of the activities pertaining to deriving knowledge from internally and externally collected data sources that database managers and SQL programmers traditionally performed. Data scientists also work closely with analysts and management in the decision making phase (Davenport & Patil, 2012). Furthermore, data lakes, which are essentially data warehouses or data marts specifically intended to serve as “sand boxes” for data scientists to experiment in, are becoming increasingly pervasive (Buytendijk, 2014). While ETL developers still play an important role, data-integration tasks increasingly entail fusing various noisy structured and unstructured data sources. Consistent with the trend toward IaaS/PaaS/DBaaS, many technologies pertaining to the value chain’s information stage are also running in the cloud and accessed via software-as-as-service (SaaS) (Buyetdijk, 2014). Big data analytics allows data scientists and modelers to use enterprise machine learning— distributed scalable online algorithms running atop Hadoop platforms that can digest the volume and variety of information at unprecedented speeds. To offer an example, big data analytics running on Hadoop allowed Sears to push personalized promotions to customers more accurately and faster, which reduced the leadtime to one week compared to eight weeks in their previous data warehouse-based implementation (McAfee\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "vii\n",
              "\n",
              "& Brynjolfsson, 2012). Complex event-processing systems can analyze real-time sensor-based spatialtemporal data (Heudecker, 2013). For instance, the U.S. grocery chain Kroger has used overhead infra-red sensors to count customers and anticipate the number of currently needed checkout lanes and the number needed in 30 minutes, which has resulted in the company’s reducing average customer wait times from four minutes to 26 seconds (Coolidge, 2013). Big data’s velocity and the trend toward data driven decision making have created an exciting paradigm shift in how organizations create and leverage knowledge for decision making. The biggest shift is organization’s consuming analytics in real time with the rise of “self-service” BI/analytics (Chandler et al., 2011). This shift is attributable to the fact that “Big data, and the fast pace and complexity of today’s marketplace, require that leaders make decisions faster than ever before” (Kiron et al., 2011, p. 5). Self-service BI/analytics allows various employees in an organization, including managers and executives, to independently generate custom reports, run basic analytical queries, and access key performance indicators across various devices without relying on IT or decision analyst support. These factors help organizations avoid time-consuming hand-offs and make decisions in an agile manner (Chandler et al., 2011). The organizational context undoubtedly has impacts on the information value chain. Firms transformed by big data are often ones where a “strong top-line mandate to use analytics supports a culture open to new ideas” (Kiron et al., 2011, p. 5). Similarly, business units or departments with a longer-standing tradition of data-driven decision making, such as finance and operations, tend to leverage big data more relative to departments such as human resources (Kiron et al., 2011). Furthermore, big data governance practices have important implications for the availability, quality, maintenance, and security of the variety of novel structured and unstructured data sources that are part of the big data information value chain.\n",
              "\n",
              "5\n",
              "\n",
              "Big Data: Implications for IS Research\n",
              "\n",
              "The big data information value chain has several implications for IS research. The first set of issues relate to deriving knowledge from big data. In this area, prior editorials have examined the effects of “big research dataset” usage in academic research. These effects primarily pertain to sensemaking, which is the process of deriving knowledge based on information extracted from big data (Lycett, 2013). Below, we discuss the epistemological/paradigmatic issues, theoretical implications, and methodological challenges pertaining to “big research datasets”. However, we must emphasize that research implications of deriving knowledge from big data extend beyond the challenges of using “big research datasets”. The disruption to the traditional information value chain attributable to big data affords a plethora of research opportunities for the three IS research traditions. For instance, as organizations collect and store more customer data than ever before, privacy, security, and ethical considerations come to the forefront. With the proliferation of NoSQL and Hadoop systems, the advent of data lakes, and the popularity of in-memory databases, we need big data modeling formalisms and integration artifacts. Increased emphasis on complex event forecasting, credibility assessment, and social media analytics presents opportunities for novel prediction/description artifacts. Social listening platforms and the Internet of things allow novel forms of analysis pertaining to user-generated content and sensor-based data rich in knowledge, opinions, emotions, location, and geographic information. More broadly, with organizations treating data as a primary asset, assessing and, in some cases, quantifying the value of volume and variety relative to the costs of veracity becomes of paramount importance to evaluate the effectiveness of big data investments. We discuss some of these opportunities in Sections 6-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "viii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Figure 4. Toward a Big Data Research Agenda for IS\n",
              "\n",
              "The second set of implications pertains to big data’s impact on decisions and actions. This area of inquiry, which few prior IS papers have focused on, can potentially view big data and its characteristics as impacting IT artifact-related perceptions and behaviors in behavioral studies. We also believe that the four Vs of big data potentially change the very nature of IT artifacts, much like communication and collaboration technologies altered decision support systems and knowledge management, which gives rise to a new class of big data IT artifacts. IS research needs to not only contribute to the design but also examine the feasibility and effectiveness of such IT artifacts for different stakeholders. For instance, with the proliferation of real-time datadriven decision making and self-service analytics, executives, managers, and front-line employees are increasingly beginning to use big data to support timely decision making, which raises questions about the tension between data and intuition, implications for the nature of decision making, and appropriate ways to quantify the value/impact of the 4Vs on decisions. The above-mentioned issues also prompt researchers to think about the effects of cognition and usability and of the broader organizational context that includes but is\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "ix\n",
              "\n",
              "not limited to organizational culture and leadership, big data investment, and adoption outcomes. Real-time analytics pipelines built on big data platforms are augmenting or automating various business processes. A question that arises is: under what circumstances can (or should) one employ such alternatives for improving business processes? Big data also presents opportunities for designing and implementing novel decisionsupport artifacts and necessitates research on assessing the value of big data IT artifacts in different organizational contexts. In the ensuing sections, we elaborate on these and other opportunities. The key takeaway is that big data is not only different but also highly disruptive to the academic research process and to practice related to data, which requires that we re-assess our research assumptions, methodologies, and substantive questions. Furthermore, due to big data’s impact on people-processtechnology, these implications extend to behavioral science, design science, and the economics of IS. In their paper interestingly entitled “Why IT Fumbles Analytics”, Marchand and Pepper (2013) discuss the usability and cognitive challenges associated with big data analytics. They argue that scholars and practitioners have focused too much on big data’s technical facets and not enough on the people and their institutional and social environments, which has created a lack of socio-technical harmony that IS implementation initiatives often need to succeed. They note that “big data and other analytics projects require people versed in the cognitive and behavioral sciences, who understand how people perceive problems, use information, and analyze data in developing solutions, ideas, and knowledge” (p. 109), which appears to be well aligned with the core strengths of the IS discipline. Keeping in mind the aforementioned sets of implications from the perspective of the information value chain, Figure 4 outlines promising areas of research for big data research in the IS discipline. In Sections 6-8, we discuss possible research areas for the three IS traditions. We reiterate that the goal here is to maintain a broad and inclusive perspective and to be illustrative rather than exhaustive in our coverage.\n",
              "\n",
              "6\n",
              "6.1\n",
              "\n",
              "A Big Data Research Agenda for Behavioral IS Research\n",
              "Epistemological Concerns of Big Data and Behavioral IS Research\n",
              "\n",
              "The four Vs, particularly volume and variety, present challenges not only regarding the changing nature of big data phenomena as they exist in practice but also regarding how, and with what assumptions, research is conducted to investigate such phenomena. Some commentators have brought to question the value of the traditional scientific model of research (which often involves constructing hypotheses based on guesses and then deductively testing the hypotheses using carefully sampled data (e.g., Gregor & Klein, 2014)) in a data-abundant environment associated with big data (Anderson, 2008; Kitchin, 2014b). Will “machinegenerated correlations” on big data be enough to specify “inherently meaningful and truthful” patterns and relationships (Kitchin, 2014b, p. 135)? Can these correlations render front-end theorizing (before empirical analysis), which is at the heart of the scientific research model, meaningless (Kitchin, 2014b)? And, even if these patterns do not lead to understanding, will we be able to accurately predict behaviors, which is all that some important stakeholders may care about (Shmueli & Koppius 2010; Agarwal & Dhar, 2014)? Is an inductive “mode of science” in development, wherein algorithms “spot[s] patterns and generates theories” (Steadman, 2013, in Kitchin, 2014b, p. 131)? Are we nearing the “end of theory” (Anderson, 2008) given that “data tells the truth” under the assumption that studies have access to exhaustive data (that is, n = all), whereas “theory is merely spin” (Kitchin, 2014b, p. 135)?\n",
              "\n",
              "6.2\n",
              "\n",
              "Behavioral IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Many scholars, including Agarwal and Dhar (2014), do not perceive a fundamental transformation in the philosophy underlying research. Rather, they see potential for using a “guided knowledge-discovery” process that leverages big data in conjunction with traditional data-collection methods to generate insights and preliminary hypotheses worthy of further examination through a hybrid process of induction, deduction, and abduction. One perspective is that big data triggers a “paradigm shift towards computational social science” research (Chang, Kauffman, & Kwon, 2014, p. 67). In this perspective, scholars have argued that new “unobtrusive” big data information sources facilitate realism and generality with appropriate levels of control. Examples include using social media and Web clickstreams to enhance customer survey data to better understand the “voice of the customer” (Kaushik, 2011, p. 9), using large social networks to understand the dynamics of influence (Aral & Walker, 2012), and including mobile sensor-based data for enhanced spatial-temporal behavior analysis. Yet, other researchers note that such data is not “neutral, objective, and pre-analytic in nature” but reflects a certain underlying world-view, philosophy, or theory-in-\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "x\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "use that has informed the design of measurement instruments (e.g. Kitchin, 2014b, p. 2). Researchers and data scientists will need to reflect on this perspective on big data when making truth claims. Another major area in the broad realm of computational social science is workforce analytics (Davenport, Harris, & Shapiro, 2010). Today, many organizations use standard statistical techniques to combine employee perceptions derived from surveys with objective data from economic reports and/or measured through technology usage logs and sensors to make connections between employee satisfaction levels and sales, productivity, retention rates, and shrinkage levels (Coco, Jamison, & Black, 2011). For example, Best Buy found that a 0.1 percent increase in employee engagement resulted in a $100,000 annual increase in revenue per store (Davenport et al., 2010). Since workforce analytics examines the interplay between employee perceptions, technology usage, and business value-related outcomes, behavioral IS researchers should be well suited to lead research studies in this important area. While the preliminary results appear promising and present a great opportunity for the IS community to further develop and apply suitable data analytic techniques on large data sets, we need to critically reflect on several issues. First, we need to examine the assumptions underlying the data (e.g., how well the data represents the population and whose interests may be excluded or overrepresented) since the condition of “n = all” does not hold in most cases (Kitchin, 2014b). Second, we need to be aware of the assumptions underlying the analytic techniques (i.e., how value-free the techniques and algorithms are, and how compatible the assumptions underlying the techniques are with the nature of data). Third, and more importantly, we need to ensure that applying big data analytics actually leads to desirable economic and humanistic outcomes for relevant stakeholders. Clearly, both qualitative and quantitative researchers have an important role to play in rethinking and refining how big data is collected, prepared, analyzed, and presented and in investigating the actual processes and consequences of using big data analytics. For example, qualitative researchers, who one might not typically think of big data researchers, can seek to contribute to this arena by examining fundamental questions. These may include: “Are decision making processes at various levels of the organization being transformed due to big data, and, if so, how?” or “What kind of fit is needed (say, between the architecture/algorithms and the organizational structure/culture) for big data initiatives to be effective in organizations, and how can one cultivate such a fit?”. When using big data sources to derive insights, privacy considerations become paramount. In fact, Barocas and Nissenbaum (2014, p. 33) contend that “big data extinguishes what little hope remains for the notice and choice regime”, which provocatively points to the futility of organizations’ sharing their policies regarding collected data and the “opt-out options” they may provide. Analytics concerns how data from various sources is assembled and mined (Fayyad et al., 1996). However, what data actually gets mined, and using what techniques, is often emergent and not necessarily defined upfront, which makes individuals particularly vulnerable to privacy invasions. Interestingly, a poll conducted by KD Nuggets showed that “data mining” has remained the prevalent term in academia, while “analytics” has become more widely adopted in industry over the past decade (KDNuggets, 2011). It appears that “data mining” has a more invasive connotation, whereas industry has carefully marketed analytics as being progressive and associated with intelligence and business value. While analytics may have made data mining more socially acceptable, the underlying privacy concerns, which big data’s characteristics such as volume and variety amplify, remain. We know about many high-profile examples of privacy infringement in both industry and academic contexts, including one where the father of a teenage daughter discovered that she was pregnant owing to Target’s highly accurate marketing efforts (Hill, 2012). Similarly, Facebook’s intentionally manipulating users’ moods has raised many critical questions (Kramer, Guillory, & Hancock, 2014; Agarwal & Dhar, 2014). Often times, organizations tout “informed consent” and upfront notice as an answer to critics; yet, as Barocas and Nissenbaum (2014, p. 32) note, “upfront notice is not possible because new classes of goods and services reside in future and unanticipated uses”. Identifying acceptable levels of intrusion, and finding the right balance (and the principles of balancing) between insights obtained due to access to big data and the infringement such access results in is an important area of inquiry for IS scholars. Similarly, it is virtually unavoidable that big data will continue to see large breaches. With the rise of self-service analytics and with access to sensitive data more pervasive than ever in and across organizations, security behavior challenges regarding compliance, insider threats, and so on will grow as important areas of research. Big data analytics involves deriving knowledge and gaining insights. The pursuit of knowledge has always had a close relationship with ethics. One primary component of ethics already mentioned is privacy considerations. Beyond privacy, firms and researchers leveraging big data may consider deontological ethical criteria of consistency, equality, accountability, integrity, and an all-around conscientiousness (e.g.,\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xi\n",
              "\n",
              "Chatterjee, Sarker, & Fuller, 2009a, 2009b). These issues underlie much of the concerns related to the “transparency paradox 3” and the “tyranny of the minority 4”.\n",
              "\n",
              "6.3\n",
              "\n",
              "Behavioral IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "As industry moves towards self-service analytics using big data, several research questions arise, such as the impact of sources and collection methods on data credibility, the impact of access to knowledge on employee satisfaction and knowledge transfer, and organizational norms for knowledge access and transfer (Chatterjee & Sarker, 2013). Further, as big data introduces novel IT artifacts that support large-scale, selfservice, real-time analyses and decision making from vastly integrated enterprise-wide analytics, behaviors and perceptions remain critical to the process of effectively converting knowledge to appropriate decisions and actions. These emerging data sources, decision making processes, and IT artifacts present an opportunity to revisit questions related to constructs, such as trust, leadership, knowledge transfer, and decision making. Along these lines, Davenport and Harris (2007) analyze numerous successful organizations to derive key elements of the “anatomy of an analytical competitor”. Scholars have used these traits to categorize organizations’ capability maturity levels from aspirational to transformed (LaValle, Lesser, Shockley, Hopkins, & Kruschwitz, 2011). Two of the important differentiating elements they identify are people and culture. With respect to these two elements, Davenport and Harris wonder: how does analytical leadership emerge? What are the characteristics of analytical executives? What role do different c-level executives play? What if executive-level commitment is lacking? What traits of big data analysts make them effective? How do organizations transform from intuition-based decision making to a data-driven decision making paradigm? Edward Deming is often falsely attributed the famous quote “In God we trust, all others bring data”. As we mention earlier, it is often (naively) believed that data and algorithms result in what is necessarily just and true. However, the importance of intuition and judgment, especially in uncertain environments cannot be underestimated (Davenport, 2006; Yetgin, Jensen, & Shaft, 2015). Given that there is a natural tension between data and intuition (Davenport & Harris, 2007), what is the ideal balance between data and intuition? What kind of theory can inform this balancing act? What role does trust play? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? More broadly, we need to understand if and how we should revise existing decision making models (e.g., Hodgkinson & Starbuck, 2008) to reflect how decisions are actually made in organizations using big data and big data analytics. Also, given people and culture can potentially act as impediments to the adoption of big data analytics in organizational settings, what theories and models are appropriate for avoiding implementation failures due to human and cultural issues? From an HCI standpoint, as dashboard-based visualizations become the norm for “managerial cockpits”, we need to investigate what the implications of the four Vs are on how users handle cognitive loads resulting from big data. Finally, we need to conduct balanced assessments of outcomes of the implementation/adoption and use of big data (i.e., big data infrastructure and big data analytics). We need to conduct critical, intensive assessments of the actual impact of big data investment and use and understand if and how one can attain instrumental benefits (such as performance and profitability) and humanistic benefits (such as empowerment and freedom). In other words, big data offers the opportunity to re-examine some of the same phenomena pertaining to decisions and actions that behavioral IS researchers have examined over the years for different waves of technological innovations.\n",
              "\n",
              "6.4\n",
              "\n",
              "Summary of Sample Big Data Research Opportunities for Behavioral IS\n",
              "\n",
              "As Table 1 shows, behavioral researchers have numerous opportunities to contribute to scholarship on big data. Some potential areas include epistemological reflections and methodological development, contributions to computational social science, and reformulations of existing theories and development of new theories of human decision making/human behaviors for the new data abundant environments using traditional or newer computational approaches. In particular, privacy, security, and ethics of big data have significant implications\n",
              "\n",
              "3\n",
              "\n",
              "4\n",
              "\n",
              "This paradox is related to the observation that “datafication” (Lycett, 2013) is assumed to make processes transparent, yet the ways in which data is collected and mined remains unknown/inaccessible to the public (Richards & King, 2013). This is related to the idea that companies tend to make inferences and generalizations based on data from the minority who agree to share data and, thereby, silence (i.e., make irrelevant) the views of those who refuse to give consent (Barocas & Nissenbaum, 2014, p. 32).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "and, hence, deserve special attention. Developing in-depth consultable case studies that capture the intricacies of applying big data approaches in complex social environments would also be of value. Note that the areas listed below in the table are not an exhaustive list of possible research avenues. Rather, they signify the breadth of possibilities and directions that have opened up from this interest in, and trend toward harnessing, big data.\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities Value chain stage(s) Possible research topics • • Deriving knowledge, decisions, and actions Epistemological concerns • • Possible areas of inquiry What implications does big data have for the traditional deductive scientific research model? Is an alternative big data-driven “inductive mode of science” in development or even feasible in the IS discipline? If so, how can we ensure the validity of such knowledge? If not, how can one use the inductive mode in conjunction with the traditional deductive model? What is the role of prediction versus explanation in big data research? How must one adapt traditional research methodologies/methods (qualitative and quantitative) to investigate phenomena of interest in big data environments? How can longitudinal big data channels, such as social media, mobile location, and Web clickstreams, enhance our understanding and explanation of user behaviors? How can sentiments and affects appearing in user-generated content, such as online word-of-mouth, inform our understanding of user behaviors and intentions? What can large online social networks reveal about patterns of influence and/or information propagation? What new insights can work logs and other unstructured sources reveal about relationships between employee actions and employee productivity, satisfaction, and/or customer-oriented outcomes? What are the threats to validity of knowledge computationally derived, and what are the ways to mitigate these threats? What is the nature of theory or theorizing that is consistent with this form of research (i.e., computational social science)? What are the principles by which one can manage invasiveness/infringement of privacy in business enterprises, in academia, and in wider society? How can big data contribute to a better understanding (and resolution) of the privacy paradox in which, on one hand, users desire personalization, innovative technologies, and novel communication channels, and, on the other hand, seek privacy and anonymity? Since informed consent is often seen as ineffective, what other controls/policies need to be put in place? How can we assess and ensure their effectiveness? What implications does big data have for consistency, equality, accountability, integrity and an all-around conscientiousness? What is the impact of sources and collection methods on data credibility? How might access to information and knowledge attained from big data affect employee satisfaction and performance? What should the organizational norms be for access and transfer of knowledge attained through big data analytics? What would be the key elements of an ethical code for organizations and analysts/data scientists using big data and big data analytics? How do organizations/individuals/groups actually make decisions in the big data environment? To what extent do traditional decision making models hold in the new environment?\n",
              "\n",
              "• • • • • • • • Privacy and security concerns •\n",
              "\n",
              "Computational social science\n",
              "\n",
              "Deriving knowledge\n",
              "\n",
              "• • • • • • Nature of decision making •\n",
              "\n",
              "Other ethical considerations\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xiii\n",
              "\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities • • • • Organizational culture and governance • • How does analytical leadership emerge? What are the characteristics of analytical executives? What role does c-level leadership play in firms’ abilities to leverage and “compete on big data analytics?” How do organizations transform from an intuition-based decision making culture to a data-driven decision making culture? What is the role of IT departments in supporting big data analytics? What big data technology architectures and configurations (or analytics techniques) fit with different types of organizational cultures (and governance)? What are the capabilities and constraints of big data information sources in supporting cognition and decision making? As dashboard-based visualizations become the norm for managers, what are the implications of the four Vs on users’ cognitive load and decision making performance? What is the ideal balance between data and intuition? What does trust in big data mean? What role does trust (e.g., trust in big data, people, and processes) play in adopting and effectively using big data? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? What are the key personality traits of good analysts and how do these traits impact data and technology usage and performance? How do we assess big data initiatives when considering the perspectives of relevant stakeholders and the potential instrumental and humanistic outcomes?\n",
              "\n",
              "Leadership\n",
              "\n",
              "• Cognition and usability •\n",
              "\n",
              "Trust and big data versus intuition\n",
              "\n",
              "• •\n",
              "\n",
              "• Adoption and adaptation of big data techniques • and technologies Big data outcomes •\n",
              "\n",
              "7\n",
              "7.1\n",
              "\n",
              "A Big Data Research Agenda for Design Science Research\n",
              "Paradigmatic Considerations for Design Science Research on Big Data\n",
              "\n",
              "We begin by discussing broader paradigmatic considerations of big data on design science; namely, 1) the effects of focusing on IT artifacts that emphasize information more than systems/technology and 2) implications of big data analytics artifacts on kernel design theories. Design is a product and a process (Walls, Widmeyer, & El Sawy, 1992; Hevner, March, Park, & Ram, 2004). The design product is a construct, model, method, and/or instantiation. The design process involves an iterative cycle of “test and learn” or “build and learn, evaluate and learn” (Simon, 1996; Nunamaker, 1992). In big data analytics, the process of deriving knowledge and insights from data is, in some ways, analogous to the design process. The most prevalent process for guiding analytics is the cross-industry standard process for data mining (CRISP-DM) (see Figure 6). CRISP-DM involves iteratively performing several phases 1) problem/business understanding, 2) data understanding, 3) data preparation, 4) modeling, 5) evaluation, and 6) deployment (Chapman et al., 2000). The first phase emphasizes the importance of tackling significant problems/opportunities and identifying data mining goals/success criteria. The second phase involves an inventory of available data sources, including assessing data quality. The third phase involves selecting appropriate sources and specific variables, and cleaning and reformatting the data. The fourth phase includes using predictive, descriptive, or prescriptive analytics method to analyze the data (Chandler et al., 2011). The fifth phase involves evaluating models and their connection to problem/business outcomes. Finally, the sixth phase involves analyzing the artifact in the field outside the lab/production environment. CRISP-DM is to data mining what the system development lifecycle (SDLC) was to traditional information systems development, with CRISP-DM similarly appearing in the introduction section of various data mining and business analytics textbooks. The commonalities between CRISP-DM and the traditional system design process (Simon, 1996; Nunamaker, 1992) presents great potential for design science research geared towards producing novel IT artifacts capable of deriving knowledge and insights from big data. For instance, following the CRISP-DM design process, new constructs, models, methods, and instantiations could enhance BI dashboards or predictive, descriptive, prescriptive, and/or diagnostic model-based technologies (Shmueli & Koppius, 2011; Chandler et al., 2011). Recently, we have already begun to see research in this\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "xiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "vein with new predictive IT artifacts developed using the design science tradition that espouses informal connections to elements of CRISP-DM (Abbasi, Zhang, Zimbra, Chen, & Nunamaker, 2010). Comparing and contrasting CRISP-DM with SDLC is also interesting for another reason. One can consider big data artifacts as a shift in the design science tradition toward more significantly emphasizing artifacts that support information relative to systems or technology. For instance, only one of the stages depicted in CRISP-DM pertains to actually deploying the artifact, whereas at least four stages relate to processing, modeling, and evaluating data/information. CRISP-DM barely emphasizes the key stages of SDLC: users’ system-related requirements, post-deployment system implementation, and system usage/evaluation. While the implications of this shift are not necessarily epistemological in nature, it does introduce paradigmatic considerations for design science. For instance, what is the appropriate balance between information and systems/technology in design research geared toward big data? Furthermore, how does research on the design of information-centric big data IT artifacts relate design science (in IS) to information and computer science? Our discipline has a clear need to have conversations on this issue.\n",
              "\n",
              "Figure 6. The CRISP-DM Analytics Process Embodies Similar Intuitions to the Iterative Design Process Advocated in the IS Design Science Tradition\n",
              "\n",
              "For predictive analytics, big data has also had an impact on the kernel theories providing design guidelines. Classical theories of statistics such as the information theory and Bayes theorem embody simple yet powerful knowledge that have guided the design and development of some of the commonly used predictive and descriptive analytics methods traditionally employed in industry and academia. However, with the growth of big data in recent years, the four Vs introduce various challenges for data mining methods grounded in these traditional statistical theories. The volume of data presents computational constraints. The sparsity and structural nuances of new varieties of data sources, such as text and multimedia, poses representational richness issues. Some have also found traditional statistical methods to be susceptible to veracity concerns. Data-mining methods grounded in the statistical learning theory overcome many of these limitations (Vapnik, 1998) and attain unprecedented results for tasks such as image recognition, text mining, phishing detection, and fraud classification. Consequently, despite appearing in the past 15 to 20 years, Vapnik’s research on the statistical learning theory has already garnered over 150,000 citations according to Google Scholar, with more than half of those citations appearing in the past five years. One can largely attribute the success of statistical learning theory-based methods to their having a strong theoretical foundation and robust analytical power that is well suited for big data. However, other big data analytics methods, such as deep learning (LeCun, Bengio, & Hinton, 2015), do not have significant theoretical underpinnings. Indeed, critics have argued that they provide power without the underlying statistical theory (Gomes, 2014). As design research continues to explore the dichotomy between prediction and explanation (Shmueli & Koppius, 2010), the precise role of kernel theories in big data IT artifacts will remain a central question. A key question is: should IS research adopt an “ends-justify-the-means” perspective in which predictive power trumps methodological\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xv\n",
              "\n",
              "transparency and explanatory potential? We discuss the broader “prediction versus explanation” issue in greater depth in Section 11.\n",
              "\n",
              "7.2\n",
              "\n",
              "Design Science Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Setting aside the aforementioned paradigmatic considerations of designing big data IT artifacts, design science has much to offer in the burgeoning realm of predictive analytics, including novel constructs, models, methods, and instantiations leveraging big data. Scholars in the IS community have also applied predictive analytics at both “micro” and “macro” levels of granularity (Brown, Abbasi, & Lau, 2015). Chang et al. (2014) refer to this range of data granularities as the micro-meso-macro data spectrum. One of the most exciting opportunities pertains to predicting/analyzing micro-level outcomes (Agarwal & Dhar, 2014). Specifically, Brown et al. (2015, p. 6) note: Micro-level predictive analytics involves making inferences about future or unknown outcomes pertaining to individual firms, people, or instances. Micro-level prediction contains greater intraentity information, including perceptual constructs, [transactions/logs on] individual behavior, and spatial-temporal indicators. We have already begun to see some exciting, cutting-edge examples of micro-level prediction. For example, Wang and Ram (2015) predicted individuals’ sequential purchase patterns using spatial, temporal, and social relationship features on a test bed encompassing three million transactions initiated by thirteen thousand customers from nearly three hundred locations. This and other related studies highlight the “art of the possible” and the “art of the valuable” for IS research on novel predictive design artifacts. Future IS research could leverage objective (e.g., observed transactions and logs) and perceptual (e.g., survey, sentiment, voice transcript, and interview) data in conjunction with various intermediate decisions and actions to predict individuals’ behaviors with applications in marketing, e-commerce, security, health, and finance (Abbasi, Lau, & Brown, 2015). There is no doubt that user-generated content sources such as social media have enhanced our understanding of various micro and macro-level phenomena in recent years (Chen et al., 2012), which presents great opportunity for developing social media analytics artifacts for collecting, monitoring, analyzing, summarizing, and visualizing social media data (Zeng et al., 2011). However, a major challenge remains in ensuring high veracity of such data sources. As Zeng et al. (2011, p. 14) note: “issues such as semantic inconsistency, conflicting evidence, lack of structure, inaccuracies, and difficulty in integrating different kinds of signals abound in social media”. We need IT design artifacts capable of identifying, quantifying, accounting for, and alleviating veracity concerns in information sources such as social media by assessing key information quality dimensions such as usefulness, relevance, and credibility. Examples of preliminary research in this vein include work pertaining to online spam and deception detection (Zhang et al., 2014). Such artifacts can be potentially beneficial in various application areas including marketing, finance, public policy, and health (Zeng et al., 2011; Abbasi & Adjeroh, 2014). Big data presents numerous opportunities for new design-oriented work pertaining to the earlier stages of the value chain; namely, data and information. There is a long-standing tradition of design science work on modeling formalisms and ontologies (Wand & Weber, 2002). New forms of user-generated content present opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. A related area involves extending classification principles from conceptual modeling to modeling of information categories in big data, which forms the basis for many forms of predictive and descriptive analytics (Parsons & Wand, 2013). Embley and Liddle (2013) expect conceptual modeling to address big data challenges by adopting the perspective that the design activity related to big data is fundamentally about structuring information. Conceptual modeling research should make big data’s volume searchable, harness the variety uniformly, mitigate the velocity with automation, and check the veracity with application constraints. The IS research community needs to direct some attention to the tasks of investigating, addressing, and/or exploiting big data’s four Vs jointly and effectively. Similarly, database design and data integration have been a major area of focus for prior works (Storey et al., 1997), and there are opportunities for research on the integration of a variety of structured and unstructured data sources available in organizational settings. As we highlight earlier when discussing the big data information value chain, the four Vs have increased the complexity of managing, storing, and integrating data. The challenge remains in investigating and establishing an acceptable architecture to integrate, manage, and implement both structured and unstructured data under one unified platform. The\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "traditional relational data model and the corresponding relational database management systems (RDBMSs) cannot meet the heterogeneity (variety) challenge of big data. Many consider NoSQL as the potential data management solution for big data, but many architectural alternatives exist that range from Hadoop/Spark to Hadoop and RDBMS in parallel to Hadoop (for unstructured data) inputting into RDBMS (Heudecker, 2013; Buytendijk, 2014). We need research to examine the feasibility, fit, and business value of such alternatives and to provide guidelines for big data architectures based on organizational and industry-level contexts. For design research pertaining to knowledge derivation and representation, big data presents both advantages (i.e., volume and variety) and disadvantages (i.e., velocity and veracity). With the large volume and a variety of data sources, big data can certainly enhance ontology learning by automatically deriving domain knowledge by mining unstructured and semi-structured data (Buitelaar, Cimiano, & Magini, 2005). For example, user-generated content contributed freely in social media presents opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. As we allude to earlier in this section, some IS scholars have recently suggested that domain ontologies may play a critical role in the conceptual model for managing and implementing big data (Embley & Liddle, 2013).\n",
              "\n",
              "7.3\n",
              "\n",
              "Design Science Research on Supporting Decisions and Actions from Big Data\n",
              "\n",
              "Scholars have long used design science to guide the design and development of various decision support systems, including group support systems, recommender systems, personalization, contextualization, and collaboration technologies (Nunamaker et al., 1991; Adomavicius & Tuzhilin, 2005; Arnott & Pervan, 2012). They have also used it to develop BI-related DSS artifacts (Chung, Chen, & Nunamaker, 2005). More recently, Lau et al. (2012) developed ABIMA, a big data business intelligence DSS for mergers and acquisitions. ABIMA integrates large volumes of financial metrics derived from structured databases with unstructured sources, such as financial news articles, search engine results, and documents crawled from the Web. The system, practitioners specializing in mergers and acquisitions evaluated, is the type of research that epitomizes how one can use design science for research on big data DSS. Other types of big data IT artifacts that support the decision making process could be ones designed to support real-time decision making that possibly incorporate user feedback-based or system-generated credibility assessments of underlying information sources (Jensen, Lowry, Burgoon, & Nunamaker, 2010; Jensen, Averbeck, Zhang, & Wright, 2013). Given that big data analytics significantly emphasizes enhancing business processes (Davenport, 2006), business process improvement driven by big data constitutes an important research area (Baesens et al., 2014). Potential avenues include developing automated artifacts for discovering and optimizing processes and employing analytics-driven methods in various internal-facing and external-facing business processes that range from operations and human resources to customer relationship management (Davenport & Harris, 2007). As an extreme example of automation, the use of big data analytics to replace human involvement from certain business processes has already begun to take shape (Davenport & Kirby, 2015); in Section 4, we mention real-time analytics pipelines that are replacing traditional business processes. However, in many contexts, big data analytics provides complementary “augmentation” to human-driven processes (Davenport & Kirby, 2015). Augmentation and automation signify a departure from the traditional human-centered computing paradigm toward autonomous computing albeit with varying degrees of separation depending on the level of human involvement. Nevertheless, this shift necessitates reconsidering guidelines for the design product and design process associated with such artifacts (e.g., requirements gathering in contexts where there are no users). When designing such artifacts, what role might theories and principles from, for example, the cognitive computing and artificial intelligence literature play? Given the dynamic and nascent technological and organizational environments related to big data, it would be interesting to see if (and how) one could productively use the action design research (ADR) method to develop robust big data IT artifacts (Sein et al., 2011). For instance, action design research incorporates provisions for varying levels of end user involvement that could provide the necessary flexibility for designing big data artifacts in contexts ranging from traditional user-centered decision support to business process augmentation/automation.\n",
              "\n",
              "7.4\n",
              "\n",
              "Summary of Design Science Research on Big Data\n",
              "\n",
              "Big data presents several wicked problems: how should IS researchers balance a big data-oriented design science research agenda with those being pursued in reference disciplines such as engineering, statistics, and computer science? Goes (2014, p. iv) touches on this challenge by noting that at least five different\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xvii\n",
              "\n",
              "departments at his institution were, in some way, explicitly related to big data research. Goes cautions us by noting that, without guidelines for shaping the IS big data research agenda, “each unit can contribute to the big data paradigm, but at present the approach resembles that well-known cartoon of making sense of an elephant by grabbing isolated parts of the animal”. In our assessment, we can say that, relative to other disciplines, IS design science researchers are uniquely positioned to provide the appropriate mix of rigor along with humanistic and instrumental relevance. Further, our research often seeks to offer generalizable design principles and guidelines abstracted from the development of contextualized big data IT artifacts that can potentially help address other important problems. The sample research opportunities in Table 2 reflect this view. We believe the design science perspective on big data analytics represents an important future area of emphasis for IS research.\n",
              "Table 2. Big Data and Design Science Research: Sample Research Opportunities Value chain stage(s) Possible research topics Possible areas of inquiry • What is the appropriate balance between information and systems/technology in design research geared toward big data in the IS discipline? • What implications does big data IT artifacts’ potential shift in focus from systems to information have for the design process? • How might the characteristics of big data affect the nature of kernel design theories that are potentially useful? • What is the IS “signature” for big data design research (i.e., what is the scope/nature of big data IS design artifacts relative to reference disciplines such as computer science, marketing, engineering, and statistics)?\n",
              "\n",
              "Deriving knowledge, decisions, and actions\n",
              "\n",
              "Paradigmatic considerations\n",
              "\n",
              "• How can one leverage the volume and variety of big data to develop novel artifacts for predicting/describing macro versus individual/micro-level phenomena or events? Novel artifacts for • How can design science research build novel artifacts for deriving knowledge prediction or from big data sources, such as user-generated content, to advance research in description other disciplines, including marketing, finance, and health? • How can design guidelines of big data analytics artifacts better compensate for the veracity of input data? What novel veracity-assessment artifacts can we develop to shed light on information relevance, usefulness, and credibility? Deriving knowledge Modeling formalisms and integration artifacts • Can new forms of user-generated content enrich existing ontologies, enable the development of new ones, and introduce new conceptual models and grammars? • What is the potential for extending classification principles from conceptual modeling to modeling of information categories in big data? • Can conceptual modeling address some of the challenges of big data by making the volume searchable, harnessing the variety uniformly, mitigating the velocity with automation, and/or checking veracity with application constraints? • How can design science inform the state-of-the-art integration, management, and implementation of organizational big data initiatives in light of the four V challenges? • What design theories do we need to guide big data architectures based on organizational and industry-level contexts?\n",
              "\n",
              "• How can IS contribute guidelines for design artifacts that support real-time Novel IT artifacts decision making from big data? for decision • What is the role of credibility assessment as a design guideline in big data support decision support systems? • What is the potential for automated process discovery and optimization? Decisions Business process • How can big data analytics improve business processes? and actions improvements • Are existing design theories sufficient for real-time big data analytics and automation environments in which run-time and autonomy considerations create nuanced design requirements? What are the alternative theories that may be valuable? Big data action design research • Given the emerging nature of big data, how can we use approaches such as action design research (ADR) to guide the development and harnessing of big data IT artifacts in organizational settings?\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "8\n",
              "8.1\n",
              "\n",
              "A Big Data Research Agenda for the Economics of IS\n",
              "Epistemological Concerns for Big Data and the Economics of IS\n",
              "\n",
              "The economics of big data has important implications for information systems. Just as scholars once used the “economics of information” to describe the value of information asymmetries in marketplaces (Stigler 1961), now, with firms competing on analytics, access to information that can enable enhanced analytical capabilities and insights facilitating differentiation has ushered a new era of “knowledge is power”. Quantifying this power is critical. Beyond some of the issues discussed in Section 6.1, the epistemological implications for the economics of IS community primarily relate to research methodology. These include the deflated p-value problem (Lin, Lucas, & Shmueli, 2013), increasing emphasis on prediction versus explanation (Shmueli & Koppius, 2010), and construct validity when using unstructured and log-based data sets; however, given that many of these issues are applicable to multiple IS research traditions, we discuss them in greater detail in Section 11.\n",
              "\n",
              "8.2\n",
              "\n",
              "Economics of IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "The value of information has been a longstanding area of inquiry in the economics of IS tradition (Banker & Kauffman, 2004). In the context of big data, assessing information’s value is more critical than ever. One research direction analyzes the relative value contributions of the four Vs (e.g., value of data volume and variety) for deriving knowledge from big data. As organizations treat data as an asset (and, in many Web 2.0 business models, as the primary asset), quantifying its value has become a major discussion topic both from a broad business value perspective (which includes the implications for third party data brokers and data markets) and from a more traditional accounting perspective. This emphasis on data as an asset has spurred infonomics: the theory, study, and discipline of assigning economic significance to information. For instance, a recent McKinsey report states that public data sources pertaining to education, energy, healthcare, transportation, and consumer finance (collectively dubbed “open data”) have the potential to create USD$3 trillion in annual business and/or societal value (Manyika et al., 2013). At a more micro level, individual organizations are routinely interested in identifying the most useful public/private data sources and quantifying the precise value of these sources. For example, Bardhan, Oh, Zheng, and Kirksey (2015) used demographic, clinical, and administrative data from 67 hospitals in northern Texas gathered over a four-year period to build models capable of predicting and describing congestive heart failure patient readmissions. Their model demonstrates the importance of health IT-related variables, which prior or baseline models have not considered impactful but that could help hospitals save millions of dollars by avoiding costly readmission-related penalties. Many other recent studies also suggest that using more data instances and variables can improve predictive capabilities (Junque de Fortuny et al., 2013). These findings raise questions regarding complexity, model management, and cost-benefit tradeoffs. Researchers wonder: if bigger is better, how much is too much? As Junque de Fortuny et al. (2013, p. 219) ask, “is it worth undertaking the investment to collect, curate, and model from larger data sets?” Prior to big data’s rise, firms were beginning to derive the last iota of predictive power from a set of data they had access to often by building predictive models that were increasingly complex. Netflix is one example. In 2006, Netflix offered a US$1 million prize to any team that could improve their existing movie recommendation models by 10 percent. The competition concluded on July 26, 2009. Over the final two years, the performance of the best predictive models improved by less than 2 percent, whereas the complexity of the solutions increased dramatically. The final winning model from a team appropriately named BellKor’s Pragmatic Chaos blended results from hundreds of underlying base models. Similarly, scholars commonly describe loan risk-assessment models at financial services firms developed in the past two decades, which largely leverage the same structured data sources, as complex, variegated black-box arrangements of models on top of models delicately combined (Derman, 2011). Former Goldman Sach’s Lead Quantitative Analyst Emmanuel Derman highlights the pitfalls of complexity and poor model management over time in his book Models Behaving Badly. He describes the role played by complex models in the 2008 financial crisis (Derman, 2011); the narrative reminds readers of the classic saying “complexity is death”. Big data creates an opportunity to not only enhance these models’ analytical capabilities but also reduce the inherent risks associated with using them. However, not all information sources are created equal. A key challenge facing organizations is finding a way to quantify the value of information that considers both insightfulness and risks.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xix\n",
              "\n",
              "Research on pricing for data sets/data sources in the booming data broker markets can help firms make more-informed decisions in the data marketplace. Here, all of the standard services management questions apply, including those that Rai and Sambamurthy (2006) articulate. What are the best strategies for bundling of data? Should firms pursue flat or usage-based monthly service rates versus one-time sales? How do alternative service rate plans impact data usage? Big data’s impact on data-based marketing and pricing, omni-channel marketing (Song, Sahoo, Srinivasan, & Chrysanthos, 2014), and attribution are other potential areas of inquiry. The value of information also raises questions about intellectual property rights, especially in the context of user-generated content. The cost of veracity can potentially offset the value of data volume and variety. The existing body of knowledge on data warehousing and business intelligence, which one can consider a close predecessor to big data, has emphasized the importance of data quality as an antecedent for the success of data warehousing initiatives (Wixom & Watson, 2001). Hence, quantifying the adverse effects of incomplete or inaccurate information is essential yet challenging for mitigating risk in the era of big data analytics. Such research could connect data quality to the effectiveness of business outcomes. The analysis of social media has garnered considerable attention from the economics of IS community in recent years with many outstanding avenues of inquiry. As organizations move towards the “socialecosystem” encompassing the use of social media for various employees and customer-oriented activities, we can ask how firms can leverage social media for internal communication and collaboration, external engagement, and listening/ideation (Zeng, Chen, & Lusch, 2010). What is the value of insights, engagement, and internal communication usage through social technologies? The interplay between social media channels and marketing effectiveness is another important area receiving considerable attention in the economics of IS community (Song et al., 2014). In that vein, questions include: what are the key factors impacting social media marketing effectiveness? How does peer influence impact social media marketing? What is the role of social media in viral marketing? A recent related stream of work examined the usefulness of location and geographic information for the analysis of choice, price, competition, and mobile marketing.\n",
              "\n",
              "8.3\n",
              "\n",
              "Economics of IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "The economics of big data analytics extend further down the information value chain beyond knowledge acquisition to decisions, actions, and their ensuing consequences. In his book entitled The Value of Business Analytics, acclaimed business analytics guru Evan Stubbs talks about the challenges business analysts and data scientists face when attempting to quantify the value of an analytics project or portfolio of initiatives (Stubbs, 2011). For example, when should firms invest in big data, and what are the potential returns? As one answer to this question, Tambe (2014) found that firms with significant existing data sets who invested in Hadoop were associated with 3 percent faster productivity growth. A related question is: what implications does big data’s ushering in the increased usage of cloud-based SaaS and DBaaS have for platform economics and strategy? Research on quantifying the business value of big data analytics, including the implications of the four Vs, could inform the existing body of knowledge. Potential research directions include work measuring the return on investment for big data technologies, the impact of data variety and veracity on quality of decision making, and the economics of real-time decisions using big data.\n",
              "\n",
              "8.4\n",
              "\n",
              "Summary of Economics of IS Research on Big Data\n",
              "\n",
              "In some ways, the economics of IS tradition is ideally suited to tackle problems pertaining to deriving knowledge from big data. In particular, beyond certain methodological adjustments (see Section 11), the empirically driven, inductive reasoning-oriented applied econometrics approach is well aligned to addressing these types of questions. The economics of IS also has an essential role to play in examining the economic value of big data insights and big data analytics-driven decision making. Table 3 summarizes some of the important research opportunities.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Table 3. Big Data and Economics of IS: Sample Research Opportunities Value chain stage(s) Deriving knowledge, decisions, and actions Possible research topics Possible areas of inquiry\n",
              "\n",
              "• How must traditional research methods be adapted to investigate big data Epistemological environments? and/or • What is the role of prediction versus explanation in big data research? methodological • How can we ensure the validity of constructs derived from noisy unstructured concerns and log-based data sources? • What is the value of various data sources and channels in terms of quality of insights, enabling new capabilities, and quantifiable business value? • Regarding the impact of data volume on insights and business value, how much is enough and how much is too much? • What is the value of data volume and variety from a risk management perspective? • As data becomes an asset, what role can third party data brokers and data markets play? What are the pricing and market structure implications? • As firms monetize user-generated content, what are the implications for intellectual property? • How can the volume and variety of data inform data-based marketing and pricing? • What are the benefits and challenges of data variety for omni-channel marketing analysis and attribution? • How can we quantify the business impact of low veracity data? • Which types of data quality issues are the most impactful?\n",
              "\n",
              "Value of data, volume, and variety\n",
              "\n",
              "Deriving knowledge Cost of veracity\n",
              "\n",
              "• What is the role of social media in enterprises for internal communication, customer engagement, and listening/ideation? • Regarding the economics of social media, what is the value of insights, Social media and engagement, and internal communication usage through social technologies? economics of IS • What are the key factors that impact social media marketing effectiveness? • How does peer influence impact social media marketing? • What is the role of social media and incentive schemes in viral marketing? Impact of location and geography • How can location and geographic information impact research on choice, pricing, and competition? • What are the implications of geo-targeting in mobile marketing?\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Quantifying value • What is the impact of data variety and veracity on the quality of decision and impact of four making? Vs on decision • What are the key factors influencing business value in the context of real-time making decision making using big data? • How do we measure the return on investment for big data technologies? Value of big data • When should firms invest in big data, and what are the potential returns? IT artifacts • As big data ushers in increased usage of cloud-based SaaS and DBaaS, what are the implications for platform economics and strategy?\n",
              "\n",
              "9\n",
              "\n",
              "Cross-tradition Research on Big Data\n",
              "\n",
              "There are many opportunities for research at the cross-sections of behavior, design, and economics of IS. In some respects, big data’s scale and complexity afford and encourage cross-tradition research projects. The work on human-computer systems design has traditionally been at the intersection of design science and behavior, such as cognitive psychology and decision science (Banker & Kaufmann, 2004). In this vein, one obvious direction is to design and develop big data IT artifacts that researchers may subsequently evaluate in terms of their positive impact on behavior (Zahedi, Abbasi, & Chen, 2015). Another connection between the design and behavioral traditions relates to research on IS strategy. Big data analytics ultimately focuses on improving business processes (Davenport & Harris, 2007) to attain a strategic competitive advantage, a perspective that is highly congruent with the resource-based view of the firm. Designing enterprise-wide big data analytics in a manner that maximizes the potential for competitive advantage in different types of industries and for different organizational cultures and governance archetypes is a potentially valuable direction. Related to this area, knowing how to align alternative big data architectures\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxi\n",
              "\n",
              "(such as ones based on Hadoop, traditional RDBMS, or hybrid models) with business strategy and a firm’s data environment and understanding important criteria and success factors in the decision making process remain important issues. Researchers can use established approaches in IS such as case studies, laboratory experiments, and survey studies in investigating such topics. Design science and the economics of IS also appear to have potential in the context of big data research. One example is cost-sensitive classification or regression in which one quantifies the values of true and false positives/negatives and incorporates them into the design of predictive artifacts (Bansal, Sinha, & Zhao, 2008; Zhao, Sinha, & Bansal, 2011). Here, existing work has mostly focused on the predictive artifact and less on the methodology for deriving cost matrix values. The value of information in big data IT artifacts represents another important area at the cross-section of design and economics of IS. Recently, many studies have designed novel IT artifacts focused on mining big data sources as decision-support aids (e.g., Lau, Liao, Wong, & Chiu, 2012). It remains unclear what the business value of such artifacts truly is with respect to key business performance indicators. Another already potent research area at the cross-section of design and economics of IS pertains to optimization (Banker & Kaufmann, 2004). Here, big data’s variety presents opportunities. For instance, online product reviews could possibly enrich product design optimization (e.g., Balakrishnan & Jacob, 1996). Similarly, consumer sentiments and demand forecasts based on user-generated content could enhance pricing optimization; however, in both of these examples, the tension between the value and veracity of social media and other user-generated data sources could present an interesting dynamic, worthy of an in-depth inquiry. Big data presents numerous opportunities at the intersection of economics of IS and behavior as well. For instance, behavioral economics is a well-established area (Tversky & Kahneman, 1974). In the context of big data, research could examine managers’ propensities to combine data of varying quality/credibility with, for instance, intuition in real-time versus non-real-time settings. Such work would borrow from theories in economics, cognitive psychology, decision making, and risk management (Banker & Kaufmann, 2004).\n",
              "\n",
              "10 Big Data: Implications for Theory\n",
              "Many scholars have reflected on big data’s possible implications on theory. One point of view, albeit extreme, is that big data renders the role of theory—sometimes seen as fictional and value-laden— unnecessary and obsolete, and replaces it with patterns derived directly from data that reflect nothing but the truth (e.g., Kitchin, 2014b). On the other hand, many scholars argue that, in the absence of theory, data lacks “order, sense and meaning” and that “theories without data are empty; data without theories are blind” (Harrington, 2005, p. 5, cited in Sarker et al., 2013, p. xiiii). While this debate is likely to continue without immediate resolution, we do not foresee theories disappearing or diminishing in importance because of research using big data. To the contrary, we foresee that some of the theories will become more robust because “researchers now have a medium for theory development through massive experimentation in the social, health, urban, and other sciences” (Agarwal & Dhar, 2014, p. 444). This is in part due to easier data collection and enhanced control and precision, realism, and generality associated with big data (Chang et al., 2014). At a broader level, we believe that scholars have the opportunity to reflect on the changing nature of the theorizing process and on the characteristics of theories developed in a data-abundant environment. From an information value chain perspective, some recent big data IS studies and editorials have touched on the role of theory when leveraging big data sources for discovering knowledge. As previously mentioned, we believe that, in addition to big data “information sources”, big data’s characteristics embodied in the four Vs afford important opportunities for research, that can both borrow from novel theories and contribute to existing theories, related to deriving knowledge as well as decisions and actions. Below, we outline a few examples. The impact of data variety and velocity on problem-solving accuracy and time constitutes an important research topic. The cognitive fit theory (CFT) provides an excellent and robust theoretical lens for examining this topic. Earlier work on CFT examined the importance of congruence between problem task and problem representation on users’ mental representation and overall problem-solving performance (Vessey, 1990). While initial studies focused on tables versus graphs, subsequent work extended CFT to other specialized representations such as maps (Dennis & Carte, 1998), and considered the impact of users’ prior domain knowledge and the effect of subtasks (Shaft & Vessey, 2006). All of these findings have important implications when examining the impact of big data characteristics for problem solving in general, and specifically in the context of dashboards depicting a variety of information in real-time. On the other hand, big data characteristics, such as variety and velocity, can also potentially offer theoretical extensions. Data\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "visualization dashboards often incorporate multiple tabs with coordinated views depicting real-time data (Andrienko & Andrienko, 2003). The effects of problem solving in such multi-representation, multi-subtask, real-time situations remain unclear, and this offers great potential to contribute to theory. CFT, with suitable adaptations, can also inform the design/construction of novel user interface artifacts (Vance, Lowry, & Egget, 2015) for presenting big data. Organizations using big data routinely ask managers and analysts to monitor and present key findings using reporting tools that integrate traditional structured data sources with novel social listening, web clickstream, sensor-based, and open data. Practitioner studies have suggested that analysts often do not perceive such tools to be useful, with obvious implications for the business value of such artifacts (Kaushik, 2011). Adoption models represent an excellent theoretical lens for examining the impact of perceived usefulness and ease of use on behavioral intention to use such reporting tools and actual use (Davis, 1986, 1989). The effects of mandatory versus voluntary reporting and trust are also important considerations (Brown, Massey, Montoya-Weiss, & Burkman, 2002; Gefen, Karahanna, & Straub, 2003). In turn, big data’s four Vs could provide important insights that can inform the extensive body of knowledge pertaining to technology adoption (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, forthcoming). For instance, the variety of data could have potentially contrasting effects on users’ perceptions of usefulness and ease of use, which users’ levels of experience may moderate. Further, as data veracity becomes increasingly relevant to big data IT artifacts, the implications for trust (in both the data and the artifact) and eventually for behavioral intention to use also present interesting issues to investigate. Chaos theory studies the behavior of dynamic systems that are highly sensitive to initial conditions, where small changes in initial conditions can yield widely diverging outcomes (Gleick, 1987; Sprott, 2003; Werndl 2009). Prior studies have already discussed the potential of big data for theory development via computational social science or massive experimentation (Agarwal & Dhar, 2014; Chang et al., 2014). Chaos theory could be beneficial in macro-level computational social science research, since it “appears to provide a means for understanding and examining many of the uncertainties, nonlinearities, and unpredictable aspects of social systems behavior” (Kiel & Elliott, 1996). Furthermore, as IS design science research explores novel predictive artifacts utilizing big data, chaos becomes an important consideration. The inclusion of big data should make predictive artifacts more accurate, stable, and valuable. However, for complex event forecasting in situations where chaos is present, prediction can be problematic since model assumptions, which are typically based on probabilities of various patterns (i.e., connections between observed initial conditions and eventual observed outcomes), may not hold true (Sprott 2003). Consequently, seemingly small errors in initial condition prediction probabilities can result in large errors between longer-term forecasts and actual outcomes (Werndl, 2009). We are already beginning to see Chaos theory concepts incorporated in problems such as weather forecasting and traffic prediction, where, in turn, the results are informing our understanding of chaos in these application areas. Somewhat related to chaos is black swan theory, which focuses on highly improbable or surprising, highimpact events that are often incorrectly rationalized in hindsight (Taleb, 2005, 2007). The key idea is that probability-centered analysis and thinking that diminishes the importance of outliers or the unobserved is problematic for appropriately managing the risks associated with black swan events. Taleb’s views on the limitations of statistics and his seemingly negative portrayal of statisticians has raised several (possibly valid) rebuttals from the statistics community (e.g., Westfall & Hilbe, 2007). Nevertheless, his central tenet appears to have merit. As big data further creates the shift towards data-driven decision making, risk management pitfalls such as attempting to predict extreme events, overreliance on the past, psychological biases against less likely outcomes, and overemphasis on standard deviations are likely to be exacerbated (Taleb, Goldstein, & Spitznagel, 2009). Black swan events also have important implications for the design of process automation relying on big data. We have already seen automated loan risk assessment and algorithmic trading engines fail miserably due to such events (Taleb, 2007; Derman, 2011). Black Swan Theory could shed light on studies examining risk considerations in data-driven decision making where traditional decision theories may be inadequate. Similarly, it can inform the design of process automation in big data environments such that the unknown unknowns are given proper consideration. In summary, big data has potentially important implications for theory. From an information value chain perspective, big data sources and associated IT artifacts have distinct implications for both knowledge acquisition and for decisions and actions (and related outcomes), which include system usage, performance, and satisfaction. The key nuances of big data artifacts stem from the four V characteristics. On one hand, these characteristics may simply inform well established IS theories by playing the role of antecedent constructs or of moderator/mediator variables. On the other hand, these characteristic can\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxiii\n",
              "\n",
              "introduce complexity and risk in IT artifacts increasingly relying on big data, thereby opening up exciting new possibilities for utilization of theories that have seen relatively limited usage in IS. Our final comment regarding theory and big data is while one cannot understate the role of “theory” in big data research, we do need to acknowledge that theory has different forms in different traditions of research, and, thus, as research community, we need to be open to different types of abstractions offered as theoretical contributions.\n",
              "\n",
              "11 Big Data: Implications for Methodology\n",
              "The characteristics of big data test beds have important implications for the norms of analyses. One significant implication that has recently garnered attention is the “deflated p-value” problem. In their Academy of Management Journal editorial, George et al. (2014) suggest that the statistical methods and metrics used to examine big data sets may need to incorporate alternative techniques from statistics, computer science, applied mathematics, and econometrics. They state (p. 323): “The typical statistical approach of relying on p values to establish the significance of a finding is unlikely to be effective because the immense volume of data means that almost everything is significant”. In addition to statistical significance and co-efficient signs, one may also need to consider effect sizes and variance when testing hypotheses on big data sets (Lin et al., 2013; George et al., 2014). As Chatfield (1995, p. 70) notes: “The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical significance, what is the practical significance of the results?”. To quantify the extensiveness of the problem, Lin et al. (2013) examined nearly 100 IS papers published between 2004 and 2010 with research test beds exceeding 10,000 instances and concluded that nearly half failed to discuss practical significance. As we note in Section 7 and as George et al. (2014) and others allude to, analyzing big data often requires using computer science-based methods grounded in machine learning and artificial intelligence rather than statistics. For instance, genetic algorithms are a computationally effective non-deterministic heuristic method for searching an NP-hard problem’s solution space and researchers/analysts have used them for variable feature selection in many predictive analytics problems involving thousands of input variables. Similarly, deep learning methods have enabled neural networks to attain impressive classification accuracies on large data sets (LeCun et al., 2015). Deep learning methods add additional layers of processes capable of learning or representing complex patterns at the expense of further degrees of separation between the model output and the underlying model intuition. Consequently, such methods also constitute a departure from traditional statistical methods, such as ordinary least squares or simple logistic regression analyses that prior IS studies have commonly used for both explaining and predicting. Shmueli and Koppius (2010) note that there is a difference between models geared toward prediction and those geared toward explanation. In many ways, big data amplifies this dichotomy as powerful non-deterministic and/or “black box” methods gain prominence for their predictive capabilities. In many cases, such methods produce answers to the “what” without the “why”. Another implication of big data sets is construct validity/credibility concerns pertaining to variables derived from user-generated, non-survey-based data sources, often characterized by low veracity. In addition to the spam and deception traits inherent to user-generated content sources (e.g., social media) and clickstreams’ data-tracking limitations, scholars operationalize many unstructured data sources as a few structured variables. We can see one prominent illustration of this point in the context of user sentiment polarity: whether the user is expressing a positive, negative, or neutral sentiment toward a given topic. Due to the volume of big data, one typically derives such constructs using software packages that rely on natural language processing methods as opposed to traditional manual coding methods. There have been thousands of studies published using social media sentiments in the recent years, including several in IS outlets. Such studies routinely make conclusions about the impact of user sentiment-related independent variables. However, scholars rarely report information on the suitability and accuracy of the underlying sentiment classification models used to operationalize the constructs, which happens despite the fact that benchmarking studies have found that many state-of-the-art sentiment analysis methods’ sentiment polarity classification performances are subpar, which affects the sentiment-related analysis and conclusions drawn from it (Hassan, Abbasi, & Zeng, 2014). Moving forward, we need research-based guidelines on how to validate variables derived from natural language, clickstream, sensor, and other big data sources.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Another important issue is to consider how the penetrative, imaginative understanding of human meanings discerned by qualitative (particularly interpretive) researchers using small data can complement (rather than be substituted by) patterns derived from big data using machines/techniques/algorithms, to be able to offer a more complete picture of the phenomenon. Indeed, an emerging stream of work illustrates how findings based on qualitative “idiographic” approaches may mutually inform findings based on computational methods (Gaskin, Berente, Lyytinen, & Yoo, 2014). We are also aware that grounded theory researchers in IS (i.e., the SIG GTM community) are looking for ways in which the grounded theory methodology (GTM) principles can be effectively utilized in big data settings. Clearly, big data is requiring us to reexamine how we analyze and validate data and interpret and discuss the findings. We need further research to assess more thoroughly the pros and cons of different methods and metrics on various types of big data sets, and to provide meaningful guidelines. In particular, due to the volume, variety, and veracity dimensions, we need to be watchful about big data’s creating “false positives” in terms of statistical significance of independent variables or considerably altering the effect size. Finding ways and principles that can aid in effectively complementing and/or triangulating big data research with small data research is another issue we must take seriously. Achieving “consilience—that is, convergence of evidence from multiple, independent, and unrelated sources”— needs to be a matter of priority (George et al., 2014, p. 324).\n",
              "\n",
              "12 Closing Thoughts\n",
              "The arena of big data/big data analytics has captured the attention and imagination of both practitioners and academics in a variety of disciplines, not just in IS. Commentators have described the big data phenomenon as a “deluge” and as having the potential to cause long-lasting impacts on practice and academia (e.g., Anderson, 2008; Kitchin, 2014b). Indeed, thought leaders and editors of leading IS journals see much reason for optimism regarding big data’s impacts on the IS discipline. For example, Goes (2014, p. viii) has encouraged the field to “embrace the changes and provide leadership in the new environment… [for which we] are uniquely positioned” and to “claim our [rightful] territory”. Agarwal and Dhar (2014) view big data as an opportunity for ushering in a “golden age for IS researchers”. Yet, not all scholars from IS and other disciplines unquestionably accept projections of such promise (e.g., Buhl, Röglinger, Moser, & Heidemann, 2014). Some, including Professor Michael Jordan, a “machine-learning maestro”, predict the onset of “big data winter” if we continue to over-promise and overhype (Gomes, 2014) without addressing fundamental epistemological and methodological issues associated with big data (e.g., Kitchin, 2014b). Furthermore, we must address the concerns regarding the erosion of privacy and, consequently, the loss of human dignity in the face of economic imperatives (e.g., Barocas & Nissenbaum, 2014) that can lead to “digital colonization” (Buhl et al., 2014) and the “subjugation” of human interests by machines and algorithms, which socio-technical scholars have for long been wary of (e.g., Bjorn-Andersen, Earl, Holst, & Mumford, 1982). In addition, the rhetoric of making academic research relevant by helping solve immediate organizational problems through big data without adequate abstraction or without designing approaches or artifacts for addressing broad classes of problems raises questions regarding how academic research differs from practice. Also, in line with Benbasat and Zmud (2003), many scholars believe that routinely engaging in big data projects without a unique disciplinary “signature” could prove to be ominous for the IS discipline in the long run. Hence, our position is one of cautious optimism. While we undoubtedly see potential for big data in contributing to a stronger and more relevant IS discipline—one that would have a significant social impact— we do not take the benefits for granted. To help big data research achieve its potential, we invite IS scholars to: a) critically engage with fundamental issues, such as epistemology, methodology, ethics, and the design of novel artifacts; b) rethink decision models proposed in the era of scarce data and adapt them for use in the current era of abundant data; and c) assess economic and humanistic outcomes of big data in the form of systematic, multi-paradigmatic research initiatives across the information chain value. Further, to ensure a healthy development of scholarship in this area, we see the need to carefully balance “real-world” problemsolving using big data/big data techniques with reflective inquiry and scholarly abstraction of knowledge in this area. We also encourage big data researchers from the IS discipline to participate in boundary-spanning interdisciplinary big data projects, but to balance engagement with other disciplines with conscious development and nurturing of big data approaches and objectives that are somewhat consistent with the IS discipline’s socio-technical heritage. In this editorial, we do not intend to provide definite answers or directions but to encourage inquiry, reflection, and debate on this topic by IS scholars embedded in diverse theoretical and methodological traditions. We\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxv\n",
              "\n",
              "are hopeful that the framework (Figure 4) and the accompanying tables (Tables 1, 2, and 3), while preliminary, will help energize the conversation on big data in the broader IS community and provide a roadmap for advancing scholarship in the area. Our final comment is related to teaching, which we believe is our raison d'être. No other academic unit has the diversity of research traditions and understanding of the business, information, technology, and human issues that are essential to comprehending the various facets of the big data value chain (e.g., Agarwal & Dhar, 2014). This diversity places us in an excellent position to offer pedagogical leadership in teaching, developing curricula, and programs and to initiate industry outreach centers (Chiang, Goes, & Stohr, 2012). In summary, we are convinced that big data is here to stay. However, we can foresee a time when big data will not be at the forefront of our conversations, as we have seen in the cases of expert systems, BPR, ebusiness, ERP, and groupware. Yet, few will disagree that the ideas underlying these topics have continued, and will continue, to be important knowledge areas informing research and practice in IS. We expect that big data research will do the same. For now, big data offers a stage for learning new lessons, re-learning and refining old lessons, and reflecting on assumptions that underlie our research endeavors and the complex ways in which technology, information, and humans interact to shape the world we live in.\n",
              "\n",
              "Acknowledgments\n",
              "We thank Kenny Cheng, Dirk Hovorka, Steven Johnson, Vijay Khatri, Brent Kitchens, Sridhar Nerur, and Tony Vance for their constructive comments on an earlier version of the editorial. The reactions of TSWIM 2015 participants to a keynote talk by one of the authors also helped shape several ideas, for which we are grateful. Likewise, we greatly appreciate feedback from faculty and doctoral students that participated in the 2015 Antai Graduate Summer School at Shanghai Jiaotong University.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "References\n",
              "Abbasi, A., & Adjeroh, D. (2014). Social media analytics for smart health. IEEE Intelligent Systems , 29 (2), 60-64. Abbasi, A., Zhang, Z., Zimbra, D., Chen, H., & Nunamaker, J. F., Jr. (2010). Detecting fake websites: The contribution of statistical learning theory. MIS Quarterly, 34(3), 435-461. Abbasi, A., Lau, R. Y. K., & Brown, D. E. (2015). Predicting Behavior. IEEE Intelligent Systems, 30(3), 35-43. Adomavicius, G., & Tuzhilin, A. (2005). Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6), 734-749. Agarwal, R., & Dhar, V. (2014). Editorial: Big data, data science, and analytics: The opportunity and challenge for IS research. Information Systems Research, 25(3), 443-448. Alavi, M., & Leidner, D. E. (2001). Review: Knowledge management and knowledge management systems: Conceptual foundations and research issues. MIS Quarterly, 25(1), 107-136. Anderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. Retrieved from http://www.wired.com/2008/06/pb-theory/ Andrienko, N., & Andrienko, G. (2003). Informed spatial decisions through coordinated views. Information Visualization, 2(4), 270-285. Aral, S., & Walker, D. (2012). Identifying influential and susceptible members of social networks. Science, 337(6092), 337-341. Arnott, D., & Pervan, G. (2008). Eight key issues for the decision support systems discipline. Decision Support Systems, 44(3), 657-672. Arnott, D., & Pervan, G. (2012). Design science in decision support systems research: An assessment using the Hevner, March, Park, and Ram Guidelines. Journal of the Association for Information Systems, 13(11), 923-949. Bardhan, I., Oh, C., Zheng, E., & Kirksey, K. (2015). Predictive analytics for readmission of patients with congestive heart failure: Analysis across multiple hospitals. Information Systems Research, 26(1), 19-39. Baesens, B., Bapna, R., Marsden, J. R., Vanthienen, J., & Zhao, J. L. (2014). Transformational issues of big data and analytics in networked business. MIS Quarterly, 38(2), 629-632. Balakrishnan, P. V., & Jacob, V. S. (1996). Genetic algorithms for product design. Management Science, 42(8), 1105-1117. Bansal, G., Sinha, A. P., & Zhao, H. (2008). Tuning data mining methods for cost-sensitive regression: A study in loan charge-off forecasting. Journal of Management Information Systems, 25(3), 315-336. Banker, R. D., & Kauffman, R. J. (2004). 50th anniversary article: The evolution of research on information systems: A fiftieth-year survey of the literature in management science. Management Science, 50(3), 281-298. Barocas, S., & Nissenbaum, H. (2014). Big data's end run around procedural privacy protections. Communications of the ACM, 57(11), 31-33. Benbasat, I., & Zmud, R. W. (2003). The identity crisis within the IS discipline: Defining and communicating the discipline's core properties. MIS Quarterly, 27(2), 183-194. Bjørn-Andersen, N., Earl, M., Holst, O., & Mumford, E. (1982). Information society: For richer, for poorer, Amsterdam: North-Holland. Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxvii\n",
              "\n",
              "Broniatowski, D., Paul, M. J., & Dredze, M. (2014). National and local influenza surveillance through Twitter: An analysis of the 2012-2013 influenza epidemic. PLoS One, 8, e83672. Brown, S. A., Massey, A. P., Montoya-Weiss, M. M., & Burkman, J. R. (2002). Do I really have to? User acceptance of mandated technology. European Journal of Information Systems, 11(4), 283-295. Brown, D. E., Abbasi, A., & Lau, R. Y. K. (2015). Predictive analytics: Predictive modeling at the micro level. IEEE Intelligent Systems, 30(3), 6-8. Buhl, H. U., Röglinger, M., Moser, F., & Heidemann, J. (2013). Big data—a fashionable topic with(out) sustainable relevance for research and practice? Business & Information Systems Engineering, 5(2), 65-69. Buitelaar, P., Cimiano, P., & Magnini, B. (Eds.). (2005). Ontology learning from text: Methods, evaluation and applications. Amsterdam: IOS Press. Buytendijk, F. (2014). Hype cycle for big data, https://www.gartner.com/doc/2814517/hype-cycle-big-data2014. Gartner. Available from\n",
              "\n",
              "Chandler, N., Hostmann, B., Rayner, N., & Herschel, G. (2011). Gartner’s business analytics framework. Gartner. Chang, R. M., Kauffman, R. J., & Kwon, Y. (2014). Understanding the paradigm shift to computational social science in the presence of big data. Decision Support Systems, 63, 67-80. Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R. (2000). CRISP-DM 1.0 Step-by-step data mining guide. SPSS. Retrieved from www.crisp-dm.org Chatfield, C. (1995). Problem solving: A statistician’s guide (2nd ed.). London: Chapman & Hall/CRC. Chatterjee, S., & Sarker, S. (2013). Infusing ethical considerations in knowledge management scholarship: Toward a research agenda. Journal of the Association for Information Systems, 14(8), 452-481. Chatterjee, S., Sarker, S., & Fuller, M. (2009a). A deontological approach to designing ethical collaboration. Journal of the Association for Information Systems, 10(3), 138-169. Chatterjee, S., Sarker, S., & Fuller, M. (2009b). Ethical information systems development: A Baumanian postmodernist perspective. Journal of the Association for Information Systems, 10(11), 787-815. Chen, H., Chiang, R. H., & Storey, V. C. (2012). Business intelligence and analytics: From big data to big impact. MIS Quarterly, 36(4), 1165-1188. Chiang, R. H., Barron, T. M., & Storey, V. C. (1994). Reverse engineering of relational databases: Extraction of an EER model from a relational database. Data & Knowledge Engineering, 12(2), 107-142. Chiang, R. H., Goes, P., & Stohr, E. A. (2012). Business intelligence and analytics education, and program development: A unique opportunity for the information systems discipline. ACM Transactions on Management Information Systems, 3(3), 1-13. Chung, W., Chen, H., & Nunamaker, J. F., Jr. (2005). A visual framework for knowledge discovery on the Web: An empirical study of business intelligence exploration. Journal of Management Information Systems, 21(4), 57-84. Christensen, C. (1997). The innovator's dilemma: When new technologies cause great firms to fail. Boston, MA: Harvard Business Review Press. Coco, C. T., Jamison, F., & Black, H. (2011). Connecting people investments and business outcomes at Lowe’s. People & Strategy, 34(2), 28-33 Coolidge, A. (2013). New technology helps Kroger speed up checkout times. The Cincinnati Inquirer. Retrieved from http://www.usatoday.com/story/money/business/2013/06/20/new-technology-helpskroger-speed-up-checkout-times/2443975/ Davenport, T. H. (2006). Competing on analytics. Harvard Business Review, 84(1), 98-107. Davenport, T. H., & Harris, J. G. (2007). Competing on analytics: The new science of winning. Boston, MA: Harvard Business Press.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Davenport, T. H., Harris, J., & Shapiro, J. (2010). Competing on talent analytics. Harvard Business Review, 88(10), 52-58. Davenport, T. H., & Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(10), 70-76. Davenport, T. H., & Kirby, J. (2015). Beyond automation. Harvard Business Review, 94(6), 59-65. Davis, F. D. (1986). A technology acceptance model for empirically testing new end-user information systems: Theory and results (Doctoral dissertation). Sloan School of Management, Massachusetts Institute of Technology. Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319-340. Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1), 107-113. Dennis, A. R., & Carte, T. A. (1998). Using geographical information systems for decision making: Extending cognitive fit theory to map-based presentations. Information Systems Research, 9(2), 194-203. Derman, E. (2011). Models. Behaving. Badly. New York, NY: Simon and Schuster. Embley, D. W., & Liddle, S. W. (2013). Big data—conceptual modeling to the rescue. In W. Ng, V. C. Storey, & J. C. Trujillo (Eds.), ER conference (vol. 8217, pp. 1-8). Springer. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996a). From data mining to knowledge discovery in databases. AI Magazine, 17(3), 37-54. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996b). The KDD process for extracting useful knowledge from volumes of data. Communications of the ACM, 39(11), 27-34. Gaskin, J., Berente, N., Lyytinen, K., & and Yoo, Y. (2014). \"Toward Generalizable Sociomaterial Inquiry: A Computational Approach for Zooming In and Out of Sociomaterial Routines,\" MIS Quarterly, 38(3), 849-871. Gefen, D., Karahanna, E., & Straub, D. W. (2003). Trust and TAM in online shopping: An integrated model. MIS Quarterly, 27(1), 51-90. George, G., Haas, M. R., & Pentland, A. (2014). Big data and management. Academy of Management Journal, 57(2), 321-326. Gleick, J. (1987). Chaos. Making a new science. New York: Viking Penguin Inc. Goes, P. (2014). Big data and IS research. MIS Quarterly, 38(3), iii-viii. Gomes, L. (2014). Machine-learning maestro Michael Jordan on the delusions of big data and other huge engineering efforts. IEEE Spectrum. Gregor, S., & Klein, G. (2014). Eight obstacles to overcome in the theory testing Genre. Journal of the Association for Information Systems, 15(11), i-xix. Han, J., Kamber, M., & Pei, J. (2006). Data mining: Concepts and techniques. New York: Morgan Kaufmann. Harrington, A. (2005). Modern social theory. New York: Oxford University Press. Hassan, A., Abbasi, A., & Zeng, D. (2013). Twitter sentiment analysis: A bootstrap ensemble framework. In Proceedings of the IEEE International Conference on Social Computing (pp. 357-364). Heudecker, N. (2013). Hype cycle for big data, https://www.gartner.com/doc/2574616/hype-cycle-big-data2013. Gartner. Available from\n",
              "\n",
              "Hevner, A. R., March, S. T., Park, J., & Ram, S. (2004). Design science in information systems research. MIS Quarterly, 28(1), 75-105.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxix\n",
              "\n",
              "Hill, K. (2012). How Target figured out a teen girl was pregnant before her father did. Forbes. http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-waspregnant-before-her-father-did/ Hodgkinson, G. P., & Starbuck, W. H. (2008). The Oxford handbook of organizational decision making. Oxford, UK: Oxford University Press. Horan, J. A. (2011). The essential CIO. IBM CIO C-Suite Studies. Retrieved from http://www935.ibm.com/services/c-suite/cio/study/ Jensen, M. L., Lowry, P. B., Burgoon, J. K., & Nunamaker, J. F., Jr. (2010). Technology dominance in complex decision making: The case of aided credibility assessment. Journal of Management Information Systems, 27(1), 175-202. Jensen, M. L., Averbeck, J. M., Zhang, Z., & Wright, K. B. (2013). Credibility of anonymous online product reviews: A language expectancy perspective. Journal of Management Information Systems, 30(1), 293-323. Junqué de Fortuny, E., Martens, D., & Provost, F. (2013). Predictive modeling with big data: Is bigger really better? Big Data, 1(4), 215-226. Kaushik, A. (2011). Web analytics 2.0: The art of online accountability and science of customer centricity. Indianapolis, IND: Wiley. KDNuggets (2011). What do you call analyzing data? KDNuggets. http://www.kdnuggets.com/polls/2011/what-do-you-call-analyzing-data.html Retrieved from\n",
              "\n",
              "Kiel, L. D., & Elliott, E. W. (1996). Chaos theory in the social sciences: Foundations and applications. University of Michigan Press. Kiron, D., Shockley, R., Kruschwitz, N., Finch, G., & Haydock, M. (2012). Analytics: The widening divide. MIT Sloan Management Review, 53(2), 1-21. Kitchin, R. (2014a). Big Data, new epistemologies and paradigm shifts. Big Data & Society, 1(1), 1-12. Kitchin, R. (2014b). The data revolution. London: Sage. Kramer A. D. I., Guillory J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788-8790. Lavalle, S., Lesser, E., Shockley, R., Hopkins, M. S., & Kruschwitz, N. (2011). Big data, analytics, and the path from insights to value. Sloan Management Review, 52(2), 21-31. Lau, R. Y. K. Liao, S. Y., Wong, K. F., & Chiu, K. W. (2012). Web 2.0 environmental scanning and adaptive decision support for business mergers and acquisitions. MIS Quarterly, 36(4), 1239-1268. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436-444. Lin, M., Lucas, H. C., Jr., & Shmueli, G. (2013). Research commentary-too big to fail: Large samples and the p-value problem. Information Systems Research, 24(4), 906-917. Lycett, M. (2013). “Datafication”: Making sense of (big) data in a complex world. European Journal of Information Systems, 22(4), 381-386. McAfee, A., & Brynjolfsson, E. (2012). Big data: The management revolution. Harvard Business Review. Retrieved from https://hbr.org/2012/10/big-data-the-management-revolution/ar Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation Manyika, J., Chui, M., Groves, P., Farrell, D., Van Kuiken, S., & Doshi, E. A. (2013). Open data: Unlocking innovation and performance with liquid information. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights/Business%20Technology/Open%20d ata%20Unlocking%20innovation%20and%20performance%20with%20liquid%20information/MGI_ OpenData_Full_report_Oct2013.ashx\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Marchand, D. A., & Pepper, J. (2013). Why IT fumbles analytics. Harvard Business Review. Retrieved from https://hbr.org/2013/01/why-it-fumbles-analytics Mayer-Schönberger, V., & Cukier, K. (2013). Big data: A revolution that will transform how we live, work, and think. Boston, MA: Houghton Mifflin Harcourt. Newman, D. (2014). Big Data Means Big Disruption. Forbes. Retrieved http://www.forbes.com/sites/danielnewman/2014/06/03/big-data-means-big-disruption/ from\n",
              "\n",
              "Nunamaker, J. F., Jr., Chen, M., & Purdin, T. (1991). Systems development in information systems research. Journal of Management Information Systems, 7(3), 89-106. Nunamaker, J. F., Jr. (1992). Build and learn, evaluate and learn. Informatica, 1(1), 1-6. Parsons, J., & Wand, Y. (2012). Extending classification principles from information modeling to other disciplines. Journal of the Association for Information Systems, 14(5), 245-273. Rai, A., & Sambamurthy, V. (2006). Editorial notes-the growth of interest in services management: Opportunities for information systems scholars. Information Systems Research, 17(4), 327-331. Redman, T. C. (2008). Data driven: Profiting from your most important business asset. Boston, MA: Harvard Business Press. Richards, N. M., & King, J. H. (2013). Three paradoxes of big data, Stanford Law Review Online, 66(41), 41-46. Sarker, S., Xiao, X., & Beaulieu, T. (2013). Qualitative studies in information systems: A critical review and some guiding principles. MIS Quarterly, 37(4), iii-xviii. Schroeck, M., Shockley, R., Smart, J., Romero-Morales, D., & Tufano, P. (2012). Analytics: The real-world use of big data. IBM Institute for Business Value. Sein, M., Henfridsson, O., Purao, S., Rossi, M., & Lindgren, R. (2011). Action design research. MIS Quarterly, 35(1), 37-56. Shaft, T. M., & Vessey, I. (2006). The role of cognitive fit in the relationship between software comprehension and modification. MIS Quarterly, 30(1), 29-55. Sharma, R., Mithas, S., & Kankanhalli, A. (2014). Transforming decision-making processes: A research agenda for understanding the impact of business analytics on organisations. European Journal of Information Systems, 23, 433-441. Shim, J. P., Warkentin, M., Courtney, J. F., Power, D. J., Sharda, R., & Carlsson, C. (2002). Past, present, and future of decision support technology. Decision Support Systems, 33(2), 111-126. Shmueli, G., & Koppius, O. (2011). Predictive analytics in information systems research. MIS Quarterly, 35(3), 553-572. Simon, H. A. (1996). The sciences of the artificial (3rd ed.). Cambridge, MA: MIT Press. Song, Y., Sahoo, N., Srinivasan, S., & Chrysanthos, D. (2014). Uncovering path-to-purchase segments in large consumer population. In Proceedings of the 24th Workshop on Information Technologies and Systems. Sprott, J. C. (2003). Chaos and time-series analysis (Vol. 69). Oxford: Oxford University Press. Steadman I. (2013). Big data and the death of the theorist. http://www.wired.co.uk/news/archive/2013-01/25/big-data-end-of-theory Wired. Retrieved from\n",
              "\n",
              "Stigler, G. (1961). The economics of information. The Journal of Political Economy, 69(3), 213-225. Storey, V. C., Chiang, R. H., Dey, D., Goldstein, R. C., & Sudaresan, S. (1997). Database design with common sense business reasoning and learning. ACM Transactions on Database Systems, 22(4), 471-512. Stubbs, E. (2011). The value of business analytics: Identifying the path to profitability. New York, NY: John Wiley & Sons. Taleb, N. (2005). Fooled by randomness: The hidden role of chance in life and in the markets (Vol. 1). Random House Incorporated.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxxi\n",
              "\n",
              "Taleb, N. N. (2007). The black swan: The impact of the highly improbable. Random House. Taleb, N. N., Goldstein, D. G., & Spitznagel, M. W. (2009). The six mistakes executives make in risk management. Harvard Business Review, 87(10), 78-81. Tambe, P. (2014). Big data investment, skills, and firm value. Management Science, 60(6), 1452-1469. Te’eni, D. (2006). Designs that fit: An overview of fit conceptualizations in HCI. In P. Zhang & D. Galletta (Eds.), Human computer interaction and management information systems: Foundations (pp. 205224). London, England: M. E. Sharpe. Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124-1131. Vapnik, V. N. (1998). Statistical learning theory (Vol. 1). New York: Wiley. Venkatesh, V, Thong, J. Y. L., & Xu, X. (Forthcoming). Unified theory of acceptance and use of technology: A synthesis and the road ahead. Journal of the Association for Information Systems. Venkatesh, V., Morris, M., Davis, G., & Davis, F. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 27(3), 425-478. Vessey, I. (1991). Cognitive fit: A theory‐based analysis of the graphs versus tables literature. Decision Sciences, 22(2), 219-240. Walls, J. G., Widmeyer, G. R., & El Sawy, O. A. (1992). Building an information system design theory for vigilant EIS. Information Systems Research, 3(1), 36-59. Wand, Y., & Weber, R. (2002). Research commentary: Information systems and conceptual modeling—a research agenda. Information Systems Research, 13(4), 363-376. Wang, Y., & Ram, S. (2015). Prediction of location-based sequential purchasing events using spatial, temporal and social patterns. IEEE Intelligent Systems, 30(3), 10-17. Werndl, C. (2009). What are the new implications of chaos for unpredictability? The British Journal for the Philosophy of Science, 60(1), 195-220. Westfall, P. H., & Hilbe, J. (2007). The black swan: Praise and criticism. The American Statistician, 61(3), 193-194. Wixom, B. H., & Watson, H. J. (2001). An empirical investigation of the factors affecting data warehousing success. MIS Quarterly, 25(1), 17-41. Yetgin, E., Jensen, M., & Shaft, T. (2015). Complacency and intentionality in IT use and continuance. AIS Transactions on Human-Computer Interaction, 7(1), 17-42. Zahedi, F. M., Abbasi, A., & Chen, Y. (2015). Fake-website detection tools: Identifying elements that promote individuals’ use and enhance their performance. Journal of the Association for Information Systems, 16(6), 448-484. Zeng, D., Chen, H., Lusch, R., & Li, S. H. (2010). Social media analytics and intelligence. IEEE Intelligent Systems, 25(6), 13-16. Zhao, H., Sinha, A. P., & Bansal, G. (2011). An extended tuning method for cost-sensitive regression and forecasting. Decision Support Systems, 51(3), 372-383. Zhang, W., Lau, R., & Li, C. (2014). Adaptive big data analytics for deceptive review detection in online social media. In Proceedings of the International Conference on Information Systems.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "About the Authors\n",
              "Ahmed Abbasi is Murray Research Professor and Associate Professor of Information Technology in the McIntire School of Commerce at the University of Virginia. He is Director of the Center for Business Analytics and a member of the Predictive Analytics Lab. His research on online fraud and security, natural language processing, social media, and e-health has been funded through multiple grants from the National Science Foundation. Ahmed received the IBM Faculty Award and AWS Research Grant for his work on Big Data. He has published over fifty peer-reviewed papers in journals and conferences, including top-tier outlets such as MIS Quarterly, Journal of the AIS, Journal of MIS, ACM Transactions on IS, IEEE Transactions on Knowledge and Data Engineering, and IEEE Intelligent Systems. One of his papers was considered a top publication by the AIS. Ahmed also won best paper awards at MIS Quarterly and WITS. His work has been featured in various media outlets, including the Wall Street Journal, the Associated Press, and Fox News. Ahmed serves as an Associate Editor for Information Systems Research, Decision Sciences Journal, ACM Transactions on MIS, and IEEE Intelligent Systems, and as an editorial review board member for the Journal of AIS. He is a senior member of the IEEE and serves on program committees for various conferences related to computational linguistics, text analytics, and data mining. He is also a co-founder and/or advisory board member for multiple predictive analytics-related companies. Suprateek Sarker is a Professor of Information Technology at the McIntire School of Commerce, University of Virginia, USA. He also serves as Visiting Distinguished Professor at Aalto University School of Business, Helsinki, Finland. He is primarily a qualitative researcher, and his past work has been published in leading outlets including the MIS Quarterly, Information Systems Research, Journal of the AIS, Journal of MIS, European Journal of Information Systems, MIS Quarterly Executive, Journal of Information Technology, IEEE Transactions, ACM Transactions on MIS, Communications of the ACM, MIS Quarterly Executive, and Decision Sciences Journal. He is currently serving as the Editor-in-Chief of the Journal of the AIS, a Senior Editor of Decision Sciences Journal, a Senior Editor (Emeritus) of MIS Quarterly, a member of the Board of Editors of Journal of MIS, and an editorial board member of IEEE Transactions on Engineering Management. Some of his past work has been funded by the National Science Foundation (NSF) and the Institute for the Study of Business Markets (ISBM). He is also a past recipient (with S. Sahay) of the Stafford Beer Medal awarded by the Operational Research Society, UK. Roger H. L. Chiang is a Professor of Information Systems at Department of Operations, Business Analytics, and Information Systems, Carl H. Lindner College of Business, University of Cincinnati. Dr. Chiang’s research interests are in business intelligence and analytics, data and knowledge management, and intelligent systems, particularly in database reverse engineering, database integration, data and text mining, sentiment analysis, document classification and clustering, domain knowledge discovery, and semantic information retrieval. He has over fifty refereed papers published by conferences and journals including ACM Transactions on Database Systems, ACM Transactions on MIS, Communications of the ACM, The DATA BASE for Advances in Information Systems, Data & Knowledge Engineering, Decision Support Systems, Journal of American Society for Information Science and Technology, Journal of Database Administration, Journal of MIS, Marketing Science, MIS Quarterly, and Very Large Data Base Journal. He has served as the Senior Editor of The DATA BASE for Advances in Information Systems, Decision Sciences, and Journal of the AIS and the associate editor of Information & Management, Journal of the AIS, Journal of Database Management, International Journal of Intelligent Systems in Accounting, Finance and Management, and MIS Quarterly.\n",
              "\n",
              "Copyright © 2016 by the Association for Information Systems. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than the Association for Information Systems must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, GA, 30301-2712 Attn: Reprints or via e-mail from publications@aisnet.org.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Copyright of Journal of the Association for Information Systems is the property of Association for Information Systems and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "fgHyaGNjuZDT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "yHyXKAIRtibr",
        "colab_type": "code",
        "outputId": "381694c5-c308-4acb-ad3f-8b0a81a7cab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10917
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc1=nlp(open(u\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\").read())\n",
        "doc1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "J\n",
              "Editorial\n",
              "\n",
              "ournal of the\n",
              "\n",
              "A\n",
              "\n",
              "ssociation for\n",
              "\n",
              "I\n",
              "\n",
              "nformation\n",
              "\n",
              "S\n",
              "\n",
              "ystems\n",
              "ISSN: 1536-9323\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "Ahmed Abbasi\n",
              "McIntire School of Commerce, University of Virginia abbasi@comm.virginia.edu\n",
              "\n",
              "Suprateek Sarker\n",
              "McIntire School of Commerce, University of Virginia School of Business, Aalto University suprateek.sarker@comm.virginia.edu\n",
              "\n",
              "Roger H. L. Chiang\n",
              "Carl H. Lindner College of Business, University of Cincinnati\n",
              "\n",
              "roger.chiang@uc.edu\n",
              "\n",
              "Abstract:\n",
              "Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data’s characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data’s disruptive effects. Keywords: Big Data, Behavioral, Business Analytics, Business Intelligence, Design Science, Economics of IS, Information Value Chain, Research Directions.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "pp. i – xxxii\n",
              "\n",
              "February\n",
              "\n",
              "2016\n",
              "\n",
              "\n",
              "ii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "1\n",
              "\n",
              "Introduction\n",
              "\n",
              "As business processes become major differentiators for organizations in many industries, organizations are increasingly using analytics to “wring every last drop” of value from those processes (Davenport, 2006). Consequently, companies now view their data as a primary business asset (Redman, 2008). In organizational settings, the information technology (IT) function is tasked with managing and integrating data as an “enabler” of data-driven business processes and decision making (Chandler, Hostmann, Rayner, & Herschel, 2011; Lycett, 2013). Big data’s rise has further amplified the importance of IT in this role (Horan, 2011), resulting in important implications for IT managers and scholars within and beyond the information systems (IS) discipline. Recent editorials and special issues in top IS journals have discussed big data analytics from different vantage points. For example, Chen, Chiang, and Storey (2012) highlight the various domains and information sources associated with big data. They also touch on their vision for evolving analytics toward big data -- from the traditional structured relational database-driven paradigm to “business intelligence and analytics 2.0”, which leverages Web and unstructured content, and to “business intelligence and analytics 3.0”, which, in addition, encompasses mobile and sensor-based data. Goes (2014) presents valuable taxonomies for big data infrastructure and big data analytics. Agarwal and Dhar (2014) discuss challenges and opportunities pertaining to big data in information systems research and note that IS researchers are well positioned to take advantage of opportunities in this area. Sharma, Mithas, and Kankanhalli (2014) offer several research questions that IS researchers need to pursue. Adding to this conversation is the call for papers for MIS Quarterly’s upcoming special issue that emphasizes the need for IS researchers to examine and exploit big data’s disruptive nature (Baesens, Bapna, Marsden, Vanthienen, & Zhao, 2014). All of these commentaries, editorials, and/or special issue introductions highlight important facets of big data research in IS. In this editorial, we synthesize takeaways from prior expositions and expand on them by focusing on the interplay between big data’s unique characteristics, these characteristics’ implications for the information value chain, and potential areas of inquiry for the three major IS research traditions. We present a framework (Figure 1) that highlights this interplay and helps one to generate (and potentially refine) a set of meaningful research questions on this topic for the IS discipline. There has been a fair amount of IS research on the information value chain, which is the cycle of converting data to information to knowledge to decisions to actions and, thereby, generate additional data. Major bodies of IS research pertaining to the information value chain have examined the derivation and management of knowledge and decision making and actions; however, the effects of big data on the value chain remain relatively unexplored. Scholars and others often define big data by four key characteristics: volume, velocity, variety, and veracity; however, big data is not simply a matter of injecting additional scale, variation, speed, or noise to research data sets. As Chris Anderson of Wired famously said (2008, emphasis added), “Because in the era of big data, more isn’t just more. More is different”. In other words, these characteristics have the propensity to disrupt the traditional information value chain and result in a new “big data information value chain”. Such disruption not only presents opportunities for novel research from within or across the different IS research traditions (e.g., positivist, interpretive, and critical behavioral research, design research, and economics of IS-based research) but also raises epistemological questions and challenges for some of the IS research traditions. This editorial proceeds as follows. First, in Sections 2, 3, and 4, we provide an overview of the traditional information value chain and related IS research, big data’s disruptive characteristics, and the resulting big data information value chain. With these sections, we highlight the profound impact of big data on people, processes, technologies, and, consequently, on organizations, industries, and virtually every facet of the world we live in 1. Second, in Sections 5 to 11, we discuss possible research directions and implications for the three traditions of IS research alluded to earlier. In a way, we highlight the fact that we have much to learn about the big data phenomena and scholars in each tradition can and need to play a role in the research endeavor. We emphasize that that, by highlighting the three traditions and certain questions in our editorial, we do not wish to exclude other forms or areas of inquiry. In fact, we believe that we will see new traditions, genres, and areas of inquiry spring out of the IS community’s engagement with phenomena related to big data.\n",
              "\n",
              "1\n",
              "\n",
              "Those already familiar with the notion of “big data” and its impacts on the value chain may prefer to skim through this discussion or perhaps proceed directly to the discussion on research directions (starting in Section 5).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "iii\n",
              "\n",
              "Figure 1. Overview of the Big Data Information Value Chain Perspective 2\n",
              "\n",
              "2\n",
              "\n",
              "The Information Value Chain\n",
              "\n",
              "The information value chain is the cyclical set of activities necessary to convert data into information and, subsequently, to transform information into knowledge (Fayyad, Piatetsky-Shapiro, & Smyth, 1996a, 1996b; Han, Kamber, & Pei, 2006), which individuals use to make decisions and take action. The decisions and actions then result in outcomes such as business value and additional data (Sharma et al., 2014). Each stage of the value chain encompasses people, processes, and technologies (Chandler et al., 2011). Information value chains operate in a given context; for instance, at the enterprise level, in a centralized IT unit, or for a specific functional or business unit. Figure 2 illustrates the traditional information value chain prior to the era of big data. The list of people, processes, and technologies shown in Figure 2 are illustrative, not exhaustive. As a further caveat, one could place certain processes and technologies differently depending on how one interprets data, information, and knowledge and how one delineates between decisions and actions. Here, we present the examples to illustrate the interplay between people, processes, and technologies across the stages of the traditional information value chain (i.e., a pre-big data era baseline). In Section 3, we contrast this traditional value chain with the new information value chain resulting from big data’s disruption. One can broadly categorize the set of activities in the information value chain into two groups: knowledge derivation and decision making (Chandler et al., 2011; Goes, 2014; Sharma et al., 2014). In the traditional information value chain, structured data is predominantly stored on premises in organizations’ data centers that use relational database management systems (RDBMS). Many organizations integrate various structured data sources into data warehouses and/or data marts using extract, transform, and load (ETL) technologies. Database administrators, database managers, and ETL developers typically perform these tasks. Programmers or data analysts then process and analyze the data stored in these systems using structured query language (SQL) and use the resulting data as input for report generators, business intelligence (BI) tools, and/or analytical models incorporated in predictive technologies. In the value chain’s knowledge stage, the people tasked with deriving knowledge from the data intersect with the decision makers; in other words, in the knowledge stage, the enablers and producers interact with the consumers of information (Chandler et al., 2011). In this stage, technologies such as knowledge management systems, corporate wikis, reporting tools, BI dashboards, and expert systems preserve or present existing knowledge or facilitate the creation of new knowledge through processes such as forecasting or reporting. Technologies such as decision support systems (DSS), recommender systems, and collaboration tools support managers and analysts’ decision -making processes. The people, process, and technologies at various stages are also influenced by contextual factors such as the organizational/department/unit culture and IT governance. Traditionally, IS research on the information value chain has also focused on the two aforementioned areas: 1) deriving knowledge and 2) decision making (Sharma et al. 2014). Scholarship in the area of deriving knowledge encompasses areas such as large bodies of work on knowledge management, structured data mining, and database design (e.g., Alavi & Leidner 2001; Chiang, Barron, & Storey, 1994; Storey, Chiang, Dey, Goldstein, & Sudaresan, 1997). The IS body of literature on decision making\n",
              "2\n",
              "\n",
              "The arrow from the big data information value chain to the IS research traditions indicates the epistemological/paradigmatic considerations and challenges big data may bring to the way we conduct research and the types of criteria we might privilege in examining the big data phenomenon. The arrow between big data’s characteristics and the information value signifies the disruptive impact of the four Vs on the traditional value chain.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "iv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "is rich and extensive. Major areas of emphasis include research on designing decision support systems (DSS) and behavioral research on the effectiveness of IT artifacts or other decision aids for supporting decision making (Nunamaker, Chen, & Purdin, 1991; Wixom & Watson, 2001; Shim et al., 2002; Arnott & Pervan, 2008).\n",
              "\n",
              "Figure 2. The Traditional Information Value Chain and Examples of the Accompanying People, Processes, and Technologies\n",
              "\n",
              "In this era, academic scholars and practitioners have tended to use relatively scarce, largely static, and deliberately sampled and collected data (Kitchin, 2014a, 2014b). Having described the traditional information value chain, we now move on to describing the key characteristics of big data and elaborating on how these characteristics might disrupt the information value chain.\n",
              "\n",
              "3\n",
              "\n",
              "Enter Big Data: The Four Vs\n",
              "\n",
              "One can separate big data and “regular-sized” data based on the presence of a set of characteristics commonly referred to as the four Vs: volume, variety, velocity, and veracity (Schroeck, Shockley, Smart, Romero-Morales, & Tufano, 2012; Goes, 2014).\n",
              "\n",
              "3.1\n",
              "\n",
              "Volume\n",
              "\n",
              "The U.S. Library of Congress, which archives both digital and offline content, has collected hundreds of terabytes of data (Manyika et al., 2011). Interestingly, the average company in 15 of 17 industry sectors in the United States has more data stored than the Library of Congress (Manyika et al., 2011), which underscores the fact that big data is pervasive across industries including finance, manufacturing, retail, health, security, technology, and sports. For a detailed discussion of various applications domains for big data, see Chen et al. (2012). Furthermore, in the vocabulary of big data, petabytes and exabytes have now replaced terabytes. For instance, large retailers each collect tens of exabytes of transactional data every year (McAfee & Brynjolfsson, 2012). To put these volumes into perspective using the classic grains of sand\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "v\n",
              "\n",
              "analogy, if a megabyte is a tablespoon of sand, a terabyte is a sandbox two-feet wide and one-inch deep, a petabyte is a mile-long beach, and an exabyte is a beach extending from Maine to North Carolina.\n",
              "\n",
              "3.2\n",
              "\n",
              "Variety\n",
              "\n",
              "Organizations are now dealing with structured, semi-structured, and unstructured data from in and outside the enterprise (Schroeck et al., 2012). The variety includes traditional transactional data, user-generated text, images, and videos, social network data, sensor-based data, Web and mobile clickstreams, and spatial-temporal data (Chen et al., 2012; McAfee & Brynjolfsson, 2012). Effectively leveraging the variety of available data presents both opportunities and challenges.\n",
              "\n",
              "3.3\n",
              "\n",
              "Velocity\n",
              "\n",
              "The speed of data creation is a hallmark of big data. For instance, Wal-Mart collects over 2.5 petabytes of customer transaction data every hour (McAfee & Brynjolfsson, 2012). With respect to unstructured data, over one billion new tweets occur every three days, and five billion search queries occur daily (Abbasi & Adjeroh, 2014). Such information has important implications for “real-time” predictive analytics in various application areas, ranging from finance to health (Bollen, Mao, & Zeng, 2011; Broniatowski, Paul, & Dredze, 2014). Simply put, analyzing “data in motion” presents new challenges because the desired patterns and insights are moving targets, which is not the case for static data.\n",
              "\n",
              "3.4\n",
              "\n",
              "Veracity\n",
              "\n",
              "The credibility and reliability of different data sources vary. For instance, social media is plagued with spam, and Web spam accounts for over 20 percent of all content on the World Wide Web (Abbasi & Adjeroh, 2014). Similarly, clickstreams from website and mobile traffic are highly susceptible to noise (Kaushik, 2011). Furthermore, deriving deep semantic knowledge from text remains challenging in many situations despite significant advances in natural language processing.\n",
              "\n",
              "4\n",
              "\n",
              "The Big Data Information Value Chain\n",
              "\n",
              "In his seminal book The Innovator’s Dilemma, Christensen (1997) introduced and elaborated on the idea of disruption as relevant here. In organizational settings, disruptive phenomena significantly alter value chains. Indeed, big data and its four “V” characteristics have had a profound impact on the people, processes, and technologies related to the information value chain. As Mayer-Schönberger & Cukier (2013, p. 19) note, “The era of big data changes how we live and interact with the world”. Simply put, big data means big disruption (Newman, 2014). Figure 3 illustrates this disruption in three ways. First, the new value chain involves a different set of people, processes, and technologies. While IT is known to exist in a constantly changing landscape, we can clearly see the accompanying changes to the people and processes attributable to big data as a disruptor. Second, there is greater amalgamation of technologies into “platforms” and processes into “pipelines” in the value chain’s knowledge-derivation phase. Third, we see greater reliance on data scientists and analysts across all stages of the value chain to support self-service and realtime decision making. Big data’s four Vs clearly change how one stores and manages data. In terms of technical considerations, data’s volume, velocity, and variety in organizations have caused IT departments to consider distributed storage architectures capable of handling large quantities of unstructured data. In terms of the technology, NoSQL (“not only SQL”) systems, such as those leveraging Hadoop (Dean & Ghemawat, 2008) and/or Spark, have emerged as being better suited for the larger volumes and variety of unstructured data, while organizations commonly use in-memory databases to exploit velocity in real-time applications (Heudecker, 2013). In addition, firms are increasingly interested in collecting social media and sensor-based data (Chen et al., 2012) to supplement the internal data sources they have traditionally relied on, which contributes to the data’s variety. However, using such data sources comes with an array of data-quality and credibility concerns, which require appropriate data-management, data-preparation, and knowledge-management activities. Data’s volume and velocity have also caused IT departments to shift physical on-premises data centers to cloud-based infrastructure-as-a-service (IaaS), platform-as-a-service (PaaS), and database-asa-service (DBaaS) offerings better suited to meet organizations’ elastic computing and storage needs (Buytendijk, 2014). The interplay between traditional schema-based structured data storage and management, new schema-less big data technologies, and organizations’ increasing reliance on cloud\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "vi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "computing can create complex big data architectures. In some respects, the key data-management and storage questions that practitioners pose have shifted to “what other internal/external data sources can we leverage” and “what kind of enterprise data infrastructure do we need to support our growing needs?”.\n",
              "\n",
              "Figure 3. The Big Data Information Value Chain and Examples of Related People, Processes, and Technologies\n",
              "\n",
              "The shift towards schema-less data storage and management coupled with organizations’ increasing desire to leverage big data as a source of competitive advantage has brought about the rise of data scientists (Davenport & Patil, 2012; McAffe & Brynjolfsson, 2012). In the words of Davenport and Patil (2012, p. 73), “data scientists are the people who understand how to fish out answers to important business questions from today’s tsunami of unstructured information”. Data scientists and scriptingoriented programmers now perform (or at least complement those who do perform) many of the activities pertaining to deriving knowledge from internally and externally collected data sources that database managers and SQL programmers traditionally performed. Data scientists also work closely with analysts and management in the decision making phase (Davenport & Patil, 2012). Furthermore, data lakes, which are essentially data warehouses or data marts specifically intended to serve as “sand boxes” for data scientists to experiment in, are becoming increasingly pervasive (Buytendijk, 2014). While ETL developers still play an important role, data-integration tasks increasingly entail fusing various noisy structured and unstructured data sources. Consistent with the trend toward IaaS/PaaS/DBaaS, many technologies pertaining to the value chain’s information stage are also running in the cloud and accessed via software-as-as-service (SaaS) (Buyetdijk, 2014). Big data analytics allows data scientists and modelers to use enterprise machine learning— distributed scalable online algorithms running atop Hadoop platforms that can digest the volume and variety of information at unprecedented speeds. To offer an example, big data analytics running on Hadoop allowed Sears to push personalized promotions to customers more accurately and faster, which reduced the leadtime to one week compared to eight weeks in their previous data warehouse-based implementation (McAfee\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "vii\n",
              "\n",
              "& Brynjolfsson, 2012). Complex event-processing systems can analyze real-time sensor-based spatialtemporal data (Heudecker, 2013). For instance, the U.S. grocery chain Kroger has used overhead infra-red sensors to count customers and anticipate the number of currently needed checkout lanes and the number needed in 30 minutes, which has resulted in the company’s reducing average customer wait times from four minutes to 26 seconds (Coolidge, 2013). Big data’s velocity and the trend toward data driven decision making have created an exciting paradigm shift in how organizations create and leverage knowledge for decision making. The biggest shift is organization’s consuming analytics in real time with the rise of “self-service” BI/analytics (Chandler et al., 2011). This shift is attributable to the fact that “Big data, and the fast pace and complexity of today’s marketplace, require that leaders make decisions faster than ever before” (Kiron et al., 2011, p. 5). Self-service BI/analytics allows various employees in an organization, including managers and executives, to independently generate custom reports, run basic analytical queries, and access key performance indicators across various devices without relying on IT or decision analyst support. These factors help organizations avoid time-consuming hand-offs and make decisions in an agile manner (Chandler et al., 2011). The organizational context undoubtedly has impacts on the information value chain. Firms transformed by big data are often ones where a “strong top-line mandate to use analytics supports a culture open to new ideas” (Kiron et al., 2011, p. 5). Similarly, business units or departments with a longer-standing tradition of data-driven decision making, such as finance and operations, tend to leverage big data more relative to departments such as human resources (Kiron et al., 2011). Furthermore, big data governance practices have important implications for the availability, quality, maintenance, and security of the variety of novel structured and unstructured data sources that are part of the big data information value chain.\n",
              "\n",
              "5\n",
              "\n",
              "Big Data: Implications for IS Research\n",
              "\n",
              "The big data information value chain has several implications for IS research. The first set of issues relate to deriving knowledge from big data. In this area, prior editorials have examined the effects of “big research dataset” usage in academic research. These effects primarily pertain to sensemaking, which is the process of deriving knowledge based on information extracted from big data (Lycett, 2013). Below, we discuss the epistemological/paradigmatic issues, theoretical implications, and methodological challenges pertaining to “big research datasets”. However, we must emphasize that research implications of deriving knowledge from big data extend beyond the challenges of using “big research datasets”. The disruption to the traditional information value chain attributable to big data affords a plethora of research opportunities for the three IS research traditions. For instance, as organizations collect and store more customer data than ever before, privacy, security, and ethical considerations come to the forefront. With the proliferation of NoSQL and Hadoop systems, the advent of data lakes, and the popularity of in-memory databases, we need big data modeling formalisms and integration artifacts. Increased emphasis on complex event forecasting, credibility assessment, and social media analytics presents opportunities for novel prediction/description artifacts. Social listening platforms and the Internet of things allow novel forms of analysis pertaining to user-generated content and sensor-based data rich in knowledge, opinions, emotions, location, and geographic information. More broadly, with organizations treating data as a primary asset, assessing and, in some cases, quantifying the value of volume and variety relative to the costs of veracity becomes of paramount importance to evaluate the effectiveness of big data investments. We discuss some of these opportunities in Sections 6-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "viii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Figure 4. Toward a Big Data Research Agenda for IS\n",
              "\n",
              "The second set of implications pertains to big data’s impact on decisions and actions. This area of inquiry, which few prior IS papers have focused on, can potentially view big data and its characteristics as impacting IT artifact-related perceptions and behaviors in behavioral studies. We also believe that the four Vs of big data potentially change the very nature of IT artifacts, much like communication and collaboration technologies altered decision support systems and knowledge management, which gives rise to a new class of big data IT artifacts. IS research needs to not only contribute to the design but also examine the feasibility and effectiveness of such IT artifacts for different stakeholders. For instance, with the proliferation of real-time datadriven decision making and self-service analytics, executives, managers, and front-line employees are increasingly beginning to use big data to support timely decision making, which raises questions about the tension between data and intuition, implications for the nature of decision making, and appropriate ways to quantify the value/impact of the 4Vs on decisions. The above-mentioned issues also prompt researchers to think about the effects of cognition and usability and of the broader organizational context that includes but is\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "ix\n",
              "\n",
              "not limited to organizational culture and leadership, big data investment, and adoption outcomes. Real-time analytics pipelines built on big data platforms are augmenting or automating various business processes. A question that arises is: under what circumstances can (or should) one employ such alternatives for improving business processes? Big data also presents opportunities for designing and implementing novel decisionsupport artifacts and necessitates research on assessing the value of big data IT artifacts in different organizational contexts. In the ensuing sections, we elaborate on these and other opportunities. The key takeaway is that big data is not only different but also highly disruptive to the academic research process and to practice related to data, which requires that we re-assess our research assumptions, methodologies, and substantive questions. Furthermore, due to big data’s impact on people-processtechnology, these implications extend to behavioral science, design science, and the economics of IS. In their paper interestingly entitled “Why IT Fumbles Analytics”, Marchand and Pepper (2013) discuss the usability and cognitive challenges associated with big data analytics. They argue that scholars and practitioners have focused too much on big data’s technical facets and not enough on the people and their institutional and social environments, which has created a lack of socio-technical harmony that IS implementation initiatives often need to succeed. They note that “big data and other analytics projects require people versed in the cognitive and behavioral sciences, who understand how people perceive problems, use information, and analyze data in developing solutions, ideas, and knowledge” (p. 109), which appears to be well aligned with the core strengths of the IS discipline. Keeping in mind the aforementioned sets of implications from the perspective of the information value chain, Figure 4 outlines promising areas of research for big data research in the IS discipline. In Sections 6-8, we discuss possible research areas for the three IS traditions. We reiterate that the goal here is to maintain a broad and inclusive perspective and to be illustrative rather than exhaustive in our coverage.\n",
              "\n",
              "6\n",
              "6.1\n",
              "\n",
              "A Big Data Research Agenda for Behavioral IS Research\n",
              "Epistemological Concerns of Big Data and Behavioral IS Research\n",
              "\n",
              "The four Vs, particularly volume and variety, present challenges not only regarding the changing nature of big data phenomena as they exist in practice but also regarding how, and with what assumptions, research is conducted to investigate such phenomena. Some commentators have brought to question the value of the traditional scientific model of research (which often involves constructing hypotheses based on guesses and then deductively testing the hypotheses using carefully sampled data (e.g., Gregor & Klein, 2014)) in a data-abundant environment associated with big data (Anderson, 2008; Kitchin, 2014b). Will “machinegenerated correlations” on big data be enough to specify “inherently meaningful and truthful” patterns and relationships (Kitchin, 2014b, p. 135)? Can these correlations render front-end theorizing (before empirical analysis), which is at the heart of the scientific research model, meaningless (Kitchin, 2014b)? And, even if these patterns do not lead to understanding, will we be able to accurately predict behaviors, which is all that some important stakeholders may care about (Shmueli & Koppius 2010; Agarwal & Dhar, 2014)? Is an inductive “mode of science” in development, wherein algorithms “spot[s] patterns and generates theories” (Steadman, 2013, in Kitchin, 2014b, p. 131)? Are we nearing the “end of theory” (Anderson, 2008) given that “data tells the truth” under the assumption that studies have access to exhaustive data (that is, n = all), whereas “theory is merely spin” (Kitchin, 2014b, p. 135)?\n",
              "\n",
              "6.2\n",
              "\n",
              "Behavioral IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Many scholars, including Agarwal and Dhar (2014), do not perceive a fundamental transformation in the philosophy underlying research. Rather, they see potential for using a “guided knowledge-discovery” process that leverages big data in conjunction with traditional data-collection methods to generate insights and preliminary hypotheses worthy of further examination through a hybrid process of induction, deduction, and abduction. One perspective is that big data triggers a “paradigm shift towards computational social science” research (Chang, Kauffman, & Kwon, 2014, p. 67). In this perspective, scholars have argued that new “unobtrusive” big data information sources facilitate realism and generality with appropriate levels of control. Examples include using social media and Web clickstreams to enhance customer survey data to better understand the “voice of the customer” (Kaushik, 2011, p. 9), using large social networks to understand the dynamics of influence (Aral & Walker, 2012), and including mobile sensor-based data for enhanced spatial-temporal behavior analysis. Yet, other researchers note that such data is not “neutral, objective, and pre-analytic in nature” but reflects a certain underlying world-view, philosophy, or theory-in-\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "x\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "use that has informed the design of measurement instruments (e.g. Kitchin, 2014b, p. 2). Researchers and data scientists will need to reflect on this perspective on big data when making truth claims. Another major area in the broad realm of computational social science is workforce analytics (Davenport, Harris, & Shapiro, 2010). Today, many organizations use standard statistical techniques to combine employee perceptions derived from surveys with objective data from economic reports and/or measured through technology usage logs and sensors to make connections between employee satisfaction levels and sales, productivity, retention rates, and shrinkage levels (Coco, Jamison, & Black, 2011). For example, Best Buy found that a 0.1 percent increase in employee engagement resulted in a $100,000 annual increase in revenue per store (Davenport et al., 2010). Since workforce analytics examines the interplay between employee perceptions, technology usage, and business value-related outcomes, behavioral IS researchers should be well suited to lead research studies in this important area. While the preliminary results appear promising and present a great opportunity for the IS community to further develop and apply suitable data analytic techniques on large data sets, we need to critically reflect on several issues. First, we need to examine the assumptions underlying the data (e.g., how well the data represents the population and whose interests may be excluded or overrepresented) since the condition of “n = all” does not hold in most cases (Kitchin, 2014b). Second, we need to be aware of the assumptions underlying the analytic techniques (i.e., how value-free the techniques and algorithms are, and how compatible the assumptions underlying the techniques are with the nature of data). Third, and more importantly, we need to ensure that applying big data analytics actually leads to desirable economic and humanistic outcomes for relevant stakeholders. Clearly, both qualitative and quantitative researchers have an important role to play in rethinking and refining how big data is collected, prepared, analyzed, and presented and in investigating the actual processes and consequences of using big data analytics. For example, qualitative researchers, who one might not typically think of big data researchers, can seek to contribute to this arena by examining fundamental questions. These may include: “Are decision making processes at various levels of the organization being transformed due to big data, and, if so, how?” or “What kind of fit is needed (say, between the architecture/algorithms and the organizational structure/culture) for big data initiatives to be effective in organizations, and how can one cultivate such a fit?”. When using big data sources to derive insights, privacy considerations become paramount. In fact, Barocas and Nissenbaum (2014, p. 33) contend that “big data extinguishes what little hope remains for the notice and choice regime”, which provocatively points to the futility of organizations’ sharing their policies regarding collected data and the “opt-out options” they may provide. Analytics concerns how data from various sources is assembled and mined (Fayyad et al., 1996). However, what data actually gets mined, and using what techniques, is often emergent and not necessarily defined upfront, which makes individuals particularly vulnerable to privacy invasions. Interestingly, a poll conducted by KD Nuggets showed that “data mining” has remained the prevalent term in academia, while “analytics” has become more widely adopted in industry over the past decade (KDNuggets, 2011). It appears that “data mining” has a more invasive connotation, whereas industry has carefully marketed analytics as being progressive and associated with intelligence and business value. While analytics may have made data mining more socially acceptable, the underlying privacy concerns, which big data’s characteristics such as volume and variety amplify, remain. We know about many high-profile examples of privacy infringement in both industry and academic contexts, including one where the father of a teenage daughter discovered that she was pregnant owing to Target’s highly accurate marketing efforts (Hill, 2012). Similarly, Facebook’s intentionally manipulating users’ moods has raised many critical questions (Kramer, Guillory, & Hancock, 2014; Agarwal & Dhar, 2014). Often times, organizations tout “informed consent” and upfront notice as an answer to critics; yet, as Barocas and Nissenbaum (2014, p. 32) note, “upfront notice is not possible because new classes of goods and services reside in future and unanticipated uses”. Identifying acceptable levels of intrusion, and finding the right balance (and the principles of balancing) between insights obtained due to access to big data and the infringement such access results in is an important area of inquiry for IS scholars. Similarly, it is virtually unavoidable that big data will continue to see large breaches. With the rise of self-service analytics and with access to sensitive data more pervasive than ever in and across organizations, security behavior challenges regarding compliance, insider threats, and so on will grow as important areas of research. Big data analytics involves deriving knowledge and gaining insights. The pursuit of knowledge has always had a close relationship with ethics. One primary component of ethics already mentioned is privacy considerations. Beyond privacy, firms and researchers leveraging big data may consider deontological ethical criteria of consistency, equality, accountability, integrity, and an all-around conscientiousness (e.g.,\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xi\n",
              "\n",
              "Chatterjee, Sarker, & Fuller, 2009a, 2009b). These issues underlie much of the concerns related to the “transparency paradox 3” and the “tyranny of the minority 4”.\n",
              "\n",
              "6.3\n",
              "\n",
              "Behavioral IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "As industry moves towards self-service analytics using big data, several research questions arise, such as the impact of sources and collection methods on data credibility, the impact of access to knowledge on employee satisfaction and knowledge transfer, and organizational norms for knowledge access and transfer (Chatterjee & Sarker, 2013). Further, as big data introduces novel IT artifacts that support large-scale, selfservice, real-time analyses and decision making from vastly integrated enterprise-wide analytics, behaviors and perceptions remain critical to the process of effectively converting knowledge to appropriate decisions and actions. These emerging data sources, decision making processes, and IT artifacts present an opportunity to revisit questions related to constructs, such as trust, leadership, knowledge transfer, and decision making. Along these lines, Davenport and Harris (2007) analyze numerous successful organizations to derive key elements of the “anatomy of an analytical competitor”. Scholars have used these traits to categorize organizations’ capability maturity levels from aspirational to transformed (LaValle, Lesser, Shockley, Hopkins, & Kruschwitz, 2011). Two of the important differentiating elements they identify are people and culture. With respect to these two elements, Davenport and Harris wonder: how does analytical leadership emerge? What are the characteristics of analytical executives? What role do different c-level executives play? What if executive-level commitment is lacking? What traits of big data analysts make them effective? How do organizations transform from intuition-based decision making to a data-driven decision making paradigm? Edward Deming is often falsely attributed the famous quote “In God we trust, all others bring data”. As we mention earlier, it is often (naively) believed that data and algorithms result in what is necessarily just and true. However, the importance of intuition and judgment, especially in uncertain environments cannot be underestimated (Davenport, 2006; Yetgin, Jensen, & Shaft, 2015). Given that there is a natural tension between data and intuition (Davenport & Harris, 2007), what is the ideal balance between data and intuition? What kind of theory can inform this balancing act? What role does trust play? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? More broadly, we need to understand if and how we should revise existing decision making models (e.g., Hodgkinson & Starbuck, 2008) to reflect how decisions are actually made in organizations using big data and big data analytics. Also, given people and culture can potentially act as impediments to the adoption of big data analytics in organizational settings, what theories and models are appropriate for avoiding implementation failures due to human and cultural issues? From an HCI standpoint, as dashboard-based visualizations become the norm for “managerial cockpits”, we need to investigate what the implications of the four Vs are on how users handle cognitive loads resulting from big data. Finally, we need to conduct balanced assessments of outcomes of the implementation/adoption and use of big data (i.e., big data infrastructure and big data analytics). We need to conduct critical, intensive assessments of the actual impact of big data investment and use and understand if and how one can attain instrumental benefits (such as performance and profitability) and humanistic benefits (such as empowerment and freedom). In other words, big data offers the opportunity to re-examine some of the same phenomena pertaining to decisions and actions that behavioral IS researchers have examined over the years for different waves of technological innovations.\n",
              "\n",
              "6.4\n",
              "\n",
              "Summary of Sample Big Data Research Opportunities for Behavioral IS\n",
              "\n",
              "As Table 1 shows, behavioral researchers have numerous opportunities to contribute to scholarship on big data. Some potential areas include epistemological reflections and methodological development, contributions to computational social science, and reformulations of existing theories and development of new theories of human decision making/human behaviors for the new data abundant environments using traditional or newer computational approaches. In particular, privacy, security, and ethics of big data have significant implications\n",
              "\n",
              "3\n",
              "\n",
              "4\n",
              "\n",
              "This paradox is related to the observation that “datafication” (Lycett, 2013) is assumed to make processes transparent, yet the ways in which data is collected and mined remains unknown/inaccessible to the public (Richards & King, 2013). This is related to the idea that companies tend to make inferences and generalizations based on data from the minority who agree to share data and, thereby, silence (i.e., make irrelevant) the views of those who refuse to give consent (Barocas & Nissenbaum, 2014, p. 32).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "and, hence, deserve special attention. Developing in-depth consultable case studies that capture the intricacies of applying big data approaches in complex social environments would also be of value. Note that the areas listed below in the table are not an exhaustive list of possible research avenues. Rather, they signify the breadth of possibilities and directions that have opened up from this interest in, and trend toward harnessing, big data.\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities Value chain stage(s) Possible research topics • • Deriving knowledge, decisions, and actions Epistemological concerns • • Possible areas of inquiry What implications does big data have for the traditional deductive scientific research model? Is an alternative big data-driven “inductive mode of science” in development or even feasible in the IS discipline? If so, how can we ensure the validity of such knowledge? If not, how can one use the inductive mode in conjunction with the traditional deductive model? What is the role of prediction versus explanation in big data research? How must one adapt traditional research methodologies/methods (qualitative and quantitative) to investigate phenomena of interest in big data environments? How can longitudinal big data channels, such as social media, mobile location, and Web clickstreams, enhance our understanding and explanation of user behaviors? How can sentiments and affects appearing in user-generated content, such as online word-of-mouth, inform our understanding of user behaviors and intentions? What can large online social networks reveal about patterns of influence and/or information propagation? What new insights can work logs and other unstructured sources reveal about relationships between employee actions and employee productivity, satisfaction, and/or customer-oriented outcomes? What are the threats to validity of knowledge computationally derived, and what are the ways to mitigate these threats? What is the nature of theory or theorizing that is consistent with this form of research (i.e., computational social science)? What are the principles by which one can manage invasiveness/infringement of privacy in business enterprises, in academia, and in wider society? How can big data contribute to a better understanding (and resolution) of the privacy paradox in which, on one hand, users desire personalization, innovative technologies, and novel communication channels, and, on the other hand, seek privacy and anonymity? Since informed consent is often seen as ineffective, what other controls/policies need to be put in place? How can we assess and ensure their effectiveness? What implications does big data have for consistency, equality, accountability, integrity and an all-around conscientiousness? What is the impact of sources and collection methods on data credibility? How might access to information and knowledge attained from big data affect employee satisfaction and performance? What should the organizational norms be for access and transfer of knowledge attained through big data analytics? What would be the key elements of an ethical code for organizations and analysts/data scientists using big data and big data analytics? How do organizations/individuals/groups actually make decisions in the big data environment? To what extent do traditional decision making models hold in the new environment?\n",
              "\n",
              "• • • • • • • • Privacy and security concerns •\n",
              "\n",
              "Computational social science\n",
              "\n",
              "Deriving knowledge\n",
              "\n",
              "• • • • • • Nature of decision making •\n",
              "\n",
              "Other ethical considerations\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xiii\n",
              "\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities • • • • Organizational culture and governance • • How does analytical leadership emerge? What are the characteristics of analytical executives? What role does c-level leadership play in firms’ abilities to leverage and “compete on big data analytics?” How do organizations transform from an intuition-based decision making culture to a data-driven decision making culture? What is the role of IT departments in supporting big data analytics? What big data technology architectures and configurations (or analytics techniques) fit with different types of organizational cultures (and governance)? What are the capabilities and constraints of big data information sources in supporting cognition and decision making? As dashboard-based visualizations become the norm for managers, what are the implications of the four Vs on users’ cognitive load and decision making performance? What is the ideal balance between data and intuition? What does trust in big data mean? What role does trust (e.g., trust in big data, people, and processes) play in adopting and effectively using big data? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? What are the key personality traits of good analysts and how do these traits impact data and technology usage and performance? How do we assess big data initiatives when considering the perspectives of relevant stakeholders and the potential instrumental and humanistic outcomes?\n",
              "\n",
              "Leadership\n",
              "\n",
              "• Cognition and usability •\n",
              "\n",
              "Trust and big data versus intuition\n",
              "\n",
              "• •\n",
              "\n",
              "• Adoption and adaptation of big data techniques • and technologies Big data outcomes •\n",
              "\n",
              "7\n",
              "7.1\n",
              "\n",
              "A Big Data Research Agenda for Design Science Research\n",
              "Paradigmatic Considerations for Design Science Research on Big Data\n",
              "\n",
              "We begin by discussing broader paradigmatic considerations of big data on design science; namely, 1) the effects of focusing on IT artifacts that emphasize information more than systems/technology and 2) implications of big data analytics artifacts on kernel design theories. Design is a product and a process (Walls, Widmeyer, & El Sawy, 1992; Hevner, March, Park, & Ram, 2004). The design product is a construct, model, method, and/or instantiation. The design process involves an iterative cycle of “test and learn” or “build and learn, evaluate and learn” (Simon, 1996; Nunamaker, 1992). In big data analytics, the process of deriving knowledge and insights from data is, in some ways, analogous to the design process. The most prevalent process for guiding analytics is the cross-industry standard process for data mining (CRISP-DM) (see Figure 6). CRISP-DM involves iteratively performing several phases 1) problem/business understanding, 2) data understanding, 3) data preparation, 4) modeling, 5) evaluation, and 6) deployment (Chapman et al., 2000). The first phase emphasizes the importance of tackling significant problems/opportunities and identifying data mining goals/success criteria. The second phase involves an inventory of available data sources, including assessing data quality. The third phase involves selecting appropriate sources and specific variables, and cleaning and reformatting the data. The fourth phase includes using predictive, descriptive, or prescriptive analytics method to analyze the data (Chandler et al., 2011). The fifth phase involves evaluating models and their connection to problem/business outcomes. Finally, the sixth phase involves analyzing the artifact in the field outside the lab/production environment. CRISP-DM is to data mining what the system development lifecycle (SDLC) was to traditional information systems development, with CRISP-DM similarly appearing in the introduction section of various data mining and business analytics textbooks. The commonalities between CRISP-DM and the traditional system design process (Simon, 1996; Nunamaker, 1992) presents great potential for design science research geared towards producing novel IT artifacts capable of deriving knowledge and insights from big data. For instance, following the CRISP-DM design process, new constructs, models, methods, and instantiations could enhance BI dashboards or predictive, descriptive, prescriptive, and/or diagnostic model-based technologies (Shmueli & Koppius, 2011; Chandler et al., 2011). Recently, we have already begun to see research in this\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "xiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "vein with new predictive IT artifacts developed using the design science tradition that espouses informal connections to elements of CRISP-DM (Abbasi, Zhang, Zimbra, Chen, & Nunamaker, 2010). Comparing and contrasting CRISP-DM with SDLC is also interesting for another reason. One can consider big data artifacts as a shift in the design science tradition toward more significantly emphasizing artifacts that support information relative to systems or technology. For instance, only one of the stages depicted in CRISP-DM pertains to actually deploying the artifact, whereas at least four stages relate to processing, modeling, and evaluating data/information. CRISP-DM barely emphasizes the key stages of SDLC: users’ system-related requirements, post-deployment system implementation, and system usage/evaluation. While the implications of this shift are not necessarily epistemological in nature, it does introduce paradigmatic considerations for design science. For instance, what is the appropriate balance between information and systems/technology in design research geared toward big data? Furthermore, how does research on the design of information-centric big data IT artifacts relate design science (in IS) to information and computer science? Our discipline has a clear need to have conversations on this issue.\n",
              "\n",
              "Figure 6. The CRISP-DM Analytics Process Embodies Similar Intuitions to the Iterative Design Process Advocated in the IS Design Science Tradition\n",
              "\n",
              "For predictive analytics, big data has also had an impact on the kernel theories providing design guidelines. Classical theories of statistics such as the information theory and Bayes theorem embody simple yet powerful knowledge that have guided the design and development of some of the commonly used predictive and descriptive analytics methods traditionally employed in industry and academia. However, with the growth of big data in recent years, the four Vs introduce various challenges for data mining methods grounded in these traditional statistical theories. The volume of data presents computational constraints. The sparsity and structural nuances of new varieties of data sources, such as text and multimedia, poses representational richness issues. Some have also found traditional statistical methods to be susceptible to veracity concerns. Data-mining methods grounded in the statistical learning theory overcome many of these limitations (Vapnik, 1998) and attain unprecedented results for tasks such as image recognition, text mining, phishing detection, and fraud classification. Consequently, despite appearing in the past 15 to 20 years, Vapnik’s research on the statistical learning theory has already garnered over 150,000 citations according to Google Scholar, with more than half of those citations appearing in the past five years. One can largely attribute the success of statistical learning theory-based methods to their having a strong theoretical foundation and robust analytical power that is well suited for big data. However, other big data analytics methods, such as deep learning (LeCun, Bengio, & Hinton, 2015), do not have significant theoretical underpinnings. Indeed, critics have argued that they provide power without the underlying statistical theory (Gomes, 2014). As design research continues to explore the dichotomy between prediction and explanation (Shmueli & Koppius, 2010), the precise role of kernel theories in big data IT artifacts will remain a central question. A key question is: should IS research adopt an “ends-justify-the-means” perspective in which predictive power trumps methodological\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xv\n",
              "\n",
              "transparency and explanatory potential? We discuss the broader “prediction versus explanation” issue in greater depth in Section 11.\n",
              "\n",
              "7.2\n",
              "\n",
              "Design Science Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Setting aside the aforementioned paradigmatic considerations of designing big data IT artifacts, design science has much to offer in the burgeoning realm of predictive analytics, including novel constructs, models, methods, and instantiations leveraging big data. Scholars in the IS community have also applied predictive analytics at both “micro” and “macro” levels of granularity (Brown, Abbasi, & Lau, 2015). Chang et al. (2014) refer to this range of data granularities as the micro-meso-macro data spectrum. One of the most exciting opportunities pertains to predicting/analyzing micro-level outcomes (Agarwal & Dhar, 2014). Specifically, Brown et al. (2015, p. 6) note: Micro-level predictive analytics involves making inferences about future or unknown outcomes pertaining to individual firms, people, or instances. Micro-level prediction contains greater intraentity information, including perceptual constructs, [transactions/logs on] individual behavior, and spatial-temporal indicators. We have already begun to see some exciting, cutting-edge examples of micro-level prediction. For example, Wang and Ram (2015) predicted individuals’ sequential purchase patterns using spatial, temporal, and social relationship features on a test bed encompassing three million transactions initiated by thirteen thousand customers from nearly three hundred locations. This and other related studies highlight the “art of the possible” and the “art of the valuable” for IS research on novel predictive design artifacts. Future IS research could leverage objective (e.g., observed transactions and logs) and perceptual (e.g., survey, sentiment, voice transcript, and interview) data in conjunction with various intermediate decisions and actions to predict individuals’ behaviors with applications in marketing, e-commerce, security, health, and finance (Abbasi, Lau, & Brown, 2015). There is no doubt that user-generated content sources such as social media have enhanced our understanding of various micro and macro-level phenomena in recent years (Chen et al., 2012), which presents great opportunity for developing social media analytics artifacts for collecting, monitoring, analyzing, summarizing, and visualizing social media data (Zeng et al., 2011). However, a major challenge remains in ensuring high veracity of such data sources. As Zeng et al. (2011, p. 14) note: “issues such as semantic inconsistency, conflicting evidence, lack of structure, inaccuracies, and difficulty in integrating different kinds of signals abound in social media”. We need IT design artifacts capable of identifying, quantifying, accounting for, and alleviating veracity concerns in information sources such as social media by assessing key information quality dimensions such as usefulness, relevance, and credibility. Examples of preliminary research in this vein include work pertaining to online spam and deception detection (Zhang et al., 2014). Such artifacts can be potentially beneficial in various application areas including marketing, finance, public policy, and health (Zeng et al., 2011; Abbasi & Adjeroh, 2014). Big data presents numerous opportunities for new design-oriented work pertaining to the earlier stages of the value chain; namely, data and information. There is a long-standing tradition of design science work on modeling formalisms and ontologies (Wand & Weber, 2002). New forms of user-generated content present opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. A related area involves extending classification principles from conceptual modeling to modeling of information categories in big data, which forms the basis for many forms of predictive and descriptive analytics (Parsons & Wand, 2013). Embley and Liddle (2013) expect conceptual modeling to address big data challenges by adopting the perspective that the design activity related to big data is fundamentally about structuring information. Conceptual modeling research should make big data’s volume searchable, harness the variety uniformly, mitigate the velocity with automation, and check the veracity with application constraints. The IS research community needs to direct some attention to the tasks of investigating, addressing, and/or exploiting big data’s four Vs jointly and effectively. Similarly, database design and data integration have been a major area of focus for prior works (Storey et al., 1997), and there are opportunities for research on the integration of a variety of structured and unstructured data sources available in organizational settings. As we highlight earlier when discussing the big data information value chain, the four Vs have increased the complexity of managing, storing, and integrating data. The challenge remains in investigating and establishing an acceptable architecture to integrate, manage, and implement both structured and unstructured data under one unified platform. The\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "traditional relational data model and the corresponding relational database management systems (RDBMSs) cannot meet the heterogeneity (variety) challenge of big data. Many consider NoSQL as the potential data management solution for big data, but many architectural alternatives exist that range from Hadoop/Spark to Hadoop and RDBMS in parallel to Hadoop (for unstructured data) inputting into RDBMS (Heudecker, 2013; Buytendijk, 2014). We need research to examine the feasibility, fit, and business value of such alternatives and to provide guidelines for big data architectures based on organizational and industry-level contexts. For design research pertaining to knowledge derivation and representation, big data presents both advantages (i.e., volume and variety) and disadvantages (i.e., velocity and veracity). With the large volume and a variety of data sources, big data can certainly enhance ontology learning by automatically deriving domain knowledge by mining unstructured and semi-structured data (Buitelaar, Cimiano, & Magini, 2005). For example, user-generated content contributed freely in social media presents opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. As we allude to earlier in this section, some IS scholars have recently suggested that domain ontologies may play a critical role in the conceptual model for managing and implementing big data (Embley & Liddle, 2013).\n",
              "\n",
              "7.3\n",
              "\n",
              "Design Science Research on Supporting Decisions and Actions from Big Data\n",
              "\n",
              "Scholars have long used design science to guide the design and development of various decision support systems, including group support systems, recommender systems, personalization, contextualization, and collaboration technologies (Nunamaker et al., 1991; Adomavicius & Tuzhilin, 2005; Arnott & Pervan, 2012). They have also used it to develop BI-related DSS artifacts (Chung, Chen, & Nunamaker, 2005). More recently, Lau et al. (2012) developed ABIMA, a big data business intelligence DSS for mergers and acquisitions. ABIMA integrates large volumes of financial metrics derived from structured databases with unstructured sources, such as financial news articles, search engine results, and documents crawled from the Web. The system, practitioners specializing in mergers and acquisitions evaluated, is the type of research that epitomizes how one can use design science for research on big data DSS. Other types of big data IT artifacts that support the decision making process could be ones designed to support real-time decision making that possibly incorporate user feedback-based or system-generated credibility assessments of underlying information sources (Jensen, Lowry, Burgoon, & Nunamaker, 2010; Jensen, Averbeck, Zhang, & Wright, 2013). Given that big data analytics significantly emphasizes enhancing business processes (Davenport, 2006), business process improvement driven by big data constitutes an important research area (Baesens et al., 2014). Potential avenues include developing automated artifacts for discovering and optimizing processes and employing analytics-driven methods in various internal-facing and external-facing business processes that range from operations and human resources to customer relationship management (Davenport & Harris, 2007). As an extreme example of automation, the use of big data analytics to replace human involvement from certain business processes has already begun to take shape (Davenport & Kirby, 2015); in Section 4, we mention real-time analytics pipelines that are replacing traditional business processes. However, in many contexts, big data analytics provides complementary “augmentation” to human-driven processes (Davenport & Kirby, 2015). Augmentation and automation signify a departure from the traditional human-centered computing paradigm toward autonomous computing albeit with varying degrees of separation depending on the level of human involvement. Nevertheless, this shift necessitates reconsidering guidelines for the design product and design process associated with such artifacts (e.g., requirements gathering in contexts where there are no users). When designing such artifacts, what role might theories and principles from, for example, the cognitive computing and artificial intelligence literature play? Given the dynamic and nascent technological and organizational environments related to big data, it would be interesting to see if (and how) one could productively use the action design research (ADR) method to develop robust big data IT artifacts (Sein et al., 2011). For instance, action design research incorporates provisions for varying levels of end user involvement that could provide the necessary flexibility for designing big data artifacts in contexts ranging from traditional user-centered decision support to business process augmentation/automation.\n",
              "\n",
              "7.4\n",
              "\n",
              "Summary of Design Science Research on Big Data\n",
              "\n",
              "Big data presents several wicked problems: how should IS researchers balance a big data-oriented design science research agenda with those being pursued in reference disciplines such as engineering, statistics, and computer science? Goes (2014, p. iv) touches on this challenge by noting that at least five different\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xvii\n",
              "\n",
              "departments at his institution were, in some way, explicitly related to big data research. Goes cautions us by noting that, without guidelines for shaping the IS big data research agenda, “each unit can contribute to the big data paradigm, but at present the approach resembles that well-known cartoon of making sense of an elephant by grabbing isolated parts of the animal”. In our assessment, we can say that, relative to other disciplines, IS design science researchers are uniquely positioned to provide the appropriate mix of rigor along with humanistic and instrumental relevance. Further, our research often seeks to offer generalizable design principles and guidelines abstracted from the development of contextualized big data IT artifacts that can potentially help address other important problems. The sample research opportunities in Table 2 reflect this view. We believe the design science perspective on big data analytics represents an important future area of emphasis for IS research.\n",
              "Table 2. Big Data and Design Science Research: Sample Research Opportunities Value chain stage(s) Possible research topics Possible areas of inquiry • What is the appropriate balance between information and systems/technology in design research geared toward big data in the IS discipline? • What implications does big data IT artifacts’ potential shift in focus from systems to information have for the design process? • How might the characteristics of big data affect the nature of kernel design theories that are potentially useful? • What is the IS “signature” for big data design research (i.e., what is the scope/nature of big data IS design artifacts relative to reference disciplines such as computer science, marketing, engineering, and statistics)?\n",
              "\n",
              "Deriving knowledge, decisions, and actions\n",
              "\n",
              "Paradigmatic considerations\n",
              "\n",
              "• How can one leverage the volume and variety of big data to develop novel artifacts for predicting/describing macro versus individual/micro-level phenomena or events? Novel artifacts for • How can design science research build novel artifacts for deriving knowledge prediction or from big data sources, such as user-generated content, to advance research in description other disciplines, including marketing, finance, and health? • How can design guidelines of big data analytics artifacts better compensate for the veracity of input data? What novel veracity-assessment artifacts can we develop to shed light on information relevance, usefulness, and credibility? Deriving knowledge Modeling formalisms and integration artifacts • Can new forms of user-generated content enrich existing ontologies, enable the development of new ones, and introduce new conceptual models and grammars? • What is the potential for extending classification principles from conceptual modeling to modeling of information categories in big data? • Can conceptual modeling address some of the challenges of big data by making the volume searchable, harnessing the variety uniformly, mitigating the velocity with automation, and/or checking veracity with application constraints? • How can design science inform the state-of-the-art integration, management, and implementation of organizational big data initiatives in light of the four V challenges? • What design theories do we need to guide big data architectures based on organizational and industry-level contexts?\n",
              "\n",
              "• How can IS contribute guidelines for design artifacts that support real-time Novel IT artifacts decision making from big data? for decision • What is the role of credibility assessment as a design guideline in big data support decision support systems? • What is the potential for automated process discovery and optimization? Decisions Business process • How can big data analytics improve business processes? and actions improvements • Are existing design theories sufficient for real-time big data analytics and automation environments in which run-time and autonomy considerations create nuanced design requirements? What are the alternative theories that may be valuable? Big data action design research • Given the emerging nature of big data, how can we use approaches such as action design research (ADR) to guide the development and harnessing of big data IT artifacts in organizational settings?\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "8\n",
              "8.1\n",
              "\n",
              "A Big Data Research Agenda for the Economics of IS\n",
              "Epistemological Concerns for Big Data and the Economics of IS\n",
              "\n",
              "The economics of big data has important implications for information systems. Just as scholars once used the “economics of information” to describe the value of information asymmetries in marketplaces (Stigler 1961), now, with firms competing on analytics, access to information that can enable enhanced analytical capabilities and insights facilitating differentiation has ushered a new era of “knowledge is power”. Quantifying this power is critical. Beyond some of the issues discussed in Section 6.1, the epistemological implications for the economics of IS community primarily relate to research methodology. These include the deflated p-value problem (Lin, Lucas, & Shmueli, 2013), increasing emphasis on prediction versus explanation (Shmueli & Koppius, 2010), and construct validity when using unstructured and log-based data sets; however, given that many of these issues are applicable to multiple IS research traditions, we discuss them in greater detail in Section 11.\n",
              "\n",
              "8.2\n",
              "\n",
              "Economics of IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "The value of information has been a longstanding area of inquiry in the economics of IS tradition (Banker & Kauffman, 2004). In the context of big data, assessing information’s value is more critical than ever. One research direction analyzes the relative value contributions of the four Vs (e.g., value of data volume and variety) for deriving knowledge from big data. As organizations treat data as an asset (and, in many Web 2.0 business models, as the primary asset), quantifying its value has become a major discussion topic both from a broad business value perspective (which includes the implications for third party data brokers and data markets) and from a more traditional accounting perspective. This emphasis on data as an asset has spurred infonomics: the theory, study, and discipline of assigning economic significance to information. For instance, a recent McKinsey report states that public data sources pertaining to education, energy, healthcare, transportation, and consumer finance (collectively dubbed “open data”) have the potential to create USD$3 trillion in annual business and/or societal value (Manyika et al., 2013). At a more micro level, individual organizations are routinely interested in identifying the most useful public/private data sources and quantifying the precise value of these sources. For example, Bardhan, Oh, Zheng, and Kirksey (2015) used demographic, clinical, and administrative data from 67 hospitals in northern Texas gathered over a four-year period to build models capable of predicting and describing congestive heart failure patient readmissions. Their model demonstrates the importance of health IT-related variables, which prior or baseline models have not considered impactful but that could help hospitals save millions of dollars by avoiding costly readmission-related penalties. Many other recent studies also suggest that using more data instances and variables can improve predictive capabilities (Junque de Fortuny et al., 2013). These findings raise questions regarding complexity, model management, and cost-benefit tradeoffs. Researchers wonder: if bigger is better, how much is too much? As Junque de Fortuny et al. (2013, p. 219) ask, “is it worth undertaking the investment to collect, curate, and model from larger data sets?” Prior to big data’s rise, firms were beginning to derive the last iota of predictive power from a set of data they had access to often by building predictive models that were increasingly complex. Netflix is one example. In 2006, Netflix offered a US$1 million prize to any team that could improve their existing movie recommendation models by 10 percent. The competition concluded on July 26, 2009. Over the final two years, the performance of the best predictive models improved by less than 2 percent, whereas the complexity of the solutions increased dramatically. The final winning model from a team appropriately named BellKor’s Pragmatic Chaos blended results from hundreds of underlying base models. Similarly, scholars commonly describe loan risk-assessment models at financial services firms developed in the past two decades, which largely leverage the same structured data sources, as complex, variegated black-box arrangements of models on top of models delicately combined (Derman, 2011). Former Goldman Sach’s Lead Quantitative Analyst Emmanuel Derman highlights the pitfalls of complexity and poor model management over time in his book Models Behaving Badly. He describes the role played by complex models in the 2008 financial crisis (Derman, 2011); the narrative reminds readers of the classic saying “complexity is death”. Big data creates an opportunity to not only enhance these models’ analytical capabilities but also reduce the inherent risks associated with using them. However, not all information sources are created equal. A key challenge facing organizations is finding a way to quantify the value of information that considers both insightfulness and risks.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xix\n",
              "\n",
              "Research on pricing for data sets/data sources in the booming data broker markets can help firms make more-informed decisions in the data marketplace. Here, all of the standard services management questions apply, including those that Rai and Sambamurthy (2006) articulate. What are the best strategies for bundling of data? Should firms pursue flat or usage-based monthly service rates versus one-time sales? How do alternative service rate plans impact data usage? Big data’s impact on data-based marketing and pricing, omni-channel marketing (Song, Sahoo, Srinivasan, & Chrysanthos, 2014), and attribution are other potential areas of inquiry. The value of information also raises questions about intellectual property rights, especially in the context of user-generated content. The cost of veracity can potentially offset the value of data volume and variety. The existing body of knowledge on data warehousing and business intelligence, which one can consider a close predecessor to big data, has emphasized the importance of data quality as an antecedent for the success of data warehousing initiatives (Wixom & Watson, 2001). Hence, quantifying the adverse effects of incomplete or inaccurate information is essential yet challenging for mitigating risk in the era of big data analytics. Such research could connect data quality to the effectiveness of business outcomes. The analysis of social media has garnered considerable attention from the economics of IS community in recent years with many outstanding avenues of inquiry. As organizations move towards the “socialecosystem” encompassing the use of social media for various employees and customer-oriented activities, we can ask how firms can leverage social media for internal communication and collaboration, external engagement, and listening/ideation (Zeng, Chen, & Lusch, 2010). What is the value of insights, engagement, and internal communication usage through social technologies? The interplay between social media channels and marketing effectiveness is another important area receiving considerable attention in the economics of IS community (Song et al., 2014). In that vein, questions include: what are the key factors impacting social media marketing effectiveness? How does peer influence impact social media marketing? What is the role of social media in viral marketing? A recent related stream of work examined the usefulness of location and geographic information for the analysis of choice, price, competition, and mobile marketing.\n",
              "\n",
              "8.3\n",
              "\n",
              "Economics of IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "The economics of big data analytics extend further down the information value chain beyond knowledge acquisition to decisions, actions, and their ensuing consequences. In his book entitled The Value of Business Analytics, acclaimed business analytics guru Evan Stubbs talks about the challenges business analysts and data scientists face when attempting to quantify the value of an analytics project or portfolio of initiatives (Stubbs, 2011). For example, when should firms invest in big data, and what are the potential returns? As one answer to this question, Tambe (2014) found that firms with significant existing data sets who invested in Hadoop were associated with 3 percent faster productivity growth. A related question is: what implications does big data’s ushering in the increased usage of cloud-based SaaS and DBaaS have for platform economics and strategy? Research on quantifying the business value of big data analytics, including the implications of the four Vs, could inform the existing body of knowledge. Potential research directions include work measuring the return on investment for big data technologies, the impact of data variety and veracity on quality of decision making, and the economics of real-time decisions using big data.\n",
              "\n",
              "8.4\n",
              "\n",
              "Summary of Economics of IS Research on Big Data\n",
              "\n",
              "In some ways, the economics of IS tradition is ideally suited to tackle problems pertaining to deriving knowledge from big data. In particular, beyond certain methodological adjustments (see Section 11), the empirically driven, inductive reasoning-oriented applied econometrics approach is well aligned to addressing these types of questions. The economics of IS also has an essential role to play in examining the economic value of big data insights and big data analytics-driven decision making. Table 3 summarizes some of the important research opportunities.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Table 3. Big Data and Economics of IS: Sample Research Opportunities Value chain stage(s) Deriving knowledge, decisions, and actions Possible research topics Possible areas of inquiry\n",
              "\n",
              "• How must traditional research methods be adapted to investigate big data Epistemological environments? and/or • What is the role of prediction versus explanation in big data research? methodological • How can we ensure the validity of constructs derived from noisy unstructured concerns and log-based data sources? • What is the value of various data sources and channels in terms of quality of insights, enabling new capabilities, and quantifiable business value? • Regarding the impact of data volume on insights and business value, how much is enough and how much is too much? • What is the value of data volume and variety from a risk management perspective? • As data becomes an asset, what role can third party data brokers and data markets play? What are the pricing and market structure implications? • As firms monetize user-generated content, what are the implications for intellectual property? • How can the volume and variety of data inform data-based marketing and pricing? • What are the benefits and challenges of data variety for omni-channel marketing analysis and attribution? • How can we quantify the business impact of low veracity data? • Which types of data quality issues are the most impactful?\n",
              "\n",
              "Value of data, volume, and variety\n",
              "\n",
              "Deriving knowledge Cost of veracity\n",
              "\n",
              "• What is the role of social media in enterprises for internal communication, customer engagement, and listening/ideation? • Regarding the economics of social media, what is the value of insights, Social media and engagement, and internal communication usage through social technologies? economics of IS • What are the key factors that impact social media marketing effectiveness? • How does peer influence impact social media marketing? • What is the role of social media and incentive schemes in viral marketing? Impact of location and geography • How can location and geographic information impact research on choice, pricing, and competition? • What are the implications of geo-targeting in mobile marketing?\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Quantifying value • What is the impact of data variety and veracity on the quality of decision and impact of four making? Vs on decision • What are the key factors influencing business value in the context of real-time making decision making using big data? • How do we measure the return on investment for big data technologies? Value of big data • When should firms invest in big data, and what are the potential returns? IT artifacts • As big data ushers in increased usage of cloud-based SaaS and DBaaS, what are the implications for platform economics and strategy?\n",
              "\n",
              "9\n",
              "\n",
              "Cross-tradition Research on Big Data\n",
              "\n",
              "There are many opportunities for research at the cross-sections of behavior, design, and economics of IS. In some respects, big data’s scale and complexity afford and encourage cross-tradition research projects. The work on human-computer systems design has traditionally been at the intersection of design science and behavior, such as cognitive psychology and decision science (Banker & Kaufmann, 2004). In this vein, one obvious direction is to design and develop big data IT artifacts that researchers may subsequently evaluate in terms of their positive impact on behavior (Zahedi, Abbasi, & Chen, 2015). Another connection between the design and behavioral traditions relates to research on IS strategy. Big data analytics ultimately focuses on improving business processes (Davenport & Harris, 2007) to attain a strategic competitive advantage, a perspective that is highly congruent with the resource-based view of the firm. Designing enterprise-wide big data analytics in a manner that maximizes the potential for competitive advantage in different types of industries and for different organizational cultures and governance archetypes is a potentially valuable direction. Related to this area, knowing how to align alternative big data architectures\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxi\n",
              "\n",
              "(such as ones based on Hadoop, traditional RDBMS, or hybrid models) with business strategy and a firm’s data environment and understanding important criteria and success factors in the decision making process remain important issues. Researchers can use established approaches in IS such as case studies, laboratory experiments, and survey studies in investigating such topics. Design science and the economics of IS also appear to have potential in the context of big data research. One example is cost-sensitive classification or regression in which one quantifies the values of true and false positives/negatives and incorporates them into the design of predictive artifacts (Bansal, Sinha, & Zhao, 2008; Zhao, Sinha, & Bansal, 2011). Here, existing work has mostly focused on the predictive artifact and less on the methodology for deriving cost matrix values. The value of information in big data IT artifacts represents another important area at the cross-section of design and economics of IS. Recently, many studies have designed novel IT artifacts focused on mining big data sources as decision-support aids (e.g., Lau, Liao, Wong, & Chiu, 2012). It remains unclear what the business value of such artifacts truly is with respect to key business performance indicators. Another already potent research area at the cross-section of design and economics of IS pertains to optimization (Banker & Kaufmann, 2004). Here, big data’s variety presents opportunities. For instance, online product reviews could possibly enrich product design optimization (e.g., Balakrishnan & Jacob, 1996). Similarly, consumer sentiments and demand forecasts based on user-generated content could enhance pricing optimization; however, in both of these examples, the tension between the value and veracity of social media and other user-generated data sources could present an interesting dynamic, worthy of an in-depth inquiry. Big data presents numerous opportunities at the intersection of economics of IS and behavior as well. For instance, behavioral economics is a well-established area (Tversky & Kahneman, 1974). In the context of big data, research could examine managers’ propensities to combine data of varying quality/credibility with, for instance, intuition in real-time versus non-real-time settings. Such work would borrow from theories in economics, cognitive psychology, decision making, and risk management (Banker & Kaufmann, 2004).\n",
              "\n",
              "10 Big Data: Implications for Theory\n",
              "Many scholars have reflected on big data’s possible implications on theory. One point of view, albeit extreme, is that big data renders the role of theory—sometimes seen as fictional and value-laden— unnecessary and obsolete, and replaces it with patterns derived directly from data that reflect nothing but the truth (e.g., Kitchin, 2014b). On the other hand, many scholars argue that, in the absence of theory, data lacks “order, sense and meaning” and that “theories without data are empty; data without theories are blind” (Harrington, 2005, p. 5, cited in Sarker et al., 2013, p. xiiii). While this debate is likely to continue without immediate resolution, we do not foresee theories disappearing or diminishing in importance because of research using big data. To the contrary, we foresee that some of the theories will become more robust because “researchers now have a medium for theory development through massive experimentation in the social, health, urban, and other sciences” (Agarwal & Dhar, 2014, p. 444). This is in part due to easier data collection and enhanced control and precision, realism, and generality associated with big data (Chang et al., 2014). At a broader level, we believe that scholars have the opportunity to reflect on the changing nature of the theorizing process and on the characteristics of theories developed in a data-abundant environment. From an information value chain perspective, some recent big data IS studies and editorials have touched on the role of theory when leveraging big data sources for discovering knowledge. As previously mentioned, we believe that, in addition to big data “information sources”, big data’s characteristics embodied in the four Vs afford important opportunities for research, that can both borrow from novel theories and contribute to existing theories, related to deriving knowledge as well as decisions and actions. Below, we outline a few examples. The impact of data variety and velocity on problem-solving accuracy and time constitutes an important research topic. The cognitive fit theory (CFT) provides an excellent and robust theoretical lens for examining this topic. Earlier work on CFT examined the importance of congruence between problem task and problem representation on users’ mental representation and overall problem-solving performance (Vessey, 1990). While initial studies focused on tables versus graphs, subsequent work extended CFT to other specialized representations such as maps (Dennis & Carte, 1998), and considered the impact of users’ prior domain knowledge and the effect of subtasks (Shaft & Vessey, 2006). All of these findings have important implications when examining the impact of big data characteristics for problem solving in general, and specifically in the context of dashboards depicting a variety of information in real-time. On the other hand, big data characteristics, such as variety and velocity, can also potentially offer theoretical extensions. Data\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "visualization dashboards often incorporate multiple tabs with coordinated views depicting real-time data (Andrienko & Andrienko, 2003). The effects of problem solving in such multi-representation, multi-subtask, real-time situations remain unclear, and this offers great potential to contribute to theory. CFT, with suitable adaptations, can also inform the design/construction of novel user interface artifacts (Vance, Lowry, & Egget, 2015) for presenting big data. Organizations using big data routinely ask managers and analysts to monitor and present key findings using reporting tools that integrate traditional structured data sources with novel social listening, web clickstream, sensor-based, and open data. Practitioner studies have suggested that analysts often do not perceive such tools to be useful, with obvious implications for the business value of such artifacts (Kaushik, 2011). Adoption models represent an excellent theoretical lens for examining the impact of perceived usefulness and ease of use on behavioral intention to use such reporting tools and actual use (Davis, 1986, 1989). The effects of mandatory versus voluntary reporting and trust are also important considerations (Brown, Massey, Montoya-Weiss, & Burkman, 2002; Gefen, Karahanna, & Straub, 2003). In turn, big data’s four Vs could provide important insights that can inform the extensive body of knowledge pertaining to technology adoption (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, forthcoming). For instance, the variety of data could have potentially contrasting effects on users’ perceptions of usefulness and ease of use, which users’ levels of experience may moderate. Further, as data veracity becomes increasingly relevant to big data IT artifacts, the implications for trust (in both the data and the artifact) and eventually for behavioral intention to use also present interesting issues to investigate. Chaos theory studies the behavior of dynamic systems that are highly sensitive to initial conditions, where small changes in initial conditions can yield widely diverging outcomes (Gleick, 1987; Sprott, 2003; Werndl 2009). Prior studies have already discussed the potential of big data for theory development via computational social science or massive experimentation (Agarwal & Dhar, 2014; Chang et al., 2014). Chaos theory could be beneficial in macro-level computational social science research, since it “appears to provide a means for understanding and examining many of the uncertainties, nonlinearities, and unpredictable aspects of social systems behavior” (Kiel & Elliott, 1996). Furthermore, as IS design science research explores novel predictive artifacts utilizing big data, chaos becomes an important consideration. The inclusion of big data should make predictive artifacts more accurate, stable, and valuable. However, for complex event forecasting in situations where chaos is present, prediction can be problematic since model assumptions, which are typically based on probabilities of various patterns (i.e., connections between observed initial conditions and eventual observed outcomes), may not hold true (Sprott 2003). Consequently, seemingly small errors in initial condition prediction probabilities can result in large errors between longer-term forecasts and actual outcomes (Werndl, 2009). We are already beginning to see Chaos theory concepts incorporated in problems such as weather forecasting and traffic prediction, where, in turn, the results are informing our understanding of chaos in these application areas. Somewhat related to chaos is black swan theory, which focuses on highly improbable or surprising, highimpact events that are often incorrectly rationalized in hindsight (Taleb, 2005, 2007). The key idea is that probability-centered analysis and thinking that diminishes the importance of outliers or the unobserved is problematic for appropriately managing the risks associated with black swan events. Taleb’s views on the limitations of statistics and his seemingly negative portrayal of statisticians has raised several (possibly valid) rebuttals from the statistics community (e.g., Westfall & Hilbe, 2007). Nevertheless, his central tenet appears to have merit. As big data further creates the shift towards data-driven decision making, risk management pitfalls such as attempting to predict extreme events, overreliance on the past, psychological biases against less likely outcomes, and overemphasis on standard deviations are likely to be exacerbated (Taleb, Goldstein, & Spitznagel, 2009). Black swan events also have important implications for the design of process automation relying on big data. We have already seen automated loan risk assessment and algorithmic trading engines fail miserably due to such events (Taleb, 2007; Derman, 2011). Black Swan Theory could shed light on studies examining risk considerations in data-driven decision making where traditional decision theories may be inadequate. Similarly, it can inform the design of process automation in big data environments such that the unknown unknowns are given proper consideration. In summary, big data has potentially important implications for theory. From an information value chain perspective, big data sources and associated IT artifacts have distinct implications for both knowledge acquisition and for decisions and actions (and related outcomes), which include system usage, performance, and satisfaction. The key nuances of big data artifacts stem from the four V characteristics. On one hand, these characteristics may simply inform well established IS theories by playing the role of antecedent constructs or of moderator/mediator variables. On the other hand, these characteristic can\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxiii\n",
              "\n",
              "introduce complexity and risk in IT artifacts increasingly relying on big data, thereby opening up exciting new possibilities for utilization of theories that have seen relatively limited usage in IS. Our final comment regarding theory and big data is while one cannot understate the role of “theory” in big data research, we do need to acknowledge that theory has different forms in different traditions of research, and, thus, as research community, we need to be open to different types of abstractions offered as theoretical contributions.\n",
              "\n",
              "11 Big Data: Implications for Methodology\n",
              "The characteristics of big data test beds have important implications for the norms of analyses. One significant implication that has recently garnered attention is the “deflated p-value” problem. In their Academy of Management Journal editorial, George et al. (2014) suggest that the statistical methods and metrics used to examine big data sets may need to incorporate alternative techniques from statistics, computer science, applied mathematics, and econometrics. They state (p. 323): “The typical statistical approach of relying on p values to establish the significance of a finding is unlikely to be effective because the immense volume of data means that almost everything is significant”. In addition to statistical significance and co-efficient signs, one may also need to consider effect sizes and variance when testing hypotheses on big data sets (Lin et al., 2013; George et al., 2014). As Chatfield (1995, p. 70) notes: “The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical significance, what is the practical significance of the results?”. To quantify the extensiveness of the problem, Lin et al. (2013) examined nearly 100 IS papers published between 2004 and 2010 with research test beds exceeding 10,000 instances and concluded that nearly half failed to discuss practical significance. As we note in Section 7 and as George et al. (2014) and others allude to, analyzing big data often requires using computer science-based methods grounded in machine learning and artificial intelligence rather than statistics. For instance, genetic algorithms are a computationally effective non-deterministic heuristic method for searching an NP-hard problem’s solution space and researchers/analysts have used them for variable feature selection in many predictive analytics problems involving thousands of input variables. Similarly, deep learning methods have enabled neural networks to attain impressive classification accuracies on large data sets (LeCun et al., 2015). Deep learning methods add additional layers of processes capable of learning or representing complex patterns at the expense of further degrees of separation between the model output and the underlying model intuition. Consequently, such methods also constitute a departure from traditional statistical methods, such as ordinary least squares or simple logistic regression analyses that prior IS studies have commonly used for both explaining and predicting. Shmueli and Koppius (2010) note that there is a difference between models geared toward prediction and those geared toward explanation. In many ways, big data amplifies this dichotomy as powerful non-deterministic and/or “black box” methods gain prominence for their predictive capabilities. In many cases, such methods produce answers to the “what” without the “why”. Another implication of big data sets is construct validity/credibility concerns pertaining to variables derived from user-generated, non-survey-based data sources, often characterized by low veracity. In addition to the spam and deception traits inherent to user-generated content sources (e.g., social media) and clickstreams’ data-tracking limitations, scholars operationalize many unstructured data sources as a few structured variables. We can see one prominent illustration of this point in the context of user sentiment polarity: whether the user is expressing a positive, negative, or neutral sentiment toward a given topic. Due to the volume of big data, one typically derives such constructs using software packages that rely on natural language processing methods as opposed to traditional manual coding methods. There have been thousands of studies published using social media sentiments in the recent years, including several in IS outlets. Such studies routinely make conclusions about the impact of user sentiment-related independent variables. However, scholars rarely report information on the suitability and accuracy of the underlying sentiment classification models used to operationalize the constructs, which happens despite the fact that benchmarking studies have found that many state-of-the-art sentiment analysis methods’ sentiment polarity classification performances are subpar, which affects the sentiment-related analysis and conclusions drawn from it (Hassan, Abbasi, & Zeng, 2014). Moving forward, we need research-based guidelines on how to validate variables derived from natural language, clickstream, sensor, and other big data sources.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Another important issue is to consider how the penetrative, imaginative understanding of human meanings discerned by qualitative (particularly interpretive) researchers using small data can complement (rather than be substituted by) patterns derived from big data using machines/techniques/algorithms, to be able to offer a more complete picture of the phenomenon. Indeed, an emerging stream of work illustrates how findings based on qualitative “idiographic” approaches may mutually inform findings based on computational methods (Gaskin, Berente, Lyytinen, & Yoo, 2014). We are also aware that grounded theory researchers in IS (i.e., the SIG GTM community) are looking for ways in which the grounded theory methodology (GTM) principles can be effectively utilized in big data settings. Clearly, big data is requiring us to reexamine how we analyze and validate data and interpret and discuss the findings. We need further research to assess more thoroughly the pros and cons of different methods and metrics on various types of big data sets, and to provide meaningful guidelines. In particular, due to the volume, variety, and veracity dimensions, we need to be watchful about big data’s creating “false positives” in terms of statistical significance of independent variables or considerably altering the effect size. Finding ways and principles that can aid in effectively complementing and/or triangulating big data research with small data research is another issue we must take seriously. Achieving “consilience—that is, convergence of evidence from multiple, independent, and unrelated sources”— needs to be a matter of priority (George et al., 2014, p. 324).\n",
              "\n",
              "12 Closing Thoughts\n",
              "The arena of big data/big data analytics has captured the attention and imagination of both practitioners and academics in a variety of disciplines, not just in IS. Commentators have described the big data phenomenon as a “deluge” and as having the potential to cause long-lasting impacts on practice and academia (e.g., Anderson, 2008; Kitchin, 2014b). Indeed, thought leaders and editors of leading IS journals see much reason for optimism regarding big data’s impacts on the IS discipline. For example, Goes (2014, p. viii) has encouraged the field to “embrace the changes and provide leadership in the new environment… [for which we] are uniquely positioned” and to “claim our [rightful] territory”. Agarwal and Dhar (2014) view big data as an opportunity for ushering in a “golden age for IS researchers”. Yet, not all scholars from IS and other disciplines unquestionably accept projections of such promise (e.g., Buhl, Röglinger, Moser, & Heidemann, 2014). Some, including Professor Michael Jordan, a “machine-learning maestro”, predict the onset of “big data winter” if we continue to over-promise and overhype (Gomes, 2014) without addressing fundamental epistemological and methodological issues associated with big data (e.g., Kitchin, 2014b). Furthermore, we must address the concerns regarding the erosion of privacy and, consequently, the loss of human dignity in the face of economic imperatives (e.g., Barocas & Nissenbaum, 2014) that can lead to “digital colonization” (Buhl et al., 2014) and the “subjugation” of human interests by machines and algorithms, which socio-technical scholars have for long been wary of (e.g., Bjorn-Andersen, Earl, Holst, & Mumford, 1982). In addition, the rhetoric of making academic research relevant by helping solve immediate organizational problems through big data without adequate abstraction or without designing approaches or artifacts for addressing broad classes of problems raises questions regarding how academic research differs from practice. Also, in line with Benbasat and Zmud (2003), many scholars believe that routinely engaging in big data projects without a unique disciplinary “signature” could prove to be ominous for the IS discipline in the long run. Hence, our position is one of cautious optimism. While we undoubtedly see potential for big data in contributing to a stronger and more relevant IS discipline—one that would have a significant social impact— we do not take the benefits for granted. To help big data research achieve its potential, we invite IS scholars to: a) critically engage with fundamental issues, such as epistemology, methodology, ethics, and the design of novel artifacts; b) rethink decision models proposed in the era of scarce data and adapt them for use in the current era of abundant data; and c) assess economic and humanistic outcomes of big data in the form of systematic, multi-paradigmatic research initiatives across the information chain value. Further, to ensure a healthy development of scholarship in this area, we see the need to carefully balance “real-world” problemsolving using big data/big data techniques with reflective inquiry and scholarly abstraction of knowledge in this area. We also encourage big data researchers from the IS discipline to participate in boundary-spanning interdisciplinary big data projects, but to balance engagement with other disciplines with conscious development and nurturing of big data approaches and objectives that are somewhat consistent with the IS discipline’s socio-technical heritage. In this editorial, we do not intend to provide definite answers or directions but to encourage inquiry, reflection, and debate on this topic by IS scholars embedded in diverse theoretical and methodological traditions. We\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxv\n",
              "\n",
              "are hopeful that the framework (Figure 4) and the accompanying tables (Tables 1, 2, and 3), while preliminary, will help energize the conversation on big data in the broader IS community and provide a roadmap for advancing scholarship in the area. Our final comment is related to teaching, which we believe is our raison d'être. No other academic unit has the diversity of research traditions and understanding of the business, information, technology, and human issues that are essential to comprehending the various facets of the big data value chain (e.g., Agarwal & Dhar, 2014). This diversity places us in an excellent position to offer pedagogical leadership in teaching, developing curricula, and programs and to initiate industry outreach centers (Chiang, Goes, & Stohr, 2012). In summary, we are convinced that big data is here to stay. However, we can foresee a time when big data will not be at the forefront of our conversations, as we have seen in the cases of expert systems, BPR, ebusiness, ERP, and groupware. Yet, few will disagree that the ideas underlying these topics have continued, and will continue, to be important knowledge areas informing research and practice in IS. We expect that big data research will do the same. For now, big data offers a stage for learning new lessons, re-learning and refining old lessons, and reflecting on assumptions that underlie our research endeavors and the complex ways in which technology, information, and humans interact to shape the world we live in.\n",
              "\n",
              "Acknowledgments\n",
              "We thank Kenny Cheng, Dirk Hovorka, Steven Johnson, Vijay Khatri, Brent Kitchens, Sridhar Nerur, and Tony Vance for their constructive comments on an earlier version of the editorial. The reactions of TSWIM 2015 participants to a keynote talk by one of the authors also helped shape several ideas, for which we are grateful. Likewise, we greatly appreciate feedback from faculty and doctoral students that participated in the 2015 Antai Graduate Summer School at Shanghai Jiaotong University.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "References\n",
              "Abbasi, A., & Adjeroh, D. (2014). Social media analytics for smart health. IEEE Intelligent Systems , 29 (2), 60-64. Abbasi, A., Zhang, Z., Zimbra, D., Chen, H., & Nunamaker, J. F., Jr. (2010). Detecting fake websites: The contribution of statistical learning theory. MIS Quarterly, 34(3), 435-461. Abbasi, A., Lau, R. Y. K., & Brown, D. E. (2015). Predicting Behavior. IEEE Intelligent Systems, 30(3), 35-43. Adomavicius, G., & Tuzhilin, A. (2005). Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6), 734-749. Agarwal, R., & Dhar, V. (2014). Editorial: Big data, data science, and analytics: The opportunity and challenge for IS research. Information Systems Research, 25(3), 443-448. Alavi, M., & Leidner, D. E. (2001). Review: Knowledge management and knowledge management systems: Conceptual foundations and research issues. MIS Quarterly, 25(1), 107-136. Anderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. Retrieved from http://www.wired.com/2008/06/pb-theory/ Andrienko, N., & Andrienko, G. (2003). Informed spatial decisions through coordinated views. Information Visualization, 2(4), 270-285. Aral, S., & Walker, D. (2012). Identifying influential and susceptible members of social networks. Science, 337(6092), 337-341. Arnott, D., & Pervan, G. (2008). Eight key issues for the decision support systems discipline. Decision Support Systems, 44(3), 657-672. Arnott, D., & Pervan, G. (2012). Design science in decision support systems research: An assessment using the Hevner, March, Park, and Ram Guidelines. Journal of the Association for Information Systems, 13(11), 923-949. Bardhan, I., Oh, C., Zheng, E., & Kirksey, K. (2015). Predictive analytics for readmission of patients with congestive heart failure: Analysis across multiple hospitals. Information Systems Research, 26(1), 19-39. Baesens, B., Bapna, R., Marsden, J. R., Vanthienen, J., & Zhao, J. L. (2014). Transformational issues of big data and analytics in networked business. MIS Quarterly, 38(2), 629-632. Balakrishnan, P. V., & Jacob, V. S. (1996). Genetic algorithms for product design. Management Science, 42(8), 1105-1117. Bansal, G., Sinha, A. P., & Zhao, H. (2008). Tuning data mining methods for cost-sensitive regression: A study in loan charge-off forecasting. Journal of Management Information Systems, 25(3), 315-336. Banker, R. D., & Kauffman, R. J. (2004). 50th anniversary article: The evolution of research on information systems: A fiftieth-year survey of the literature in management science. Management Science, 50(3), 281-298. Barocas, S., & Nissenbaum, H. (2014). Big data's end run around procedural privacy protections. Communications of the ACM, 57(11), 31-33. Benbasat, I., & Zmud, R. W. (2003). The identity crisis within the IS discipline: Defining and communicating the discipline's core properties. MIS Quarterly, 27(2), 183-194. Bjørn-Andersen, N., Earl, M., Holst, O., & Mumford, E. (1982). Information society: For richer, for poorer, Amsterdam: North-Holland. Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxvii\n",
              "\n",
              "Broniatowski, D., Paul, M. J., & Dredze, M. (2014). National and local influenza surveillance through Twitter: An analysis of the 2012-2013 influenza epidemic. PLoS One, 8, e83672. Brown, S. A., Massey, A. P., Montoya-Weiss, M. M., & Burkman, J. R. (2002). Do I really have to? User acceptance of mandated technology. European Journal of Information Systems, 11(4), 283-295. Brown, D. E., Abbasi, A., & Lau, R. Y. K. (2015). Predictive analytics: Predictive modeling at the micro level. IEEE Intelligent Systems, 30(3), 6-8. Buhl, H. U., Röglinger, M., Moser, F., & Heidemann, J. (2013). Big data—a fashionable topic with(out) sustainable relevance for research and practice? Business & Information Systems Engineering, 5(2), 65-69. Buitelaar, P., Cimiano, P., & Magnini, B. (Eds.). (2005). Ontology learning from text: Methods, evaluation and applications. Amsterdam: IOS Press. Buytendijk, F. (2014). Hype cycle for big data, https://www.gartner.com/doc/2814517/hype-cycle-big-data2014. Gartner. Available from\n",
              "\n",
              "Chandler, N., Hostmann, B., Rayner, N., & Herschel, G. (2011). Gartner’s business analytics framework. Gartner. Chang, R. M., Kauffman, R. J., & Kwon, Y. (2014). Understanding the paradigm shift to computational social science in the presence of big data. Decision Support Systems, 63, 67-80. Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R. (2000). CRISP-DM 1.0 Step-by-step data mining guide. SPSS. Retrieved from www.crisp-dm.org Chatfield, C. (1995). Problem solving: A statistician’s guide (2nd ed.). London: Chapman & Hall/CRC. Chatterjee, S., & Sarker, S. (2013). Infusing ethical considerations in knowledge management scholarship: Toward a research agenda. Journal of the Association for Information Systems, 14(8), 452-481. Chatterjee, S., Sarker, S., & Fuller, M. (2009a). A deontological approach to designing ethical collaboration. Journal of the Association for Information Systems, 10(3), 138-169. Chatterjee, S., Sarker, S., & Fuller, M. (2009b). Ethical information systems development: A Baumanian postmodernist perspective. Journal of the Association for Information Systems, 10(11), 787-815. Chen, H., Chiang, R. H., & Storey, V. C. (2012). Business intelligence and analytics: From big data to big impact. MIS Quarterly, 36(4), 1165-1188. Chiang, R. H., Barron, T. M., & Storey, V. C. (1994). Reverse engineering of relational databases: Extraction of an EER model from a relational database. Data & Knowledge Engineering, 12(2), 107-142. Chiang, R. H., Goes, P., & Stohr, E. A. (2012). Business intelligence and analytics education, and program development: A unique opportunity for the information systems discipline. ACM Transactions on Management Information Systems, 3(3), 1-13. Chung, W., Chen, H., & Nunamaker, J. F., Jr. (2005). A visual framework for knowledge discovery on the Web: An empirical study of business intelligence exploration. Journal of Management Information Systems, 21(4), 57-84. Christensen, C. (1997). The innovator's dilemma: When new technologies cause great firms to fail. Boston, MA: Harvard Business Review Press. Coco, C. T., Jamison, F., & Black, H. (2011). Connecting people investments and business outcomes at Lowe’s. People & Strategy, 34(2), 28-33 Coolidge, A. (2013). New technology helps Kroger speed up checkout times. The Cincinnati Inquirer. Retrieved from http://www.usatoday.com/story/money/business/2013/06/20/new-technology-helpskroger-speed-up-checkout-times/2443975/ Davenport, T. H. (2006). Competing on analytics. Harvard Business Review, 84(1), 98-107. Davenport, T. H., & Harris, J. G. (2007). Competing on analytics: The new science of winning. Boston, MA: Harvard Business Press.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Davenport, T. H., Harris, J., & Shapiro, J. (2010). Competing on talent analytics. Harvard Business Review, 88(10), 52-58. Davenport, T. H., & Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(10), 70-76. Davenport, T. H., & Kirby, J. (2015). Beyond automation. Harvard Business Review, 94(6), 59-65. Davis, F. D. (1986). A technology acceptance model for empirically testing new end-user information systems: Theory and results (Doctoral dissertation). Sloan School of Management, Massachusetts Institute of Technology. Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319-340. Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1), 107-113. Dennis, A. R., & Carte, T. A. (1998). Using geographical information systems for decision making: Extending cognitive fit theory to map-based presentations. Information Systems Research, 9(2), 194-203. Derman, E. (2011). Models. Behaving. Badly. New York, NY: Simon and Schuster. Embley, D. W., & Liddle, S. W. (2013). Big data—conceptual modeling to the rescue. In W. Ng, V. C. Storey, & J. C. Trujillo (Eds.), ER conference (vol. 8217, pp. 1-8). Springer. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996a). From data mining to knowledge discovery in databases. AI Magazine, 17(3), 37-54. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996b). The KDD process for extracting useful knowledge from volumes of data. Communications of the ACM, 39(11), 27-34. Gaskin, J., Berente, N., Lyytinen, K., & and Yoo, Y. (2014). \"Toward Generalizable Sociomaterial Inquiry: A Computational Approach for Zooming In and Out of Sociomaterial Routines,\" MIS Quarterly, 38(3), 849-871. Gefen, D., Karahanna, E., & Straub, D. W. (2003). Trust and TAM in online shopping: An integrated model. MIS Quarterly, 27(1), 51-90. George, G., Haas, M. R., & Pentland, A. (2014). Big data and management. Academy of Management Journal, 57(2), 321-326. Gleick, J. (1987). Chaos. Making a new science. New York: Viking Penguin Inc. Goes, P. (2014). Big data and IS research. MIS Quarterly, 38(3), iii-viii. Gomes, L. (2014). Machine-learning maestro Michael Jordan on the delusions of big data and other huge engineering efforts. IEEE Spectrum. Gregor, S., & Klein, G. (2014). Eight obstacles to overcome in the theory testing Genre. Journal of the Association for Information Systems, 15(11), i-xix. Han, J., Kamber, M., & Pei, J. (2006). Data mining: Concepts and techniques. New York: Morgan Kaufmann. Harrington, A. (2005). Modern social theory. New York: Oxford University Press. Hassan, A., Abbasi, A., & Zeng, D. (2013). Twitter sentiment analysis: A bootstrap ensemble framework. In Proceedings of the IEEE International Conference on Social Computing (pp. 357-364). Heudecker, N. (2013). Hype cycle for big data, https://www.gartner.com/doc/2574616/hype-cycle-big-data2013. Gartner. Available from\n",
              "\n",
              "Hevner, A. R., March, S. T., Park, J., & Ram, S. (2004). Design science in information systems research. MIS Quarterly, 28(1), 75-105.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxix\n",
              "\n",
              "Hill, K. (2012). How Target figured out a teen girl was pregnant before her father did. Forbes. http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-waspregnant-before-her-father-did/ Hodgkinson, G. P., & Starbuck, W. H. (2008). The Oxford handbook of organizational decision making. Oxford, UK: Oxford University Press. Horan, J. A. (2011). The essential CIO. IBM CIO C-Suite Studies. Retrieved from http://www935.ibm.com/services/c-suite/cio/study/ Jensen, M. L., Lowry, P. B., Burgoon, J. K., & Nunamaker, J. F., Jr. (2010). Technology dominance in complex decision making: The case of aided credibility assessment. Journal of Management Information Systems, 27(1), 175-202. Jensen, M. L., Averbeck, J. M., Zhang, Z., & Wright, K. B. (2013). Credibility of anonymous online product reviews: A language expectancy perspective. Journal of Management Information Systems, 30(1), 293-323. Junqué de Fortuny, E., Martens, D., & Provost, F. (2013). Predictive modeling with big data: Is bigger really better? Big Data, 1(4), 215-226. Kaushik, A. (2011). Web analytics 2.0: The art of online accountability and science of customer centricity. Indianapolis, IND: Wiley. KDNuggets (2011). What do you call analyzing data? KDNuggets. http://www.kdnuggets.com/polls/2011/what-do-you-call-analyzing-data.html Retrieved from\n",
              "\n",
              "Kiel, L. D., & Elliott, E. W. (1996). Chaos theory in the social sciences: Foundations and applications. University of Michigan Press. Kiron, D., Shockley, R., Kruschwitz, N., Finch, G., & Haydock, M. (2012). Analytics: The widening divide. MIT Sloan Management Review, 53(2), 1-21. Kitchin, R. (2014a). Big Data, new epistemologies and paradigm shifts. Big Data & Society, 1(1), 1-12. Kitchin, R. (2014b). The data revolution. London: Sage. Kramer A. D. I., Guillory J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788-8790. Lavalle, S., Lesser, E., Shockley, R., Hopkins, M. S., & Kruschwitz, N. (2011). Big data, analytics, and the path from insights to value. Sloan Management Review, 52(2), 21-31. Lau, R. Y. K. Liao, S. Y., Wong, K. F., & Chiu, K. W. (2012). Web 2.0 environmental scanning and adaptive decision support for business mergers and acquisitions. MIS Quarterly, 36(4), 1239-1268. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436-444. Lin, M., Lucas, H. C., Jr., & Shmueli, G. (2013). Research commentary-too big to fail: Large samples and the p-value problem. Information Systems Research, 24(4), 906-917. Lycett, M. (2013). “Datafication”: Making sense of (big) data in a complex world. European Journal of Information Systems, 22(4), 381-386. McAfee, A., & Brynjolfsson, E. (2012). Big data: The management revolution. Harvard Business Review. Retrieved from https://hbr.org/2012/10/big-data-the-management-revolution/ar Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation Manyika, J., Chui, M., Groves, P., Farrell, D., Van Kuiken, S., & Doshi, E. A. (2013). Open data: Unlocking innovation and performance with liquid information. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights/Business%20Technology/Open%20d ata%20Unlocking%20innovation%20and%20performance%20with%20liquid%20information/MGI_ OpenData_Full_report_Oct2013.ashx\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Marchand, D. A., & Pepper, J. (2013). Why IT fumbles analytics. Harvard Business Review. Retrieved from https://hbr.org/2013/01/why-it-fumbles-analytics Mayer-Schönberger, V., & Cukier, K. (2013). Big data: A revolution that will transform how we live, work, and think. Boston, MA: Houghton Mifflin Harcourt. Newman, D. (2014). Big Data Means Big Disruption. Forbes. Retrieved http://www.forbes.com/sites/danielnewman/2014/06/03/big-data-means-big-disruption/ from\n",
              "\n",
              "Nunamaker, J. F., Jr., Chen, M., & Purdin, T. (1991). Systems development in information systems research. Journal of Management Information Systems, 7(3), 89-106. Nunamaker, J. F., Jr. (1992). Build and learn, evaluate and learn. Informatica, 1(1), 1-6. Parsons, J., & Wand, Y. (2012). Extending classification principles from information modeling to other disciplines. Journal of the Association for Information Systems, 14(5), 245-273. Rai, A., & Sambamurthy, V. (2006). Editorial notes-the growth of interest in services management: Opportunities for information systems scholars. Information Systems Research, 17(4), 327-331. Redman, T. C. (2008). Data driven: Profiting from your most important business asset. Boston, MA: Harvard Business Press. Richards, N. M., & King, J. H. (2013). Three paradoxes of big data, Stanford Law Review Online, 66(41), 41-46. Sarker, S., Xiao, X., & Beaulieu, T. (2013). Qualitative studies in information systems: A critical review and some guiding principles. MIS Quarterly, 37(4), iii-xviii. Schroeck, M., Shockley, R., Smart, J., Romero-Morales, D., & Tufano, P. (2012). Analytics: The real-world use of big data. IBM Institute for Business Value. Sein, M., Henfridsson, O., Purao, S., Rossi, M., & Lindgren, R. (2011). Action design research. MIS Quarterly, 35(1), 37-56. Shaft, T. M., & Vessey, I. (2006). The role of cognitive fit in the relationship between software comprehension and modification. MIS Quarterly, 30(1), 29-55. Sharma, R., Mithas, S., & Kankanhalli, A. (2014). Transforming decision-making processes: A research agenda for understanding the impact of business analytics on organisations. European Journal of Information Systems, 23, 433-441. Shim, J. P., Warkentin, M., Courtney, J. F., Power, D. J., Sharda, R., & Carlsson, C. (2002). Past, present, and future of decision support technology. Decision Support Systems, 33(2), 111-126. Shmueli, G., & Koppius, O. (2011). Predictive analytics in information systems research. MIS Quarterly, 35(3), 553-572. Simon, H. A. (1996). The sciences of the artificial (3rd ed.). Cambridge, MA: MIT Press. Song, Y., Sahoo, N., Srinivasan, S., & Chrysanthos, D. (2014). Uncovering path-to-purchase segments in large consumer population. In Proceedings of the 24th Workshop on Information Technologies and Systems. Sprott, J. C. (2003). Chaos and time-series analysis (Vol. 69). Oxford: Oxford University Press. Steadman I. (2013). Big data and the death of the theorist. http://www.wired.co.uk/news/archive/2013-01/25/big-data-end-of-theory Wired. Retrieved from\n",
              "\n",
              "Stigler, G. (1961). The economics of information. The Journal of Political Economy, 69(3), 213-225. Storey, V. C., Chiang, R. H., Dey, D., Goldstein, R. C., & Sudaresan, S. (1997). Database design with common sense business reasoning and learning. ACM Transactions on Database Systems, 22(4), 471-512. Stubbs, E. (2011). The value of business analytics: Identifying the path to profitability. New York, NY: John Wiley & Sons. Taleb, N. (2005). Fooled by randomness: The hidden role of chance in life and in the markets (Vol. 1). Random House Incorporated.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxxi\n",
              "\n",
              "Taleb, N. N. (2007). The black swan: The impact of the highly improbable. Random House. Taleb, N. N., Goldstein, D. G., & Spitznagel, M. W. (2009). The six mistakes executives make in risk management. Harvard Business Review, 87(10), 78-81. Tambe, P. (2014). Big data investment, skills, and firm value. Management Science, 60(6), 1452-1469. Te’eni, D. (2006). Designs that fit: An overview of fit conceptualizations in HCI. In P. Zhang & D. Galletta (Eds.), Human computer interaction and management information systems: Foundations (pp. 205224). London, England: M. E. Sharpe. Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124-1131. Vapnik, V. N. (1998). Statistical learning theory (Vol. 1). New York: Wiley. Venkatesh, V, Thong, J. Y. L., & Xu, X. (Forthcoming). Unified theory of acceptance and use of technology: A synthesis and the road ahead. Journal of the Association for Information Systems. Venkatesh, V., Morris, M., Davis, G., & Davis, F. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 27(3), 425-478. Vessey, I. (1991). Cognitive fit: A theory‐based analysis of the graphs versus tables literature. Decision Sciences, 22(2), 219-240. Walls, J. G., Widmeyer, G. R., & El Sawy, O. A. (1992). Building an information system design theory for vigilant EIS. Information Systems Research, 3(1), 36-59. Wand, Y., & Weber, R. (2002). Research commentary: Information systems and conceptual modeling—a research agenda. Information Systems Research, 13(4), 363-376. Wang, Y., & Ram, S. (2015). Prediction of location-based sequential purchasing events using spatial, temporal and social patterns. IEEE Intelligent Systems, 30(3), 10-17. Werndl, C. (2009). What are the new implications of chaos for unpredictability? The British Journal for the Philosophy of Science, 60(1), 195-220. Westfall, P. H., & Hilbe, J. (2007). The black swan: Praise and criticism. The American Statistician, 61(3), 193-194. Wixom, B. H., & Watson, H. J. (2001). An empirical investigation of the factors affecting data warehousing success. MIS Quarterly, 25(1), 17-41. Yetgin, E., Jensen, M., & Shaft, T. (2015). Complacency and intentionality in IT use and continuance. AIS Transactions on Human-Computer Interaction, 7(1), 17-42. Zahedi, F. M., Abbasi, A., & Chen, Y. (2015). Fake-website detection tools: Identifying elements that promote individuals’ use and enhance their performance. Journal of the Association for Information Systems, 16(6), 448-484. Zeng, D., Chen, H., Lusch, R., & Li, S. H. (2010). Social media analytics and intelligence. IEEE Intelligent Systems, 25(6), 13-16. Zhao, H., Sinha, A. P., & Bansal, G. (2011). An extended tuning method for cost-sensitive regression and forecasting. Decision Support Systems, 51(3), 372-383. Zhang, W., Lau, R., & Li, C. (2014). Adaptive big data analytics for deceptive review detection in online social media. In Proceedings of the International Conference on Information Systems.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "About the Authors\n",
              "Ahmed Abbasi is Murray Research Professor and Associate Professor of Information Technology in the McIntire School of Commerce at the University of Virginia. He is Director of the Center for Business Analytics and a member of the Predictive Analytics Lab. His research on online fraud and security, natural language processing, social media, and e-health has been funded through multiple grants from the National Science Foundation. Ahmed received the IBM Faculty Award and AWS Research Grant for his work on Big Data. He has published over fifty peer-reviewed papers in journals and conferences, including top-tier outlets such as MIS Quarterly, Journal of the AIS, Journal of MIS, ACM Transactions on IS, IEEE Transactions on Knowledge and Data Engineering, and IEEE Intelligent Systems. One of his papers was considered a top publication by the AIS. Ahmed also won best paper awards at MIS Quarterly and WITS. His work has been featured in various media outlets, including the Wall Street Journal, the Associated Press, and Fox News. Ahmed serves as an Associate Editor for Information Systems Research, Decision Sciences Journal, ACM Transactions on MIS, and IEEE Intelligent Systems, and as an editorial review board member for the Journal of AIS. He is a senior member of the IEEE and serves on program committees for various conferences related to computational linguistics, text analytics, and data mining. He is also a co-founder and/or advisory board member for multiple predictive analytics-related companies. Suprateek Sarker is a Professor of Information Technology at the McIntire School of Commerce, University of Virginia, USA. He also serves as Visiting Distinguished Professor at Aalto University School of Business, Helsinki, Finland. He is primarily a qualitative researcher, and his past work has been published in leading outlets including the MIS Quarterly, Information Systems Research, Journal of the AIS, Journal of MIS, European Journal of Information Systems, MIS Quarterly Executive, Journal of Information Technology, IEEE Transactions, ACM Transactions on MIS, Communications of the ACM, MIS Quarterly Executive, and Decision Sciences Journal. He is currently serving as the Editor-in-Chief of the Journal of the AIS, a Senior Editor of Decision Sciences Journal, a Senior Editor (Emeritus) of MIS Quarterly, a member of the Board of Editors of Journal of MIS, and an editorial board member of IEEE Transactions on Engineering Management. Some of his past work has been funded by the National Science Foundation (NSF) and the Institute for the Study of Business Markets (ISBM). He is also a past recipient (with S. Sahay) of the Stafford Beer Medal awarded by the Operational Research Society, UK. Roger H. L. Chiang is a Professor of Information Systems at Department of Operations, Business Analytics, and Information Systems, Carl H. Lindner College of Business, University of Cincinnati. Dr. Chiang’s research interests are in business intelligence and analytics, data and knowledge management, and intelligent systems, particularly in database reverse engineering, database integration, data and text mining, sentiment analysis, document classification and clustering, domain knowledge discovery, and semantic information retrieval. He has over fifty refereed papers published by conferences and journals including ACM Transactions on Database Systems, ACM Transactions on MIS, Communications of the ACM, The DATA BASE for Advances in Information Systems, Data & Knowledge Engineering, Decision Support Systems, Journal of American Society for Information Science and Technology, Journal of Database Administration, Journal of MIS, Marketing Science, MIS Quarterly, and Very Large Data Base Journal. He has served as the Senior Editor of The DATA BASE for Advances in Information Systems, Decision Sciences, and Journal of the AIS and the associate editor of Information & Management, Journal of the AIS, Journal of Database Management, International Journal of Intelligent Systems in Accounting, Finance and Management, and MIS Quarterly.\n",
              "\n",
              "Copyright © 2016 by the Association for Information Systems. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than the Association for Information Systems must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, GA, 30301-2712 Attn: Reprints or via e-mail from publications@aisnet.org.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Copyright of Journal of the Association for Information Systems is the property of Association for Information Systems and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "05X3bEL5nir7",
        "colab_type": "code",
        "outputId": "8f58e164-8f74-4365-c91d-4efe2a3822d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "p_klein_als1_Sents = [sent for sent in doc1.sents if 'p <' in sent.string]\n",
        "p_klein_als1_Sents\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "5aKxY9DYDQTW",
        "colab_type": "code",
        "outputId": "a7147142-8c6c-4f89-a7ce-1befa4f0c417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "significant_at1_sents = [sent for sent in doc1.sents if 'significant at' in sent.string]\n",
        "significant_at1_sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Pshpae2H_Q5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "509a4b22-d559-4018-d6b4-c37409df67c8"
      },
      "cell_type": "code",
      "source": [
        "p_groß_als_Sents = [sent for sent in doc1.sents if 'P >' in sent.string]\n",
        "p_groß_als_Sents"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "RV5ewjMGBCT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8624
        },
        "outputId": "4deb4a89-e812-4f58-ddbf-48531d730bf8"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc2 = nlp(open(u\"BairdA#MillerC#RaghuT#SinhaR_2016_Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives_Information Systems Research_2.txt\").read())\n",
        "doc2\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This article was downloaded by: [130.89.97.167] On: 17 February 2017, At: 08:09 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA\n",
              "\n",
              "Information Systems Research\n",
              "Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org\n",
              "\n",
              "Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives\n",
              "Aaron Baird, Chadwick J. Miller, T. S. Raghu, Rajiv K. Sinha\n",
              "\n",
              "To cite this article: Aaron Baird, Chadwick J. Miller, T. S. Raghu, Rajiv K. Sinha (2016) Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives. Information Systems Research 27(2):282-301. http://dx.doi.org/10.1287/isre.2016.0621 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages\n",
              "\n",
              "INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org\n",
              "\n",
              "\n",
              "Information Systems Research\n",
              "Vol. 27, No. 2, June 2016, pp. 282–301 ISSN 1047-7047 (print) ISSN 1526-5536 (online) http://dx.doi.org/10.1287/isre.2016.0621 © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives\n",
              "Aaron Baird\n",
              "J. Mack Robinson College of Business, Georgia State University, Atlanta, Georgia 30302, abaird@gsu.edu\n",
              "\n",
              "Chadwick J. Miller\n",
              "Carson College of Business, Washington State University, Pullman, Washington 99164, chadwick.j.miller@wsu.edu\n",
              "\n",
              "T. S. Raghu\n",
              "W. P. Carey School of Business, Arizona State University, Tempe, Arizona 85287, raghu.santanam@asu.edu\n",
              "\n",
              "Rajiv K. Sinha\n",
              "Deceased\n",
              "W. P. Carey School of Business, Arizona State University, Tempe, Arizona 85287\n",
              "\n",
              "H\n",
              "\n",
              "ypercompetitive consumer software markets pit incumbents against free alternatives and pirates. Although the extant literature has studied ﬁrm level strategic responses to consumer heterogeneity and piracy, there is a lack of understanding of consumer reactions to digital goods choice sets that include ﬁrm product extensions such as the introduction of premium or free alternatives. With context-dependent preferences as the theoretical basis, this study systematically examines the impact of piracy controls and product line extensions on welfare in a consumer software market context (i.e., willingness to pay (WTP) and changes in consumer and producer surplus). In two controlled experiments using double-bound-dichotomous-choice WTP elicitation, we investigate how piracy controls and product line extensions impact two different platforms of the same software (PC Adobe applications and mobile Adobe applications) in terms of propensity to pirate and WTP. We show that introducing a premium or free vertical extension has different impacts on consumers’ WTP for the focal product depending on whether it is a low-cost or high-cost market even when controlling for individual differences, such as price fairness perceptions, product feature value, brand perceptions, etc. By contrast, piracy controls reduce piracy rates but have a limited impact on consumer WTP for the focal product in both contexts. By calculating the overall welfare of the market, we show that there is alignment in consumer and producer interests at current and estimated optimal price levels in both high-cost and low-cost markets. However, the introduction of a free product extension leads to a higher surplus in the high-cost market, whereas the introduction of the premium product extension leads to a higher surplus in the low-cost market. Keywords : product line extensions; versioning; free alternatives; willingness to pay; piracy; context-dependent preferences History : Ram Gopal, Senior Editor; Sudip Bhattacharjee, Associate Editor. This paper was received on October 19, 2011, and was with the authors 28 months for 3 revisions. Published online in Articles in Advance May 6, 2016.\n",
              "\n",
              "1.\n",
              "\n",
              "Introduction\n",
              "\n",
              "Incumbents in digital consumer markets often ﬁnd their offerings competing with pirated and free alternatives. For instance, a recent Business Software Alliance (BSA) report found that over 50% of personal computer users around the world admit to pirating software, which contributed to an estimated $63 billion shadow market for software products (BSA 2011). Furthermore, the emergence of free alternatives in virtually all consumer software categories creates signiﬁcant new challenges for incumbents (Bryce et al. 2011). For example, Microsoft Ofﬁce faces this form of competition from OpenOfﬁce, Libre Ofﬁce, NeoOfﬁce, and Google Docs, all of which are free to consumers. Adobe Photoshop Elements, already a highly pirated consumer\n",
              "282\n",
              "\n",
              "software (Muchmore 2012), also competes against free alternatives such as GIMP, Pixlr.com, and others. As a result of these pirated and free marketplace alternatives, incumbents have been forced to consider piracy control and product line extension strategies to compete. Piracy controls—such as license restrictions and product activation keys—are intended to prevent consumers from illegally appropriating the software product. Product line extensions, on the other hand, involve the offering of product versions at different price/quality tiers that are intended to appeal to distinct consumer segments with varying needs and levels of willingness to pay (WTP). Thus, ﬁrms attempt to maximize their market penetration and minimize piracy using a combination of piracy controls and vertical product extensions.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "283 individual difference variables, the key result in our research is the differential impact of vertical extension type on consumers’ WTP in high-cost and low-cost markets. When a premium product line extension is introduced in a high-cost software market, consumers’ WTP for the focal software is lower and consumers’ propensity to pirate is higher than when a free product extension is introduced. This ﬁnding is reversed in low-cost markets. A premium product extension in the mobile market increases consumers’ WTP for the focal software and lowers propensity to pirate when compared to a free product extension. Additionally, we ﬁnd that piracy controls are effective in reducing consumers’ propensity to pirate in both high-cost and low-cost markets. However, piracy controls have a limited impact on WTP for the focal product. Finally, using a demand analysis, we show that in both markets, consumer and producer interests are aligned. In a high-cost market, consumer and producer surplus are highest when piracy controls are used and a free product extension is introduced. In the low-cost market, consumer and producer surplus are highest when no piracy control is used and a premium product extension is introduced. Overall, our research provides important managerial recommendations for product line extensions and piracy controls in different market conditions. In what follows, we provide the theoretical background for our hypotheses, describe our empirical work, and provide detailed analyses and results. Finally, we end with a discussion of our ﬁndings in comparison to prior theorizing and offer concluding remarks.\n",
              "\n",
              "Importantly, a major gap in the extant literature is the paucity of research that uses the consumer as the unit of analysis to examine theoretical predictions with regard to product extensions—or the introduction of a new product that is a modiﬁcation or reformulation of an existing product (Wilson and Norton 1989)—in competitive settings. Consumer reaction in this context can act as a distinct and signiﬁcant contributor to ﬁrm strategies and outcomes, particularly for ﬁrms competing in digital markets. Although some research has focused on consumers and their reactions to piracy controls in the face of independent free alternatives (i.e., Raghu et al. 2009, Sinha and Mandel 2008), researchers have not investigated how vertical product extension and piracy control strategies should be used in tandem. Importantly, previous research has restricted its focus to WTP measures (Raghu et al. 2009, Sinha and Mandel 2008) and although WTP is an important construct in the consumer context, it does not fully measure the overall demand impacts of piracy on the incumbent. Shifts in consumer segments because of the presence of alternatives can impact producer surplus even when the average WTP of the demand distribution is unaffected. In this research, we contribute to the existing literature by investigating how consumer reactions to product line extensions and piracy control strategies impact not only consumers’ piracy intentions and WTP valuations but also producer and consumer surplus in digital goods markets. Using context-dependent preferences as the theoretical framework (Tversky and Simonson 1993), we develop insights into consumer valuation of a focal product as ﬁrms introduce vertical product extensions— in the form of premium or free alternatives—with high or low levels of piracy control. To our knowledge, this is the ﬁrst research to take into account the heterogeneity of different forms of software markets. As such, we consider two hedonic software consumption contexts where Adobe Photoshop Elements (representative of the high-cost traditional PC market) and Adobe Photoshop Touch (representative of the low-cost emergent mobile market) are the focal digital products. We utilize two controlled experiments to measure consumers’ WTP and piracy intentions associated with these focal products in a contingent valuation framework (Hanemann 1984, 1994). Subsequently, we also estimate demand curves under different experimental conditions to compute consumer, producer, and overall surplus. The ﬁndings from our controlled experimental settings and analyses contribute to the existing literature by providing insights into the notion that vertical extensions of a product line can (a) impact consumers WTP valuations differently depending on the market type, and (b) be an effective approach to combating piracy in consumer software markets. Controlling for several\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "2.\n",
              "\n",
              "Background and Hypotheses\n",
              "\n",
              "With the ﬁrm as the focus of analysis, prior research has suggested a variety of strategies for alleviating competitive challenges and piracy in software markets. For instance, incumbent ﬁrms can manage piracy through prevention controls (Banerjee 2003) such as the imposition of license key and activation requirements during or after installation of the software. These preventive controls are successful because they increase the end-user cost of pirating (Gopal and Sanders 1997). However, piracy control mechanisms often burden both pirates and legitimate users alike. Although this approach can be effective for reducing piracy, the associated burden or inconvenience may also induce consumers to react negatively to piracy control measures when the measures clash with existing social norms (Depoorter et al. 2005). Another approach to managing piracy—which many powerful incumbents have taken—is to reduce the prices of existing products in consumer markets (Liebowitz and Margolis 1999, Nascimento and Vanhonacker 1988). However, because of consumers’ affective reaction to a zero price point (Shampanier\n",
              "\n",
              "\n",
              "284 et al. 2007) lowering the prices of existing offerings may not be enough to compete against free alternatives. As a result, instead of changing the prices of current product offerings, incumbents could pursue product line extensions to broaden their market reach and more effectively compete against free and pirated alternatives (Sun et al. 2004). With an existing product portfolio, a product line extension is an extremely attractive option to incumbents in software markets because a ﬁrm can offer multiple versions with almost negligible impacts on product development costs. As a viable strategy, software versioning—through the use of vertical product line extensions—is broadly supported as a means of combating piracy (Chellappa and Shivendu 2005, Wu and Chen 2008). For instance, consumers may recalibrate their brand and product perceptions when presented with product line extensions (Loken and John 1993, Heath et al. 2011) leading to changes in WTP valuations and piracy intentions of the focal product. However, how consumer reactions to piracy controls and product line extensions (1) impact consumers’ WTP valuations and piracy intentions of a focal product as well as (2) impact consumer and producer surplus in the marketplace is, as yet, an unanswered question. We ﬁll this gap in the research because there is good reason to believe that beyond affecting piracy rates, the introduction of vertical extensions should also impact the WTP of a focal product (Heath et al. 2011). Thus, ﬁrms need to be cognizant of how the expansion of their product lineup will not only impact piracy rates but also consumers’ WTP valuations of the incumbent ﬁrm’s focal product. Additionally, we show how these two variables impact the consumer and producer surplus in the marketplace. 2.1. Product Line Extensions We draw on context-dependent preferences (Tversky and Simonson 1993) as the theoretical basis for understanding the impact of vertical product extensions\n",
              "Figure 1 (Color online) Context Dependent Preferences\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "on the WTP valuations, piracy intentions, and the overall market welfare for a focal product. For instance, classical choice theory stipulates that preference among options is independent of the other options in the set. By contrast, context-dependent choice theory states that consumers’ preferences are speciﬁcally inﬂuenced by the contextual nature of the choice task—the focus of this research—and also by their experiences and beliefs (Payne et al. 1992), which we control for in this research. We make use of this theory because it represents the reality that consumers face when they are considering a variety of options within a market. For instance, when making a purchase decision, consumers not only consider the characteristics of the focal product but also consider the characteristics of other available alternatives, including versions of other products offered by the same producer. Context-dependent preference theory suggests that consumer preferences will change depending on several factors that exist in the market. These factors can include the relative price of products, the number of offerings in a consideration set, and even how the offerings in a market are introduced (i.e., the introduction of a free or premium vertical product extension). If a consumer’s preference for a focal product can be inﬂuenced by how the producer enters the market, then the way in which an incumbent changes their product lineup should also impact consumers’ evaluations of the focal product even if the ﬁnal product lineup ends up identical (despite differences in introduction strategy); see Figure 1. We develop our main research hypotheses with the underlying assumption that context-dependent variables—such as the introduction of vertical product extensions—impact consumers’ valuations of a focal product; see Figure 2. When presented with alternatives for a product, consumer cognitive processing focuses on the value offered by the product’s attributes. Higher perceived quality association, variety, and innovativeness are the key attributes that drive consumer valuation\n",
              "\n",
              "Free\n",
              "\n",
              "Focal\n",
              "\n",
              "Free\n",
              "\n",
              "Focal\n",
              "\n",
              "Premium\n",
              "\n",
              "Focal\n",
              "\n",
              "Premium\n",
              "\n",
              "Free\n",
              "\n",
              "Focal\n",
              "\n",
              "Premium\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "285\n",
              "\n",
              "Figure 2\n",
              "\n",
              "Theoretical Model\n",
              "\n",
              "Vertical product extension (free vs. premium)\n",
              "\n",
              "H1A and H1B H2\n",
              "\n",
              "WTP valuations for focal product\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "H4 Piracy controls H3 Piracy intentions for focal product\n",
              "\n",
              "Piracy intentions controls • • • • • • • Share Feature rating Justice Social influence Brand perceptions Previous use Familiarity with competition • • • • • • •\n",
              "\n",
              "WTP valuations controls Performance Feature rating Justice Social influence Brand perceptions Previous use Familiarity with competition\n",
              "\n",
              "(Heath et al. 2011). These effects will be salient in software markets since software products are often reviewed and evaluated through feature set comparisons and performance benchmarking. A higher level of capability or a larger set of features improve focal product valuations because consumers are likely to associate the product and the ﬁrm with higher quality (Kirmani et al. 1999, Heath et al. 2011, Zeithaml 1988). When a software producer offers higher capability versions of a product, it can signal the superior technical capabilities of the producer and a path to upgrade the focal product in the near future. On the other hand, when incumbents offer product line extensions at the lower end to match low or zero price competitors, consumers may lower the performance expectations of the focal product (Heath et al. 2011) but perceive it as more accessible. Although quality associations, variety, and innovativeness are compelling reasons for a consumer to make a purchase, it is unclear if their effects will hold when the comparison set involves a free alternative, such as a zero price version of the focal product. This is particularly important considering that many software developers are introducing free versions of their software in the marketplace (Reid 2015). Context dependent preferences suggest that quality assumptions (associated with premium products) and the zero price effect (associated with free products) will impact how consumers view a focal product—both piracy intentions and WTP valuations—within the same consideration set.\n",
              "\n",
              "With regard to a free vertical extension within the framework of context-dependent preferences theory, it can be shown that consumers evaluate a zero price differently from nonzero prices. Shampanier et al. (2007) develop a formal theoretical argument to show the higher valuations of zero price products. Furthermore, they explore several psychological explanations for why consumers regard zero as a special price and conclude that affective evaluation may be one plausible cause. That is, when a product is offered at a zero price it evokes greater positive affect in consumers compared to a nonzero price. For a premium vertical extension, a similar argument can be made that consumers’ associate a product with more features and a higher price with quality (Zeithaml 1988). As such, when a premium product with additional features is introduced as a vertical extension, consumers’ quality assumptions should also pass to the focal product (Hertzendorf 1993). Thus, there are strong theoretical reasons to conclude that the introduction of a free vertical extension and/or a premium vertical extension will beneﬁt the WTP valuations associated with a focal product. However, extant research has not previously investigated how a vertical extension will impact WTP valuations and piracy intentions as well as producer and consumer surpluses associated with these ﬁrm actions. As a result, it remains unclear when the quality associations due to a premium vertical extension or the positive affect from a free vertical extension—both of which are common strategies to expand product lines (see\n",
              "\n",
              "\n",
              "286 Randall et al. 1998, Reid 2015)—will impact consumers’ WTP valuations of a focal product. We believe that the type of market—low cost or high cost—will determine which vertical extension will most beneﬁt the focal product. In a high-cost market—such as with the traditional PC software platform where the products are featurerich—consumers will already have high-quality expectations (Zeithaml 1988), which should negate the impact of the quality associations related to the premium vertical extension. Instead, in the high-cost market, offering a free alternative with limited features may more likely evoke positive affect. As such, we believe that the positive affect from a zero price vertical extension should positively impact consumers’ WTP valuations of a focal product (Erat and Bhaskaran 2012, Shampanier et al. 2007). Formally stated, we hypothesize the following: Hypothesis 1A (Free Vertical Extension WTP). In a high-cost market, consumers’ WTP valuations of the focal product will be higher for a free vertical product line extension than for a premium vertical product line extension. Conversely, we suggest that in a low-cost market—or mobile platform—the zero price affective evaluation should not have an impact on the valuation of the focal product. Since the barrier to entry to the focal product is already low, the introduction of a free vertical extension should have a muted impact on the focal product’s WTP valuations. Instead, we anticipate that in the low-cost market, consumers’ WTP valuations of a focal product will be more positively impacted by a premium vertical product extension (Hertzendorf 1993). When a premium product is introduced in the low-cost market, quality associations from the premium, feature-rich product (Zeithaml 1988) should increase the WTP valuations associated with the focal product. Formally stated, we hypothesize the following: Hypothesis 1B (Premium Vertical Extension WTP). In a low-cost market, consumers’ WTP valuations of the focal product will be higher for a premium vertical product line extension than for a free vertical product line extension. Despite the differences between WTP evaluations that we suggest are impacted by market type, we do not believe there is a theoretical argument for ﬁnding differences in piracy rates as a function of market type. Rather, in both a high-cost and low-cost market the introduction of a free vertical extension should lead to lower piracy rates of the focal product because the existence of a free vertical extension gives consumers— who may pirate because they are not willing to pay any money for the software—legitimate access to the software, albeit at a reduced feature level, for free (Sinha and Mandel 2008, Raghu et al. 2009). In other words, the incremental utility gained from pirating is lower when a free alternative is available. The efforts\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "and costs of pirating may no longer make pirating an attractive option for many consumers. Formally stated, we hypothesize the following: Hypothesis 2 (Vertical Extensions and Piracy Intentions). In both low-cost and high-cost markets, consumers’ piracy intentions will be higher for a premium vertical product line extension than for a free vertical product line extension. 2.2. Piracy Controls Piracy controls are intended to force or encourage consumers to purchase legitimate software copies. The predominant approach to piracy control at the individual level is preventive control (i.e., the use of license, activation keys, and/or registration). Many consumer software products require online registration and may even track machine identiﬁcation information to dissuade piracy. Piracy controls therefore increase the search and acquisition costs of illegitimate software copies (Conner and Rumelt 1991, Gopal and Sanders 1997). In addition, piracy controls highlight the software producer’s rights to the product and force potential pirates to reconsider the moral and ethical considerations of illegal copying (Raghu et al. 2009). Overall, even though it imposes costs on legitimate users, piracy controls are considered highly effective for software products. Thus, the effect of increased acquisition costs for pirated goods on piracy rates should remain in the same direction for both high-cost and low-cost markets. Formally stated, we hypothesize the following: Hypothesis 3 (Piracy Controls and Piracy Intentions). In both low-cost and high-cost markets, piracy controls will result in lower consumer propensity to pirate the focal product when compared to no piracy controls. Although the presence of piracy controls should reduce the piracy rate because they impose additional costs on the pirate to circumvent the controls, it is less likely that piracy controls affect consumer WTP. However, the presence of a free alternative, in addition to providing potential pirates free access, may also sensitize legitimate consumers against burdensome registration, usage, and installation restrictions. Given that a free alternative can be installed on an unlimited number of computers (or allow online use), consumers may view licensing restrictions to be intrusive. For example, in the context of digital music piracy, user incentives—through investments in enhancing product quality—have been shown to be more effective than piracy controls (Sinha and Mandel 2008). As such, any impact on consumer WTP valuation can only be through elevated inconvenience expectations associated with product use (e.g., requirements of registration/ dealing with multiple devices). Thus, we anticipate a negative impact of piracy controls on WTP valuations. Formally stated, we hypothesize the following:\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "287 introduce either a premium product line extension (Adobe CS6, retail $149) or a free extension (Adobe Express, free online). In Study 2, we test our theory in the mobile software market (representative of a lowcost market) by making use of Adobe Photoshop Touch (retail $4.99) as our focal product. For our treatment, we introduce either a premium product line extension (Adobe Photoshop Touch Exp, retail $9.99) or a free product line extension (Adobe Photoshop Express, free online). 3.1. Individual Difference Covariates In addition to ﬁrm determined contexts that inﬂuence preferences as outlined in our four hypotheses, we also sought to control for individual differences in consumers’ beliefs and experiences (Payne et al. 1992) that are inherent in context dependent preferences. Although many have not been previously addressed in the information systems (IS) literature, the individual difference variables mentioned in §7 are important because there are theoretical reasons for why they should have a direct impact on consumers’ WTP valuations and/or piracy intentions. By controlling for these variables we eliminate them as alternative explanations for the context-dependent effects in the two different market types (low cost and high cost). Although most of the covariates are included in both the piracy control and WTP equations, consumers’ propensity to share is restricted to the piracy control equation and performance expectations is restricted to the WTP equation for theoretical reasons and empirical identiﬁcation purposes (we require at least one variable to distinguish the two equations). We measured participants’ “propensity to share” because the decision to pirate software relies on another consumer’s decision to share the software that they have purchased legally. Raghu et al. (2009) measured consumers’ propensity to share software when investigating WTP valuations in the presence of open source software and free software. We included this covariate in this analysis because we expect consumers’ propensity to share to impact consumers’ decision to pirate the software. Additionally, we measured consumers’ “performance expectations” of the software because their perceptions of quality should impact their overall perception of the value of a product (Zeithaml 1988). As a result, we anticipate that quality expectations, such as how well the product is expected to perform, will also play a role in consumers’ WTP valuations. We also measured “price fairness” because it is well known that notions of fairness and legitimacy inﬂuence the psychological evaluation of goods (Ajzen et al. 2000, Campbell 2007) and if consumers perceive a price to be unfair, they may decide to punish the ﬁrm by either not buying (and pirating the software) or switching to a competing product. As a result, price fairness is likely\n",
              "\n",
              "Hypothesis 4 (Piracy Controls and WTP). In both low-cost and high-cost markets, piracy controls will result in lower consumer WTP valuations of the focal product when compared to no piracy controls.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "3.\n",
              "\n",
              "Overview of Studies\n",
              "\n",
              "In this research, we examine how the introduction of a free or premium vertical extension and piracy controls impact consumers’ WTP valuations of a focal product and consumer/producer surplus. We chose Adobe Photoshop products for this research for four primary reasons. First, our focus is to examine the impact of product line extensions in consumer markets. Most software products are targeted at both professional and consumer audiences. For example, Microsoft Ofﬁce is used by both consumers and businesses. As such, consumer decisions to purchase Microsoft Ofﬁce may be driven by their use in the business environment. Adobe Photoshop products present a context where purchase decisions in the nonprofessional consumer market are predominantly driven by hedonic intentions rather than utilitarian needs. Second, our research spans both low-cost and high-cost markets. Adobe products are ideal for this research as Adobe offers products in both the PC market and the mobile market, both of which deal with high rates of piracy (Smith 2015, Jacobson 2012). As such, we can control for brand preferences but still investigate differences in the PC (high-cost) and mobile (low-cost) markets. Third, Adobe Photoshop products are among the most pirated consumer software products in the industry. In fact, the number of BitTorrent seeds for Adobe Photoshop is more than double that of Microsoft Ofﬁce (Muchmore 2012). Finally, Adobe recently made changes to its product line with the introduction of a free limited capability version (Photoshop Express for PC and mobile) and a premium version (Adobe CS6 for PC, and Adobe Photoshop Touch Exp for mobile market). Thus, Adobe Photoshop products enable us to use real product line extensions in our experimental settings, thereby increasing the external validity of the experimental design. As hypothesized, we anticipate that consumers’ WTP valuations of the focal product will be impacted differently depending on whether the market is high cost (such as a traditional PC software market) or low cost (such as the relatively new emergent mobile software market). To test our theory, we conduct two controlled experiments.1 In Study 1, we test our theory in a traditional PC software market (representative of a high-cost market) using Adobe Photoshop Elements (retail $99) as our focal digital product. For our treatment, we\n",
              "1\n",
              "\n",
              "Institutional Review Board (IRB) approval was obtained prior to collection of all study data.\n",
              "\n",
              "\n",
              "288 to affect consumers’ choice (i.e., decision to purchase or not) through their WTP valuations and should also have an impact on consumers’ piracy intentions. We measured “social inﬂuence” because a large body of research has found that consumers purchase products and services that are used by inﬂuential others (e.g., Venkatesh et al. 2012). This implies that consumers are looking for software that meets their objective needs and is also used by those around them—and in particular by those who have inﬂuence over them. Notably, it is unclear whether social inﬂuence will push consumers toward legitimate purchases (increase WTP and decrease piracy intentions) or illegitimate purchases (decrease WTP and increase piracy intentions). We also measured “product feature value.” IS research has traditionally focused on usefulness and ease of use as important determinants of systems adoption. Borrowing from this perspective, Raghu et al. (2009) showed that the perception of product usefulness is an important value determinant. We also posit that when consumers feel there is a high feature value in a product, they will be more willing to purchase the product through legitimate channels. Thus, high feature value should have a negative impact on consumers’ propensity to pirate. Most analytical models of piracy behavior make this fundamental assumption (Conner and Rumelt 1991, Gopal and Sanders 1997). In addition, we measured participants’ previous use of photo editing software as well as participants’ familiarity with other free photo editing software products. Using these covariates we are able to control for differences in participants’ product level and competition level knowledge of each market. We include participants’ previous use and familiarity with the competition in both the piracy and WTP valuations equations. Finally, we measured consumers’ “brand perceptions.” It is well known that consumers often form an afﬁnity to certain brands. In turn, strong positive perceptions toward a speciﬁc brand of product may inﬂuence consumers to remain loyal to the focal product irrespective of the price, external stimuli, other alternatives, or market type (Kirmani et al. 1999, Homburg et al. 2005, Park et al. 2010, Heath et al. 2011). As a result, consumers with strong brand perceptions should react differently to piracy controls and product line extensions than consumers with weak brand perceptions. We include this variable as a covariate and a moderator of consumers WTP valuations in our analysis. In §4, we discuss our WTP estimation methodology for both studies. We then review Studies 1 and 2 followed by a demand analysis for each study.\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "4.\n",
              "\n",
              "WTP Methodology for Studies 1 and 2\n",
              "\n",
              "We use a demand-based approach to estimate consumers’ WTP valuations and piracy intentions for the focal product. However, to estimate changes in incumbent product demand due to product line extensions and piracy controls, it is important to account for pirates, i.e., those individuals who would never purchase the incumbent’s product at any nonzero price. In other words, it is essential that our model allows for the existence of an endogenously estimated group of consumers whose WTP for the legitimate product is always zero. To do so, we utilize the product piracy model proposed recently in Sinha et al. (2010). This model is an effective way to achieve our goals because it allows us to test the impact of product line extension and piracy control while isolating and controlling for those consumers who would use but never pay for the product, i.e., the likely pirates. The empirical model in Sinha et al. (2010) builds on the double-bound-dichotomous-choice (DBDC) contingent valuation method (CVM) that originated in economics (Bishop and Heberlein 1979; Hanemann 1984, 1994). The CVM has also been increasingly used in the literature on private goods, particularly to assess WTP while using survey or experimental data collection methods (Cameron and James 1987, Park and MacLachlan 2008, Sinha and Mandel 2008, Raghu et al. 2009, Lopes and Galletta 2006). Many alternative techniques for estimating WTP have also been proposed in the literature. These range from very simple procedures that merely ask consumers to state their WTP, to more sophisticated models such as lotteries, auctions, and choice-based experiments such as conjoint analysis. Although there is no consensus that any one method is clearly preferable to the other (Stevens et al. 2000, Hanley et al. 2001, Foster and Mourato 2003), empirical evidence suggests that carefully designed CVM studies yield both reliable and valid estimates of WTP in survey-based experimental research (Arrow et al. 1993, Carson et al. 2001). CVM experiments have been used in both public good and private good contexts. By deﬁnition, a public good exhibits qualities of nonrivalry (simultaneous consumption by one individual does not diminish consumption for other individuals) and nonexcludability (one cannot completely prevent individuals from consuming the good). However, it is difﬁcult to identify a purely public good. In a review of CVM studies, Murphy and Stevens (2004) suggest that CVM studies may be less prone to bias in private good contexts. Since software products embody some characteristics of both public and private goods, the CVM approach is particularly appropriate for this study. Survey-based studies that have tested CVM approaches in a private goods context include sports cards (List 2001), pens\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "289 Individuals are randomly assigned to one of the bid sets. The bid sets used in the literature for WTP estimation with known product prices, anchor the middle bid set at the prevailing price and introduce bid sets on either side of the anchor (Sinha and Mandel 2008, Raghu et al. 2009, Sinha et al. 2010). The four outcomes may be represented as (No-No) if WTP < BL , (No-Yes) if BL ≤ WTP < B1 , (Yes-No) if B1 ≤ WTP < BH , and (Yes-Yes) if BH ≤ WTP. The main advantage of the DBDC CVM approach over simply asking respondents to state a price they are willing to pay (which has been known to be inﬂuenced by random cues) and conjoint models is that it more closely resembles consumer choice in an actual market situation, where consumers are given prices (as opposed to being faced with open-ended questions, attribute descriptions, or lotteries). For instance, conjoint analysis elicits preference information on attribute bundles that are useful from a product design perspective. However, given our focus on WTP in the presence of alternatives, we are less interested in pair-wise attribute bundle valuations and more interested in the relative overall valuation of a product already in the market. The simple Yes/No responses place limited cognitive burden on the respondents. In addition, since the stated prices vary across subsamples, no single respondent can inﬂuence the ﬁnal outcome. As a result, the DBDC CVM technique is less susceptible to gaming and strategic behavior (Mitchell and Carson 1989). To incorporate the consumer segment that will never buy the product, Sinha et al. (2010) develop a mixture model for the DBDC CVM approach by asking the (No-No) group if they would pay anything at all. Those who responded negatively (No-No-No group) were considered likely to be pirates. We reﬁne this approach further by identifying individuals who (a) indicate that they would pirate, or (b) indicate that they would borrow the product from someone else, as true pirates.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "(Wertenbroch and Skiera 2002) and quasi-public goods such as paying for recycling (Aadland and Caplan 2006). A number of researchers have used CVM to measure WTP for digital goods including Homburg et al. (2005), Sinha and Mandel (2008), Raghu et al. (2009), and Sinha et al. (2010). A concern in CVM-based WTP elicitation is hypothetical bias—where respondents overstate their true willingness to pay for the product. A “cheap talk” procedure (List 2001) has been used to address this issue and we utilize a similar approach in this research. Importantly, the factorial experimental design mitigates the hypothetical bias since the experiment is designed to show relative WTP valuations across experimental conditions (i.e., the relative impact of vertical product extensions and piracy controls) rather than ﬁnding a speciﬁc value of WTP. 4.1. Contingent Valuation and WTP We present the main ideas underlying the DBDC CVM here and explain the estimation procedure in §4.2. WTP is deﬁned as the amount that a consumer is willing to forego that makes the consumer indifferent between purchasing and not purchasing. In a random utility framework, a vector of covariates is assumed to affect the utility underlying the consumer WTP. Rather than asking respondents to state their WTP, DBDC CVM procedure presents respondents with bids and asks them whether they would be willing to pay that amount. Speciﬁcally, respondents are presented with a sequence of two bids from the set (BL < B1 < BH ) and asked if their willingness to pay equals or exceeds those bids. The sequence of bids depends on the answer (yes/no) to the ﬁrst bid (B1 ) as shown in Figure 3. Multiple bid sets are used in the WTP survey such that the support for the WTP distribution can be enhanced with more information. The main objective is to place upper and lower bounds on a consumer’s unobserved true WTP. Multiple bid sets are used in the WTP survey such that the support for the WTP distribution can be enhanced with more information.\n",
              "Figure 3 (Color online) Contingent Valuation Flow Chart\n",
              "\n",
              ".O\n",
              "\n",
              "7OULD YOU\u0000PAY\u0000 \u0004%\u0011\u001f\n",
              "\n",
              "9ES\n",
              "\n",
              ".O\n",
              "\n",
              "7OULD YOU\u0000PAY\u0000 \u0004%/\u001f\n",
              "\n",
              "9ES\n",
              "\n",
              ".O\n",
              "\n",
              "7OULD YOU\u0000PAY\u0000 \u0004%+\u001f\n",
              "\n",
              "9ES\n",
              "\n",
              "7OULD\u0000YOU\u0000PAY ANYTHING\u0000AT\u0000ALL\u001f\n",
              "\n",
              "$ONE\n",
              "\n",
              "$ONE\n",
              "\n",
              "7HAT\u0007S\u0000THE\u0000MAXIMUM AMOUNT\u0000YOU\u0000WOULD\u0000PAY\u001f\n",
              "\n",
              "\n",
              "290 4.2. Estimation Procedure The empirical approach is designed to estimate product demand under conditions of product piracy. Accordingly, the conceptualization of piracy involved the following two-step procedure. Step A. Estimating WTP. First, we asked all respondents in the (No-No) group whether they would be willing to “pay anything at all.” This gave us the “NoNo-No” group of respondents (i.e., those respondents who said “No” to the initial bid presented to them, “No” to the subsequent lower follow-up bid, and ﬁnally also “No” to the follow-up question asking them whether they would pay anything at all for the software). Then, to rule out other factors for not wishing to pay anything at all, we asked all of the respondents in the (No-No-No) group additional questions that represent their reasoning for the three successive No’s: (a) would use a pirated copy of Adobe Photoshop; (b) borrow Adobe Photoshop from someone else; (c) use a free alternative to Adobe Photoshop; (d) purchase a different photo editing software or service from another company; and (e) do not need Adobe Photoshop. Only those respondents from the (No-No-No) group who marked (a) or (b) were considered likely to be pirates. Hence, for individual j Pr(No-No) = Pr WTPj ≤ B1j and WTPj ≤ BLj = Pr WTPj ≤ B1j WTPj ≤ BLj Pr WTPj ≤ BLj = Pr WTPj ≤ BLj = F BLj since BLj < B1j (1)\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "for piracy, we deﬁne two groups of consumers as follows: (a) Group A consists of individuals whose WTP is “always zero.” These are the “true” pirates and are observed with probability pj . This probability may be modeled as a function of individual covariates Xij in a logit model. ˜ consists of individuals whose WTP (b) Group A is “not always zero.” These individuals may or may not purchase the software and are observed with probability 1 − pj . We do not know which group a randomly drawn individual belongs to, but we do know that Pr WTP = 0 A Xij = 1 and that Pr WTP = k A Xij = 0, k > 0. Hence Pr WTP = 0 Xij ˜ Xij · Pr A ˜ = Pr WTP = 0 A + Pr WTP = 0 A Xij Pr A ˜ Xij · Pr A ˜ + Pr A = Pr WTP = 0 A ˜ Xij = pj + 1 − pj Pr WTP = 0 A Pr WTP = k Xij ˜ Xij Pr A ˜ = Pr WTP = k A + Pr WTP = k A Xij Pr A ˜ Xij Pr A ˜ = Pr WTP = k A ˜ Xij = 1 − pj Pr WTP = k A (6) (5)\n",
              "\n",
              "where F BLj is the cumulative distribution function (cdf) for WTP with parameter vector . The covariates and experimental controls are used to estimate the parameter vectors. Similarly Pr (Yes-Yes) = Pr WTPj > B1j and WTPj > BHj = 1 − F BHj Pr (Yes-No) = Pr WTPj > B1j and WTPj ≤ BHj = F BHj − F B1j and (3) (2)\n",
              "\n",
              "Letting nnn, nn, ny, yn, and yy represent dummy indicators for the (No-No-No), (No-No), (No-Yes), (Yes-No), and (Yes-Yes) group, respectively, Equations (1)–(6) yield a sample log-likelihood of\n",
              "N\n",
              "\n",
              "LL =\n",
              "j =1\n",
              "\n",
              "nnn ln pj + nn − nnn ln 1 − pj F BLj + ny ln 1 − pj F B1j + yn ln 1 − pj F BHj + yy ln 1 − pj 1 − F BHj − F BLj − F B1j (7)\n",
              "\n",
              "Pr (No-Yes) = Pr WTPj ≤ B1j and WTPj > BLj = F B1j − F BLj (4)\n",
              "\n",
              "Equations (1)–(4) represent the probabilities of observing the different responses to each of the individual bids and yield the likelihood function for estimating the mean WTP for the sample. Step B. Jointly Estimating WTP and Propensity for Piracy. To jointly estimate WTP and the propensity\n",
              "\n",
              "The parameters of the distribution can be speciﬁed as a function of covariates. Consequently, the model is able to incorporate the differential impact of covariates on both the probability of piracy (pj ) and the distribution of WTP for “nonpirates.” This feature is used in the empirical application for ascertaining the impact of experimental conditions on piracy and WTP. Estimation of the piracy rate enables us to identify market size for different price offerings and compute the consumer and producer surplus. Finally, the probability of piracy\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "291 licensed for use on one primary computer or other device and, optionally, on one secondary computer or other device. Additionally, it was explained to the respondent that the user may not be able to use certain features of the software if it was not activated. Conﬁrmation of the respondents’ understanding of the strength of the piracy controls was obtained by asking follow-up, veriﬁcation questions. Respondents randomly assigned to the low piracy control condition were shown a much more liberal licensing agreement that speciﬁcally asked the respondent to assume that the Adobe licensing agreement allowed the purchaser to (1) install and use a copy of Adobe Photoshop Elements on as many computers and devices as they wish, even if they only purchased one copy of the software; (2) copy Adobe Photoshop Elements as many times as they wish for use on any computer or device; and (3) give copies of Adobe Photoshop Elements to others. Additionally, it was explained to the participants that registration was not required and a license key was not needed for activation. Full features of the product would be completely available immediately after any installation of the product. Conﬁrmation of the respondents’ understanding of the nonexistent piracy control was obtained by asking three follow-up, veriﬁcation questions (all survey instruments are available as part of Online Appendix A (available as supplemental material at http://dx.doi.org/10.1287/isre.2016.0621)). Respondents randomly assigned to the low vertical extension (LE) experimental condition were ﬁrst introduced to the “existing” product line prior to the introduction of the low or free extension. Thus, participants ﬁrst viewed the focal product (Adobe Photoshop Elements) along with the premium product (Adobe CS6) with a description of the prices of each and the features included within each. Next, respondents were presented with a description of Adobe Photoshop Express, which is a free online product that provides a limited set of the most commonly used Adobe Photoshop Elements features. Participants were explicitly shown how all three Adobe products compared to one another. Basic features such as retouching or manipulating pictures’ color, size, effects, etc., were emphasized and the price ($0 as compared to $99.99)\n",
              "\n",
              "can be estimated with a simple logit model speciﬁed for the probabilities in (7). Since the objective is to estimate the WTP distribution, the estimation procedure provides the aggregate statistical properties of the WTP distribution and not the WTP at the individual respondent level.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "5.\n",
              "\n",
              "Study 1\n",
              "\n",
              "The purpose of this study is to test how the introduction of a free or premium extension and different levels of piracy controls impact the WTP valuations, piracy intentions, and consumer/producer surplus in a highcost or traditional PC software market. In this study, we make use of three Adobe Photoshop products: (1) the online free version of Adobe Photoshop (the low vertical extension), (2) Adobe Photoshop Elements (the focal product), and (3) Adobe CS6 (the premium vertical extension). We expect that consumers will give lower WTP values to products with high piracy controls, but give higher WTP values to products with low piracy controls. Additionally, since consumers’ product evaluations are context dependent, we anticipate that in this high-cost market consumers will give higher WTP valuations to a focal product when a free vertical extension is introduced as opposed to when a premium vertical extension is introduced. 5.1. Design and Procedure We recruited 815 participants from Amazon’s Mechanical Turk and conducted a 2 (piracy control: Low versus High) × 2 (Low Extension (LE) versus High Extension (HE)) between subjects experiment with Adobe Photoshop (PS) Elements as the focal product (see Table 1). All respondents were provided with an initial description of Adobe Photoshop Elements, the current retail price of $99.99, and a summary of basic features (e.g., image editing, creating photo books, sharing images on social media platforms, etc.). Participants were at least 18 years of age. Respondents were randomly assigned to one of the four experimental conditions. Those in the high piracy control condition were presented with information regarding the standard Adobe Photoshop Elements licensing agreement stating that the software was for the exclusive use of the primary user and that it was\n",
              "Table 1 Experimental Design\n",
              "\n",
              "LOW Extension (LE) Free, limited version called Adobe Photoshop Express PC Low: No piracy controls on PS elements PC High: Piracy controls on PS elements PC Low + LE PC High + LE\n",
              "\n",
              "HIGH Extension (HE) Adobe Photoshop CS6 (Study 1 high-cost market—$149.99) or Adobe Photoshop Touch Exp (Study 2 low-cost market—$9.99) PC Low + HE PC High + HE\n",
              "\n",
              "\n",
              "292 was also emphasized. Follow-up veriﬁcation questions were used to verify understanding of the essential aspects of this free offering (i.e., free, used online, only basic features offered). Respondents randomly assigned to the high vertical extension (HE) experimental condition were ﬁrst shown the focal product (Adobe Photoshop Elements) along with the free product (Adobe Photoshop Express) in conjunction with the corresponding prices and features. Participants were then given a description of Adobe Photoshop CS6 for a total of $149.99. Participants were explicitly shown how all three Adobe products compared to one another. The photo editing and sharing features of Adobe CS6 were also emphasized along with the price. Respondents were subsequently asked questions to verify understanding of the essential aspects of this offering (i.e., the additional features included in Adobe CS6). We want to be very clear that in both the high and low vertical extension conditions, participants were asked to make a WTP valuation while viewing all three versions of the product (Adobe Photoshop Express, Adobe Photoshop Elements, and Adobe CS6). The only difference between the high and low vertical extension conditions—the context in which the valuation was dependent—was how the existing market was portrayed prior to the introduction of the vertical extension (see Figure 1). Thus, in the low vertical extension condition participants viewed the premium product and focal product before the free extension was introduced. In the high vertical extension condition, participants viewed the free and focal product before the premium extension was introduced. That the end result is the same for both conditions is consistent with the context dependent preferences literature (Tversky and Simonson 1993) and allows us to assess how incumbents can best approach a market. In other words, we do not assess the ﬁnal product lineup, but rather the impact of how incumbents sequence the full product lineup in relation to the focal product. Using this experimental design, we are able to isolate the speciﬁc context dependent preferences that we are interested in investigating—the introduction of a vertical product extension in a high-cost market. As a result of this random assignment, individual differences (such as previous experience with the product, feature valuations, beliefs about the ﬁrm, etc.) are averaged across all experimental conditions. Thus, causality can be hypothesized to be the result of the randomly assigned independent variables: vertical product extensions (high versus low) and privacy controls (high versus low). Prior to continuing with the survey, respondents gave a valuation of each of the basic features common to all of the products considered in the experiment (e.g., organizing photos, retouching photos, sharing photos on social media, etc.) based on a 7 point Likert scale\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "(1–Not at all valuable; 4–Neutral; 7–Extremely valuable). Feature comparisons for the vertical extensions were also separately presented in the corresponding experimental conditions. Respondents were asked to provide their WTP for the focal product (Adobe Photoshop Elements) after exposure to the experimental conditions and after responding to experimental design veriﬁcation questions (manipulation checks). Respondents were randomly assigned to one of ﬁve potential bid sets for use in CVM evaluation. Using the Adobe Photoshop Elements market price of $99.99 as the basis, we built ﬁve distinct bid sets: ($59.99–$39.99–$79.99); ($79.99– $59.99–$99.99); ($99.99–$79.99–$119.99); ($119.99–$99.99– $139.99); and ($139.99–$119.99–$159.99), where each number represents (bid1, subsequent lower bid, subsequent higher bid). We pretested and piloted the contingent valuation survey language and the bid sets with a subject group similar to the one used in this study. From an estimation perspective several bid sets are required. The ﬁve bid sets used here are centered on Adobe Photoshop Elements’ offered price of $99.99. The results of the CVM section of the survey were used to estimate the WTP distribution. The estimated WTP distribution and piracy rate (derived from those who will never pay anything) was then used to analyze the demand distribution—which enables us to estimate consumer and producer surplus as well as the welfare maximizing price point for the different experimental conditions. Finally, respondents were exposed to questions for multi-item measures including brand perceptions, perceived price fairness, propensity to share, social inﬂuence, product feature value, and performance expectations. The items are described in Table 2 and were adapted from relevant previous literature. 5.2. Descriptive Statistics We reached out to a total of 815 (of whom 814 completed the study) participants using Amazon’s Mechanical Turk (see Table 3 for a description of the sample). Previous researchers have shown that variances in completion time do have an impact on the accuracy of responses in survey research (Meade and Bartholomew 2012). The mean response time was 8.5 minutes and the median was 8 minutes. This study included approximately 1,700 words of instructions and questions and should take at least ﬁve minutes to read—the average individual reads approximately 300 words per minute (Nelson 2012). Researchers have found that participants who take studies too quickly are not paying attention (Steelman et al. 2014). As a result, we removed respondents from our sample who took less than ﬁve minutes to complete the study, for validity purposes. Additionally, any participants that took an extreme amount of time to complete the study—longer than 30 minutes, which is more than 4 standard deviations\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "293\n",
              "\n",
              "Table 2 Construct\n",
              "\n",
              "Survey Constructs and Items Items The Adobe brand is important to me. The Adobe brand is relevant to me. I have frequently thought about the Adobe brand. I am conﬁdent with my evaluation of the Adobe brand. I am certain that my evaluation of Adobe as a brand is correct. The retail price of Adobe Photoshop Elements—$99.99 (Touch—$4.99) is fair. The retail price of Adobe Photoshop Elements—$99.99 (Touch—$4.99) is reasonable. The retail price of Adobe Photoshop Elements—$99.99 (Touch—$4.99) is just. I like to share the things that I have. Everybody beneﬁts when we share what we have with each other. I share what I have so others don’t have to spend money on the same things. I use the same photo editing software as my friends. I pay attention to the software that my friends use, and then make similar purchase decisions. Others that I know use the same photo editing software as I do. I use the same photo editing software as the people in my life who are important to me. I use the same photo editing software as those who have some inﬂuence on my behavior. I use the same photo editing software as those whose opinions I value. I am conﬁdent in the ability of Adobe Photoshop Elements (Touch) to perform well. I am certain that Adobe Photoshop Elements (Touch) would perform as well as similar products. Considering the performance of Adobe Photoshop Elements (Touch), there would not be much risk in choosing to use Adobe Photoshop Elements (Touch). Have you used ANY type of photo editing software at least once (e.g., Photoshop Express, Photoshop Elements, Photoshop CS6, or non-Adobe products)? I am very familiar with the free online photo editing software called Instagram; Pixlr.com; GIMP (GNU Image Processor); FotoFlexer; LunaPic; ColorBright; ClearFilm. Source Park et al. (2010)\n",
              "\n",
              "Brand perceptions\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Price fairness\n",
              "\n",
              "Ajzen et al. (2000)\n",
              "\n",
              "Propensity to share\n",
              "\n",
              "Raghu et al. (2009)\n",
              "\n",
              "Social inﬂuence\n",
              "\n",
              "Ajzen (1991)\n",
              "\n",
              "Performance expectations\n",
              "\n",
              "Ostrom and Iacobucci (1998)\n",
              "\n",
              "Previous use Familiarity with competition\n",
              "\n",
              "above the average response time—were removed to ensure that extreme response times did not impact our manipulations and subsequent responses (Leiner and Doedens 2010). Finally, the manipulation check questions were utilized to determine the ﬁnal set of responses to include in the study, any respondents who\n",
              "Table 3\n",
              "\n",
              "answered more than one of the three manipulation check questions incorrectly for each of the experimental factors were also removed (Berinsky et al. 2014). A total of 711 (87.3%) participants qualiﬁed for the study, however, the inclusion of the entire sample in the analysis did not alter our ﬁndings. Approximately\n",
              "\n",
              "Survey Sample Descriptive Statistics (for Both Studies) Study 1: PC market Study 2: Mobile market 746 49.6% 18–25: 27.9%; 26–34: 35.4%; 35–54: 28.2%; >55: 8.4% <30K: 34.9% 30K–50K: 28.1% >50K: 37.0% Less than four year college: 56.7% four year college: 32.7% Higher than four year college: 10.6% 4.47 (1.27) 4.96 (1.01) 5.05 (1.13) 5.05 (1.41) 5.15 (1.10) 3.97 (1.29) 5.49 (0.95) 0.88 (0.33) 16.60 (7.02)\n",
              "\n",
              "Number of respondents Gender: Male Age\n",
              "\n",
              "Income\n",
              "\n",
              "814 56.8% 18–25: 29.5%; 26–34: 36.6%; 35–54: 26.1%; >55: 7.7% <30K: 33.2% 30K–50K: 27.1% >50K: 39.7% Less than four year college: 54.9% four year college: 37.1% Higher than four year college: 8.0% 4.32 (1.40) 4.69 (1.12) 4.78 (1.18) 3.73 (1.60) 5.23 (1.08) 3.92 (1.28) 5.38 (1.05) 0.88 (0.32) 16.50 (6.96)\n",
              "\n",
              "Education\n",
              "\n",
              "Expertise (of 7) Feature rating (of 7) Brand perceptions (of 7) Price fairness (of 7) Propensity to share (of 7) Social inﬂuence (of 7) Performance expectations (of 7) Previous use (0 or 1) Familiarity with competition (sum; each brand rated out of 7)\n",
              "\n",
              "\n",
              "294 equal numbers of respondents were exposed to each experimental condition (PC Low + LE = 178 respondents; PC High + LE = 177 respondents; PC Low + HE = 176 respondents; PC High + HE = 180 respondents). 5.3. WTP Distribution Using the maximum likelihood model for WTP distribution estimation given the propensity for piracy that we described earlier, the CVM data collected from the survey were ﬁtted to a Gumbel distribution because they exhibited the best ﬁt (Akaike information criterion; AIC). Three equations were utilized to simultaneously estimate coefﬁcients associated with the piracy rate and the WTP distribution location and scale. As a ﬁrst step, the cell means for WTP and piracy rates were estimated without introducing any covariates.2 The dollar value of mean WTP was obtained through estimation of Gumbel distribution parameters (location and scale). The probability of piracy is given by p = e x / 1 + e x , where x is the estimated pirating propensity equation. The cell means are reported in Table 4 (standard deviation for the study sample WTP was 40.9). Highest mean WTP is observed in the cell (PC = LOW, EXT = LE), and is signiﬁcantly different from that of (HIGH, HE) (p = 0 01), marginally signiﬁcantly different from (LOW, HE) (p = 0 10), but not signiﬁcantly different from that of (HIGH, LE) (p = 0 29). The (HIGH, HE) cell mean was the lowest WTP and is marginally signiﬁcantly different from that of (HIGH, LE) (p = 0 10), but not different from (LOW, HE) (p = 0 29). The cell mean in the (LOW, HE) condition is also not different from that in (HIGH, LE) (p = 0 55). The piracy rate is lowest for the (HIGH, LE) condition and highest in the (LOW, HE). As predicted in Hypothesis 1A, in a high-cost market consumers’ WTP for the focal product is higher when a free vertical extension is introduced, than when a premium vertical extension is introduced. We anticipated this outcome because in a high-cost market quality signals are already associated with price. Thus, the introduction of a premium product does not improve quality associations or WTP valuations. However, when a free vertical extension is introduced, the focal product WTP valuations beneﬁt from the positive affect boost associated with the free offering. We also ﬁnd support for Hypothesis 2 that premium vertical extensions increase consumers’ propensity to pirate. Finally, we ﬁnd support for Hypothesis 3 that piracy controls will have a negative impact on piracy intentions, and Hypothesis 4 that piracy controls will have a negative impact on WTP.\n",
              "2\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Table 4\n",
              "\n",
              "Cell Means for the Experimental Conditions EXT = LE EXT = HE 72.41 14 69.85 6\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "PC = LOW Mean WTP ($) Piracy rate (%) PC = HIGH Mean WTP ($) Piracy rate (%)\n",
              "\n",
              "76.42 10 73.86 4\n",
              "\n",
              "6.\n",
              "\n",
              "Study 2\n",
              "\n",
              "The purpose of Study 2 is to investigate whether vertical product extensions have the same impact within a low-cost software market. In this study, we also make use of three Adobe Photoshop products but we use applications that are available on mobile platforms: Photoshop Express (free low vertical extension), Photoshop Touch (focal product), and Photoshop Touch Exp (premium vertical extension). In this study, we expect to ﬁnd that in a low-cost market, consumers’ WTP valuations will be higher for premium product extensions than low—or free—product extensions, the opposite of high-cost markets (Study 1). 6.1. Design and Procedure We use the same study design, procedure, and WTP estimations used in Study 1. However, we change the products and the bid sets to accurately represent the product logos and prices reﬂected in low-cost markets such as what is exhibited on the mobile platform. Thus, using the Adobe Photoshop Touch market price of $4.99 as the basis, we built ﬁve distinct bid sets: ($1.99– $0.49–$3.49); ($3.49–$1.99–$4.99); ($4.99–$3.49–$6.49); ($6.49–$4.99–$7.99); and ($7.99–$6.49–$9.49), where each set represents (bid1, subsequent lower bid, subsequent higher bid). 6.2. Descriptive Statistics We reached out to a total of 776 (746 of whom completed the study) participants using Amazon’s Mechanical Turk. Just as in Study 1, we removed participants who completed the survey in less than 5 minutes or took longer than 30 minutes (mean response time was 9.3 minutes and the median was 8 minutes) and respondents who answered more than one out of the three manipulation check questions incorrectly in each experimental factor for validity purposes. A total of 623 (83.5%) participants qualiﬁed for the study (see Table 3 for a description of the sample). Approximately equal numbers of respondents were exposed to each experimental condition (PC Low + LE = 157 respondents; PC High + LE = 145 respondents; PC Low + HE = 163 respondents; PC High + HE = 158 respondents). 6.3. WTP Distribution As in Study 1, we used the maximum likelihood model for WTP distribution estimation and ﬁtted the CVM\n",
              "\n",
              "Median values—(LOW, LE): $76.64; (HIGH, LE): $72.07; (LOW, HE): $70.64; (HIGH, HE): $68.06.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "295 were adequate (all correlations < 0 40), the construct items loaded separately on distinct factors in a rotated (oblique) conﬁrmatory factor analysis, the average variance extracted (AVE) for these constructs were all above 0.5, and the square roots of the AVEs for each construct were higher than all interconstruct correlations, suggesting good discriminant validity. The Cronbach’s alphas were greater than 0.70 and the composite reliabilities (CR) were all greater than 0.70, suggesting good convergent reliability. We also created a composite for perceived feature value. The Cronbach’s alpha was greater than 0.70 and the CR was also greater than 0.70. The AVE was less than the preferred 0.50, however, the square root of the AVE was greater than the interconstruct correlations. Therefore, given that our method is maximum likelihood based, we consider this to be a reliable and valid composite. Table 6 reports the result of a simultaneous equation approach. Among the covariates in the ﬁrst equation and their impact on piracy rates, increased “perceived feature” value has a negative impact on piracy rates in all models, supporting existing research that higher feature valuations are likely to result in reduced piracy rates (Conner and Rumelt 1991, Gopal and Sanders 1997). Additionally, higher levels of perceived “price fairness” are also likely to decrease overall piracy rates. “Sharing” propensity also has a signiﬁcant positive effect in all models, suggesting that positive attitudes toward sharing increase piracy rates in all software markets. Consumers in mobile markets may be more comfortable with the idea of sharing because many applications in the mobile industry are designed to share (i.e., photos, gaming, messages, locations, etc.). We ﬁnd no signiﬁcant impact of “social inﬂuence” in any of the markets, which suggests that piracy is not speciﬁcally a function of ownership among a consumer’s network. We also ﬁnd that “brand perception” has a negative marginally signiﬁcant relationship with piracy rate in the Adobe (traditional high-cost) market, suggesting that a strong brand may actually decrease consumers’ propensity to pirate software. Finally, neither “Previous Use” nor “Familiarity with Competition” had a signiﬁcant impact on piracy rates. Among the covariates in the second equation and their impact on the WTP distribution, “feature rating,” “price fairness,” and “social inﬂuence” all positively and signiﬁcantly impacted WTP. Thus, increases to any of these variables also increase overall WTP valuations of the focal product. Additionally, we ﬁnd that “performance” has a marginally signiﬁcant positive relationship on WTP valuations in the high-price market and a signiﬁcant positive relationship on WTP valuations in the low-price market. Participants’ familiarity with the competition did not have a signiﬁcant impact in the WTP equations for either the low cost\n",
              "\n",
              "Table 5\n",
              "\n",
              "Cell Means for the Experimental Conditions EXT = LE EXT = HE 5.77 6 5.13 1\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "PC = LOW Mean WTP ($) Piracy rate (%) PC = HIGH Mean WTP ($) Piracy rate (%)\n",
              "\n",
              "5.16 7 4.51 2\n",
              "\n",
              "data we collected to a Gumbel distribution (AIC). We used the same procedure as in Study 1 to calculate the cell means for WTP and piracy rates without introducing any covariates.3 The cell means are reported in Table 5 (standard deviation for the study sample WTP was 3.61). The highest mean WTP is observed in the cell (PC = LOW, EXT = HE), and is signiﬁcantly different from that of (HIGH, LE) (p < 0 001 , (HIGH, HE) (p = 0 002), and (LOW, LE) (p = 0 003) cells. The lowest cell mean was in the (HIGH, LE) cell and is signiﬁcantly different from the (LOW, LE) (p = 0 002) and (HIGH, HE) (p = 0 003) cells. The cell mean in (LOW, LE) condition is not different from that in the (HIGH, HE) cell (p = 0 88). Thus, we ﬁnd evidence to support Hypothesis 1B that consumers’ WTP valuations in a low-cost market will be higher when a premium rather than free vertical product extension is introduced into the market. We expected this result because in a low-cost market the addition of the premium product extension provides a quality signal that extends to the focal product. However, because the market is already low cost, the beneﬁt of the affective evaluation from the free product extension is negligible. Additionally, we replicate the ﬁndings of Study 1 and ﬁnd additional support for Hypothesis 3 that piracy controls will have a negative impact on piracy intentions and Hypothesis 4 that piracy controls will have a negative impact on WTP valuations. However, we ﬁnd the opposite results from Study 1 with regard to Hypothesis 2, where in the low-cost market, premium vertical extensions decrease consumers’ propensity to pirate.\n",
              "\n",
              "7.\n",
              "\n",
              "Covariate Models and Demand Analysis for Studies 1 and 2\n",
              "\n",
              "7.1. Covariate Models Composites for ﬁve constructs—sharing propensity, perceived price fairness, expected product performance, social inﬂuence, and brand attitude—were formed from the corresponding multi-item measures. The psychometric properties of the constructs were assessed for reliability and validity using standard construct evaluation approaches summarized by MacKenzie et al. (2011). The correlations between the constructs\n",
              "3\n",
              "\n",
              "Median values were close to the means—(LOW, LE): $5.00; (HIGH, LE): $4.36; (LOW, HE): $5.62; (HIGH, HE): $4.97.\n",
              "\n",
              "\n",
              "296\n",
              "Table 6 Piracy Rate and WTP Distribution Estimation Results (1) PC (2) PC (3) MOBILE (4) MOBILE\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Extension\n",
              "\n",
              "Piracy rate eqn. 0 198 0 198 0 69 0 69 −1 133∗∗∗ −3 69 0 352∗ 1 99 −0 688∗∗∗ −3 32 −0 506∗∗ −3 08 −0 102 −0 58 −0 290† −1 92 −0 13 −0 31 0 114 03 −2 487∗ −2 26 −1 133∗∗∗ −3 69 0 352∗ 1 99 −0 688∗∗∗ −3 32 −0 506∗∗ −3 08 −0 102 −0 58 −0 290† −1 92 −0 13 −0 31 0 114 03 −2 487∗ −2 26\n",
              "\n",
              "−0 131 −0 30 −1 448∗∗ −2 74 0 880∗∗ 2 72 −1 296∗∗∗ −3 89 −0 406† −1 88 −0 254 −0 89 0 101 04 −0 821 −1 53 0 44 0 73 −3 695∗ −2 11\n",
              "\n",
              "−0 131 −0 30 −1 448∗∗ −2 74 0 880∗∗ 2 72 −1 296∗∗∗ −3 89 −0 406† −1 88 −0 254 −0 89 0 101 04 −0 821 −1 53 0 44 0 73 −3 695∗ −2 11 0 540∗ 2 32 −0 36 −1 56 0 665∗∗∗ 3 81 1 389∗∗∗ 64 1 515∗∗∗ 10 29 0 278∗ 2 03 −0 0625 −0 26 0 0326 0 11 −0 593∗ −2 05\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Piracy control Propensity to share Feature rating Price fairness Social inﬂuence Brand perceptions Previous use Familiarity with competition Constant\n",
              "\n",
              "Extension Piracy control Performance expectations Feature rating Price fairness Social inﬂuence Brand perceptions\n",
              "\n",
              "WTP dist. eqn. (location parameter) 0 134 0 909 0 440† 0 04 03 1 93 5 432† 1 74 4 398† 1 95 13 89∗∗∗ 5 37 21 65∗∗∗ 12 2 6 838∗∗∗ 3 65 −1 914 −0 88 2 427 0 79 4 288† 1 91 14 16∗∗∗ 5 55 21 82∗∗∗ 12 73 6 488∗∗∗ 3 51 −8 637∗∗ −2 76 13 79∗∗∗ 38 −2 182 −0 60 −3 37 −0 68 −2 258 −0 65 61 95∗∗∗ 5 88 −2 405 −0 49 −1 234 −0 36 60 35∗∗∗ 58 1 020∗ 25 −0 118 −0 38 3 065∗∗∗ 3 34 2 067∗∗∗ 20 55 623 1,416.7 1,509.8 −687.4 −0 318 −1 42 0 672∗∗∗ 3 86 1 337∗∗∗ 6 25 1 474∗∗∗ 10 16 0 316∗ 2 32 −0 313† −1 83\n",
              "\n",
              "or high cost market. Previous use of a photo editing software only had a signiﬁcant (positive) impact on the WTP equation in the Android (low-cost) market. We expect that this is due to the fact that both the product and the market are relatively new. Thus, consumers who are more familiar with the product and how well it functions are willing to pay more for the product. Finally, we ﬁnd that although the main effect of “brand perceptions” is inconsistent across the different equations, it moderates WTP valuations for highand low-priced markets differentially. For instance, in the high-cost market, brand perception interacts with piracy controls such that as brand perceptions increase, consumers’ WTP for products with piracy controls also increases. However, consumers’ WTP for products with no piracy controls decreases. This would suggest that for products in a high-cost market, brand perceptions have a magnifying effect on WTP valuations because consumers see the inherent value in piracy controls. In the low-cost market on the other hand, brand perceptions have a marginally signiﬁcant interaction with the product line extension. In this case, the premium vertical extension increases consumers’ WTP more for consumers with low brand perceptions of the product than for consumers with high brand perceptions of the product. Importantly, by controlling for these variables, we can eliminate them as alternative explanations for the vertical product extension context dependent effects and piracy control differences in both the low-cost and high-cost markets. A key advantage of simultaneous estimation of WTP and piracy rate is that it enables the computation of overall conditional demand (given piracy) for the focal product in response to the changes in the experimental conditions. We present the demand analysis results in §7.2. Assuming zero marginal costs of production, expected revenue was used as a proxy for producer surplus. 7.2. Demand Analyses Given the piracy rates and WTP distribution parameters (distribution scale and location) estimated for each experimental condition (without covariates), a demand analysis was conducted (for more details, see the demand analysis spreadsheet example shown in Online Appendix B). The conditional probability of purchase was estimated using the scale and location parameters for the estimated WTP distribution. Prices were incremented in $5.00 intervals from $0 to $160 (based on the highest bid-set value in the experiment—$159.99). The probability of purchase was discounted by the piracy rate to arrive at an estimated percentage of consumers with a WTP at the given price (i.e., demand). Revenue to the producer was then calculated by multiplying the given price by the portion of those consumers with an estimated willingness to pay at that price. As an example, with piracy\n",
              "\n",
              "Piracy control × Brand perceptions Extension × Brand perceptions Previous use Familiarity with competition Constant\n",
              "\n",
              "1 051∗∗ 2 58 −0 126 −0 40 3 033∗∗∗ 33 2 061∗∗∗ 20 59 623 1,416.4 1,518.4 −685.2\n",
              "\n",
              "Constant N AIC BIC ll\n",
              "\n",
              "(Scale parameter) 24 50∗∗∗ 23 99∗∗∗ 20 11 20 1 711 1,597.6 1,693.5 −777.8 711 1,587.9 1,692.9 −770.9\n",
              "\n",
              "Note. The t statistics are in parentheses. † p < 0 10; ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "297 whether they are in a high-cost or low-cost market. Importantly, we want to point out that the results of the demand analysis suggest that although piracy controls may reduce piracy intentions (Hypothesis 3), producers and consumers are both best served by a reduction in piracy controls. In the PC (high-cost) market, we suggest that an incumbent can increase welfare by (a) reducing prices, (b) introducing piracy controls, and (c) extending the existing product line at the low end. As suggested earlier, the free vertical extension creates positive affect that impacts consumers’ preference for the focal product. As such, a ﬁrm that currently has offerings in a high-cost market would beneﬁt from introducing a free vertical extension of their product. Allowing consumers to try out the product prior to committing to the purchase price has the additional potential to create positive affect toward the focal product. In the high-cost PC market, this may be particularly enticing to consumers. In the mobile (low-cost) market, we show that an incumbent can increase demand by (a) minimally reducing prices (by ∼$0.5), (b) reducing piracy controls, and (c) extending the existing product line at the high end. We suggest that this is due to the quality signal that introducing a premium extension has on the focal product. Future research should examine the price point at which these two vertical extension strategies meet. A better understanding of this difference will be useful for managerial decisions made when digital products exist within the same market but at discrepant (very high versus very low) price points.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "control = Low and product extension = LE, consider an offer price of $25. The estimated piracy rate for the experimental condition was approximately 10%. The log-logistic conditional probability for WTP = $25 was 94.00%. The estimated demand at that price was (94.00% × (1%–10%)) approximately 85%. Therefore, revenue at this price point was (85% × 100 × $25) approximately $2,114. The revenues computed at each price point form the basis for calculating producer surplus (for the given price, under the given experimental conditions). Consumer surplus at the retail price of the focal product was calculated by ﬁnding the area under the estimated distribution curve beyond the retail price of the focal product ($99.99–PC market, $4.99–mobile market). We deﬁned optimal price as the price point at which producer surplus was maximized. The consumer surplus at the optimal point would be the area under the estimated distribution curve beyond the optimal price point. The demand analyses for each of the four experimental conditions of both Studies 1 and 2 are summarized in Tables 7 and 8. Remarkably, in both high-cost and low-cost contexts, producer and consumer interests are aligned at both current and optimal prices and our investigation of consumer and producer surplus has useful insights for software markets. Whereas academic literature on pricing strategies and piracy primarily focus on producer surplus, we empirically show the impact of these ﬁrm strategies on consumer and producer surplus. Thus, we are able to suggest different approaches for incumbents to introduce vertical product extensions with different levels of piracy control depending on\n",
              "Table 7 Study 1. Welfare and Demand Analyses (PC Market) PC = 0, EXT = 0 Price $50.00 $55.00 $60.00 $65.00 $70.00 $75.00 $80.00 $85.00 $90.00 $95.00 $100.00 $105.00 Consumer welfare Producer welfare Total welfare Optimal price Consumer welfare Producer welfare Total welfare % WTP 65.06 59.96 54.79 49.65 44.68 39.94 35.49 31.38 27.62 24.21 21.15 18.43 Revenue ($) 3 3 3 3 3 2 2 2 2 2 2 1 253 12 298 04 287 11 227 47 127 32 995 18 839 32 667 29 485 75 300 28 115 44 934 77\n",
              "\n",
              "PC = 1, EXT = 0 % WTP 66.35 60.87 55.38 50.00 44.83 39.95 35.41 31.23 27.43 24.01 20.94 18.21 Revenue ($) 3 3 3 3 3 2 2 2 2 2 2 1 317 45 347 93 322 83 250 07 138 40 996 58 832 92 654 92 469 03 280 63 094 03 912 54\n",
              "\n",
              "PC = 0, EXT = 1 % WTP 58.55 53.58 48.64 43.82 39.22 34.90 30.88 27.21 23.87 20.87 18.19 15.81 Revenue ($) 2 2 2 2 2 2 2 2 2 1 1 1 927 47 947 05 918 31 848 55 745 60 617 23 470 71 312 52 148 21 982 37 818 63 659 79\n",
              "\n",
              "PC = 1, EXT = 1 % WTP 60.84 55.44 50.13 45.01 40.16 35.63 31.46 27.65 24.21 21.13 18.39 15.97 Revenue ($) 3 3 3 2 2 2 2 2 2 2 1 1 042 14 049 40 007 90 925 68 811 09 672 20 516 41 350 22 179 13 007 60 839 16 676 47\n",
              "\n",
              "Welfare (given a $99.99 price for a single license of Adobe Photoshop Elements) 359 34 355 52 308 68 2 115 44 2 094 03 1 818 63 2 474 78 2 449 55 2 127 31 55 00 1 977 71 3 298 04 5 275 75 Welfare (at the optimal price for each column) 55 00 1 984 85 3 347 93 5 332 78 55 00 1 736 53 2 947 05 4 683 59\n",
              "\n",
              "312 01 1 839 16 2 151 17 55 00 1 778 22 3 049 40 4 827 62\n",
              "\n",
              "\n",
              "298\n",
              "Table 8 Study 2. Welfare and Demand Analyses (Mobile Market) PC = 0, EXT = 0 Price $2.50 $2.75 $3.00 $3.25 $3.50 $3.75 $4.00 $4.25 $4.50 $4.75 $5.00 $5.25 Consumer welfare Producer welfare Total welfare Optimal price Consumer welfare Producer welfare Total welfare % WTP 71.11 68.24 65.29 62.28 59.25 56.21 53.19 50.21 47.29 44.44 41.68 39.01 Revenue ($) 177 77 187 67 195 88 202 43 207 37 210 78 212 75 213 38 212 78 211 08 208 38 204 82 PC = 1, EXT = 0 % WTP 67.11 63.91 60.70 57.50 54.33 51.22 48.17 45.22 42.36 39.61 36.98 34.47 Revenue ($) 167 78 175 76 182 10 186 87 190 16 192 06 192 70 192 18 190 63 188 16 184 90 180 96\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "PC = 0, EXT = 1 % WTP 78.51 75.93 73.20 70.35 67.40 64.38 61.32 58.24 55.17 52.14 49.15 46.23 Revenue ($) 196 27 208 81 219 61 228 64 235 89 241 42 245 26 247 52 248 27 247 64 245 75 242 73\n",
              "\n",
              "PC = 1, EXT = 1 % WTP 74.92 71.86 68.73 65.54 62.32 59.10 55.90 52.75 49.67 46.66 43.75 40.95 Revenue ($) 187 29 197 63 206 19 213 00 218 12 221 62 223 62 224 21 223 51 221 66 218 77 214 98 72 61 218 77 291 37 4 25 102 90 224 21 327 11\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Welfare (given a $4.99 price for a single license of Adobe Touch for Android) 69 19 60 99 82 15 208 38 184 90 245 75 277 57 245 89 327 91 4 25 98 01 213 38 311 39 Welfare (at the optimal price for each column) 4 00 97 30 192 70 290 00 4 50 103 46 248 27 351 73\n",
              "\n",
              "Whereas the extant literature supports lower levels of piracy controls from the perspective of improving producer surplus (Conner and Rumelt 1991, Givon et al. 1995, Gopal and Sanders 1997), we show that relaxed piracy controls may actually be welfare maximizing, especially in low-cost markets. Moreover, at the current marketplace prices, relaxed piracy controls improve welfare in both markets. Additionally, the optimal prices computed indicate that both consumer and producer surplus can increase in response to price reductions irrespective of experimental conditions, however, care should be taken with this implication because our main concern is with the relative WTP elicitation rather than the absolute WTP elicitation.\n",
              "\n",
              "cost (such as the mobile market) or high cost (such as the traditional PC market). We present an approach for computing consumer and producer surplus under various conditions by estimating both WTP and piracy rate. This approach overcomes the shortcomings of WTP as the sole proxy for consumer valuation by also accounting for changes in piracy rates in the market that result from ﬁrm level decisions. As such, we ﬁnd there are substantial differences in how producers should approach both high-cost and low-cost markets to maximize consumer and producer welfare. 8.1. Willingness to Pay We ﬁnd strong support for our research hypotheses related to consumer valuation. The WTP for the focal product is most responsive to a product line extension and not as responsive to changes in piracy control. The WTP ﬁndings build on the nascent demand-side research on piracy and WTP in the IS literature. We ﬁnd support for Hypothesis 1A, that in a high-cost market consumers’ WTP valuations are highest when a free vertical extension is introduced. We anticipated this result because in a high-cost market the positive affect associated with the introduction of the free product positively inﬂuences consumers’ valuations of the focal product. We ﬁnd the reverse effect in a low-cost market and, consequently, support for Hypothesis 1B. Consumers’ WTP valuations in a low-cost market were highest when a premium vertical extension was introduced. We also anticipated this outcome because of the fact that in a low-cost market a quality association— such as the one provided by the introduction of a\n",
              "\n",
              "8.\n",
              "\n",
              "Discussion\n",
              "\n",
              "The hypercompetitive nature of software markets and the competition with zero priced alternatives has created uncertainty as to how producers should align their market offerings. Consider the free online Adobe Photoshop that is currently available to PC consumers and, conversely, the free mobile Adobe Photoshop application that is available to mobile platform users (Reid 2015). Using context dependent preferences as our theoretical background, we contribute to the literature by showing that producers can inﬂuence the WTP valuations of their consumers by how they introduce vertical product extensions into the market and through the piracy controls that they place on those products. Importantly, we show that these effects are different depending on whether the market is low\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "299 to improve both consumer and producer surplus in high-cost and low-cost markets. Price fairness perception is a signiﬁcant psychological driver of piracy rate. A reason why piracy rates are high in the digital goods industry is that incumbents often set prices that are well above consumers’ WTP for the markets they address. For instance, in an analysis of global software piracy, Gopal and Sanders (2000) advocated for country speciﬁc software product prices as an antidote to extremely high piracy rates in emerging economies. Interestingly, a similar phenomenon was observed in the music industry where the unbundling of music CDs into singles somewhat mitigated the downward spiral of rampant music piracy through peer-to-peer sharing. Software vendors also create distinct versions for different market segments. The ﬁndings in this study indicate that there is further room for improvement in this area. For instance, we ﬁnd a signiﬁcant impact of feature rating on piracy rate. As the feature rating increases, piracy intentions decrease. Firms can mitigate piracy intentions through improved content and features they include as part of their software offering. 8.3. Limitations We note some limitations of this study so that the results and implications can be understood in the right context. First, as with any contingent valuation study, external validity can be limited because of the lack of incentive compatibility in eliciting consumer WTP. We ﬁnd that the average WTP in our experimental conditions range from $70–$76 for the high-cost market and $4.5–$5.7 for the low-cost market depending on the experimental condition. These prices are below the market price anchor of $99 for Adobe Photoshop Elements, but span the $4.99 cost of Adobe Photoshop Touch. However, the WTP numbers are quite close to the offered price even with the incentive compatibility issues. Importantly, the estimations and our subsequent implications relied more on the relative comparisons between experimental conditions than the absolute values of WTP. A second limitation is that we assume zero marginal costs for the incumbent in the demand analysis and surplus computation. Whereas this assumption is consistent with extant literature on software products, care should be exercised in extending this line of reasoning to other product contexts where marginal costs may be material (e.g., products hosted on cloud computing platforms). Future research in this area should examine and extend the ﬁndings in other market segments.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "premium vertical extension—would be a valuable signal to consumers. An important ﬁnding in Raghu et al. (2009) was that the usefulness of the product had a signiﬁcant impact on WTP. We reﬁned the deﬁnition of usefulness further in this study to elicit feature level valuations of the focal product and ﬁnd similar results. Essentially, consumer hedonic preferences for features will drive the WTP for products similar to Adobe Photoshop. Extant IS literature has seldom considered the ramiﬁcations of market oriented attributes of software products. We also ﬁnd that social inﬂuence is an important construct in determining consumer WTP valuations in both highcost and low-cost software markets in support of the extant literature that inﬂuential others have a positive impact on consumer adoption. Additionally, we ﬁnd there are positive effects of performance perceptions (marginally signiﬁcant in the high-cost market and signiﬁcant in the low-cost market) and price fairness perceptions on consumers WTP valuations. Finally, we showed that brand perceptions have a different moderating effect on consumers’ WTP valuations such that they interact with piracy controls (in the high-cost market) and product line extensions (in the low-cost market). These results in particular, point to a new direction in IS research where consumer software product valuations also consider brand perceptions in pricing and piracy control to rule out alternative explanations that could be the result of individual difference variables. 8.2. Piracy Rate Another key contribution of this work is the estimation of the propensity to pirate among subjects for consumer software. As would be expected, estimated piracy rates in the high-cost market were higher than those in the low-cost market. Irrespective of product extension controls, introduction of piracy controls lowered the piracy rate in both markets. However, the impact of vertical product line extension on piracy rate was different for both markets. In the high-cost market, consumers’ piracy intentions were higher when a premium extension was introduced, which also coincided with lower WTP valuations. However, in the low-cost market, consumers’ piracy intentions were highest when a free vertical extension was introduced, in contrast to previous research and our own Hypothesis 2. A key beneﬁt of piracy rate estimation is the ability to compute producer and consumer surplus under the different experimental conditions. This enabled us to estimate optimal product prices for each cell. We ﬁnd that at current and optimal prices, maximum consumer and producer surplus are well aligned in both high-cost and low-cost markets. Optimal prices in both markets were lower than the suggested price due to piracy. Thus, price reductions may have the potential\n",
              "\n",
              "9.\n",
              "\n",
              "Conclusion\n",
              "\n",
              "The main contribution of this research is the development and application of the context-dependent preference theory framework to analyzing consumer\n",
              "\n",
              "\n",
              "300 valuation of software products in two different markets. Using two controlled experimental settings, we demonstrate the importance of product extension strategy in traditional high-cost and emergent low-cost consumer software markets. We show that individual differences in consumer perceptions also play an important role in determining propensity to pirate and willingness to pay, which also implies that researchers will improve the validity of their ﬁndings by controlling for these variables. Our ﬁndings, in the context of two speciﬁc consumer software products, show that welfare improvements are dependent on the type of market and the order in which product line extensions are introduced. Moreover, based on the empirical distribution, we show that price reductions may have the potential to improve both consumer and producer surplus. Although context-dependent preference theory framework opens up a promising new line of research into consumer software product markets, a number of important issues remain. Switching costs and loyalty concerns are important sources of competitive advantage for incumbents. It will be important to test if consumer valuations are moderated by these important factors. Another important consideration in product line extensions is moving into unrelated product categories (Heath et al. 2011). There is surprisingly little research in the IS literature on product diversiﬁcation and its impacts on incumbents in consumer software markets. Finally, the increasing shift toward subscription-based and service oriented approaches to software provisioning provide rich opportunities for IS researchers to make meaningful theoretical extensions in the domain of consumer software markets. Supplemental Material\n",
              "Supplemental material to this paper is available at http://dx .doi.org/10.1287/isre.2016.0621.\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "Acknowledgments\n",
              "The authors would like to thank the senior editor, the associate editor, and the anonymous reviewers for their constructive feedback and recommendations. The authors dedicate this paper to our late friend, colleague, and coauthor, Rajiv K. Sinha.\n",
              "\n",
              "References\n",
              "Aadland D, Caplan AJ (2006) Cheap talk reconsidered: New evidence from CVM. J. Econom. Behav. Organ. 60(4):562–578. Ajzen I (1991) The theory of planned behavior. Organ. Behav. Human Decision Processes 50(2):179–211. Ajzen I, Rosenthal LH, Brown TC (2000) Effects of perceived fairness on willingness to pay. J. Appl. Soc. Psych. 30(12):2439–2450. Arrow K, Solow R, Portney PR, Leamer EE, Radner R, Schuman H (1993) Report of the NOAA panel on contingent valuation. Federal Register 58(10):4601–4614. Banerjee DS (2003) Software piracy: A strategic analysis and policy instruments. Internat. J. Indust. Organ. 21(1):97–127.\n",
              "\n",
              "Berinsky AJ, Margolis MF, Sances MW (2014) Separating the shirkers from the workers? Making sure respondents pay attention on self-administered surveys. Amer. J. Political Sci. 58(3):739–753. Bishop RC, Heberlein TA (1979) Measuring values of extramarket goods: Are indirect measures biased? Amer. J. Agricultural Econom. 61(5):926–930. Bryce DJ, Dyer JH, Hatch NW (2011) Competing against free. Harvard Bus. Rev. 89(6):104–111. Business Software Alliance (2011) Global business software piracy study. http://globalstudy.bsa.org/2011/downloads/study_pdf/ 2011_BSA_Piracy_Study-Standard.pdf. Cameron TA, James MD (1987) Efﬁcient estimation methods for “closed-ended” contingent valuation surveys. Rev. Econom. Statist. 69(2):269–276. Campbell MC (2007) “Says who?!” How the source of price information and affect inﬂuence perceived price (un) fairness. J. Marketing Res. 44(2):261–271. Carson RT, Flores NE, Meade NF (2001) Contingent valuation: Controversies and evidence. Environ. Resource Econom. 19(2): 173–210. Chellappa RK, Shivendu S (2005) Managing digital piracy: Pricing and sampling strategies for digital experience goods in vertically segmented markets. Inform. Systems Res. 16(4):400–417. Conner KR, Rumelt RP (1991) Software piracy: An analysis of protection strategies. Management Sci. 37(2):125–139. Depoorter B, Parisi F, Vanneste S (2005) Problems with the enforcement of copyright law: Is there a social norm backlash? Internat. J. Econom. Bus. 12(3):361–369. Erat S, Bhaskaran SR (2012) Consumer mental accounts and implications to selling base products and add-ons. Marketing Sci. 31(5):801–818. Foster V, Mourato S (2003) Elicitation format and sensitivity to scope. Environment. Resource Econom. 24(2):141–160. Givon M, Mahajan V, Muller E (1995) Software piracy: Estimation of lost sales and the impact on software diffusion. J. Marketing 59(1):29–37. Gopal RD, Sanders GL (1997) Preventive and deterrent controls for software piracy. J. Management Inform. Systems 13(4):29–47. Gopal RD, Sanders GL (2000) Global software piracy: You can’t get blood out of a turnip. Comm. ACM 43(9):82–89. Hanemann WM (1984) Welfare evaluations in contingent valuation experiments with discrete responses. Amer. J. Agricultural Econom. 66(3):332–341. Hanemann M (1994) Valuing the environment through contingent valuation. J. Econom. Perspect. 8(4):19–43. Hanley N, Mourato S, Wright RE (2001) Choice modelling approaches: A superior alternative for environmental valuation? Hanley N, Roberts CJ, eds. Issues in Environmental Economics (Blackwell Publishing, Oxford, UK), 185–212. Heath TB, DelVecchio D, McCarthy MS (2011) The asymmetric effects of extending brands to lower and higher quality. J. Marketing 75(4):3–20. Hertzendorf M (1993) I’m not a high-quality ﬁrm–but I play one on TV. RAND J. Econom. 24(2):236–247. Homburg C, Koschate N, Hoyer WD (2005) Do satisﬁed customers really pay more? A study of the relationship between customer satisfaction and willingness to pay. J. Marketing 69(2):84–96. Jacobson M (2012) Dear Android: A 9:1 piracy rate for games is not good enough. Wired Magazine (May 2), http://www.wired.co.uk/ news/archive/2012-05/02/android-market-game-piracy. Kirmani A, Sood S, Bridges S (1999) The ownership effect in consumer responses to brand line stretches. J. Marketing 63(1):88–101. Leiner DJ, Doedens S (2010) Test-retest reliability in the research practice of the online survey. Jackob N, Zerback T, Jandura O, Maurer M, eds. The Internet as Research Tool and Object in Communication Science, Research Methods and Logic of Communication Science, Vol. 6 (von Halem, Cologne, Germany), 316–331. Liebowitz SJ, Margolis SE (1999) Causes and consequences of market leadership in application software. Competition Innovation Personal Comput. Indust., 1–37.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Baird et al.: Product Line Extensions in Software Markets\n",
              "Information Systems Research 27(2), pp. 282–301, © 2016 INFORMS\n",
              "\n",
              "301\n",
              "Raghu TS, Sinha R, Vinze A, Burton O (2009) Willingness to pay in an open source software environment. Inform. Systems Res. 20(2):218–236. Randall T, Ulrich K, Reibstein D (1998) Brand equity and vertical product line extent. Marketing Sci. 17(4):356–379. Reid B (2015) How to download Adobe Photoshop CS2 for free legally. Redmond Pie (May 24), http://www.redmondpie.com/download -adobe-photoshop-cs2-for-free-legally-while-you-still-can/. Shampanier K, Mazar N, Ariely D (2007) Zero as a special price: The true value of free products. Marketing Sci. 26(6):742–757. Sinha RK, Mandel N (2008) Preventing digital music piracy: The carrot or the stick? J. Marketing 72(1):1–15. Sinha RK, Machado F, Sellman C (2010) Don’t think twice, it’s all right: DRM, peer-to-peer piracy and the pricing of digital music. J. Marketing 74(2):40–54. Smith D (2015) Android still has a massive piracy problem. Business Insider (January 8), http://www.businessinsider.com/android -piracy-problem-2015-1. Steelman ZR, Hammer BI, Limayem M (2014) Data collection in the digital age: Innovative alternatives to student samples. MIS Quart. 38(2):355–378. Stevens TH, Belkner R, Dennis D, Kittredge D, Willis C (2000) Comparison of contingent valuation and conjoint analysis in ecosystem management. Ecological Econom. 32(1):63–74. Sun B, Xie J, Cao HH (2004) Product strategy for innovators in markets with network effects. Marketing Sci. 23(2):243–254. Tversky A, Simonson I (1993) Context-dependent preferences. Management Sci. 39(10):1179–1189. Venkatesh V, Thong JYL, Xu X (2012) Consumer acceptance and use of information technology: Extending the uniﬁed theory of acceptance and use of technology. MIS Quart. 36(1):157–178. Wertenbroch K, Skiera B (2002) Measuring consumers’ willingness to pay at the point of purchase. J. Marketing Res. 39(2):228–241. Wilson LO, Norton JA (1989) Optimal entry timing for a product line extension. Marketing Sci. 8(1):1–17. Wu SY, Chen PY (2008) Versioning and piracy control for digital information goods. Oper. Res. 56(1):157–172. Zeithaml VA (1988) Consumer perceptions of price, quality, and value: A means-end model and synthesis of evidence. J. Marketing 52(3):2–22.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "List JA (2001) Do explicit warnings eliminate the hypothetical bias in elicitation procedures? Evidence from ﬁeld auctions for sportscards. Amer. Econom. Rev. 91(5):1498–1507. Loken B, John DR (1993) Diluting brand beliefs: When do brand extensions have a negative impact? J. Marketing 57(3):71–84. Lopes AB, Galletta DF (2006) Consumer perceptions and willingness to pay for intrinsically motivated online content. J. Management Inform. Systems 23(2):203–231. MacKenzie SB, Podsakoff PM, Podsakoff NP (2011) Construct measurement and validation procedures in MIS and behavioral research: Integrating new and existing techniques. MIS Quart. 35(2):293–334. Meade AW, Bartholomew CS (2012) Identifying careless responses in survey data. Psych. Methods 17(2):437–455. Mitchell RC, Carson RT (1989) Using Surveys to Value Public Goods: The Contingent Valuation Method (Resources for the Future, Washington, DC). Muchmore M (2012) Stolen software: Piracy hits more than movies and music. PC Magazine (January 25), http://www.pcmag.com/ article2/0,2817,2399318,00.asp. Murphy JJ, Stevens TH (2004) Contingent valuation, hypothetical bias, and experimental economics. Agriculture Resource Econom. Rev. 33(2):182–192. Nascimento F, Vanhonacker WR (1988) Optimal strategic pricing of reproducible consumer products. Management Sci. 34(8): 921–937. Nelson B (2012) Do you read fast enough to be successful? The most successful people I know don’t just read—they inhale information. Forbes (June 4), http://www.forbes.com/sites/brettnelson/ 2012/06/04/do-you-read-fast-enough-to-be-successful/. Ostrom AL, Iacobucci D (1998) The effect of guarantees on consumers’ evaluation of services. J. Services Marketing 12(5):362–378. Park CW, MacInnis D, Priester J, Eisingerich A, Iacobucci D (2010) Brand attachment and brand attitude strength: Conceptual and empirical differentiation of two critical brand equity drivers. J. Marketing 74(6):1–17. Park JH, MacLachlan DL (2008) Estimating willingness to pay with exaggeration bias-corrected contingent valuation method. Marketing Sci. 27(4):691–698. Payne JW, Bettman JR, Johnson EJ (1992) Behavioral decision research: A constructive processing perspective. Annual Rev. Psych. 43(1):87–131.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "D4euMV0a_uKd",
        "colab_type": "code",
        "outputId": "4dcf5ff0-27f8-44f3-8634-0f06c04b8550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc2.sents if 'p =' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The probability of piracy is given by p = e x / 1 + e x , where x is the estimated pirating propensity equation.,\n",
              " Highest mean WTP is observed in the cell (PC = LOW, EXT = LE), and is signiﬁcantly different from that of (HIGH, HE) (p = 0 01), marginally signiﬁcantly different from (LOW, HE) (p = 0 10), but not signiﬁcantly different from that of (HIGH, LE) (p = 0 29).,\n",
              " The (HIGH, HE) cell mean was the lowest WTP and is marginally signiﬁcantly different from that of (HIGH, LE) (p = 0 10), but not different from (LOW, HE) (p = 0 29).,\n",
              " The cell mean in the (LOW, HE) condition is also not different from that in (HIGH, LE) (p = 0 55).,\n",
              " The highest mean WTP is observed in the cell (PC = LOW, EXT = HE), and is signiﬁcantly different from that of (HIGH, LE) (p < 0 001 , (HIGH, HE) (p = 0 002), and (LOW, LE) (p = 0 003) cells.,\n",
              " The lowest cell mean was in the (HIGH, LE) cell and is signiﬁcantly different from the (LOW, LE) (p = 0 002) and (HIGH, HE),\n",
              " (p = 0 003) cells.,\n",
              " The cell mean in (LOW, LE) condition is not different from that in the (HIGH, HE) cell (p = 0 88).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "lCl6ZB50Diud",
        "colab_type": "code",
        "outputId": "8d2d91e2-278c-406d-b8bf-5aaf3fbbfc15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "p_klein_als_Sents = [sent for sent in doc2.sents if 'p <' in sent.string]\n",
        "p_klein_als_Sents "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The highest mean WTP is observed in the cell (PC = LOW, EXT = HE), and is signiﬁcantly different from that of (HIGH, LE) (p < 0 001 , (HIGH, HE) (p = 0 002), and (LOW, LE) (p = 0 003) cells.,\n",
              " † p < 0 10; ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              " \n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "Hctw8pBFFV-t",
        "colab_type": "code",
        "outputId": "8ef3c444-46fb-4e86-b0e2-03f21d615726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16304
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc3 = nlp(open(u\"SaundersA#BrynjolfssonE_2016_Valuing Information Technology Related Intangible Assets_MIS Quarterly_1.txt\").read())\n",
        "doc3\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RESEARCH ARTICLE\n",
              "\n",
              "VALUING INFORMATION TECHNOLOGY RELATED INTANGIBLE ASSETS1\n",
              "Adam Saunders\n",
              "Sauder School of Business, University of British Columbia, 2053 Main Mall, Vancouver, BC V6T 1Z2 CANADA {adam.saunders@sauder.ubc.ca}\n",
              "\n",
              "Erik Brynjolfsson\n",
              "Sloan School of Management, Massachusetts Institute of Technology, 100 Main Street, Cambridge, MA 02142 U.S.A. {erikb@mit.edu}\n",
              "\n",
              "In this article, we assess the value of information technology related intangible assets and then use data on business practices and management capabilities to understand how this value is distributed across firms. Using a panel of 127 firms over the period 2003–2006, we replicate and extend the finding from Brynjolfsson, Hitt, and Yang (2002) that $1 of computer hardware is correlated with more than $10 of market value. We account for the “missing $9” by broadening the definition of IT to include capitalized software, and then include all purchased and internally developed software, other internal IT services, IT consulting, and IT-related training (whether or not it is capitalized by the firm). In addition, we use data on IT-related business practices in order to analyze the distribution of IT-related intangibles within the sample. Our results suggest that the “invisible” IT not accounted for on balance sheets is being priced into the market value of firms. We also estimate that there is a 45% to 76% premium in market value for the firms with the highest organizational IT capabilities (based on separate measures of human resource practices, management practices, internal IT use, external IT use, and Internet capabilities), as compared to those with the lowest organizational IT capabilities. Our results thus suggest that contributions of IT to value depend heavily on other factors, and are not a rising tide that lifts all boats. Keywords: IT value, market value, IT-related intangibles, IT capabilities, intangible assets, R&D value, brand value\n",
              "\n",
              "Introduction1\n",
              "A company’s intangible assets—especially those related to information technology—are not well captured on corporate balance sheets. The vast majority of intangible spending is expensed; that is, not treated as investments that build assets on the balance sheet (Lev 2000, p. 91). One rare exception is\n",
              "\n",
              "goodwill,2 while the other notable exception is a small portion of software development costs.3 Thus, any spending on busi\n",
              "2\n",
              "\n",
              "When one company buys another, the acquirer adds the net assets of the target to its balance sheet. The additional value of what the acquirer paid over and above the net assets of the target is then added as goodwill to the acquirer’s balance sheet. Goodwill, thus, does not include intangibles created outside of mergers and acquisitions. Lev (2000) notes other rare cases of balance sheet intangibles, such as movie rights and commissions paid for life insurance and mortgages.\n",
              "\n",
              "1\n",
              "\n",
              "Vijay Gurbaxani was the accepting senior editor for this paper. Ram Gopal served as the associate editor.\n",
              "\n",
              "3 Software spending in the application development stage of a project (including designing, coding, installing, or testing) can be capitalized. However, any spending in the preliminary stage of a software project is expensed,\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1, pp. 83-110/March 2016\n",
              "\n",
              "83\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "ness process reengineering, such as mapping business processes or reconfiguring the work force, must be treated as an expense (Ernst & Young 1998). Yet, it is precisely this restructuring of the firm that forms a significant component of technology investments. While IT accounts for about 30% of all business investment,4 publicly available data on firm-level IT investment and IT practices are severely lacking. For example, the most recent free and publicly available IT spending data set was last published in 1997.5 In a well-known study, Brynjolfsson, Hitt, and Yang (2002) (hereafter BHY 2002) found that $1 of computer hardware is correlated with more than $10 of market value, suggesting that there are at least $9 of unmeasured IT-related intangible assets for every $1 of measured hardware. However, the lack of available data on IT-related intangibles has hampered efforts to understand the nature of this relationship between IT and business value. In this paper, we set out to find the “missing $9” with a market value approach based on IT-related spending and practices data. Our hypothesis is that while IT-related intangible assets are often absent from the balance sheet, they are reflected in a company’s market value. Consider three traditional bellwether components of the Dow Jones Industrial Average: Caterpillar, 3M, and Home Depot. In Figure 1, we illustrate the sizable differences between each of their market values and their balance sheet assets. Specifically, we argue that while important complements to IT investment—like changes in business processes—may be intangible, they are not unmeasurable when the right data and methods are applied (Brynjolfsson and Hitt 2005). Our analysis relies on a balanced panel of annual data from 127 firms over the period 2003–2006, which provides us with broader and more recent IT spending estimates compared to\n",
              "which might include hiring consultants to evaluate the need for the software in the first place. The same is true for spending in the post-implementation stage, which includes training, maintenance, or support. Even when it comes to software that is ready to use off-the-shelf (known as prepackaged software in National Income and Product Accounts), the Bureau of Economic Analysis (Moylan 2001, p. 3) noted: “Although in theory prepackaged software purchases with a useful life of at least one year should be capitalized, most are treated as an expense.”\n",
              "4\n",
              "\n",
              "data used in BHY 2002 (which relied on 1987–1997 data from Computer Intelligence). We then use a separate data set on IT competencies to analyze whether IT-related intangible assets are evenly distributed among firms or concentrated in leading firms. Applying econometric methods, we find that IT intangibles are a significant driver of market value even though they are “invisible” on the balance sheet. The work presented below is the first we are aware of that replicates the original BHY 2002 finding that at least $10 of market value is associated with every $1 of hardware assets. This is the case even though we use (1) different source data that measure IT spending,6 (2) a different panel of firms, and (3) a different time period. While BHY 2002 theorized a missing $9 of IT intangibles for every $1 of hardware, their data set was limited to hardware data. In this work, we are able to directly account for the missing piece of intangible IT by including capitalized software in a market value estimating equation, as well as all purchased and internally developed software, other internal IT services, IT consulting, and IT-related training, whether or not this spending is actually capitalized by the firm. We find that $1 of this broadest measure of IT assets is closely correlated with $1 of market value, the equilibrium value predicted by economic theory (Hall 2000, 2001).7 Our results demonstrate that the invisible IT not accounted for on the balance sheet is being priced into the market value of firms, suggesting that IT assets defined by accounting standards capture only a fraction of the business value of IT.8\n",
              "\n",
              "6 BHY 2002 used IT data from Computer Intelligence Infocorp (CII), whereas this research uses data from the SeeIT Survey (described in further detail below).\n",
              "\n",
              "The equilibrium value predicted by economic theory is that $1 of any asset should be valued by the market at roughly $1. BHY 2002 also found that that $1 of non-IT physical assets and $1 of financial assets were each correlated with roughly $1 of market value. We find this as well. Interestingly, we find that $1 of research and development (R&D) and $1 of brand assets are each associated with significantly more than $1 of market value.\n",
              "8\n",
              "\n",
              "7\n",
              "\n",
              "Source: Bureau of Economic Analysis, National Income and Products Accounts Table 5.3.5. “Private Fixed Investment by Type.” This is the sum of information processing equipment (line 10) and software (line 17) divided by total nonresidential fixed investment (line 2).\n",
              "\n",
              "The data was published as part of the InformationWeek 500, an annual ranking of the 500 firms that are most innovative users of IT. Since 1998, this annual ranking has not included IT spending. As of 2014, the list has been shortened and renamed the InformationWeek Elite 100.\n",
              "\n",
              "5\n",
              "\n",
              "Statement of Position (SOP) 98-1, “Accounting for the Costs of Computer Software Developed or Obtained for Internal Use” was developed by the American Institute of Certified Public Accountants (AICPA). Emerging Issues Task Force (EITF) 97-13, “Accounting for Costs Incurred in Connection with a Consulting Contract or an Internal Project that Combines Business Process Reengineering and Information Technology Transformation” was developed by the Financial Accounting Standards Board (FASB). These documents constitute a set of guidelines that firms use in determining whether spending on IT and business process reengineering projects can be expensed or capitalized, depending on the type of spending and the stage of the project.\n",
              "\n",
              "84\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Figure 1. The Value of Three Components of the Dow Jones Industrial Average as Compared to Their Balance Sheet Assets (Data from December 31, 2010)\n",
              "\n",
              "Beyond quantifying the size of IT intangibles, our research examines how these intangibles are distributed within our sample. We do so by relying on a measurement of organizational IT capabilities (ITC) used in Aral and Weill (2007). ITC is based on how management capability and human resource capability facilitate or inhibit IT investment, how IT is used in internal communications and with suppliers, and the company’s Internet capabilities. We assign firms grades of “A” through “F” based on their ITC scores, and analyze the relationship between these categories and firms’ corresponding market value.9 We find that firms with the highest ITC (in the top 5% of the sample) have a 45% to 76% greater market value than the firms with the lowest ITC (in the bottom 5% of the sample), indicating that IT-related intangible assets are far from being commodities. Rather, they can be a significant value differentiator between firms. The remainder of our paper is organized as follows: the next section reviews the relevant literature, and is followed by our conceptual framework and econometric model. We then describe the data used in this study, present our results, and conclude with a summary of findings and implications for future research.\n",
              "\n",
              "Literature\n",
              "While the most visible assets of a modern corporation may be its structures, equipment, and other physical property, for many firms the most valuable assets are intangible. These include intellectual property like patents and copyrights, brand capital whether created by advertising or word of mouth, business processes and methods, and even the organization of the firm and allocation of decision rights. More formally, our definition of intangible assets is that used by Lev (2000, p. 5): Assets are claims to future benefits, such as the rents generated by commercial property, interest payments derived from a bond, and cash flows from a production facility. An intangible asset is a claim to future benefits that does not have a physical or financial (a stock or a bond) embodiment. A patent, a brand, and a unique organizational structure (for example, an Internet-based supply chain) that generate cost savings are intangible assets. Research on the economics of IT has established that computer hardware investments are typically accompanied by substantial software, organizational, and process investments, and that productivity of IT investments is much higher when these complementary investments are also made.10 These are\n",
              "10\n",
              "\n",
              "9\n",
              "\n",
              "Using various categories allows for more flexibility in the association between ITC and market value over assuming a monotonic relationship, especially for very high or low values of ITC.\n",
              "\n",
              "For a lengthy discussion and review, see Brynjolfsson and Saunders (2010).\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "85\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "the sorts of intangible investments we seek to more carefully measure and analyze in this paper. According to recent estimates by the Bureau of Economic Analysis, U.S. businesses hold more than $2.3 trillion worth of intellectual property assets,11 and invest a further $600 billion in such assets each year.12 Nakamura (2001), using a wider definition of intangibles, analyzed economy-wide spending data on R&D, software, and advertising, as well as aggregate data on labor inputs and corporate operating margins, and estimated that intangible assets in U.S. corporations could total as much as $5 trillion. Corrado et al. (2009), using different methods but an even broader metric that includes organizational capital, found almost $4 trillion in intangible assets in the U.S. economy. While such aggregate estimates are noteworthy, there is a paucity of data at the firm level. We seek to contribute to the literature using a combination of firm-level intangible asset data and market value equations. Our research complements and extends other empirical work that uses financial markets to value IT and other intangible assets. Anderson et al. (2003) used estimating equations relating the market value of the firm to its book value, earnings, R&D, and Y2K spending. They found that $1 of Y2K spending was correlated with an average of $30 to $40 of market value (with one of their estimates being as high as $62 of market value). Their interpretation was that the high values for Y2K spending were likely due to complementary investments in organizational assets as well as improvements to the supply chain as a whole. While this is the most plausible and intuitive explanation, they did not have the data to empirically demonstrate the value of IT-related intangibles. Our study uses IT practice and capabilities data alongside intangible IT spending data to quantify the value of IT-related intangibles and examine how they are distributed in the sample. While existing studies have examined the relationship of IT, organizational design, and market value, our work more directly measures and quantifies the IT-related intangible assets (or IT intangibles for short) through an estimating equation. For instance, Lev and Radhakrishnan (2005) used the firm’s sales, general, and administrative expenses (SG&A) as a proxy for organizational capital13 in a sample of publicly traded companies. They found that this measure of organizational capital was significant in explaining a firm’s sales, and\n",
              "\n",
              "that it was also highly correlated with the firm’s spending on IT. Their organizational capital and IT measures together explained market value beyond traditional measures such as book value and growth potential. Their results, while quite powerful, relied purely on spending data.14 However, our work builds on their findings as we use organizational practice data as well as specific IT intangibles spending data to estimate IT-related organizational capital. Our study uses a panel of data that spans multiple industries across time, which enables us to control for more confounding factors than would be possible in a single cross section. For example, Rai et al. (2006) constructed a model to relate IT infrastructure to better supply chain management capabilities and improved performance. While they presented a potentially more sophisticated measure of IT capabilities, their data were a one-time cross section of firms that allowed them to focus only on the manufacturing and retail sectors. In addition, the performance measures in their work were subjective; while their sample contained 110 firms, they could obtain objective performance measures from Compustat for only 57 of them. A second study relying on a single cross section of firms was conducted by Subramani (2004), who used a detailed questionnaire of IT usage in 131 firms to gauge the benefits of using supply chain management systems. The firms in that survey were supplier firms to a single large retailer in Canada. In contrast to these studies, we use a longitudinal panel and are able to include a lagged value of market value in an effort to control for unobserved factors that could contribute to higher market value. In addition to quantifying IT-related intangibles, our findings support the observation that IT investments are significantly riskier than non-IT investments (Dewan et al. 2007). IT capabilities are neither easy to create nor copy because they involve a system of practices. While copying any one piece might be straightforward, an organizational system as a whole is very difficult to duplicate (Brynjolfsson and Milgrom 2012; Brynjolfsson et al. 1997; Milgrom and Roberts 1990, 1995; Porter 1996). Our finding that the highest ITC firms are correlated with 45% to 76% more value than the lowest ITC firms fits with this framework. The rewards are higher for firms that have built an interlocking system of complementary IT capabilities because such investments involved significant risk: many changes to the workplace often need to be made, and the interaction effects of any new practices with existing practices can be difficult to anticipate. By incorporating estimated values of R&D and brand directly in a market value equation, our analysis complements other\n",
              "Based on the InformationWeek 500 annual list of IT spending published until 1997.\n",
              "14\n",
              "\n",
              "11 Consisting of software, research and development, and entertainment, literary, and artistic originals. See BEA Fixed Assets Table 2.1, line 77. 12\n",
              "\n",
              "See BEA NIPA Table 5.3.5, “Private Fixed Investment by Type,” line 16.\n",
              "\n",
              "We use the term organizational capital to be consistent with the literature, which often uses capital and assets interchangeably.\n",
              "\n",
              "13\n",
              "\n",
              "86\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "approaches in the literature that value such intangibles using discounted ex post future returns and production functions. Hand (2003), using a net present value (NPV) profitability model, found that the NPV of R&D and brand is significantly positive and that the firms that were the largest spenders on R&D and advertising were the ones with the highest returns on those assets. Lev (2004) noted that companies with the greatest amount of R&D assets had the highest risk-adjusted returns between 1983 and 2000, implying that “R&Dintensive companies were systematically underpriced by the market” (p. 110). Barth et al. (1998) found that brand value estimates are a significant and positive predictor of share prices and future returns. Using a production function framework, Seethamraju (2003) estimated the value of trademarks, and found that these estimated values are reflected in share prices. These four studies demonstrate that despite their virtual absence from the balance sheet, R&D and advertising are highly valuable investments. Using the market value equation framework outlined within the next section, our analysis is another lens through which to quantify the value of these intangibles.\n",
              "\n",
              "For example, while Procter and Gamble (P&G) was worth more than $237 billion at the end of June 2012,15 the company listed just $132 billion in assets on its balance sheet. While we can measure physical assets for publicly traded companies because of the accounting regulations that require their inclusion on disclosed balance sheets, measuring their intangible assets poses significant challenges: with the exception of goodwill or other purchased intangibles, they are virtually invisible on financial statements. One of the key contributions of this work is to construct several types of intangible assets and directly measure their values in an empirical estimation of (1). We do not assume that the residual of the firm’s market value above and beyond the firm’s book value is due entirely to intangibles. Continuing with the P&G example, we do not begin with the assumption that the other $105 billion of its market value is simply equal to the sum of its intangible assets. Rather, in our approach, we estimate how much each type of asset— physical, financial, or intangible—contributes to the entire $237 billion of the company’s market value. We begin with the null hypothesis that $1 of an asset should contribute $1 to a firm’s market value. To test this hypothesis, we construct three types of intangible assets and include them in a market value equation: IT intangibles, advertising, and R&D. We also add physical and financial assets to this equation, and then estimate the relationships between all asset types and market value. We control for factors such as industry and year in an estimation of (1). In any given year, the market value of two firms that operate in different industries will differ because of unobserved differences due to barriers to entry, regulation, or competition, for example. There may also be time-varying unobserved factors, such as excessive optimism or pessimism on the part of investors, that necessitate controlling for year in an estimation of (1).\n",
              "\n",
              "Conceptual Framework\n",
              "We begin with the simple, yet elegant, principle that the total value of financial claims on the firm should be equal to the sum of the firm’s physical, financial, and intangible assets (Baily 1981; Hall 2000, 2001). While one can measure the total number of dollars spent on coding software, running experiments, or keyword advertising, how this spending translates into intangible asset value is often difficult to quantify. We argue that financial markets provide an important way to value intangible assets beyond the balance sheet and other input metrics such as spending. We model the value of financial claims against the firm, MV, as the sum of each of its n assets, Ai (based on the model in BHY 2002, pp. 150-151):\n",
              "\n",
              "Econometric Model\n",
              "In order to relate the market value of the firm to its various assets, we use the following estimation equation:\n",
              "\n",
              "MV =  Ai\n",
              "i =1\n",
              "\n",
              "n\n",
              "\n",
              "(1)\n",
              "\n",
              "In other words, If all assets can be documented and no adjustment costs are incurred in making them fully productive, buying a firm is equivalent to buying a collection of separate assets. Thus the market value of a firm is simply equal to the current stock of its capital assets (ibid, p. 151).\n",
              "\n",
              "MVit = β0 + β1 Kit + β2 Fit + β3 ITit + β4 Rit + β5 Bit + controls + εit\n",
              "\n",
              "(2)\n",
              "\n",
              "As the sum of equity plus total liabilities. Alternatively, it was worth $198 billion using the sum of equity plus debt.\n",
              "\n",
              "15\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "87\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "The value of all financial claims on the firm (equity plus liabilities) is placed on the left-hand side of equation (2) and is denoted by MV. Subscripts i and t represent firm i in year t. We use various categories of assets on the right side of the equation. The first category is physical, non-IT assets (also described as ordinary assets in the literature), represented by K. This category includes all non-IT equipment and structures. Next, we have other assets, F, which represents total balance sheet assets minus ordinary assets. Included in F are inventories, receivables, cash, and other accounting assets, such as goodwill. The next term is IT, which represents IT assets. We use three different measures of IT in our analysis. The first measure includes purchased hardware only, the second measure includes capitalized hardware and software, and the third and broadest measure of IT includes all hardware and software, internal IT services, IT consulting, and IT-related training. In our broader measures of IT, we combine both tangible (such as hardware) and intangible (such as software and services) assets, rather than attempting to value each asset type separately.16 The term R represents R&D assets, and B represents brand assets. We include controls for year and industry, as well as a dummy variable for firms that do not report R&D, and a dummy variable for firms that do not report advertising.17 We scale all dummy variables to control for firm size by multiplying each of them by the firm’s total balance sheet assets.18 Given that our data are in panel form, we estimate equation (2) using generalized least squares (GLS) to address potential serial correlation of the error terms and heteroskedasticity. As one of our objectives is to estimate the distribution of IT intangible assets among the sample firms, we do not use fixed effects or first differences, as this would overcorrect for the\n",
              "\n",
              "effects we are looking to measure.19 In addition, as the time period of our panel is relatively short, if we use fixed effects or first differences, any measurement error in the slowly changing right-hand side variables could significantly bias the coefficients downward (Griliches and Hausman 1986). To estimate the value of $1 of ordinary assets, K, we use net property, plant, and equipment (PP&E) as listed on the balance sheet, and subtract IT assets that would be included in K to avoid double-counting.20 We use the unadjusted value of other assets, F, as listed on the balance sheet, and we assume this approximates a current-cost measure. To measure IT, R, and B, we use the perpetual inventory method (PIM) to construct assets based on depreciation rates and price deflators from the Bureau of Economic Analysis (BEA) where possible.21 Our null hypothesis is as follows: β1 = β2 = β3 = β4 = β5 = 1. If any of these coefficients are greater than 1, then it means that firms, on average, are reaping greater benefit from their assets than the replacement costs of those assets. However, this does not mean that there is a “free lunch” in the market, especially when it comes to intangibles. Rather, there are potential adjustment costs and omitted variables to consider (BHY 2002). In particular, BHY 2002 found that $1 of computer hardware assets was correlated with more than $10 of market value, and reasoned that this was due to omitted IT\n",
              "\n",
              "19\n",
              "\n",
              "One of the principle assumptions in random effects estimation is that the firm effect is uncorrelated with all of the explanatory variables. However, as Hall et al. (2005, p. 26) note, this assumption would be inappropriate in estimating the value of R&D: R&D tends to change slowly over time, a firm’s R&D intensity is highly correlated with its individual effect; in fact, it is an important component of what creates differences across firms, so removing these effects would entail an overcorrection. This is likely true of IT intangibles and advertising as well.\n",
              "\n",
              "16\n",
              "\n",
              "Lev (2000, p. 7) remarked on how difficult this would be to do: the demarcation lines between intangible assets and other forms of capital are often blurry. Intangibles are frequently embedded in physical assets (for example, the technology and knowledge contained in an airplane) and in labor (the tacit knowledge of employees), leading to considerable interaction between tangible and intangible assets in creation of value. Although net PP&E is listed in historical-cost dollars (based on the year the asset was purchased) rather than a true replacement or current-cost (how much it could cost to purchase the asset in that year), converting this value into a current-cost measure is challenging because firms are not required to disaggregate their investment by type (such as equipment or structures). Converting historical-cost PP&E into a current-cost measure would entail a number of assumptions about the mix of investment, and since measuring such physical assets is not the main focus of this work, we follow the prevailing practice in the IS literature to leave net PP&E as reported by the firm as is (e.g., see Brynjolfsson and Hitt 2003; BHY 2002). Nevertheless, we do attempt such an adjustment as a robustness check, as detailed in the results section.\n",
              "21 In the absence of published data from the BEA (such as the depreciation rate for advertising), we use values based on the prevailing practice in the literature. 20\n",
              "\n",
              "17 Since spending on R&D and advertising is required to be disclosed where it is material to the firm, we assume a value of 0 for firms that do not report it.\n",
              "\n",
              "This allows for a more realistic interpretation of the dummy variables. The effects of industry or year on market value are much more likely to be related to the size of the firm than to simply be a constant dollar amount across all firm sizes.\n",
              "\n",
              "18\n",
              "\n",
              "88\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "intangible assets. When the authors included an interaction term comprised of organizational practices plus hardware in their estimating equations, the coefficient on hardware alone fell significantly, suggesting up to $9 of related assets correlated with each dollar of hardware assets. In this paper, we seek to measure the missing $9 by using data on IT intangible spending in an estimating equation for market value. However, cost-based measures may not be enough to quantify the value of intangible assets. If two firms spend $20 million each on an enterprise resource planning (ERP) system, it is reasonable to expect that the value of each system is going to be firm specific. For example, P&G adopted an approach to a technology-driven and highly coordinated supply chain that involved careful attention to more than just the technology. As director of customer services and logistics for P&G in North America, Lamar Johnson noted: Our approach in the 1980s was not completely software driven…we developed our attitude way before there were computers on everybody’s desk or an Internet. All technology does is facilitate these practices; it is not what creates them. It’s the culture and work process that drives them (Stankevich 2003, p. 43). A number of recent papers have shown that complementary business practices are necessary to get the full value from IT (Bartel et al. 2007; Bloom et al. 2012; Bresnahan et al. 2002; Brynjolfsson and Hitt 2003; Brynjolfsson et al. 2002; Crespi et al. 2007; Dedrick et al. 2003; McKinsey Global Institute 2001; Pilat 2004). Moreover, research has shown that such practices do not work as effectively on an individual basis as they do within a cohesive system (Athey and Stern 1998; Brynjolfsson et al. 1997; Milgrom and Roberts 1990, 1995).22 Thus, to examine the distribution of IT intangibles, we construct a variable to capture management capabilities and organizational IT practices. This variable is based on a measure called organizational IT capabilities from Aral and Weill (2007). It is constructed from five components, which quantify how management capability and human resource capability facilitate or inhibit IT investment, how IT is used in internal communications and with suppliers, and the company’s Internet capabilities. We examine whether the value from IT-related intangibles is evenly distributed across firms within the sample, or whether most of the value is concentrated in firms with high ITC. Moreover, given the complementarities one might expect from investments in IT\n",
              "See Brynjolfsson and Saunders (2010, Chapter 4) for a review of key papers in the area of IT and complementarities.\n",
              "22\n",
              "\n",
              "hardware, software, and services alongside workplace practices and skills, we interact ITC with measures of IT. We construct ITC as a standardized variable (mean 0, variance 1). We then create dummy variables based on the firm’s ITC score, akin to academic letter grades of “A” through “F”.23 If a firm’s ITC is in the bottom 5% of the sample, then ITC_F = 1. Otherwise, it is equal to 0. If ITC is between the 5th and 15th percentiles, then ITC_D = 1. Otherwise, it is 0. The variable ITC_B = 1 if ITC is between the 85th and 95th percentiles, 0 otherwise, and ITC_A = 1 if ITC is in the top 5% of the sample, and 0 otherwise. The baseline group of ITC_C firms is in the middle 70% of the distribution, between the 15th and 85th percentiles.24 Using this set of dummy variables, we construct the following estimating equation:\n",
              "\n",
              "MVit = β0 + β1 Kit + β2 Fit + β3 ITit + β4 Rit + β5 Bit + β6 EMPit ∗ ICT _ Fi + β7 EMPit ∗ ITC _ Di + β8 EMPit ∗ ICT _ Bi + β9 EMPit ∗ ICT _ Ai + β10 ITit ∗ ICT _ Fi + β11 ITit ∗ ICT _ Di + β12 ITiy ∗ ICT _ Bi + β13 ITit ∗ ICT _ Ai + controls + εit\n",
              "\n",
              "(3)\n",
              "\n",
              "For the baseline group of ITC_C firms (whose dummy is omitted), the total contribution of IT assets to market value is β3 @ IT dollars. For an ITC_F firm, the total contribution of IT assets to market value would be (β3 + β10) @ IT + β6 @ EMP dollars. For an ITC_A firm, the contribution of IT assets to market value would be (β3 + β13) @ IT + β9 @ EMP dollars. The null hypothesis underlying this estimating equation is that the eight coefficients β6 through β13 are equal to zero.\n",
              "\n",
              "Data\n",
              "Sample Construction\n",
              "Our data set consists of 508 firm-year observations from 2003 through 2006, based on a balanced panel of 127 publicly traded U.S. firms across a broad range of industries. We construct our sample by starting with the firms that participated in the Social and Economic Explorations of IT (SeeIT) survey, a project based at the MIT Sloan School to poll com23\n",
              "\n",
              "We use five categories: A, B, C, D, and F, as there is no academic letter grade of E. Given that ITC is a standardized variable, the middle 70% of the distribution is within approximately one standard deviation of the mean.\n",
              "24\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "89\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "panies about IT spending and technology usage covering data from 2005 through 2006.25 We match the publicly traded firms from the survey to Compustat Industrial Annual, and eliminate firms without data for market value, total assets, employment, or PP&E. We drop a small handful of firms with implausibly high IT asset estimates, as compared to PP&E from Compustat.26 In addition, we drop companies incorporated outside the United States, in order to eliminate confounding effects from firms subject to different tax laws, markets, cultures, and regulations. Furthermore, we exclude IT producers, financial firms, and natural resource firms such as mining and oil companies. As compared to firms in the rest of the economy, IT producers face different input prices for computer hardware and software since such firms use the IT they produce themselves (BHY 2002).27 Thus, we drop firms with primary industry codes in computer and electronic product manufacturing (NAICS 334), software publishing (NAICS 5112), data processing, hosting, and related services (NAICS 518), and computer system design and related services (NAICS 5415). We exclude financial corporations (NAICS 52) as their balance sheets are fundamentally different than those of other firms in the economy.28 Mining and oil firms (NAICS 21 and 324) hold significant assets whose market values fluctuate with the potentially volatile market price of their underlying commodities, yet such changes are not reflected in the book value of assets on the balance sheet. Because of the large potential changes to the left-side variable (market value) from such swings in input costs or assets, without resultant changes to the right-side variables, we exclude these firms.29\n",
              "\n",
              "To create a balanced panel, we keep firms that have complete data in every year from 2003 through 2006. All of the firmlevel data is constructed on a fiscal year-end basis. The summary statistics of the sample can be found in Table 1.\n",
              "\n",
              "Variable Construction and Data Sources\n",
              "Market Value We define market value as the sum of all financial claims on the firm at the end of each fiscal year, as shown in equation (4): MV = PSTK + (PRCC_F * CSHO) + LT (4)\n",
              "\n",
              "MV, or market value, is the sum of three variables. The first is the value of preferred stock (Compustat mnemonic PSTK), the second is the price of common stock at the end of the fiscal year, (PRCC_F) times the number of outstanding shares of common stock (CSHO), and the third variable is total liabilities (LT). IT Assets: Three Measures The firm-level IT spending data is summarized in Table 2. On average, each firm in our sample spent $229.8 million per year on IT, of which $29.6 million, or 12.9%, was on hardware. We convert this annual IT spending into three different measures of IT assets, progressively broadening the definition each time: 1. Purchased Hardware: This is the measure from BHY 2002, which uses hardware owned by the firm (regardless of the extent to which the firm capitalizes hardware). Capitalized Hardware and Software: This is an estimate of the IT assets that would be included on the balance sheet of the company. This measure does not include uncapitalized purchases of hardware that would have been in the first measure, Purchased Hardware. All IT: Our broadest measure of IT assets is derived from virtually all IT spending by the firm—hardware, purchased and internally developed software, other internal IT services, external IT services (such as consulting), and IT-related training—whether or not the firm capitalizes or expenses such spending.30\n",
              "\n",
              "25\n",
              "\n",
              "Described further in Appendix A.\n",
              "\n",
              "This occurs when our estimate for capitalized IT assets is larger than all of PP&E, implying that ordinary (non-IT) assets are negative. In addition, IT producers face different accounting rules when it comes to capitalizing software. If software is developed internally and is only used by the firm itself, Statement of Position 98-1 from the AICPA applies. If the software will also be sold to other customers, then Statement 86 from the Financial Accounting Standards Board (FASB) applies. Financial firms have high levels of “other” assets that may affect our estimate for the coefficient of F. Government sources such as the Federal Reserve’s Flow of Funds and the BEA’s Integrated Macroeconomic Accounts for the United States separate financial firms from the rest of the economy. A number of papers that examine intangible assets and Tobin’s q (the ratio of market value to total assets) dropped financial firms from their analyses (Berger and Ofek 1995; Hall 2000, 2001; McGahan and Porter 1999, 2003). Such firms are often based on a single commodity input factor, magnifying the effect of such price swings.\n",
              "29 28 27\n",
              "\n",
              "26\n",
              "\n",
              "2.\n",
              "\n",
              "3.\n",
              "\n",
              "We omit leased hardware and software, which is a relatively small component of IT spending.\n",
              "\n",
              "30\n",
              "\n",
              "90\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 1. Variable Means for Sample, 2003-2006 ($Millions)\n",
              "Mean 31,422.1 52.8 156.5 505.9 Standard Deviation 60,251.6 109.1 278.9 970.7 Minimum 295.7 .3 1.2 5.0 Maximum* 354,347.9 622.8 1,517.0 5,411.7\n",
              "\n",
              "Market Value IT Assets Purchased Hardware Capitalized Hardware and Software Purchased Hardware, All Software, Other Internal IT Assets, IT Consulting, ITRelated Training Ordinary Assets Other Assets R&D Assets Brand Assets Employment (000s)\n",
              "\n",
              "5,455.5 12,965.4 2,583.5 331.9 58.4\n",
              "\n",
              "11,746.6 38,239.4 7,574.8 956.0 77.0\n",
              "\n",
              "2.6 53.3 0 0 .4\n",
              "\n",
              "72,539.3 245,509.5 44,004.9 5,891.8 465.0\n",
              "\n",
              "Note: Total of 508 observations. *To avoid disclosure, we list the maximum as the average of the 10 largest observations.\n",
              "\n",
              "Table 2. Average IT Spending per Firm, 2003-2006 ($Millions)\n",
              "Mean 29.6 33.9 41.1 114.1 11.1 229.8 Standard Deviation 57.5 61.1 73.2 198.2 18.5 401.8 Minimum .2 .3 .3 .9 .03 1.9 Maximum* 327.0 326.4 388.5 1,073.9 95.4 2,168.1\n",
              "\n",
              "Hardware Prepackaged Software External IT Services (e.g., business process consulting, integration services) Internal IT Services (e.g., custom software, design, maintenance, administration) IT-Related Training Total IT Spending\n",
              "\n",
              "Note: Total of 508 observations. *To avoid disclosure, we list the maximum as the average of the 10 largest observations.\n",
              "\n",
              "In Figure 2, we illustrate IT spending and assets for the 2003– 2006 sample. We estimate that our first and narrowest measure of IT assets, Purchased Hardware, averaged $52.9 million per firm during the sample period.31 The same firms averaged $156.9 million of Capitalized Hardware and Software, our second and broader measure of IT. Finally, the sample firms averaged $505.9 million of IT assets based on our third and broadest measure, All IT. In Appendix A, we elaborate further on how IT spending is converted into each IT asset measure.\n",
              "\n",
              "Hardware To construct firm-level hardware assets, we begin with total annual hardware spending as reported by the firm. We convert each of these flows into a constant-dollar (2005) measure using the investment price deflator for computers and peripherals from the BEA.32 As is the practice of the BEA, we assume spending occurred halfway through the year.33 We use the BEA rate of depreciation for computers and periph-\n",
              "\n",
              "32 Bureau of Economic Analysis, National Income and Products Accounts Table 5.3.4 “Price Indexes for Private Fixed Investment by Type,” line 11.\n",
              "\n",
              "Consisting of $39.9 million of hardware that is capitalized, and $13.0 million that is not capitalized.\n",
              "\n",
              "31\n",
              "\n",
              "33\n",
              "\n",
              "We make this assumption for all subsequent asset calculations.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "91\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "2003-2006 Average Annual Spending ($mm) $29.6\n",
              "\n",
              "IT Spending\n",
              "64% 18%\n",
              "\n",
              "IT Assets\n",
              "Purchased Hardware, capitalized\n",
              "\n",
              "2003-2006 Average Assets ($mm) $39.9\n",
              "\n",
              "Hardware Spending\n",
              "\n",
              "Purchased Hardware, not capitalized 18% Leased Hardware, not capitalized\n",
              "\n",
              "$13.0\n",
              "\n",
              "$0.0\n",
              "\n",
              "$33.9\n",
              "\n",
              "Prepackaged Software Spending\n",
              "\n",
              "53% 25%\n",
              "\n",
              "Purchased Prepackaged Software, capitalized\n",
              "\n",
              "$21.7\n",
              "\n",
              "Purchased Prepackaged Software, not capitalized 22% Leased Prepackaged Software, not capitalized\n",
              "\n",
              "$10.5\n",
              "\n",
              "$0.0\n",
              "\n",
              "$114.1\n",
              "\n",
              "Internal IT Services Spending\n",
              "\n",
              "34% 16%\n",
              "\n",
              "Custom and Own-Account Software, capitalized\n",
              "\n",
              "$95.0\n",
              "\n",
              "Custom and Own-Account Software, not capitalized 50% Other Internal IT Assets, not capitalized\n",
              "\n",
              "$49.3\n",
              "\n",
              "$126.0\n",
              "\n",
              "$41.1\n",
              "\n",
              "External IT Services Spending\n",
              "\n",
              "100%\n",
              "\n",
              "IT Consulting, not capitalized\n",
              "\n",
              "$96.8\n",
              "\n",
              "$11.1\n",
              "\n",
              "IT-Related Training\n",
              "\n",
              "100%\n",
              "\n",
              "IT-Related Training, not capitalized\n",
              "\n",
              "$53.7\n",
              "\n",
              "Total Capitalized $156.5 $229.8 Total ($mm) Total Uncapitalized $349.4 Total ($mm) $505.9\n",
              "\n",
              "Figure 2. Converting IT Spending into IT Assets\n",
              "\n",
              "92\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "erals, which is approximately 33 percent.34 Each year’s constant-dollar estimate is then converted back into a currentdollar, or replacement cost measure, using the appropriate price deflator for computers and peripherals.35 Our spending data cover the period of 2003–2006, and thus we impute earlier values of hardware spending to generate estimates of hardware assets. We start with the firm’s reported hardware spending in 2003, and apply the industry-level growth rate of investment in computers and peripherals from the BEA to estimate the historical values of firm-level hardware spending.36 Prepackaged Software Similar to hardware, we convert all prepackaged software spending by the firm into a constant-dollar measure by using the BEA depreciation rate of 55% per year as well as the appropriate investment price deflators.37 We use the BEA growth rate of investment in prepackaged software at the industry level to impute earlier firm-level spending estimates prior to 2003.38 Finally, our annual prepackaged software\n",
              "\n",
              "asset estimates are then converted from constant dollars into current dollars by using the appropriate price deflators.39 Custom and Own-Account Software In contrast to prepackaged software that is ready to use offthe-shelf, custom software is “tailored to the specifications of a business enterprise or government unit” (BEA 2000, p. 3). Own-account software is “in-house expenditures for new or significantly-enhanced software created by business enterprises or government units for their own use” (BEA 2000, p. 4). As the SeeIT survey does not ask for further breakdown of spending beyond “Internal IT Services,” and the BEA price deflators and depreciation rates of 33% per year are the same for both custom and own-account software, we aggregate these two types of software assets in our estimates.40 We use the BEA growth rate of investment in custom software at the industry level to impute earlier firm-level spending estimates prior to 2003. In its calculations of IT assets, the BEA estimates that 50% of programmer and system analyst time is spent on new software development (BEA 2000, p. 5). Thus, we allocate 50% of internal IT services spending toward Custom and OwnAccount Software assets, and the other 50% toward Other Internal IT assets. While a 50% split is the prevailing practice of the BEA, the agency acknowledges that this ratio is an approximation (BEA 2000, p. 5). Thus, as a robustness check, we derive results using other ratios in order to examine the sensitivity of our analyses to this assumption. Other Internal IT Assets The 50% of internal IT services spending that is not allocated toward Custom and Own-Account Software assets is allocated toward Other Internal IT assets. This includes spending on activities such as design, maintenance, or administration of IT. There are no published price deflators or depreciation rates available from the BEA for this type of spending. Thus, as a price deflator, we use the BEA gross output deflator for\n",
              "\n",
              "34 The BEA does not publish a depreciation rate for computers and peripherals. Rather, it publishes depreciation schedules for the various subcomponents of hardware, such as mainframes, printers, terminals and displays, tape drives, and storage devices (see BEA [2003], M-30, available at http://www.bea.gov/national/FA2004/index.asp). Thus, we impute an aggregated depreciation rate for computers and peripherals based on combining BEA Fixed Assets Tables 2.1, 2.4, and 2.7. While the rate derived varies slightly from year to year, it is very close to 33% (ranging from 33% to 35% during the sample period by our estimates). Our calculations are available upon request. 35\n",
              "\n",
              "We compute an end-of-year price deflator based on BEA Fixed Assets Tables 2.1 and 2.2, as end-of-year price indexes for computers and peripherals are not published and they vary slightly from the published mid-year investment price deflators (as the BEA assumes investment is made in the middle of the year).\n",
              "\n",
              "The BEA does not publish a chain-type quantity index for computers and peripherals. Thus, we aggregated the various subcomponents of hardware from the nonresidential detailed estimates of the BEA, available at http://www.bea.gov/national/FA2004/Details/Index.html. We then create a Fisher chain-type quantity index for computers and peripherals extending back to 1998. Since our asset estimates for IT rely more on imputation for 2003 and 2004 than for 2005 and 2006, as a robustness check we later run estimates based only on 2005 and 2006 asset values.\n",
              "37\n",
              "\n",
              "36\n",
              "\n",
              "BEA depreciation rates for various types of equipment and software are available at http://www.bea.gov/national/FA2004/Tablecandtext.pdf. The investment price deflator for prepackaged software is available from the nonresidential detailed estimates of investment (chain-type quantity index), available at http://www.bea.gov/national/FA2004/Details/Index.html.\n",
              "\n",
              "For all asset calculations that follow, we (1) use appropriate price deflators to convert nominal, current-dollar flows into constant-dollar flows; (2) use the appropriate depreciation rates to create estimates of assets in constant dollars; and (3) convert each year’s assets in constant dollars into currentdollar assets using the appropriate price deflators.\n",
              "40 See depreciation rates for each at http://www.bea.gov/national/FA2004/ Tablecandtext.pdf. The investment price deflators for custom and ownaccount software are from the nonresidential detailed estimates of investment (chain-type quantity index), available at http://www.bea.gov/national/ FA2004/Details/Index.html.\n",
              "\n",
              "39\n",
              "\n",
              "We use the chain-type quantity index of investment for prepackaged software, available at http://www.bea.gov/national/FA2004/Details/Index.html.\n",
              "\n",
              "38\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "93\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "NAICS 541512, computer systems design services. To impute historical spending at the firm level before 2003, we use the industry-level growth rate of investment in custom software, applied to the 2003 firm-level value of spending on Other Internal IT assets. We use the depreciation rate for firm-specific resources found in Corrado et al. (2005, 2009), which amounts to about 37.5% per firm in our sample.41 IT Consulting Spending on external IT services, which includes business process consulting and integration services, is converted into an IT Consulting asset. We use an approximate 37.5% depreciation rate and the gross output deflator for NAICS 541512 (the same as for Other Internal IT assets) to convert currentdollar annual spending as reported by the firm into constantdollar measures. To impute firm-level spending in years before 2003, we use the industry-level growth rate of investment in custom software, applied to the 2003 firm-level value of spending on IT Consulting. As part of our extended analyses, we also report results with IT Consulting assets depreciated by a rate of either 15% or 60%. IT-Related Training We use the annual spending reported by the firm for ITrelated training and convert it to an IT-Related Training asset. We use an approximate 37.5% depreciation rate and the gross output deflator for NAICS 541512 (the same as for Other Internal IT assets and IT Consulting) to convert current-dollar spending into annual asset measures. To impute firm-level spending in years before 2003, we use the industry-level growth rate of investment in custom software, applied to the 2003 firm-level value of spending on IT-related training. As part of our extended analyses, we also report results with ITRelated Training assets depreciated by a rate of either 15% or 60%. IT Capabilities (ITC) ITC is based on the work of Aral and Weill (2007). It is the sum of five components, which include management capabilities, human resource capabilities, IT usage in internal and\n",
              "41 The formula for depreciation of firm-specific resources in Corrado et al. (2005, 2009) is the average of the depreciation rates of R&D and advertising. Since we use a 15% depreciation rate for R&D for most firms and a 60% depreciation rate for advertising, this averages to a depreciation rate of about 37.5%. As an alternative to this baseline assumption, we also consider depreciation rates for Other Internal IT assets ranging from 15% to 60% in our analyses.\n",
              "\n",
              "external communications, and Internet capabilities.42 We list the components and summary statistics of ITC in Table 3, and the distribution of ITC (from ITC_A through ITC_F) in Table 4. Each of the five components of ITC is constructed from the sum of several questions, which are measured on a 1 to 5 scale within the 2005 and 2006 SeeIT surveys. To reduce measurement error, we average the answers from both years and give each firm a single value for each component in the sample period. We standardize each of the five component sums to mean 0, variance 1 variables. We then add those five standardized components, and restandardize the resultant sum to create the final mean 0, variance 1 variable that represents ITC. Research and Development (R&D) Assets We begin with annual R&D spending as reported by the firm (Compustat mnemonic XRD) as far back as 1983 (or the first year reported by the firm), and apply BEA price deflators and depreciation rates to estimate R&D assets.43 The R&D depreciation rate for firms in transportation and warehousing industries (NAICS 48-49) is 18%. In the chemicals industry (NAICS 325), the R&D depreciation rate is 11%, and for all other firms it is 15%.44 Approximately half of the firms in our sample report nonzero values of R&D, and as firms are required to report R&D spending if it exceeds 1% of sales (Zhao 2002, p. 156), we assume a value of zero R&D spending for firms that do not report R&D. We create a dummy variable equal to 1 for any firms with no R&D assets in our estimating equations. For firms that report R&D generally but have some values missing, we impute the missing values by taking the ratio of R&D to sales for the trailing or leading five years and applying it to the value of sales in the year(s) of missing R&D. In accordance with accounting standards, R&D spending is entirely expensed by firms.\n",
              "\n",
              "We do not have enough response data to create the sixth component from Aral and Weill, which measured the degree of digitization in purchases and sales. The formulation of ITC is based on case studies of two manufacturing firms, two financial services firms, and 7-Eleven Japan. See Aral and Weill (pp. 767-770) for further details.\n",
              "43\n",
              "\n",
              "42\n",
              "\n",
              "Although we use the perpetual inventory method and not a strict service life, after 20 years of 15% depreciation per year, the value of each dollar invested in R&D would have deteriorated by more than 96%.\n",
              "\n",
              "The BEA rate for R&D depreciation in the Computers and Electronics industry is 16.5%, but we do not consider IT producers in our sample.\n",
              "\n",
              "44\n",
              "\n",
              "94\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 3. Components of IT Capabilities (ITC)\n",
              "ITC is the standardized sum (mean 0, standard deviation 1) of the five factors below. Please rate whether the following factors at your company facilitate or inhibit the ability to make new information technology investments on a scale from 1 to 5, with 1 being “inhibits significantly,” 3 being “no effect,” and 5 being “facilitates significantly.” HR Capabilities Average Std. Dev. Technical skills of existing IT staff 4.56 .50 Business skills of existing IT staff 4.52 .50 Ability to hire competent IT staff 2.53 1.18 Skills of end-users 3.54 .57 Management Capability Business unit involvement in IT projects 2.60 1.14 Senior management support 2.55 1.14 Please rate how important the following methods are for (internal communications, communications with suppliers) in your company on a scale from 1 to 5, with 1 being “not at all important,” 3 being “moderately important,” and 5 being “extremely important.” Internal Communications Email 4.53 .50 Mobile electronic mail (e.g., BlackBerry) 4.49 .50 Instant messaging 3.62 .60 Company Intranet 3.54 .55 Wireless (including phone and pager) 4.44 .50 Communications with Suppliers Email 4.54 .50 Mobile electronic mail (e.g., BlackBerry) 4.56 .50 Instant messaging 3.50 .50 Internet 3.59 .49 Wireless (including phone and pager) 4.44 .50 Please identify to what extent your company uses Internet technology to perform each of the tasks on a scale from 1 (no use of the Internet) to 5 (fully automated via the Internet). Internet Capability Sales force management 4.55 .50 Employee performance measurement 3.08 .88 Training 3.09 .83 Post-sales customer support 3.01 .85\n",
              "\n",
              "Table 4. Distribution of Capabilities (ITC)\n",
              "Category ITC_A ITC_B ITC_C ITC_D ITC_F Total Meaning 95th percentile and above 85th-95th percentile 15th-85th percentile 5th-15th percentile 5th percentile and below Number of Observations 24 52 356 52 24 508\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "95\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Brand Assets To construct brand assets, we begin with advertising spending as reported by the firm (Compustat mnemonic XAD). Approximately 50% of the firms in our sample report advertising expenditures. For firms that do not report advertising, we use a database maintained by Kantar Media called Ad$pender, which estimates advertising spending for approximately 95% of the firms covered by Compustat. For any remaining firms without Compustat or Ad$pender advertising spending data, we assume a value of zero for advertising. To convert flows of advertising spending into brand assets, we start with annual advertising spending as far back as 1995 (or the first year reported by the firm), and use a 60% annual depreciation rate (following Corrado et al. 2005, 2009) with a price deflator based on the Producer Price Index (PPI) for advertising agencies. In accordance with accounting standards, all advertising expenditures are entirely expensed by firms. We also create a dummy variable equal to 1 for any firms with no brand assets in our estimating equations. Ordinary Assets Ordinary assets (i.e., physical, non-IT assets) are equal to the Compustat measure of net property, plant, and equipment (Compustat mnemonic PPENT), minus our estimate for capitalized IT.45 Net PP&E as reported by the firm is in historical-cost dollars rather than current-cost dollars. While current-cost estimates would be more desirable in our market value estimations, in the absence of disaggregated investment data by type (such as equipment and structures), we leave PPENT as is.46 Other Assets We define other assets as total balance sheet assets (Compustat mnemonic AT) minus net PP&E. Other assets include inventories, financial assets such as accounts\n",
              "We use the BEA definition of physical assets, which is equipment plus structures. The Bureau of Labor Statistics has a different definition of assets, which includes inventories and undeveloped land. To be fully consistent across the source data for depreciation rates, price deflators, and investment growth, we use the BEA as the definitive source for all data in this paper where possible. Further detail for the BLS definition of assets can be found in BLS (1983, Appendix C).\n",
              "46 As a robustness check, we adjust net PP&E as reported by the firm to a current-cost estimate by assuming the firm invests in equipment and structures at the industry rate. The industry-level estimates of current- and historical-cost assets are found in BEA Fixed Assets Tables 3.1ES and 3.3ES. Our alternative calculations are available upon request. 45\n",
              "\n",
              "receivable, cash, other liquid assets, and any balance sheet intangibles such as goodwill. Sector Controls We create sector dummy variables, and in order to have at least 20 observations in each sector, we aggregate similar NAICS sectors. We list our sector classifications in Table 5.\n",
              "\n",
              "Results\n",
              "Summary\n",
              "Using our 2003–2006 data set, we first replicate the BHY 2002 finding that $1 of computer hardware is correlated with about $10 of market value. We then account for the missing $9 of value by broadening the definition of IT assets to capture virtually all IT spending, whether the firm treats this spending as capitalized or expensed. One dollar of the broadest measure of IT assets is correlated with close to its theoretical value of $1 of market value. However, our results suggest that this value is not spread evenly throughout the sample. Rather, it is positively correlated with firms that possess the highest IT capabilities, such as the ITC_A firms. The estimated difference in market value between ITC_A and ITC_F firms is striking: holding all physical as well as intangible assets of the firm constant, we estimate an average difference in market value of 52% between the ITC_A and the ITC_F firms. This finding is robust to alternative specifications and assumptions, and our point estimates of the difference in market value range from 45% to 76%. The estimated premium of ITC_A firms is consistent with the observations that IT investments are riskier than ordinary investments (Dewan et al. 2007), or that those firms with the highest IT capabilities simply execute better than their peers, perhaps reflecting better management. Thus, the evidence suggests that the market generously rewards the firms with the highest IT capabilities. In addition, we estimate that $1 of R&D and $1 of brand assets are each correlated with slightly more than $1 of market value, further suggesting the importance of intangible assets in differentiating firms. While the coefficients for R&D and brand are significantly greater than 1 in virtually all specifications, the coefficient for brand is measured with larger standard errors. More research is necessary to quantify the strength of brand, determine how\n",
              "\n",
              "96\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 5. Sector Dummy Variables\n",
              "Sector Dummy Variables 1. Agriculture, Utilities, Construction 2. Nondurable Process Manufacturing: Paper Products; Chemical Products 3. Other Nondurable Manufacturing: Food, Beverage and Tobacco Products; Textile Mills and Textile Product Mills; Apparel and Leather and Allied Products; Printing and Related Support Activities; Plastics and Rubber Products 4. Durable Manufacturing, High-Tech: Electrical Equipment, Appliances and Components; Motor Vehicles, Bodies and Trailers, and Parts; and Other Transportation Equipment 5. Durable Manufacturing, Non High-Tech: Wood Products; Nonmetallic Mineral Products; Primary Metals; Fabricated Metal Products; Machinery; Furniture and Related Products; Miscellaneous Manufacturing 6. Wholesale and Retail Trade 7. Transportation and Warehousing 8. Information 9. Real Estate and Rental and Leasing, Professional, Scientific and Legal Services; Management of Companies and Enterprises; Administrative and Support services, Waste Management and Remediation Services 10. Educational Services; Health Care and Social Assistance 11. Arts, Entertainment and Recreation; Accommodation and Food Services; Other Services except Government NAICS Codes 11, 22, 23 322, 325 311-316, 323, 326 335, 336 Observations 40 104 28\n",
              "\n",
              "40\n",
              "\n",
              "321, 327, 331, 332, 333, 337, 339 44-45 48-49 51 53, 54, 55, 56\n",
              "\n",
              "80\n",
              "\n",
              "68 32 24 32\n",
              "\n",
              "61, 62 71, 72, 81 Total\n",
              "\n",
              "24 36 508\n",
              "\n",
              "it varies between firms, and identify the correct industryspecific depreciation rates.\n",
              "\n",
              "Sample Validation\n",
              "Because we use the replacement cost of computers (the cost to replace a firm’s stock of computers in the dollars of that year), and not the historical cost of computers (the cost of computers in the year in which they were purchased), in theory the market value of $1 of replacement-cost computers should not differ from year to year. This approach allows us to compare our results directly to BHY 2002. From 1987 (the first year of the BHY 2002 data set) through 2006 (the last year of our data set), the replacement cost of hardware held by businesses in the United States grew 215%, from $76 billion to more than $161 billion (Figure 3). This barely outpaced the Consumer Price Index (CPI), which grew 175% during the same period. However, the story is very different when we take into account the quality changes to computing over this time: U.S. businesses held 32 times as much computing\n",
              "\n",
              "power at the end of 2006 as they did in 1987.47 Even though 20 years of technological change have produced computers of remarkably different quality, the equilibrium market value of $1 of new 1987 computers in 1987 should not be different than the market value of $1 of new 2006 computers in 2006. We find that the ratio of intangible IT spending to hardware spending in our sample is similar to that of the economy as a whole during the sample period. In our sample, we estimate $6.76 in intangible IT spending for every $1 of hardware spending from 2003 to 2006 (Table 2).48 Within the U.S. economy from 2003 to 2006, for every $1 of hardware investment, businesses spent $6.13 in software, internal IT services, and external IT services (including training). Intangible in-\n",
              "\n",
              "47 Source: Bureau of Economic Analysis, Fixed Assets Table 2.2. “ChainType Quantity Indexes for Net Stock of Private Fixed Assets, Equipment and Software, and Structures by Type,” line 5. The value of the quantity index was 3.507 in 1987, and 113.598 in 2006 (with 2005 as the base year, equal to 100).\n",
              "\n",
              "This is calculated as all IT spending ($229.8 million), minus hardware ($29.6 million), divided by hardware ($29.6 million), which is $6.76.\n",
              "\n",
              "48\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "97\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Figure 3. Computer Hardware Owned by Businesses in the United States, 1987–2006 (Source: BEA Fixed Assets Table 2.1, “Current-Cost Net Stock of Private Fixed Assets, Equipment and Software, and Structures by Type,” line 5)\n",
              "\n",
              "Figure 4. IT Spending by Businesses in the United States, 1990–2006 (Source: BEA NIPA Table 5.3.5, “Private Fixed Investment by Type,” and authors’ calculations from the Census Bureau’s Service Annual Survey)\n",
              "\n",
              "98\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "vestments in IT have grown significantly in the United States over the period 1990–2006, from $95 billion in 1990 to more than $450 billion in 2006 (shown in Figure 4).49\n",
              "\n",
              "Market Value as a Function of IT Assets\n",
              "In Table 6, we illustrate market value estimation of equation (2) for our three different measures of IT, moving from the narrowest measure of IT assets (purchased hardware) to the broadest definition (all IT). Column 1 is an attempt to replicate the results of BHY 2002. One dollar of computer assets, defined as hardware only, is correlated with more than $10 of market value, significantly above the theoretical value of $1. In contrast, $1 of “ordinary” and “other” assets is correlated with close to $1 of market value.50 BHY 2002 did not maintain that $1 of hardware itself is worth more than $10, and neither do we. Our hypothesis is that hardware is correlated with unmeasured IT intangibles. Adding R&D and brand assets reduces the coefficient for IT in column 2, which is what one would expect if there is a positive synergy between IT and R&D (Bardhan et al. 2013) or IT and product variety (Gao and Hitt 2012).51\n",
              "Source: Hardware is from BEA NIPA Table 5.3.5, “Private Fixed Investment by Type,” line 11. Software is from the same BEA table, line 17, and consists of the sum of prepackaged, custom, and own-account investment. IT services are the sum of internal IT services and external IT services. For internal IT services, we use an amount equal to investment in ownaccount software because the BEA allocates 50% of all internal IT spending for own-account software. Thus, we include the other 50% of all internal IT spending here. Spending on external IT services is from the Service Annual Survey. From 1998 to 2006, we use revenue in NAICS industry 5415 (computer systems design and related services) minus 541511 (custom computer programming services), as NAICS 541511 is allocated for custom software and thus is already counted. For 1990–1997, we use the revenue from SIC industries 7373, 7376, and 7379 as this most closely matches the definitions of NAICS industry 5415 excluding 541511. However, this is not an exact match. Thus, we further adjust the SIC estimates to match the NAICS definitions by multiplying the SIC estimates from 1990 to 1997 by the 1998 ratio of NAICS to SIC revenue in these industries (1998 being the only year in which data under both NAICS and SIC is available). We begin with 1990, the first year with data available for SIC codes 7373, 7376, and 7379 in the Service Annual Survey. Our results from OLS estimation (not shown here) are also similar to BHY 2002, and are qualitatively similar to our GLS results. One dollar of computer assets is correlated with more than $10 of market value, and $1 of “ordinary” and “other” assets are each correlated with close to $1 of market value. When we broaden the definition of IT, we estimate coefficients closer to 1. One dollar of R&D and advertising are each associated with significantly more than $1 of market value, and we estimate a 68% difference in market value between ITC_A and ITC_F firms. BHY 2002 used the ratios of current-year R&D to sales and advertising to sales as controls. Using these ratios instead of R&D and advertising assets in column 2 results in a higher IT hardware coefficient closer to the result from column 1.\n",
              "51 50 49\n",
              "\n",
              "Although $1 of hardware is associated with more than $10 of market value, $1 of IT defined more broadly as all capitalized IT (both capitalized hardware and software) is associated with approximately $7 of market value (Table 6, column 3). When we include measures of R&D and brand assets, $1 of capitalized IT is associated with about $5 of market value (column 4). We then relax the assumption that the firm capitalizes hardware and software at the industry average.52 Instead, we use a hypothetical upper bound on capitalized IT by assuming the firm capitalizes all purchases and payroll for hardware and software.53 Even in such a case, $1 of this definition of IT is correlated with more than $3 of market value (column 5), suggesting that there are still complementary intangible IT assets that are unaccounted for. We then use our broadest definition of IT, which includes IT services and training that were not included in narrower measures. In these specifications, $1 of IT is correlated with close to $1 of market value, its theoretical value (Table 6, columns 6 and 7).54 Our interpretation is that by progressively broadening the definition of IT to include previously unmeasured intangibles, we are better capturing IT assets. We examine the robustness of this result by using different assumptions and measures of the dependent and independent variables, as seen in Table 7.\n",
              "\n",
              "Robustness of Market Value Results\n",
              "Alternative Dependent Variables We begin by using two different definitions of market value. In the first case (Table 7, column 2), we adjust the face value of long-term debt (Compustat mnemonic DLTT) to reflect the additional premium (or discount) of the market value of bonds to the face value of bonds. For data to adjust long-term debt, we start with the Mergent Fixed Income Securities Database, with data on approximately 180,000 corporate bond issues. We extract the unique CUSIP identifier and issue information for each bond. We match this information to the Trade Reporting and Compliance Engine (TRACE) database, a product of the Financial Industry Regulatory Authority (FINRA). The TRACE database represents more than 99% of U.S. cor52\n",
              "\n",
              "Based on the Annual Capital Expenditure Survey (ACES) data. See appendix A for more detail. We do not have a specification for capitalized IT if the firm capitalizes 0% of IT spending, as that variable would be zero. Recall our conceptual framework: each dollar’s worth of an asset—if properly measured—should be correlated with about $1 of market value.\n",
              "54 53\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "99\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 6. Market Value as a Function of the Assets of the Firm, 2003–2006\n",
              "(1) Information Technology (IT) – Hardware only Purchased Hardware (BHY 2002 measure) Information Technology (IT) – Capitalized only Capitalized Hardware, Capitalized Software Information Technology (IT) – All Hardware, All Software, Other Internal IT Assets, IT Consulting, IT-Related Training Ordinary Assets (K) Other Assets (F) R&D (R) Brand (B) Number of Observations Assumptions 508\n",
              "Firm capitalized all purchased hardware only\n",
              "\n",
              "(2) 8.75 (3.89)\n",
              "\n",
              "(3)\n",
              "\n",
              "(4)\n",
              "\n",
              "(5)\n",
              "\n",
              "(6)\n",
              "\n",
              "(7)\n",
              "\n",
              "11.31 (3.51)\n",
              "\n",
              "7.19 (1.39)\n",
              "\n",
              "5.68 (1.52)\n",
              "\n",
              "3.61 (1.01) 1.53 (.41) 1.15 (.46) 1.48 (.14) 1.80 (.18) 1.63 (.23) 2.49 (1.44) 508\n",
              "Virtually all IT spending included in IT assets (except leased hardware & software)\n",
              "\n",
              "1.20 (.08) 1.12 (.09)\n",
              "\n",
              "1.48 (.14) 1.80 (.18) 1.65 (.23) 2.57 (1.45) 508\n",
              "Firm capitalized all purchased hardware only\n",
              "\n",
              "1.22 (.08) 1.09 (.10)\n",
              "\n",
              "1.48 (.14) 1.81 (.17) 1.65 (.23) 2.48 (1.38)\n",
              "\n",
              "1.48 (.14) 1.81 (.18) 1.64 (.23) 2.53 (1.41) 508\n",
              "Firm capitalized 100% of purchases and payroll for hardware & software\n",
              "\n",
              "1.20 (.08) 1.12 (.09)\n",
              "\n",
              "508\n",
              "Firm capitalized industry average of purchases and payroll for hardware & software\n",
              "\n",
              "508\n",
              "Firm capitalized industry average of purchases and payroll for hardware & software\n",
              "\n",
              "508\n",
              "Virtually all IT spending included in IT assets (except leased hardware & software)\n",
              "\n",
              "Note: All regressions are GLS, with correction for heteroskedasticity and serial correlation. Controls for sector, year, no R&D, and no advertising are included (where each control is multiplied by total balance sheet assets).\n",
              "\n",
              "Table 7. Market Value as a Function of the Assets of the Firm with Additional Specifications, 2003–2006\n",
              "(1)\n",
              "Information Technology (IT) – All Hardware, All Software, Other Internal IT Assets, IT Consulting, IT-Related Training Ordinary Assets (K) Other Assets (F) R&D (R) Brand (B) Lagged MV (HQ in High-Tech Spillover Area = 1) * Total Assets Number of Observations Assumptions 508\n",
              "Baseline result (From Table 6, column 7)\n",
              "\n",
              "(2)\n",
              ".86 (.44) 1.35 (.19) 1.81 (.20) 1.94 (.22) 3.39 (1.44)\n",
              "\n",
              "(3)\n",
              "1.12 (.49) 1.25 (.20) 1.76 (.24) 1.05 (.27) 1.20 (1.40)\n",
              "\n",
              "(4)\n",
              "1.15 (.47) .72 (.11) 1.39 (.19) 1.68 (.24) 2.62 (1.51)\n",
              "\n",
              "(5)\n",
              ".93 (.47) 1.49 (.14) 1.81 (.18) 1.63 (.23) 2.49 (1.44)\n",
              "\n",
              "(6)\n",
              "1.37 (.44) 1.47 (.14) 1.80 (.18) 1.64 (.23) 2.49 (1.44)\n",
              "\n",
              "(7)\n",
              ".73 (.28) 1.49 (.14) 1.79 (.18) 1.61 (.24) 2.47 (1.47)\n",
              "\n",
              "(8)\n",
              "1.68 (.62) 1.48 (.14) 1.81 (.18) 1.66 (.23) 2.52 (1.42)\n",
              "\n",
              "(9)\n",
              ".75 (.37) 1.12 (.14) 1.85 (.17) 1.02 (.22) .33 (1.13) .28 (.04)\n",
              "\n",
              "(10)\n",
              "1.15 (.44) 1.45 (.13) 1.73 (.17) 1.47 (.24) 2.24 (1.39)\n",
              "\n",
              "1.15 (.46) 1.48 (.14) 1.80 (.18) 1.63 (.23) 2.49 (1.44)\n",
              "\n",
              ".25 (.09) 508\n",
              "Market value adjusted for market value of bonds\n",
              "\n",
              "508\n",
              "Market value = equity + debt\n",
              "\n",
              "508\n",
              "\n",
              "508\n",
              "\n",
              "508\n",
              "\n",
              "508\n",
              "\n",
              "508\n",
              "\n",
              "508\n",
              "Lagged market value included\n",
              "\n",
              "508\n",
              "Headquarters located in high-tech metropolitan area\n",
              "\n",
              "Ordinary 0% of 100% of assets internal IT internal IT estimated at services services current-cost spending for spending for instead of custom & custom & historical- own-account own-account cost software software\n",
              "\n",
              "IT IT consulting, consulting, IT training, IT training, and other and other internal IT internal IT assets at assets at 15% 60% depreciation depreciation\n",
              "\n",
              "Note: All regressions are GLS, with correction for heteroskedasticity and serial correlation. Controls for sector, year, no R&D, and no advertising are included (where each control is multiplied by total balance sheet assets).\n",
              "\n",
              "100\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "porate bond market activity for 30,000 issuers. From January 2003 through December 2006, the database contains more than 18 million trades. For each bond, we keep the last recorded price for the end of the fiscal year. We aggregate the face and market values of all outstanding bonds for each company, and match them to our sample. Finally, we take the ratio of the aggregate market value of a company’s outstanding bonds to their aggregate face value, and multiply it by the face value of the company’s long-term debt. Our main definition of market value—equity plus liabilities— is consistent with the sum of all financial claims to the firm (Hall 2000, 2001). However, in column 3, we use an alternative definition of the market value of the firm consistent with the empirical corporate finance literature: equity plus long-term and short-term debt.55 We note that with either alternative market value definition, our results do not change substantially. In column 2, the coefficient on IT drops somewhat, while the coefficients of R&D and brand in column 3 fall closer to 1. Assumptions for Ordinary Assets We then use current-cost estimates of ordinary assets instead of historical-cost (or book-value) estimates in column 4. We use BEA data on current-cost assets by industry56 and divide this by the historical-cost estimates of assets by industry,57 and then multiply this ratio by the historical-cost estimate of net PP&E for the firm (Compustat mnemonic PPENT). In this estimation, while other coefficients remain the same, the coefficient for ordinary assets falls below 1. To the extent that firms invest in different physical assets that differ from the industry average, this leads to measurement error, biasing the coefficient toward zero (Griliches and Hausman 1986).\n",
              "\n",
              "Assumptions about Internal IT Services Spending Our baseline assumption for internal IT services spending by the firm was that 50% was for own-account and custom software, and the remaining 50% was for other internal IT assets such as design, maintenance, and administration. In columns 5 and 6, we relax this assumption: In column 5, we assume that 0% of internal IT services spending is for own-account and custom software, while all of it is on other internal IT assets. In column 6, we assume the opposite is true, wherein 100% of internal IT services spending is toward own-account and custom software, and 0% is on other internal IT assets. It is worth noting that these upper and lower bound assumptions change the coefficient on IT assets only somewhat. Depreciation Rates for IT Consulting, Training, and Other Internal IT Assets Instead of our baseline depreciation rate of 37.5% for IT consulting, IT training, and other internal IT assets, we use 15% as an alternative assumption in column 7, and 60% in column 8.58 The coefficient on IT assets is 0.73 with a 15% depreciation rate, and 1.68 with a 60% depreciation rate. While the coefficient on IT is closest to 1 with our baseline estimate of 37.5%, neither estimates from column 7 nor column 8 are significantly different from 1. Lagged Market Value We include market value lagged by one year to capture timevarying unobserved firm effects in column 9. The coefficient on lagged market value is positive and significant, indicating that unobserved qualities of the firm, such as management quality, are serially correlated over time. The coefficient on IT is reduced to 0.75, although it is not significantly different from 1.59\n",
              "\n",
              "Thus, the mnemonics are PSTK + (PRCC_F * CSHO) + DLTT + DLCC. See Berger and Ofek (1995), Lang and Stulz (1994), and Villalonga (2004a, 2004b).\n",
              "56 See BEA Fixed Assets Table 3.1ES. An ideal calculation would start with a firm-level series of investment in each of equipment and structures, apply the appropriate deflators and depreciation rates to each series, and then add them together. However, firms are not required to record disaggregated investment by type, and thus we assume that the firm invests in a physical asset mix at the same rate as its industry. 57\n",
              "\n",
              "55\n",
              "\n",
              "We do not vary the depreciation rates for hardware, prepackaged software, or own-account and custom software in our additional specifications because these rates are published by the BEA, the most authoritative source available for macroeconomic data. Interestingly, the coefficient on brand is reduced to 0.33 and is not significantly different from zero, suggesting that advertising spending has weak explanatory power once previous market value is controlled for. The effects of powerful, long-lasting, and valuable brands are correlated with firm value but are not fully captured by advertising spending measures alone. A source of future work lies in capturing brand strength more directly.\n",
              "59\n",
              "\n",
              "58\n",
              "\n",
              "See BEA Fixed Assets Table 3.3ES.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "101\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Accounting for Potential Spillovers It has been well demonstrated that technological change leads to positive spillovers that extend beyond firm boundaries (Cheng and Nault 2007; David 1990; Tambe and Hitt 2014). If firms with high levels of IT assets were predominately located in areas of abundant high-tech spillovers, the IT variable could be, in effect, taking credit for such spillovers. To test whether this might be the case, we create a dummy variable equal to 1 if the firm is headquartered in an area that is well known for high-tech spillovers.60 Approximately 20% of our sample (96 observations) is headquartered in such an area. Column 10 is a specification that includes this dummy variable multiplied by total balance sheet assets, to scale for firm size and to achieve consistency with other dummy variables. While the coefficient on this dummy variable is positive and significant, the remaining coefficients are almost exactly the same as our baseline estimate in column 1.\n",
              "\n",
              "Our point estimates imply that an ITC_A firm with the sample average amount of IT assets ($505.9 million) is correlated with $7.4 billion more in market value than an ITC_C firm,62 while an ITC_F firm with the sample average amount of IT assets is correlated with $2.6 billion less in market value than an ITC_C firm.63 This makes the difference between an average ITC_A and ITC_F firm equal to about $10 billion in market value, significant at the 1% level and very large in a practical sense—amounting to about one-third of the average firm’s market value in the sample.\n",
              "\n",
              "Robustness of Market Value Results with ITC\n",
              "In Table 9, we illustrate our baseline result, along with a set of results based on alternative assumptions, from estimating equation (3). In Table 10, we then compare the percentage differences in market value between ITC_A, ITC_B, ITC_D, and ITC_F firms with ITC_C firms based on the findings from Table 9. Note that in the baseline result in column 1, the estimated difference in market value between ITC_A firms and ITC_F firms is 53%. Our market value results with ITC are robust to a variety of different conditions, including different definitions of market value and ordinary assets, different assumptions about how internal IT services are converted into IT intangible assets, and usage of alternative depreciation rates. Furthermore, the results are robust to specifications that include lagged market value and high-tech spillovers.64 The effects of ITC on market value change monotonically within each specification, with differences in value between ITC_A and ITC_F firms ranging from 45% to 76%. The contribution of IT to market value appears to be concentrated in a relatively small number of firms with very high IT capabilities, and firms with the lowest IT capabilities are correlated with large discounts in market value compared to their more IT-enabled competitors. As a result, firms with average or below average IT capabilities are not deriving maximum value from their IT assets.\n",
              "\n",
              "Market Value Results with IT Capabilities (ITC)\n",
              "Although $1 of our broadest measure of IT is correlated with close to $1 of market value on average, our results suggest that high ITC firms account for a disproportionate share of this value. In Table 8, column 1, we include the ITC_A through ITC_F dummy variables multiplied by firm employment.61 Our point estimate is that ITC_A firms are each worth an extra $119,810 more per employee than an ITC_C firm. Note that these interaction effects decrease monotonically from ITC_A firms through ITC_F firms (although we do not estimate a statistically significant difference in value between ITC_C and ITC_D firms). This is consistent with a doseresponse relationship and increases our confidence in our interpretation. Using the specification in column 2, where ITC_A through ITC_F are multiplied instead by IT assets, we also find striking differences in value. Every $1 of IT assets is worth $14.63 more in an ITC_A firm than an ITC_F firm. Again, the effects decrease monotonically for each ITC group.\n",
              "We use the 10 cities listed in Wired Magazine’s “10 Top Tech Towns”: Austin, Boston, Los Angeles, New York City, Orlando, Pittsburgh, RaleighDurham, the San Francisco Bay Area, Seattle, and Washington, DC. The list is available at http://www.wired.com/wired/archive/15.01/geekcities.html. We use the greater metropolitan area for each city. This is defined by the Office of Management and Budget (OMB) as the metropolitan statistical area (MSA), and is also used by the U.S. Census Bureau. For the San Francisco Bay area and Raleigh-Durham, we use a slightly broader definition, called the combined statistical area (CSA), which aggregates nearby MSAs together based on commuting patterns. We scale variables ITC_A through ITC_F by employment to account for firm size. As these variables are based on workplace practices, we scale them by total employees rather than total assets.\n",
              "61 60\n",
              "\n",
              "Measurement Error and its Impact on the Results\n",
              "Although it is necessary to consider measurement error given the inherent difficulty of quantifying intangible assets, we do not see it as a source of serious concern that would change the\n",
              "62\n",
              "\n",
              "14.63 * 505.9 = $7.4 billion. (-5.21 * 505.9) = -$2.6 billion.\n",
              "\n",
              "63\n",
              "\n",
              "Note that average IT asset levels change from columns 5 through 8 because of the alternative assumptions used to construct them.\n",
              "\n",
              "64\n",
              "\n",
              "102\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 8. Market Value as a Function of the Assets of the Firm and ITC\n",
              "(1) Information Technology (IT) – All Hardware, All Software, Other Internal Assets, IT Consulting, ITRelated Training Ordinary Assets (K) Other Assets (F) R&D (R) Brand (B) ITC_A * IT ITC_B * IT ITC_D * IT ITC_F * IT ITC_A * Employment ITC_B * Employment ITC_D * Employment ITC_F * Employment Number of Observations 119.81 (57.26) 78.11 (15.10) -4.66 (22.44) -171.89 (49.62) 508 508 .67 (.36) 1.58 (.13) 1.94 (.17) 1.56 (.22) 3.24 (1.26) (2) .71 (.36) 1.51 (.13) 1.91 (.16) 1.58 (.23) 3.58 (1.34) 14.63 (5.09) 9.77 (1.94) -.75 (1.67) -5.21 (3.93)\n",
              "\n",
              "Note: All regressions are GLS, with correction for heteroskedasticity and serial correlation. Controls for sector, year, no R&D, and no advertising are included (where each control is multiplied by total balance sheet assets). Employment levels are reported in thousands.\n",
              "\n",
              "interpretation of our results. First, the simplest interpretations of measurement error would likely bias the coefficient estimates downward (Griliches and Hausman 1986), so in the case of R&D and brand, in which we obtain positive and significant coefficient estimates greater than 1, we may in fact be underestimating the true contribution of these intangible assets to market value. Next, we consider ITC. While reducing ITC to a single construct risks oversimplification, if it were subject to serious measurement error, it would be very unlikely that dummy variables ITC_A through ITC_F would explain market value in order. While there are potentially more sophisticated models of IT capabilities cited in the literature, such papers generally do not have (1) coverage of firms across multiple\n",
              "\n",
              "industries, and (2) repeated observations of the same firm over time. We then consider whether there was significant measurement error in the IT asset estimates. Such error would likely bias any coefficient estimates on IT assets downward, and possibly produce the same set of qualitative results as illustrated in Table 6, wherein the broader the definition of IT assets, the smaller the coefficient of IT. While we cannot rule this out altogether, we do not consider this to be a likely possibility that explains the bulk of our results. First, our firm-level survey results measuring intangible IT spending within sample firms are in line with economy-wide spending on IT intangibles (as discussed above and shown in Figure 4). This consistency makes it less likely that firm-level estimates of IT\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "103\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 9. Market Value as a Function of the Assets of the Firm and ITC: Alternative Specifications\n",
              "(1) Information Technology (IT) – All Hardware, All Software, Other Internal IT Assets, IT Consulting, ITRelated Training Ordinary Assets (K) Other Assets (F) R&D (R) Brand (B) ITC_A * IT ITC_B * IT ITC_D * IT ITC_F * IT ITC_A * Employment ITC_B * Employment ITC_D * Employment ITC_F * Employment Lagged FV (HQ in High-Tech Spillover Area = 1) * Total Assets Number of Observations Assumptions 508\n",
              "Baseline result\n",
              "\n",
              "(2) .35 (.35) 1.45 (.16) 1.84 (.17) 1.92 (.20) 3.26 (1.20) 16.59 (4.53) .83 (2.18) -.40 (1.68) .68 (3.35)\n",
              "\n",
              "(3) .37 (.40) 1.30 (.19) 1.80 (.22) .95 (.26) 1.71 (1.40) 23.73 (6.15) 5.06 (2.01) -.31 (1.97) 4.11 (4.86) -26.15 (30.45) 47.68 (11.14) 7.14 (23.34) -174.56 (76.28)\n",
              "\n",
              "(4) .59 (.36) .80 (.09) 1.58 (.18) 1.58 (.23) 3.80 (1.34) 18.55 (5.91) 2.97 (2.23) -.86 (1.77) 2.39 (3.43) -45.67 (35.76) 61.91 (16.53) 8.97 (24.55) -271.45 (63.13)\n",
              "\n",
              "(5) .37 (.37) 1.56 (.12) 1.91 (.16) 1.55 (.22) 3.50 (1.24) 19.50 (5.60) -2.81 (2.68) -.76 (1.70) -.56 (3.78) -68.60 (38.80) 79.96 (19.56) -4.11 (21.81) -174.74 (56.66)\n",
              "\n",
              "(6) .87 (.35) 1.55 (.13) 1.93 (.16) 1.49 (.23) 2.87 (1.37) 17.15 (5.60) 3.15 (3.21) -.71 (1.69) -.86 (4.16) -40.63 (33.78) 58.91 (16.69) 8.37 (13.23) -132.59 (62.53)\n",
              "\n",
              "(7) .35 (.21) 1.57 (.12) 1.90 (.16) 1.54 (.22) 3.41 (1.26) 9.69 (3.46) 1.89 (1.34) -.39 (1.00) -.64 (2.41) -31.83 (40.27) 63.54 (16.95) -3.87 (22.56) -158.87 (58.94)\n",
              "\n",
              "(8) .84 (.48) 1.56 (.12) 1.91 (.16) 1.52 (.22) 3.45 (1.25) 27.33 (7.96) 3.05 (2.91) -.87 (2.26) -.76 (5.37) -65.25 (41.09) 66.11 (17.07) -4.17 (22.35) -167.75 (57.94)\n",
              "\n",
              "(9) .29 (.33) 1.25 (.13) 1.88 (.15) .98 (.23) 1.39 (1.14) 14.44 (4.60) 1.69 (1.93) -.70 (1.39) .75 (3.41) -17.18 (22.32) 50.00 (15.43) 5.31 (20.10) -139.42 (52.09) .23 (.04)\n",
              "\n",
              "(10) .68 (.35) 1.57 (.14) 1.79 (.17) 1.12 (.25) 2.88 (1.34) 20.12 (6.37) 3.08 (2.15) -.52 (2.10) .25 (4.25) -51.94 (31.17) 63.33 (12.19) -5.61 (24.68) -176.85 (63.01)\n",
              "\n",
              ".55 (.35) 1.56 (.12) 1.91 (.16) 1.53 (.22) 3.42 (1.25) 17.53 (5.34) 2.33 (2.17) -.62 (1.68) -.53 (3.83)\n",
              "\n",
              "-44.66 -42.37 (30.67) (27.93) 66.03 74.69 (17.29) (18.90) -4.31 -17.97 (22.40) (22.38) -170.35 -205.79 (58.14) (63.31)\n",
              "\n",
              ".27 (.10) 508\n",
              "Market value adjusted for value of bonds\n",
              "\n",
              "508\n",
              "Market value = equity + debt\n",
              "\n",
              "508\n",
              "Ordinary assets estimated at currentcost instead of historicalcost\n",
              "\n",
              "508\n",
              "0% of internal IT services spending for custom & ownaccount software\n",
              "\n",
              "508\n",
              "100% of internal IT services spending for custom & ownaccount software\n",
              "\n",
              "508\n",
              "IT consulting, IT training, and other internal IT assets at 15% depreciation\n",
              "\n",
              "508\n",
              "IT consulting, IT training, and other internal IT assets at 60% depreciation\n",
              "\n",
              "508\n",
              "Lagged market value included\n",
              "\n",
              "508\n",
              "Headquarters located in high-tech metropolitan area\n",
              "\n",
              "Note: All regressions are GLS, with correction for heteroskedasticity and serial correlation. Controls for sector, year, no R&D, and no advertising are included (where each control is multiplied by total balance sheet assets). Empowerment levels are reported in thousands.\n",
              "\n",
              "104\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 10. Relative Valuation of ITC_A, ITC_B, ITC_D, and ITC_F to ITC_C Firms\n",
              "(1) Percent above/below an ITC_C firm ITC_A ITC_B ITC_C ITC_D ITC_F Avg. IT asset ($mms) Avg. employment (000s) Assumptions 20% 16% — -2% -33% 505.9 58.4\n",
              "Baseline result\n",
              "\n",
              "(2) 35% 15% — -4% -37% 505.9 58.4\n",
              "Market value adjusted for value of bonds\n",
              "\n",
              "(3) 33% 17% — 1% -26% 505.9 58.4\n",
              "Market value = equity + debt\n",
              "\n",
              "(4) 21% 16% — 0% -47% 505.9 58.4\n",
              "Ordinary assets estimated at currentcost instead of historicalcost\n",
              "\n",
              "(5) 43% 11% — -2% -33% 487.6 58.4\n",
              "0% of internal IT services spending for custom & ownaccount software\n",
              "\n",
              "(6) 21% 16% — 0% -26% 524.1 58.4\n",
              "100% of internal IT services spending for custom & ownaccount software\n",
              "\n",
              "(7) 20% 17% — -2% -31% 848.7 58.4\n",
              "IT consulting, IT training, and other internal IT assets at 15% depreciation\n",
              "\n",
              "(8) 20% 16% — -2% -32% 368.8 58.4\n",
              "IT consulting, IT training, and other internal IT assets at 60% depreciation\n",
              "\n",
              "(9) 20% 12% — 0% -25% 505.9 58.4\n",
              "Lagged market value included\n",
              "\n",
              "(10) 23% 17% — -2% -32% 505.9 58.4\n",
              "Headquarters located in high-tech metropolita n area\n",
              "\n",
              "Note: Percentages are based on the specifications in Table 9. Note that average IT asset levels change from columns 5 through 8 because of the alternative assumptions used to construct them.\n",
              "\n",
              "assets (as constructed from intangible IT spending) greatly differ from their true values. Second, if our firm-level estimates of IT assets were subject to significant measurement error, it would be very unlikely that our IT asset variable could explain market value in order when interacted with variables ITC_A through ITC_F in Tables 8 and 9. We rerun the results from Tables 8 and 9 based on a subset of the data from 2003 to 2004,65 which tests the robustness of imputing pre-2003 data66 as well as the assumption that ITC is quasi-fixed.67 The results based on the 2003–2004 subset\n",
              "\n",
              "are very similar to those based on the 2003–2006 sample. In addition, we reran the specifications without brand and R&D asset variables, and similar results were obtained for all of the IT-related intangible variables. Table 11 displays the correlation matrix of our sample. Neither ordinary assets nor other assets are correlated with our measures of IT, which suggests that the high valuations for IT are not being driven by a correlation between IT and ordinary or other assets.\n",
              "\n",
              "Limitations of the Intangible Assets Data\n",
              "65\n",
              "\n",
              "Not shown for brevity. Results available upon request.\n",
              "\n",
              "The 2003 and 2004 IT asset estimates rely more on imputation than do the same estimates from 2005 and 2006, as our IT spending data runs from 2003 to 2006.\n",
              "67 Since our organizational IT and management practice data were measured in 2005 and 2006, analyzing the results based on 2003 and 2004 tests the robustness of our assumption that ITC is quasi-fixed and thus is applicable to those years as well. As BHY (2002, p. 146) note, many organizational practices are embedded in a firm’s culture and thus are slow-changing. Therefore, a firm’s ITC in 2003 and 2004 should be closely correlated with its ITC in 2005 and 2006. In a sense, one can think of the 2005–2006 values as proxies for the 2003–2004 values, albeit noisy ones. Moreover, since we are using categories (ITC_A through ITC_F) rather than exact values of ITC, our results are more robust against measurement error within the ITC variable. Since ITC as constructed (spreading the 2005–2006 average across all four years) is a strong predictor of market value from 2003 to 2006, it would be very unlikely that the 2005 and 2006 values for ITC are radically different than the 2003 and 2004 values. If there were no relationship\n",
              "\n",
              "66\n",
              "\n",
              "While our results are robust to a number of different assumptions, there are limitations to our intangibles data. First, the R&D and brand variables are constructed based on respective spending on each of those assets. As a result of this limitation, it is beyond the scope of this work to compare the relative importance of R&D, brand, or IT based solely on the coefficients from the estimating equations. For example, although the coefficient of brand is higher than that of the broadest measure of IT, this could be the result of missing brand-related intangibles (in the same way that the coefficient of hardware was quite high until more IT-related intangibles\n",
              "\n",
              "between the 2003-2004 values of ITC and the 2005-2006 values, then ITC would not be a reliable predictor of market value from 2003 to 2006.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "105\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Table 11. Correlation Table for Variables in Sample\n",
              "Variable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Market Value IT Hardware Capitalized IT Broadest IT Ord. Assets Other Assets R&D Advertising No R&D No Advertising ITC ITC_A ITC_B ITC_C ITC_D ITC_F 1 1.00 .10** .10** .10** .83*** .91*** .84*** .82*** 1.00 .96*** 1.00 .98*** .04 .07 .03 .02 .98*** 1.00 .03 .05 .03 .02 -.06 -.01* .03 -.01 .06 .03 -.07* -.03 .04 .08* .04 .03 -.10** -.01 .02 -.02 .04 .04 -.07 -.03 1.00 .82*** 1.00 .60*** .61*** -.04 -.07 -.09** -.02 -.08* -.02 .09** .04 .78*** 1.00 .75*** .76*** 1.00 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
              "\n",
              "-.23*** -.09** -.08* .02 .02 -.04 .00 .05 -.03 -.03* .02 -.03 .05 .04 -.07* -.03\n",
              "\n",
              "-.19*** -.36*** -.18*** 1.00 -.06 -.01 -.02 -.05 .01 .07* -.04 -.03 -.04 .07 -.07 -.09* .14*** .02** -.09** -.04 -.06 -.06 .04 .04 -.01 -.03 .07* -.01 -.04 .16*** 1.00 .04 .09** -.09** .11** 1.00 .41*** 1.00 .42*** -.08 .08* 1.00\n",
              "\n",
              "-.34*** -.52*** 1.00 -.11*** -.52*** 1.00 -.08* -.34*** -.08* 1.00\n",
              "\n",
              "-.20*** -.09** -.01 -.06\n",
              "\n",
              "-.49*** -.08 -.47*** -.05\n",
              "\n",
              "Note: Total of 508 observations for all variables. ***p < .01, **p < .05, *p < .10\n",
              "\n",
              "were included in the estimating equation). Rather, our analyses suggest the necessity of determining and capturing more fine-grained measures of intangible assets in an effort to better demonstrate and explain market value as derived from IT-related intangible assets. Due to our sample size, there is not enough of an overlap to make a meaningful comparison between ITC in this work and ORG from BHY 2002. However, we do not see this as a significant impediment to these research findings, as ORG was based primarily on management and organization-related variables and ITC is based more on specific IT-related capabilities of firms.\n",
              "\n",
              "structed data set, replicate their finding that $1 of computer hardware is correlated with more than $10 of market value. Using our expanded definition of IT, which includes hardware, software that is both purchased and developed internally, other internal IT services, IT consulting, and IT-related training, we account for the missing $9 by directly estimating the value of a broader set of IT assets in a market value equation. Furthermore, our results suggest that IT is not a rising tide that lifts all boats. Instead, differences in management and organizational IT capabilities (e.g., management practices, HR practices, internal IT use, external IT use, and Internet capabilities) largely account for the value of IT intangibles. Firms with the highest IT capabilities (ITC) have significantly higher market value compared to firms with the lowest IT capabilities. Holding all tangible and intangible assets of the firm fixed, we estimate that the firms in the highest ITC category have 45% to 76% greater market value than the firms in the lowest ITC category. Our results suggest that the previously invisible IT intangibles not accounted for on the balance sheet are being priced into the market value of firms, providing further evidence that assets constructed in accordance with accounting standards such as SOP 98-1 or EITF 97-13 measure only a fraction of the business value of IT.\n",
              "\n",
              "Conclusion\n",
              "In this work, we set out to answer the following questions: First, can we quantify the value of IT-related intangible assets, most of which are invisible on corporate balance sheets? Second, can we use data on IT-related business practices and management capabilities to analyze whether this value is evenly distributed across firms or concentrated in leading firms? Using a panel of 127 firms over the period 2003–2006, we create comprehensive asset measures based on IT-related intangible spending at the firm level. We build on the framework by BHY 2002 and, using our newly con-\n",
              "\n",
              "106\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Gaps of this magnitude among otherwise similar firms are indicative of comparable gaps in management understanding and execution. Documenting and quantifying this set of relationships among IT, management practices, and market value is not only a step toward understanding the growing importance of IT-related intangibles, but also a crucial step toward better management, and ultimately greater returns on investment in IT for firms across the economy.\n",
              "\n",
              "Acknowledgments\n",
              "SAP supplied generous funding for this research. Lorin Hitt, Paul Hofmann, Jason Yotopoulos, and the participants of the Workshop on Information Systems and Economics all provided excellent suggestions and guidance. We are thankful to the participants and review team of the 2010 International Conference on Information Systems, where this work won the Best Conference Paper award. We are also grateful to Vijay Gurbaxani, senior editor, as well as the associate editor and two anonymous reviewers for their helpful feedback on this article. We thank Peter Weill and Stephanie Woerner at the MIT Center for Information Systems Research for their help in supplying us with the data set from the Social and Economic Implications of Information Technology (SeeIT) project, funded by NSF Grant IIS-0085725. Emily Dietrich and Stefanus Soegiarto provided valuable research assistance. All errors are, of course, our own.\n",
              "\n",
              "References\n",
              "Anderson, M. C., Banker, R. D., and Ravindran, S. 2003. “The New Productivity Paradox,” Communications of the ACM (46:3), pp. 91-94. Aral, S., and Weill, P. 2007. “IT Assets, Organizational Capabilities, and Firm Performance: How Resource Allocations and Organizational Differences Explain Performance Variation,” Organization Science (18:5), pp. 763-780. Athey, S., and Stern, S. 1998. “An Empirical Framework for Testing Theories About Complementarity in Organizational Design,” Working Paper 6600, National Bureau of Economic Research. Baily, M. N. 1981. “Productivity and the Services of Capital and Labor,” Brookings Papers on Economic Activity (1981:1), pp. 1-65. Bardhan, I., Krishnan, V., and Lin, S. 2013. “Business Value of Information Technology: Testing the Interaction Effect of IT and R&D on Tobin’s Q,” Information Systems Research (24:4), pp. 1147-1161. Bartel, A., Ichniowski, C., and Shaw, K. 2007. “How Does Information Technology Affect Productivity? Plant-Level Comparisons of Product Innovation, Process Improvement, and Worker Skills,” Quarterly Journal of Economics (122:4), pp. 1721-1758.\n",
              "\n",
              "Barth, M. E., Clement, M. B., Foster, G., and Kaznik, R. 1998. “Brand Values and Capital Market Valuation,” Review of Accounting Studies (3:1-2), pp. 41-68. BEA. 2000. “Recognition of Business and Government Expenditures for Software as Investment: Methodology and Quantitative Impacts, 1959-98,” Bureau of Economic Research, Washington, DC. BEA. 2001. “Estimation of Software in the U.S. National Accounts: New Developments,” Bureau of Economic Research, Washington, DC (available at http://www.bea.gov/papers/ pdf/USSoftware.pdf). BEA. 2003. Fixed Assets and Consumer Durable Goods in the United States, 1925-97, Washington, DC: U.S. Government Printing Office, Bureau of Economic Research. Berger, P. G., and Ofek, E. 1995. “Diversification’s Effect on Firm Value,” Journal of Financial Economics (37:1), pp. 39-65. Bloom, N., Sadun, R., and Van Reenen, J. 2012. “Americans Do IT Better: US Multinationals and the Productivity Miracle,” American Economic Review (102:1), pp. 167-201. BLS. 1983. Trends in Multifactor Productivity, 1948-81, Bulletin of the United States Bureau of Labor Statistics (2178), Washington, DC. Bresnahan, T. F., Brynjolfsson E., and Hitt, L. M. 2002. “Information Technology, Workplace Organization, and the Demand for Skilled Labor: Firm-Level Evidence,” Quarterly Journal of Economics (117:1), pp. 339-376. Brynjolfsson, E., and Hitt, L. M. 2003. “Computing Productivity: Firm-Level Evidence,” Review of Economics and Statistics (85:4), pp.793-808. Brynjolfsson, E., and Hitt, L. M. 2005. “Intangible but Not Unmeasurable: Some Thoughts on the Measurement and Magnitude of Intangible Assets,” Remarks for Chapter 12 in Measuring Capital in the New Economy, C. Corrado, J. Haltiwanger, and D. Sichel (eds.), Chicago: University of Chicago Press/National Bureau of Economic Research Studies in Income and Wealth, pp. 557-576. Brynjolfsson, E., Hitt, L. M., and Yang, S. 2002. “Intangible Assets: Computers and Organizational Capital,” Brookings Papers on Economic Activity (2002:1), pp. 137-181. Brynjolfsson, E., and Milgrom, P. 2012. “Complementarity in Organizations” in The Handbook of Organizational Economics, R. Gibbons and J. Roberts (eds.), Princeton, NJ: Princeton University Press, pp. 11-56. Brynjolfsson, E., Renshaw, A. A., and Van Alstyne, M. 1997. “The Matrix of Change,” Sloan Management Review (38:2), pp. 37-54. Brynjolfsson, E., and Saunders, A. 2010. Wired for Innovation: How Information Technology is Reshaping the Economy, Cambridge, MA: MIT Press. Cheng, Z. J., and Nault, B. R. 2007. “Industry Level SupplierDriven IT Spillovers,” Management Science (53:8), pp. 1199-1216. Corrado, C., Hulten, C., and Sichel, D. 2005. “Measuring Capital and Technology: An Expanded Framework,” in Measuring Capital in the New Economy, C. Corrado, J. Haltiwanger, and D. Sichel (eds.), Chicago: University of Chicago Press/National\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "107\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Bureau of Economic Research Studies in Income and Wealth, pp. 11-46. Corrado, C., Hulten, C., and Sichel, D. 2009. “Intangible Capital and Economic Growth,” Review of Income and Wealth (55:3), pp. 661-685. Crespi, G., Criscuolo, C., and Haskel, J. 2007. “Information Technology, Organisational Change, and Productivity Growth: Evidence from UK Firms,” Discussion Paper No. 783, Centre for Economic Performance, London School of Economics. David, P. A. 1990. “The Dynamo and the Computer: An Historical Perspective on the Modern Productivity Paradox,” The American Economic Review (80:2), pp. 355-361. Dedrick, J., Gurbaxani, V., and Kraemer, K. L. 2003. “Information Technology and Economic Performance: A Critical Review of the Empirical Evidence,” ACM Computing Surveys (35:1), pp. 1-28. Dewan, S., Shi, C., and Gurbaxani, V. 2007. “Investigating the Risk–Return Relationship of Information Technology Investment: Firm-Level Empirical Analysis,” Management Science (53:12), pp. 1829-1842. Ernst & Young. 1998. Accounting for the Costs of Computer Software Developed or Obtained for Internal Use: Statement of Position 98-1, Assurance & Advisory Business Services, November. Gao, G., and Hitt, L. 2012. “Information Technology and Trademarks: Implications for Product Variety,” Management Science (58:6), pp. 1211-1226. Griliches, Z., and Hausman, J. A. 1986. “Errors in Variables in Panel Data,” Journal of Econometrics (31:1), pp. 93-118. Hall, B. H., Jaffe, A., and Trajtenberg, M. 2005. “Market Value and Patent Citations,” The Rand Journal of Economics (36:1), pp. 16-38. Hall, R. E. 2000. “E-Capital: The Link Between the Stock Market and the Labor Market in the 1990s,” Brookings Papers on Economic Activity (2000:2), pp. 73-118. Hall, R. E. 2001. “The Stock Market and Capital Accumulation,” The American Economic Review (91:5), pp. 1185-1202. Hand, J. R. M. 2003. “The Increasing Returns-to-Scale of Intangibles,” in Intangible Assets: Values, Measures and Risks, J. Hand and B. Lev (eds.), New York: Oxford University Press, pp. 303-331. Lang, L. H. P., and Stulz, R. M. 1994. “Tobin’s q, Corporate Diversification, and Firm Performance,” Journal of Political Economy (102:6), pp. 1248-1280. Lev, B. 2000. Intangibles: Management, Measurement, and Reporting, Washington, DC: Brookings Institute Press. Lev, B. 2004. “Sharpening the Intangibles Edge,” Harvard Business Review (82:6), pp. 109-116. Lev, B., and Radhakrishnan, S. 2005. “Valuation of Organizational Capital,” in Measuring Capital in the New Economy, C. Corrado, J. Haltiwanger, and D. Sichel (eds.), Chicago: University of Chicago Press/National Bureau of Economic Research Studies in Income and Wealth, pp. 73-110. McGahan, A. M., and Porter, M. E. 1999. “The Persistence of Shocks to Profitability,” The Review of Economics and Statistics (81:1), pp. 143-153.\n",
              "\n",
              "McGahan, A. M., and Porter, M. E. 2003. “The Emergence and Sustainability of Abnormal Profits,” Strategic Organization (1:1), pp. 79-108. McKinsey Global Institute. 2001. US Productivity Growth 1995-2000: Understanding the Contribution of IT Relative to Other Factors (available at http://www.mckinsey.com). Milgrom, P., and Roberts, J. 1990. “The Economics of Modern Manufacturing: Technology, Strategy, and Organization,” The American Economic Review (80:3), pp. 511-528. Milgrom P., and Roberts, J. 1995. “Complementarities and Fit: Strategy, Structure and Organizational Change in Manufacturing,” Journal of Accounting and Economics (19:2-3), pp. 179-208. Nakamura, L. 2001. “What is the US Gross Investment in Intangibles? (At Least) One Trillion Dollars a Year!,” Working Paper No. 01-15, Federal Reserve Bank of Philadelphia. Pilat, D. 2004. “The ICT Productivity Paradox: Insights from Micro Data,” OECD Economic Studies (38:1), pp. 37-65. Porter, M. E. 1996. “What Is Strategy?,” Harvard Business Review (74:6), pp. 61-78. Rai, A., Patnayakuni, R., and Seth, N. 2006. “Firm Performance Impacts of Digitally Enabled Supply Chain Integration Capabilities,” MIS Quarterly (30:2), pp. 225-246. Seethamraju, C. 2003. “The Value Relevance of Trademarks,” in Intangible Assets: Values, Measures and Risks, J. Hand and B. Lev (eds.), New York: Oxford University Press, pp. 228-247. Stankevich, D. 2003. “The Human Link in Supply Chain Technology,” Retail Merchandiser (43:2), pp. 43-45. Subramani, M. 2004. “How Do Suppliers Benefit from Information Technology Use in Supply Chain Relationships?,” MIS Quarterly (28:1), pp. 45-73. Tambe, P., and Hitt, L. M. 2014. “Job Hopping, Knowledge Spillovers, and Regional Returns to Information Technology Investments,” Management Science (60:2), pp. 338-355. Villalonga, B. 2004a. “Diversification Discount or Premium? New Evidence from the Business Information Tracking Series,” Journal of Finance (59:2), pp. 479-506. Villalonga, B. 2004b. “Does Diversification Cause the Diversification Discount?,” Financial Management (33:2), pp. 5-27. Zhao, R. 2002. “Relative Value Relevance of R&D Reporting: An International Comparison,” Journal of International Financial Management and Accounting (13:2), pp. 153-174.\n",
              "\n",
              "About the Authors\n",
              "Adam Saunders is an assistant professor at the Sauder School of Business at the University of British Columbia. His research aims to quantify technology-related intangible assets that are playing a significant role in generating market value and productivity. He is also studying how IT is changing the competitive playing field among U.S. firms. With Erik Brynjolfsson, he is the coauthor of Wired for Innovation: How Information Technology is Reshaping the Economy (MIT Press, 2010). Prior to coming to the Sauder School, Adam was a lecturer and postdoctoral fellow at the Wharton School of the University of Pennsylvania and previously worked for\n",
              "\n",
              "108\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "the White House’s Council of Economic Advisers in Washington, D.C. He holds a Ph.D. in Management from MIT, and an A.B. in Economics summa cum laude with a Certificate in Finance from Princeton University. Erik Brynjolfsson is the Schussel Family Professor at the MIT Sloan School, director of the Massachusetts Institute of Technology’s Initiative on the Digital Economy, a research associate\n",
              "\n",
              "at the NBER, and chairman of MIT Sloan Management Review. His research examines the effects of information technologies on business strategy, productivity and performance, Internet commerce, and intangible assets. He is the author or coauthor of several books, including The Second Machine Age. He holds Bachelor’s and Master’s degrees from Harvard University in Applied Mathematics and Decision Sciences and a Ph.D. from MIT in Managerial Economics.\n",
              "\n",
              "Appendix A\n",
              "Variable Construction\n",
              "SeeIT Survey Detail\n",
              "The IT spending and practice data is from the SeeIT survey. The data covers IT spending from 2003 to 2006, and IT practices in 2005 and 2006. Approximately 600 companies participated in the survey, and about half of them were publicly traded. The survey was conducted by telephone in 2005 and 2006 with a single point of contact in each company. The majority of respondents were CIOs, those in IT finance functions, or those in IT project management functions. The IT spending questions covered hardware, prepackaged software, external IT services (e.g., business process consulting and integration services), internal IT services (e.g., custom software, design, maintenance, and administration), and IT-related training.\n",
              "\n",
              "Methodology of Converting IT Spending into IT Assets\n",
              "To allocate IT spending into the three measures of IT assets (purchased hardware, capitalized hardware and software, and all IT), we begin by estimating the percentage of IT spending that is leased or purchased in each category of Table 2. We then estimate how purchases or payroll are either capitalized or expensed by the firm. In the absence of direct data from the firms themselves (as publicly traded firms are not required to list capitalized IT as a separate item on their balance sheets), we use an industry-level census survey, called the Annual Capital Expenditure Survey (ACES). The ACES contains the Information and Communication Technology Supplement, which details hardware and software spending in each of the 20 major two-digit NAICS sectors by leases or by purchases and payroll. The survey also divides purchases and payroll data into either capitalized or expensed spending by sector. Therefore, we assume that firm spending on IT is allocated as leases or purchases and payroll at the sector average rate, and that any purchases and payroll would also be capitalized at the sector average rate. We list the summary statistics for hardware and software capitalization and leases for our sample in Table A1. Analysis of the sample demonstrated that an average of 82.2% of hardware spending was for purchases (while the remainder was leased) and, overall, an average of 64.0% of hardware spending was capitalized (which ranged from 51.2% to 84.7% among sectors). An average of 53.1% of software spending was capitalized, and this value ranged from 24.5% to 68.7% among sectors within our sample. Below, we elaborate further on how IT assets are divided into each of our three measures of IT.\n",
              "\n",
              "Hardware\n",
              "To account for hardware assets in our three measures of IT, we divide each year’s hardware spending by the firm into three categories as illustrated in Figure 2: capitalized purchases (which average 64% of IT spending), uncapitalized purchases (which average 18%), and leases (which comprise the other 18%). We do not convert spending on hardware leases into hardware assets. Hardware assets that are considered within our first measure of IT assets (the BHY 2002 measure of purchased hardware) average $52.9 million per firm. Hardware assets that are included in the second measure, capitalized IT, average $39.9 million per firm. In the third and broadest measure of all IT, hardware assets average $52.9 million per firm.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "109\n",
              "\n",
              "\n",
              "Saunders & Brynjolfsson/Valuing IT-Related Intangible Assets\n",
              "\n",
              "Prepackaged Software\n",
              "We use the industry-level capitalization ratios from the ACES data to divide prepackaged software spending into three categories: capitalized purchases (which average 53% of spending), uncapitalized purchases (which average 25%), and leases (which comprise the other 22%). Prepackaged software assets included in the second measure of IT assets, capitalized IT, average $21.7 million per firm. In the third and broadest measure of all IT, prepackaged software assets average $32.2 million per firm. We do not convert spending on prepackaged software leases in our prepackaged software asset estimates.\n",
              "\n",
              "Custom and Own-Account Software\n",
              "Spending on internal IT services as detailed in the SeeIT survey covered activities such as custom software, design, maintenance, and administration. We divide this spending into Custom Software, Own-Account Software, and Other Internal IT assets (see Figure 2). Half of all annual spending is assumed to be on Custom and Own-Account Software, and the other half is assumed to be on Other Internal IT assets (BEA 2000, p. 5). We divide the internal IT services spending that is allocated toward Custom and Own-Account Software assets into capitalized and uncapitalized categories according to the industry-level ratios from the ACES. As illustrated in Figure 2, we assume that 34% of internal IT services spending is capitalized, while the remainder is expensed. On average, firms in our sample have $95.0 million per year of Custom and Own-Account Software assets included in our second measure of capitalized IT, and average $144.3 million of Custom and Own-Account Software included in our third and broadest measure of all IT.\n",
              "\n",
              "Other Internal IT Assets\n",
              "Spending in this area is assumed to be entirely expensed. Firms in our sample average about $126.0 million per year of Other Internal IT assets, and this asset category is only included in the third and broadest measure of all IT.\n",
              "\n",
              "IT Consulting\n",
              "We assume the firm fully expenses any spending on external IT services,68 and thus IT Consulting assets, which average $96.3 million per year in our sample, are included only in the third and broadest measure of all IT.\n",
              "\n",
              "IT-Related Training\n",
              "We assume that firms fully expense spending on IT-Related Training in accordance with SOP 98-1, and thus, any IT-Related Training assets are included only in the third and broadest measure of all IT. These assets average $53.7 million per year in the sample.\n",
              "\n",
              "Table A1. Capitalization Ratios of Hardware and Software Spending for 2003-2006 Sample\n",
              "Spending Type Hardware purchases and leases Purchases, capitalized Purchases, not capitalized Leases, not capitalized Software purchases, payroll, and licensing Purchases and payroll, capitalized Purchases and payroll, not capitalized Leases, not capitalized Mean 100.0% 64.0% 18.2% 17.8% 100.0% 53.1% 24.8% 22.1% Standard Deviation — 10.0% 6.2% 5.3% — 8.8% 8.3% 3.9% Minimum — 51.2% 5.4% 5.4% — 24.5% 12.0% 11.6% Maximum — 84.7% 30.0% 26.0% — 68.7% 59.2% 34.9%\n",
              "\n",
              "68 While it is clear from SOP 98-1 that all business process consulting is to be expensed, it is possible that some integration services that are part of the application stage of software development could be capitalized. In the absence of more detailed data on (1) the extent of external IT services spending that included integration services, and (2) the extent to which any such services were in the application stage of software development, we assume all such spending is entirely expensed by the firm.\n",
              "\n",
              "110\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Copyright of MIS Quarterly is the property of MIS Quarterly and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "aVcSQ3VqD3IY",
        "colab_type": "code",
        "outputId": "0687a7a0-d6f0-45a3-a7b0-a9579d34b31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc3.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[**p < .01, **p < .05, *p < .10\n",
              " \n",
              " were included in the estimating equation).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "CyzT95U4L1yM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_xQ61kCFQKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L67nM9ebE0ws",
        "colab_type": "code",
        "outputId": "c1736993-5bc0-499f-82aa-a8254dde6243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc2.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The highest mean WTP is observed in the cell (PC = LOW, EXT = HE), and is signiﬁcantly different from that of (HIGH, LE) (p < 0 001 , (HIGH, HE) (p = 0 002), and (LOW, LE) (p = 0 003) cells.,\n",
              " † p < 0 10; ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              " \n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "azGFpTwBFIDR",
        "colab_type": "code",
        "outputId": "9211e1f7-4653-4c99-8e27-a669206f32a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "p_großer_als_Sents = [sent for sent in doc2.sents if 'p >' in sent.string]\n",
        "p_großer_als_Sents"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "QCWNBZubF4ta",
        "colab_type": "code",
        "outputId": "5711ee47-b05f-4fe3-a13d-b8992553b53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14597
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc4 = nlp(open(u\"KwonH#SoH#HanS#OhW_2016_Excessive Dependence on Mobile Social Apps - A Rational Addiction Perspective_Information Systems Research_4.txt\").read())\n",
        "doc4\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This article was downloaded by: [130.89.97.167] On: 17 February 2017, At: 05:55 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA\n",
              "\n",
              "Information Systems Research\n",
              "Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org\n",
              "\n",
              "Excessive Dependence on Mobile Social Apps: A Rational Addiction Perspective\n",
              "Hyeokkoo Eric Kwon, Hyunji So, Sang Pil Han, Wonseok Oh\n",
              "\n",
              "To cite this article: Hyeokkoo Eric Kwon, Hyunji So, Sang Pil Han, Wonseok Oh (2016) Excessive Dependence on Mobile Social Apps: A Rational Addiction Perspective. Information Systems Research 27(4):919-939. http://dx.doi.org/10.1287/isre.2016.0658 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages\n",
              "\n",
              "INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org\n",
              "\n",
              "\n",
              "Information Systems Research\n",
              "Vol. 27, No. 4, December 2016, pp. 919–939 ISSN 1047-7047 (print) ISSN 1526-5536 (online) https://doi.org/10.1287/isre.2016.0658 © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "Excessive Dependence on Mobile Social Apps: A Rational Addiction Perspective\n",
              "Hyeokkoo Eric Kwon, Hyunji So\n",
              "College of Business, Korea Advanced Institute of Science and Technology, Seoul 02455, Korea {hkkwon7@business.kaist.ac.kr, namjjong@business.kaist.ac.kr}\n",
              "\n",
              "Sang Pil Han\n",
              "W. P. Carey School of Business, Arizona State University, Tempe, Arizona 85281, shan73@asu.edu\n",
              "\n",
              "Wonseok Oh\n",
              "College of Business, Korea Advanced Institute of Science and Technology, Seoul 02455, Korea, wonseok.oh@kaist.ac.kr\n",
              "\n",
              "D\n",
              "\n",
              "rawing on the rational addiction framework, this study explores the digital vulnerabilities driven by dependence on mobile social apps (e.g., social network sites and social games). Rational addicts anticipate the future consequences of their current behaviors and attempt to maximize utility from their intertemporal consumption choices. Conversely, myopic addicts tend toward immediate gratiﬁcation and fail to fully recognize the future consequences of their current consumption. In lieu of conducting self-report surveys or aggregatelevel demand estimation, this research examines addictive behaviors on the basis of consumption quantity at an individual level. To empirically validate rational addiction in the context of social app consumption, we collect and analyze 13-month, individual-level panel data on the weekly app usage of thousands of smartphone users. Results indicate that the average social app user conducts herself in a forward-looking manner and rationally adjusts consumption over time to derive optimal utility. The subgroup analysis, however, indicates that substantial variations in addictiveness and forward-looking propensities exist across demographically diverse groups. For example, addictive behaviors toward social network sites are more myopic in nature among older, less-educated, high-income groups. Additionally, the type of social app moderates the effects of demographic characteristics on the nature of addictive behaviors. We provide implications that policymakers can use to effectively manage mobile addiction problems, with the recommendations focusing on asymmetric social policies (e.g., information- and capacity-enhancing measures). Keywords : addiction; mobile; rational addiction; myopic addiction; digital vulnerability; IT and health impact; mobile social apps; app consumption; econometrics; panel data History : Rob Fichman, Ram Gopal, Alok Gupta, Sam Ransbotham, Senior Editors; Sam Ransbotham, Associate Editor. This paper was received on March 1, 2015, and was with the authors 8.5 months for 2 revisions. Published online in Articles in Advance November 3, 2016.\n",
              "\n",
              "1.\n",
              "\n",
              "Introduction\n",
              "\n",
              "Social applications (“apps”) on mobile platforms shift how we communicate, socialize, learn, and play. Social network sites (SNSs) and platform-based social games—arguably, the two most dominant forms of social apps—progressively integrate into users’ everyday lives. The global number of SNS users is now 2 billion, nearly 30% of the entire world population today (Kemp 2015). Mobile platforms are ubiquitous; as a result, at least 1.6 billion users maintain an active social media presence (as of January 2015). Social games (e.g., Candy Crush, Anipang), which enable social interaction among players through exchanging points, gifting, and competing via leaderboards, have rapidly become prevalent digital pastimes for mobile users (Fields 2014). To illustrate the formidability of social games as competitors in the digital industry,\n",
              "919\n",
              "\n",
              "Candy Crush achieved an audience reach of 100 million in a little over a year—an accomplishment that Facebook realized in ﬁve years (Gould 2015). This feat positions the social game parallel to Twitter, with Candy Crush earning nearly double the revenue that the microblogging platform generated during the last quarter of 2014. Yet, the burgeoning popularity of social apps as catalysts for interpersonal connection is both a blessing and a curse. On one hand, social apps promote social relationships and assert solidarity with friends. The portability, convenience, and on-demand accessibility offered by mobile devices make perpetual contact possible; users maintain a constant sense of camaraderie and kinship with the people in their social circles by sharing daily routines or playing games. This new social paradigm has consequently reshaped and, to a\n",
              "\n",
              "\n",
              "920 certain extent, enriched the social fabric and conventions of society. On the other hand, the social fever engendered by these mobile apps could turn into social fear. Medical professionals and scholars are increasingly concerned that overdependence on social apps can give rise to feelings of social inadequacy and consequently exert adverse ramiﬁcations on human behaviors. Heavy or excessive use of social networks and gaming apps on smartphones fosters habits that can easily develop into addictive conduct, similar to the potential escalation of experimentation to dependence on alcohol, cigarettes, or drugs. A study by Zhang (2014) shows that the average user devoted 2.53 hours daily to social platforms: 1.72 and 0.81 hours to social media sites (e.g., Facebook) and microblogging sites (e.g., Twitter), respectively (Bennet 2015). People compulsively check their SNS feeds—while eating, attending meetings, crossing the street, or driving—for fear of missing a satisfying “social update” (Hedges 2014). Some even interrupt their sleep to keep pace with the unfolding of digital life events. A similar addictive behavior is exhibited by Candy Crush’s 93 million users, who play the game at an average of more than 10 times every day (Makuch 2014). Medical professionals periodically alert the public to the potential of heavy SNS or game dependence to cause health crises, including depression, bipolar disorder, and histrionic personality disorder (Moreno et al. 2011). As the mobile era evolves, the apparent addictive preoccupation with and digital vulnerabilities to SNSs and social games have become vexing social challenges. In academic circles, the debate regarding addictive behaviors continues to intrigue scholars. Numerous researchers and scientists espouse an exclusive deﬁnition of addiction as an acute irrational behavior and as a chronic disease that necessitates psychological and medical treatment. Much of the scholarship maintains that the basic economic principles, such as the imposition of high prices, are ineffective bases in devising a cure for addictive behaviors (Chaloupka 1988). This perspective is challenged by Becker and Murphy (1988), who argue that addiction to substances (e.g., cigarettes and alcohol) can also be explained by utilitymaximizing rational human behavior that can be controlled by exogenous factors (e.g., price adjustments). This reasoning is the essence of rational addiction theory (Becker and Murphy 1988), which holds that addicts are rational in that anticipated future consequences of current consumption inspire sensible conduct intended to help these individuals arrive at a choice characterized by maximum lifetime utility. When addicts expect the future prices of addictive goods to rise, for example, they reduce their current consumption of these goods because the increase diminishes the marginal utility of current and future\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "consumption. This frame of reference starkly contrasts with “myopic” addiction theory, in which one’s current assessment of utility depends solely on the past and current consumption of addictive goods (Pollak 1970, 1976). Although myopic addicts recognize that their past consumption heavily inﬂuences their current consumption decisions because of an accumulated stock of substances (i.e., reinforcement effects), they disregard the consequences of past and current choices on future consumption decisions when availing of current consumption options. These “short-sighted” individuals are predisposed toward immediate gratiﬁcation and fail to recognize the future harmful consequences caused by their current consumption of addictive goods (Chaloupka 1991). These qualities translate to a greater risk and higher vulnerability to the detrimental effects of compulsive behavior among past-driven, myopic addicts than among forward-looking rational addicts. With the aforementioned considerations in mind, we explore social “app-diction” (deﬁned as the excessive dependence on social apps) through the lens of rational addiction theory. In this regard, speciﬁc research questions pursued are as follows: On the basis of the rational addiction framework, do “average” SNS or social game addicts exhibit rational or myopic addictive behaviors? Assuming the presence of individual heterogeneity, which demographic groups (classiﬁed by age, gender, education, and income) are more vulnerable to myopic addiction? Do SNSs more signiﬁcantly induce susceptibility to myopic addiction than do social games? Should addiction and vulnerability to social apps be self-regulated or government regulated? What policies will be effective in curbing social app addiction? Building on the theoretical insights offered by Becker and Murphy (1988), this study derives a framework designed to enrich our understanding of rational addiction to social apps. We draw on this rational choice framework to investigate individuals’ addictive behaviors across the various time horizons (i.e., past, present, and future) over which they attempt to maximize lifetime utility from their intertemporal consumption choices. Previous rational addiction studies concentrated on validating whether addiction itself is a rational behavior at the aggregate level. The current research expands such holistic inquiries to occasion a ﬁne-grained understanding of individual addictive behaviors on social apps. The approach embraced in this work contributes to developing nuanced knowledge of the fundamental mechanisms and dynamics that underlie the digital addiction driven by social apps. To empirically test the presence of addiction among social app users, we collected and analyzed 13-month,\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "921 the interplay between heredity and the social environment (Peele 1985). Academic communities have actively showcased addiction issues and their negative effects on humanity. A deﬁciency worth looking into, nonetheless, is that most scholarly works on addiction extensively focus on physical substances (e.g., drugs (Everitt and Robins 2005), cigarettes (Naqvi et al. 2007), or alcohol (Herz 1997)). Minimal attention has been devoted to mobile addiction—a case of being “caught in the Net,” unable to rein in the compulsive use of social apps. 2.1. Internet and Social App Addiction The extant literature on information technology (IT)triggered addiction focuses primarily on the antecedents and consequences of digital addiction. Internet addiction poses as much of a detriment as does substance addiction because it can throw one’s life out of balance and cause physical damage (Young 1998). This survey-driven study reveals that Internet addicts suffer from social, professional, and affective impairment and disruption. Addiction researchers often employ mass communication theory (e.g., Morris and Ogan 1996) to establish a conceptual foundation for elucidating Internet addiction, with the frameworks reﬂecting the social and psychological origins of human needs. Internet addiction can also be deﬁned and understood in terms of the behavioral core elements (i.e., salience, mood modiﬁcation, tolerance, withdrawal, conﬂict, and relapse) that form addiction (Grifﬁths 2000). Selfcontrol is negatively related to online game addiction, whereas aggression and narcissistic identity are positively associated with it (Kim et al. 2008). Notwithstanding the scholarly scrutiny dedicated to Internet addiction, the literature has paid little attention to the issues of addiction to SNSs or social games. An emerging body of work has recently called for a new understanding of the mechanisms and causes, as well as an innovative treatment, of addiction to IT-based social applications. Similar to substance abusers, SNS addicts are vulnerable to many adverse biopsychological consequences, including mood modiﬁcation, salience, tolerance, withdrawal, conﬂict, and relapse (Grifﬁths 2005). Some of the signiﬁcant correlates that predict the tendency toward SNS addiction are self-identity and a desire for belongingness (Pelling and White 2009), social activities and relationship building (Kim et al. 2009), and dysfunctional coping (Kuss and Grifﬁths 2011), such as escapism and avoidance. Regulated use of SNSs and relapse prevention derived from effective deterrence schemes formulated with cognitive-behavioral therapies can successfully inhibit SNS dependence (Echeburúa and de Corral 2010). Although addiction to online or video games, particularly among teenagers, has gained increasing recognition as a viable object to study (see Ng and\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "individual-level panel data on the weekly usage of such apps by thousands of smartphone users. To date, most scholarly works on addiction are based on either self-report surveys or demand estimation at the aggregate level. The susceptibility of these methods to structural biases and inaccuracies casts doubt on their cogency and usefulness (Fowler 1992). The survey-based approach reﬂects only user perceptions and not actual behaviors. Similarly, aggregate-level demand estimation pays limited attention to the individual differences in addictive propensities among users and contemplates purchase quantity rather than actual consumption. Precisely quantifying the extent of consumption among smokers and drinkers would be extremely difﬁcult, if not impossible. We improved the quality of consumption measurement by tracing individual real app consumption, which was determined in seconds over a 13-month period. This study is among the ﬁrst to examine addictive behaviors on the basis of individuals’ actual consumption quantities. Our analysis suggests that average users of SNSs or social games exhibit rational addictive behaviors when using the social apps. This ﬁnding theoretically and empirically illuminates numerous conundrums as to why legal sanctions may inefﬁciently curb digital addiction. We also found that addiction vulnerability is associated with the extent of consumption inertia manifested through reinforcement effects, the sensitivity to social liquidity (i.e., network effects), and the degree of propensity for a forward-looking mindset. The interpretation of these results, however, should be tempered by the signiﬁcant variations observed across diverse demographic groups with respect to the nature of addictive behaviors. Certain sociodemographic groups exhibit rational addiction behaviors, whereas others fall prey to myopic addiction. Moreover, these group differences are relativized by app characteristics; the extent of myopic addiction is more pronounced among users of social networking apps than among users of social game apps. Our ﬁndings collectively suggest that addiction to social apps is driven by many factors, but we also derive evidence that effectively addressing these addiction issues is feasible through the adoption of basic economic principles.\n",
              "\n",
              "2.\n",
              "\n",
              "Theoretical Background\n",
              "\n",
              "Addiction encompasses uncontrollable and irrational behaviors motivated by the desire to experience pleasurable and euphoric effects despite the potentially harmful consequences of the said behaviors (Pollak 1970). Medical scientists treat addiction as a chronic disorder caused by biological or neurological predispositions, whereas sociopsychologists evaluate it as an irrepressible response that is engendered by\n",
              "\n",
              "\n",
              "922 Wiemer-Hastings 2005, Kim et al. 2008, Xu et al. 2012), the dearth of inquiries into the addictive qualities of platform-based social games (e.g., Candy Crush Saga) continues to prevent a more solid explanation of addiction to social apps. Psychological factors, including loneliness, perceived gratiﬁcation, and self-control, are signiﬁcantly associated with social game addiction (Chen and Leung 2015). Computational difﬁculties behind the simple play (Walsh 2014) and network externalities and gratiﬁcations inherent in social games (Wei and Lu 2014) are also identiﬁed as key factors contributing to addiction. A few studies have examined the addictive potential of SNSs on the basis of demographic or personal characteristics. Research (e.g., Correa et al. 2010) indicates that adolescent users with narcissistic traits are more vulnerable to SNS addiction than are adult users. The same study reveals that addiction to SNSs is more pronounced among men with neurotic traits than among women with the same qualities. No signiﬁcant gender difference was detected for addiction to social games, but occupation was found to be signiﬁcantly associated with addictive tendencies (Chen and Leung 2015). Interestingly, both extroverts and introverts develop a penchant for SNS engagement, but for different reasons; while the former are prone to SNS addiction because of the social enhancement factor, the latter’s predilections are stimulated by the need for social compensation (Amichai-Hamburger and Vinitzky 2010). Although varied in scope and context, most addiction studies on SNSs or social games, including all of the works discussed here, focus exclusively on surveybased approaches that must depend on subjective, perceptual, and often inaccurate human cognition (Fowler 1992). The majority of such investigations are also grounded in behavioral theories involving cognitive psychology. Furthermore, medical and psychological studies on SNS or social game addiction all have, as their basis, a type of “disease model,” in which compulsive and irrational behaviors are viewed as originating neurological, genetic, and biological causes that are curable only by lifelong abstinence. The present study relies on an economics framework and uses real consumption data to provide a fresh and unique insight into addiction to social apps. 2.2. A Rational Choice View of Addiction The deﬁning features of addictive behaviors, such as irrationality, compulsion, overindulgence, and loss of control, have been questioned by Becker and Murphy (1988). Using the ﬁndings of Stigler and Becker (1977) and Iannaccone (1984) as a prelude to their analytical derivation, Becker and Murphy (1988) proposed the rational addiction model in which addicts are not “myopic” but rational. Rational, far-sighted addicts anticipate the future consequences of their current\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "behaviors and act prudently in their own best interests to maximize discounted utility. The most essential aspect of the theory is that, by weighing the effects of their actions on the future, addicts, who have full knowledge of the consequences of their addiction, strategically calculate the expected beneﬁts (e.g., utility gains from taking drugs) against costs (e.g., negative impacts on health). They arrive at a rational choice that maximizes lifetime utility based on their stable preferences. For example, drug addicts may be cognizant that their current drug use will stimulate greater future drug consumption and that continued drug use will result in negative consequences (e.g., adverse health). Nevertheless, they may appraise the utility gained from taking drugs as outweighing the discounted reduction in utility arising from negative consequences. Using this example and many others, Becker and Murphy (1988) construe addiction as rational, utility-maximizing behavior, just like any other actions based on economic considerations. To illustrate further, in previous myopic models of addictive behaviors, addicts neither consider the future consequences of their current consumption nor factor future prices of addictive goods into their current consumption decision making (Becker and Murphy 1988). Instead, they choose current consumption based primarily on past consumption. This myopic perspective regards addiction as merely the positive interaction and complementarity between past and current consumption of addictive goods, without reference to future consumption or external factors (e.g., prices). As opposed to this backward-looking, myopic model, the rational addiction framework maintains that addicts decide on current consumption with future consumption and utility in mind to maximize discounted utility. This forward-looking paradigm, which falls under the umbrella of the rational choice model (Calvert 1985), centers on the dynamics of current consumption in response to anticipated future prices of addictive goods. For example, rational addicts may proactively reduce their current consumption of tobacco when they anticipate that prices will rise in the future. They recognize that the anticipated price increase will lower the marginal utility of their current and future consumption. By contrast, myopic addicts do not cut their current consumption in response to expected increases in future prices. In constructing the rational addiction model, Becker and Murphy (1988) assume that an individual’s utility at time t depends on three elements: consumption of addictive goods, Ct ; addictive stock, At ; and consumption of other goods, Yt . In addition, these authors explicate three distinctive aspects of addiction: withdrawal, reinforcement, and tolerance. Withdrawal corresponds to a decline in current utility due\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "923 of future consumption (i.e., forward-looking) on current consumption. Scholars (e.g., Grossman and Chaloupka 1998, Baltagi and Grifﬁn 2002) have since leveraged the empirical formality speciﬁed in Equation (1), testing the theory of rational addiction by estimating the following model parameters: 1 and 2 . The positive effect of past consumption on current consumption 1 signiﬁes addiction, and the addictive behavior is considered rational when the effect of future consumption on current consumption ( 2 ) exhibits the same direction as 1 . Most empirical studies ﬁnd signiﬁcant and positive values for 1 and 2 in diverse habit-forming contexts, including consumption of cigarettes (Becker et al. 1994), alcohol (Baltagi and Grifﬁn 2002), cocaine (Grossman and Chaloupka 1998), opium (Liu et al. 1999), caffeine (Olekalns and Bardsley 1996), and gambling (Mobilia 1993).\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "to reduced current consumption. Reinforcement is evident when past consumption prompts the desire for current consumption and increase the marginal utility of current consumption. Finally, increased tolerance shows that the greater the addictive stock (i.e., cumulative past consumption of the addictive substance), the lower the current utility (Becker and Murphy 1988). The idiosyncrasies of addiction give rise to a behavioral pattern in which past consumption of addictive goods induces current consumption by inﬂuencing the marginal utility of both current and future consumption (Becker and Murphy 1988). This intertemporal, dependent demand structure, often called adjacent complementarity (Ryder and Heal 1973), epitomizes the key conceptual building blocks that underlie the rational addiction model. Consumers become addicted if their past consumption positively affects their current consumption. An addictive behavior is thought to be rational when current consumption is positively dependent on future utility and consumption. In a subsequent study, Becker et al. (1994) scrutinize the model of rational addiction by conducting an empirical investigation of the effect of higher future cigarette prices on current consumption of cigarettes. In their empirical framework, they assume that addicts make rational decisions about their consumption of both nonaddictive and addictive goods within their lifetime budget. The model assumes a quadratic utility function in Ct At , and Yt to elicit the demand function (for derivations, see Becker et al. 1994, p. 398). Finally, they identify the resulting demand structure of addictive goods, Ct , which denotes current consumption as a function of both past and future consumption, and the current price of the addictive goods, Pt Ct =\n",
              "0\n",
              "\n",
              "3.\n",
              "\n",
              "Rational Addiction to Mobile Social Apps\n",
              "\n",
              "+\n",
              "\n",
              "1 Ct −1\n",
              "\n",
              "+\n",
              "\n",
              "2 Ct +1\n",
              "\n",
              "+\n",
              "\n",
              "3 Pt\n",
              "\n",
              "(1)\n",
              "\n",
              "where 1 = −uCA / ucc + uAA / 1 + r , 2 = 1 / 1 + r , 3 = / ucc + uAA / 1 + r , r is an interest rate, and is the marginal utility of wealth. In this speciﬁcation, uCA , uCC , and uAA represent second-order derivatives of the utility function. Becker et al. (1994) assume that ucc and uAA are negative, which indicates that the utility function is concave with respect to the consumption, and the second derivative of the addictive stock is negative. The sign of uCA speciﬁes whether addictive stock A increases or decreases the marginal utility of current consumption, uC . Becker et al. (1994) interpret the positive value of 1 as evidence for addictiveness of substances (e.g., cigarettes) since a higher marginal utility of current consumption driven by a greater addictive stock (i.e., uCA > 0) represents reinforcement effects. Moreover, if 2 is signiﬁcant and positive, addictive behaviors toward the substance are assumed to be rational. The rationale supporting this premise is that addicts consider not only the effects of past consumption (i.e., backward-looking) but also the effects\n",
              "\n",
              "This study extends the model of rational addiction (Becker and Murphy 1988) to investigate whether addiction to social apps (e.g., SNSs and social games) follows the patterns of utility-maximizing, rational behaviors. Addiction to social apps can be viewed as one form of technological addiction (Turel et al. 2011). We reﬁne the analytical and empirical components of the frameworks architected by Becker and Murphy (1988) and Becker et al. (1994) in the context of mobile social apps, which are nonphysical and monetary-free commodities. We test the models on panel data gathered weekly, examining app consumption behaviors at the individual user level. This approach is in contrast to those of previous studies in which aggregate-level and yearly or quarterly data were used. 3.1. Social Exchanges as Economic Actions Social exchange theory (Homans 1958), which originates from the scientiﬁc traditions of neoclassical economics paradigms, offers a conceptual foundation for comprehending social relationships through economic principles. This canonical theory maintains that social behaviors can be construed as the upshot of negotiated exchange processes, in which rational actors navigating in social situations select behaviors that maximize their own self-interests. If the reward or utility derived from social interaction outweighs the punishment or cost, they cultivate the ensuing relationship. Under an excessively high cost, however, they suspend interpersonal associations. Regarded as an economic metaphor for social relationships, social exchange theory is underpinned by several key premises. First, individuals engaging in social relationships can rationally and successfully gauge the costs and beneﬁts\n",
              "\n",
              "\n",
              "924 of social exchanges. Second, individuals involved in exchange processes rationally seek to maximize payoffs or rewards to satisfy their basic social needs. Finally, exchange processes and outcomes alter power and privilege structures in social groups because of the competitive nature of social systems. In our model, the act of consuming social apps (e.g., exchanging messages through SNSs or playing social games) is viewed as an enacted exchange process, in which app users endeavor to maximize payoffs by enhancing their social presence and privileges within the time frame in which the interaction takes place. Users of social apps are forward-looking and rational in that they anticipate the future consequences of their current app consumption. Their social exchange relationships are determined by strategic reward–cost calculations. Consistent with the notion of adjacent complementarity, the current consumption of social apps increases the marginal utility of future consumption; as a result, users increase their current app consumption when they expect social liquidity1 to rise in the future. Users consuming social apps are therefore typiﬁed with a drive to form and maintain stable social bonds and attachments, which are reinforced through the use of such apps. 3.2. Social Liquidity and Exchange Dynamics In Becker and Murphy’s (1988) model, the price of addictive substances regulates addicts’ intertemporal consumption preferences; that is, the anticipated future prices of addictive commodities inﬂuence current consumption because they affect future stocks and consumption patterns. As indicated in the model, governments, for example, can motivate smokers to lessen their current cigarette consumption by preannouncing a cigarette tax increase. In this scenario, increased future prices negatively affect current consumption because future consumption and utility may decrease. Becker et al. (1994) ﬁnd that a 10% increase in cigarette prices can reduce current consumption by as much as 7.5% in the long run. Moreover, as a consequence of the preannouncement a 10% price increase in one period reduces consumption in the previous period by 0.6% and in the subsequent period by 1.5%. These correlation patterns demonstrate the intertemporal associations in cigarette demand that result from rational addictive behaviors. For addicts of mobile social apps, no direct payment is necessary to consume the apps once they have been downloaded and installed onto mobile devices. In fact, mobile apps can be distinguished from physical addictive goods (e.g., cigarettes and alcohol) in several ways. Most mobile apps are available free of charge, but most physical addictive goods are not. In addition, while\n",
              "1\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "Details on social liquidity are presented in Section 3.2.\n",
              "\n",
              "negative health-related consequences from excessive doses of physical addictive substances can be immediately recognized, physical damage caused by heavy consumption of mobile apps may be relatively difﬁcult to notice in the short term. Moreover, mobile apps can be downloaded and consumed by anyone, anywhere, whereas physical addictive goods, like cigarettes, can be purchased and consumed only by adult consumers in speciﬁc areas. Finally, interaction and reciprocity stimulate the consumption of social mobile apps, such as SNSs and social games, but such social intercourse, although relevant, is less important in the consumption of physical addictive products. Just as price affects the consumption of physical commodities, social liquidity (i.e., the ease with which one can establish interpersonal relationships) determines how much users consume the social commodities (e.g., social apps). In ﬁnancial markets, market liquidity, deﬁned as how easily an asset can be traded and converted to cash with minimal impact on price, often has a signiﬁcant effect on traders’ willingness to engage in trading activities. If all else is equal, the greater the liquidity within a market, the larger the beneﬁts investors can gain. The size of the market (e.g., the number of active traders) often serves as a key catalyst that galvanizes the degree of market liquidity (Pagano 1989). Furthermore, because of the network externality effect, the principle of liquidity attracts liquidity is often demonstrated in the ﬁnancial market (Roll et al. 2009). In this respect, utility from consuming social apps for users is contingent on the amount of social liquidity they perceive to be available. This is similar to the notion of network effects (Katz and Shapiro 1985) in which a user’s utility derived from consuming a particular social app escalates exponentially as the number of people using it increases. Recently, several studies focusing on onlinebased social networks (e.g., Susarla et al. 2012, Zeng and Wei 2013) demonstrated the important role such positive consumption externalities play in disseminating user-generated content across social network platforms (e.g., YouTube and Flickr). Consequently, rational, forward-looking app users form expectations about changes in social liquidity in future periods and establish optimal consumption plans that maximize their discounted utility in the present. Following the lead of Freud et al. (1930) in his research on social dynamics, theorists regard the need to belong and social attachment as intrinsic human motivators (Baumeister and Leary 1995). In certain respects, social platforms (e.g., Facebook) can be viewed as social markets where individuals seek and reﬁne social attachments by “trading” (i.e., making or breaking) interpersonal relationships. Relationships may be newly formed, strengthened, and often ended on Facebook and in social games. In essence, several\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "925 The resulting demand function of social apps represents current consumption, Ci t , as a function of past (Ci t−1 and future consumptions (Ci t+1 , as well as the current degree of social liquidity, Lt Ci t =\n",
              "0\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "features unique to these platforms, such as Facebook “likes” and in-app currency/credit exchanges, are designed to increase social liquidity. These artifacts foster and lubricate social exchanges through shared emotions, thoughts, and gossip. Users consume social apps to form or maintain lasting, positive, and affectively pleasant relationships with other people within or across their usual social boundaries (Ellison et al. 2007, Steinﬁeld et al. 2008). Furthermore, they often endeavor to enhance their social presence and authority by receiving recognition and soliciting caring comments from others. The more users involved and the greater the social liquidity within a platform, the higher the utility derived from the exchanges. As a consequence, social liquidity (i.e., the number of users available with whom social relationships can be cultivated) inﬂuences how actively individuals engage in online social activities (Fang et al. 2013). Social liquidity also affects their ability to utilize their time resources efﬁciently for the purpose of forming and maintaining interpersonal relationships. Therefore, rational app addicts will anticipatorily increase their current consumption of social apps when they expect future social liquidity to increase. They appraise the utility gained from consuming social apps as outweighing the discounted reduction in utility arising from the negative consequences of doing so (e.g., escapism, procrastination, preoccupation, poor time management). 3.3. The Model Based on the discussion above, we posit that an individual’s utility depends on the factors speciﬁed in Equation (2) Ui t = u C i\n",
              "t\n",
              "\n",
              "+\n",
              "\n",
              "1 Ci t −1\n",
              "\n",
              "+\n",
              "\n",
              "2 Ci t +1\n",
              "\n",
              "+\n",
              "\n",
              "3 Lt\n",
              "\n",
              "(4)\n",
              "\n",
              "where 1 = −uCA / ucc + uAA / 1 + r , 2 = 1 / 1 + r , 3 = −uLC / ucc + uAA / 1 + r , r is an interest rate, and ULC is a second-order partial derivatives of the utility function with respect to liquidity and consumption. In keeping with Becker et al. (1994), a positive value of 1 indicates that a good is addictive. In addition, a positive value of 2 denotes that an addiction is the result of rational forward-looking behavior. According to Becker and Murphy (1988, p. 682), individuals with a greater preference for the present consumption are more likely to become addicted and behave myopically than those with a greater preference for the future. Thus, the ratio of the estimated coefﬁcients on past consumption to those on future consumption, 1 / 2 , implies the rate of time preference. As r = 1 / 2 − 1, the estimated interest rate captures the rate of time preference.\n",
              "\n",
              "4.\n",
              "\n",
              "Empirical Validation\n",
              "\n",
              "We provide a brief overview of the empirical background and variables derived from the data and illustrate our econometric speciﬁcation models. In addition, we discuss how we identiﬁed the estimates of the parameters, after which we present our ﬁndings. 4.1. Empirical Context and Data Description We constructed a panel data set consisting of information on app time use for two mobile social apps—a widely used social networking app (Facebook) and a popular social gaming app (Anipang). Facebook is a major SNS through which people build and maintain relationships. For example, users may become Facebook friends, sharing their thoughts, opinions, photos, videos, and links to other sites they ﬁnd interesting. Anipang is a platform-based, social puzzle game. The mobile messaging platform allows users to ﬁnd friends who are also playing the puzzle game. The basic structure of Anipang is similar to another popular social puzzle game—Candy Crush. After completing a round, players can check their ranking compared to those of their friends in a social messaging app with a leaderboard for those who play the game. Each game only lasts for one minute. To continue the game, players must acquire in-app currency in the form of “hearts.” Players may choose to wait several minutes to get more hearts. Alternatively, players give each other hearts as gifts or get more hearts by inviting friends on the social messaging app list to join the\n",
              "\n",
              "Yi\n",
              "\n",
              "t\n",
              "\n",
              "Ai\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "(2)\n",
              "\n",
              "where Ci t is the amount of social app consumption of individual i at time t . In our context, no direct monetary cost is associated with consuming social apps, but a user incurs an opportunity cost, i.e., the value of what other activities the user could have done during that time. Yi t refers to the time spent on these outside options of i at time t . Accordingly, it can be expressed as a function of Ci t , [Yi t = W − Ci t ], where W represents the total amount of time (i.e., 168 hours per week) allowed to each individual on any given day, denoting a time budget constraint. Ai t indicates the amount of addictive stock of i at time t . Finally, Lt reﬂects the degree of social liquidity at time t . A utility-maximizing demand function subject to the time budget can be derived using the following ﬁrst-order condition (see Appendix A for the detailed model derivation process) u Ci\n",
              "t\n",
              "\n",
              "Ai Ci t\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "+\n",
              "\n",
              "1 u Ci 1+r\n",
              "\n",
              "t +1\n",
              "\n",
              "Ai t+1 Lt+1 =0 Ci t\n",
              "\n",
              "(3)\n",
              "\n",
              "\n",
              "926 game. These mechanisms encourage users to communicate continuously with other users. The data used in this study were provided by Nielsen Koreanclick, the Korean division of the global market research company specializing in online and mobile Internet audience measurement. Audience measurement measures how many people are in an audience and how long they remain, usually in relation to television viewership (e.g., Nielsen ratings) or trafﬁc on websites and mobile apps. Nielsen Koreanclick maintains a panel of mobile app users, ranging in age from 7 to 69 years old, who are selected by stratiﬁed random sampling. After individuals agree to be panel members, they download and install a Nielsen Mobile App on their mobile devices. This app runs in the background and collects data on panel members’ use of mobile apps and the mobile websites even while disconnected. The meter app regularly transmits encrypted log ﬁles to a server via a secure cellular connection or Wi-Fi. We used data collected between October 1, 2012, and October 27, 2013 (56 weeks). The data for panel members included individual-level weekly information on these users’ activities on the aforementioned social apps throughout the sampling period. In addition, we acquired individual-level information on user demographics such as age, gender, income, and education. 4.2. Model Estimation and Identiﬁcation To identify the impact that past and future consumption of a particular mobile social app has on the current consumption at the individual user level, we consider the following econometric model: Ci t =\n",
              "0\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "+\n",
              "\n",
              "1 Ci t −1\n",
              "\n",
              "+\n",
              "\n",
              "2 Ci t +1\n",
              "\n",
              "+\n",
              "\n",
              "3 Lt\n",
              "\n",
              "+\n",
              "\n",
              "i\n",
              "\n",
              "+\n",
              "\n",
              "t\n",
              "\n",
              "+ ui\n",
              "\n",
              "t\n",
              "\n",
              "(5)\n",
              "\n",
              "where the subscript i denotes the ith user, the subscript t denotes the t -th week, Ci t is the consumption of a mobile social app (measured in seconds) by user i at time t , Lt is the number of active users in the platform during time t , i is a user-speciﬁc effect, t is a weekspeciﬁc effect, and ui t is a remainder disturbance. Use of the rational addiction model in the context of mobile social apps poses several endogeneity problems due to the presence of lead and lag with respect to the dependent variable, the potential simultaneity between the number of active users and the dependent variable, and a potential serial correlation among the disturbances. To address these issues, we adopt Blundell and Bond’s (1998) system generalized method of moments (GMM) estimator which uses the lagged differences of endogenous variables as instruments for the model in levels, in addition to the lagged levels of endogenous variables as instruments for the model in ﬁrst differences. The system GMM estimator is considered more robust than difference GMM because the former combines the moment conditions for the model\n",
              "\n",
              "in ﬁrst differences with the moment conditions for the model in levels, whereas the latter focuses solely on the ﬁrst set of conditions (Blundell et al. 2001).2 By contrast to typical models that use dynamic panel data (e.g., Arellano and Bond 1991), our model includes not only Ci t−1 but also Ci t+1 as endogenous exploratory variables. This feature prevents us from employing typical two-period lagged variables (e.g., Ci t−2 as instruments for endogenous variables because Ci t−2 is a direct function of Ci t−1 , which itself is an endogenous variable. Given this situation, we used higher lagged variables Ci t−3 Ci t−k and Li t−3 Li t−k as instruments for the model in ﬁrst differences and Ci t−2 and Li t−2 as instruments for the model in levels. To determine whether this set of instruments is valid for use with our data and satisﬁes the requirements necessary for analyzing panel data, we performed the Hansen test for the overidentiﬁcation of instruments. We also carried out the difference-inHansen test of instrument exogeneity, which includes not only the lagged levels of endogenous variables (i.e., instruments for difference GMM) but also the lagged differences of endogenous variables (i.e., additional instruments for system GMM). The results conﬁrm the validity of our instruments (10% signiﬁcance level). Additionally, we conducted auto-regression (AR) tests for the autocorrelation of the residuals to ensure the appropriateness of our lagged endogenous variables. By construction, the residuals of the differenced Equation (5) should exhibit serial correlation because of the lead and lag of the dependent variable; that is, we should reject Arellano–Bond (Arellano and Bond 1991) tests for AR(1) and AR(2). However, if the assumption of serial independence in the original errors is warranted, the differenced residuals should not exhibit signiﬁcant AR(3) behavior. Our results are consistent with our expectations in that they allow for the rejection of the Arellano–Bond test for AR(1) and AR(2) but not for AR(3) at the 5% signiﬁcance level. This set of instruments is thus deemed suitable for use with the panel data. 4.3. Main Results Table 1 presents the descriptive statistics for the variables included in the analysis. The average user in our sample spent 3,039 and 4,889 seconds (or 51 and 81 minutes, respectively) per week on the SNS and social game, respectively. Younger users spend almost ﬁve times more time on SNSs than do older counterparts, but the latter group devotes twice as much time on social games compared to the former. No difference was detected between female and male users with\n",
              "2\n",
              "\n",
              "We replicated the same analysis using the difference GMM estimation. Although the magnitude of coefﬁcients slightly varies, the overall results remain more or less unchanged (see Appendix B).\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "927\n",
              "Figure 1 Weekly Trends of Consumption and Number of Users\n",
              "\n",
              "Table 1 App SNS Social game\n",
              "\n",
              "Summary Statistics Variables C L C L Mean 3 039 18 1 383 50 4 888 95 757 16 Age 7–29 30–69 1,140 56.2 5,629 74.9 Education High school graduate University graduate 1,706 61.8 5,122 73.6 Std. dev. 5 345 58 111 87 6 729 00 369 57 Min. 0 1,223 0 377 Max. 46 1 122 1 748 623 330 730\n",
              "\n",
              "Weekly total consumption (106 sec.)\n",
              "\n",
              "25\n",
              "SNS Social game\n",
              "\n",
              "20\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "15\n",
              "\n",
              "Gender Female 3,640 45.1 4,989 50.1 Income Below $3,000 3,086 25 3,983 24.8 Above $3,000 3,023 75 5,187 75.2 Male 2,544 54.9 4,787 49.9\n",
              "\n",
              "10\n",
              "\n",
              "SNS Social Game\n",
              "\n",
              "C % C %\n",
              "\n",
              "5,468 43.8 2,680 25.1\n",
              "\n",
              "5\n",
              "0 20 40 60\n",
              "\n",
              "Week\n",
              "2,000\n",
              "\n",
              "SNS Social Game\n",
              "\n",
              "C % C %\n",
              "\n",
              "1,949 9.9 5,859 13.4\n",
              "\n",
              "Weekly average number of active users\n",
              "\n",
              "1,500\n",
              "\n",
              "Notes. C indicates the consumption of a mobile social app (measured in seconds), and L denotes the number of active users in the platform. For the education category, 768 and 237 users of SNSs and social games, respectively, were identiﬁed as students who were enrolled in high school or below.\n",
              "\n",
              "1,000\n",
              "\n",
              "500 0 20 40 60\n",
              "\n",
              "respect to consumption amount for social games, but the former spent more time on SNSs than did the latter. High school graduates with no college degree consume both SNSs and social games more actively than do university graduates. Finally, high- and low-income users are almost evenly distributed in terms of SNS usage time, but the latter cluster spends far more time on social games than the former. Figure 1 graphically displays how the number of users and their usage amount changed over time. Table 2 shows the main results of the model obtained using the system GMM estimation (Blundell and Bond 1998). We ﬁnd that ˆ 1 and ˆ 2 are positive and statistically signiﬁcant for both social apps, which indicates that users exhibit rational addictive behaviors when utilizing mobile social apps. Interest rates are also positive and statistically signiﬁcant for both social apps, demonstrating that, in our context, the reinforcement effect from past consumption is larger than the forward-looking rationality of future consumption. For example, the ﬁrst column (i.e., Facebook) shows that the interest rate is positive (0.214) due to the larger coefﬁcient of past consumption (0.244) than the coefﬁcient of future consumption (0.201). Similarly, in the second column (i.e., Anipang), the coefﬁcient of past consumption (0.275) is larger in magnitude than the coefﬁcient of future consumption (0.252); thus, the interest rate is positive (0.09). For both social apps, exogenous factors that promoted past consumption by\n",
              "\n",
              "Week\n",
              "\n",
              "10% would increase current consumption by between 2% and 3%. Similarly, a shock that would increase consumption by 10% in the future would lead to a rise of between 2% and 3% in current consumption. We ﬁnd that ˆ 3 is positive and statistically significant for both social apps (1.934 and 4.174 for SNS and social games, respectively), which suggests that\n",
              "Table 2 GMM Estimates of Rational Addiction Model Ci Variables Ci Ci\n",
              "t −1 t\n",
              "\n",
              "Social networking service Facebook 0 244∗∗∗ 0 037 0 201∗∗∗ 0 024 1 934∗∗∗ 0 248 0 214 107,685 2,710 0 833 0 196\n",
              "\n",
              "Social game Anipang 0 275∗∗∗ 0 018 0 252∗∗∗ 0 058 4 174∗∗∗ 0 597 0 091 52,071 1,824 0 862 0 828\n",
              "\n",
              "ˆ1 ˆ2\n",
              "\n",
              "t +1\n",
              "\n",
              "Lt ˆ 3 Interest rate, r Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "Note. Robust standard errors are in parentheses. ∗∗∗ p < 0 001.\n",
              "\n",
              "\n",
              "928 as social liquidity grows, rational app addicts increase their consumption of social apps. For example, SNS apps allow people to communicate with each other socially. They can exchange messages and receive automatic notiﬁcation when their friends update their proﬁles. They can also regulate their app consumption according to the number of active users on the platform. In the context of social game apps, leader scoreboards help users learn about and predict the usage patterns of their friends more accurately and manage their intertemporal consumption wisely. Furthermore, players often invite their friends to be able to play more. Such incentives built into social games can also make players regulate their consumption rationally. Drawing on the insights provided by Becker et al. (1994), we identify three types of elasticity related to intertemporal liquidity to quantify how changes in the past, current, and future social liquidity may affect the volume of current consumption. Figure 2 presents a schematic representation that visualizes the effects of changes in liquidity in period = 20 on app consumption in period ± t t ≥ 0 with respect to the two types of social apps. Speciﬁcally, Figure 2 displays percentage increases in app consumption in period t in response to a 1% increase in liquidity in period t = 0. To illustrate further, t = 0 denotes the effects of changes in current liquidity on current consumption. While t = −5 denotes the extent to which changes in future liquidity ﬁve weeks after the current period t = 0 affect current consumption t = −5 , t = 5 indicates how variations in past liquidity ﬁve weeks prior to the current period t = 0 inﬂuenced the volume of current consumption t=5 . Figure 2 portrays several noteworthy empirical regularities surrounding the differential effects of social liquidity on use of the two types of social apps. Users of SNS are more sensitive to changes in social liquidity than users of social games. In addition, users of both apps become most responsive to the current liquidity change. Furthermore, as opposed to social game\n",
              "Figure 2\n",
              "1.0\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "users, who react similarly to the same level of liquidity increases in both future and past periods, users of SNS appear to be more sensitive to such increases in past periods than those in the future. 4.4. Subsample Analyses Addictive behaviors may vary based on demographic factors and user characteristics (Chaloupka 1991). Social inﬂuence and latent homophily have been found to signiﬁcantly affect the behaviors of online users (Ma et al. 2014). To account for the varying degrees of addiction to mobile social apps according to user-related factors (e.g., age, gender, education, and income), we performed estimations using separate demand equations for different user groups. Speciﬁcally, we divided the sample into two subsamples according to age (7–29 years old and 30–69 years old) and gender (male and female), two different subsamples according to education (high school graduates and university graduates), and two more subsamples according to income (≤ $3,000/month and > $3,000/month).3 Our subgroup analysis indicates that substantial addictiveness and rationality (time preferences) variations exist across groups of diverse demographic characteristics (Table 3). In the case of the SNS (panel 1), older users exhibit only myopic usage patterns and no rational addiction behaviors ( ˆ 2 , p > 0 05). Given their expanded social circle, older users are likely to exhibit more relational diversity and complexity than younger users (Antonucci and Akiyama 1987). The latter maintains relatively more homogeneous relationships because their social circle is narrower and thinner (e.g., schools) than that of older users (Morris et al. 2004). Diverse and complex social ties may cause older users to experience more difﬁculty in exercising forwardlooking relational management than that encountered by their younger counterparts (Baumeister et al. 1998). A similar pattern was observed among users with less education. Higher-income groups do not exhibit rational addiction behaviors and are characterized as myopic addicts. The ﬁndings on social games align with expectations (panel 2); that is, younger groups exhibit signiﬁcant myopic addiction propensities. In addition to myopic addiction, rational addiction is more pronounced in some groups than others. On the basis of the procedures recommended by Clogg et al. (1995), we carried out a formal statistical test to conﬁrm whether signiﬁcant differences exist across various demographic groups in terms of the degree of\n",
              "3\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "Social Liquidity Elasticity\n",
              "\n",
              "Increase in app consumption (%)\n",
              "\n",
              "SNS 0.8\n",
              "\n",
              "Social game\n",
              "\n",
              "0.6\n",
              "\n",
              "0.4\n",
              "\n",
              "0.2\n",
              "\n",
              "0 – 20 –15 –10 –5 0 5 10 15 20\n",
              "\n",
              "t (week relative to week of liquidity increase)\n",
              "\n",
              "The age category was divided into two groups (younger and older) on the basis of the PewResearch Center’s classiﬁcation scheme (see http://www.pewinternet.org/2015/04/01/us-smartphone-use -in-2015/). We used the same reference to separate high-income users from low-income users. The income category includes users only in the 30–69 age group.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "929\n",
              "\n",
              "Table 3\n",
              "\n",
              "Subsample Analyses Results Age 7–29 30–69 Ct Female Ct Gender Male Ct Education High school graduate Ct University graduate Ct Below $3,000 Ct Income Above $3,000 Ct\n",
              "\n",
              "Variable\n",
              "\n",
              "Ct\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "Panel 1: SNS Ci\n",
              "t −1\n",
              "\n",
              "ˆ1\n",
              "\n",
              "z -score Ci t +1 ˆ2 Lt ˆ 3 z -score Interest rate r Observations Number of users Serial correlation (p-value) Instrument validity (p-value) Ci ˆ1\n",
              "\n",
              "0 313∗∗∗ 0 129∗∗∗ 0 043 0 036 3 29∗∗∗ 0 274∗∗∗ 0 017 2 732∗∗∗ 0 430 3 99∗∗∗ 0 142 51,372 1,189 0 547 0 645 0 221 0 060\n",
              "∗∗∗\n",
              "\n",
              "0 335∗∗∗ 0 051 1.44 0 270∗∗∗ 0 020 2 636∗∗∗ 0 406 3 46∗∗∗ 0 241 47,513 1,223 0 745 0 062\n",
              "\n",
              "0 255∗∗∗ 0 023 0 180∗∗∗ 0 032 0 942∗∗∗ 0 274 0 417 60,172 1,487 0 853 0 328\n",
              "\n",
              "0 484∗∗∗ 0 152∗∗∗ 0 092 0 027 3 46∗∗∗ 0 166 0 127 2 275∗∗∗ 0 548 2 49∗∗∗ Myopic 10,297 268 0 985 0 720 0 329∗∗∗ 0 031 0.94 0 434∗∗∗ 0 049 2 635∗∗∗ 0 473 −1 32 −0 242 7,853 244 0 115 0 421 −0 055 39,484 1,343 0 427 0 194 0 310∗∗∗ 0 060 3 718∗∗∗ 0 674 0 099∗∗∗ 0 038 0 776∗∗ 0 251 0 542 64,811 1,674 0 110 0 123 0 293∗∗∗ 0 022\n",
              "\n",
              "0 309∗∗∗ 0 145∗∗∗ 0 051 0 037 2 61∗∗∗ 0 255∗∗∗ 0 070 0 240∗∗∗ 0 091 −1 42 0 212 10,794 294 0 658 0 320 0 326∗∗∗ 0 055 0.02 0 337∗∗∗ 0 325∗∗∗ 0 083 0 057 2 950∗∗∗ 3 719∗∗∗ 0 581 0 670 −0 87 −0 033 −0 001 10,092 31,780 307 1,059 0 032 0 566 0 026 0 566 0 081 0 056 0 673∗ 0 291 Myopic 45,519 1,227 0 061 0 240 0 325∗∗∗ 0 019\n",
              "\n",
              "0 045 0 053 0 745∗∗∗ 0 251 Myopic 56,313 1,521 0 044 0 196 0 307 0 017\n",
              "∗∗∗\n",
              "\n",
              "Panel 2: Social game\n",
              "t −1\n",
              "\n",
              "z -score Ci t +1 ˆ2 Lt ˆ3 z -score Interest rate, r Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "−1 38 0 124 0 092 3 337∗∗∗ 0 706 −0 73 Myopic 10,199 458 0 075 0 819 0 299∗∗∗ 0 055 4 014∗∗∗ 0 610 0 027 41,872 1,366 0 968 0 188\n",
              "\n",
              "0 244∗∗∗ 0 327∗∗∗ 0 021 0 022 −2 73∗∗∗ 0 191∗∗∗ 0 354∗∗∗ 0 036 0 071 5 241∗∗∗ 2 827∗∗∗ 0 571 0 589 2 94∗∗∗ 0 278 −0 076 27,668 24,403 914 910 0 946 0 778 0 680 0 847\n",
              "\n",
              "Notes. Robust standard errors are in parentheses. z -scores indicate cross-group differences in coefﬁcients (Clogg et al. 1995). ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              "\n",
              "addictiveness and responsiveness to social liquidity.4 For example, on the basis of the extent of addictiveness, addiction to SNS is more pronounced for younger, less-educated, and lower-income users. However, no gender difference was detected with respect to SNS addiction. Furthermore, addicts who are younger, female, and less-educated are more responsive to social liquidity within SNSs than other groups. Male users of social games are more prone to addiction but less\n",
              "We conducted a formal statistical test proposed by Clogg et al. (1995), in addition to a simple comparison of coefﬁcients on the basis of their absolute values, to understand the differences across the various demographic groups. Clogg et al. (1995) suggested a z-test of cross-group differences in regression coefﬁcients that is based on the following z-score: z = ˆ 2 − ˆ 1 / s 2 ˆ 1 + s 2 ˆ 2 1/2 , where ˆ 1 and ˆ 2 are estimated regression coefﬁcients of each group (i.e., subsample), and the function s 2 returns the estimated variance. Using the statistical procedures commonly adopted in diverse disciplines (e.g., Amato and Sobolewski 2001, Hribar and McInnis 2012, Jiang et al. 2012), we identiﬁed differences across various demographic clusters.\n",
              "4\n",
              "\n",
              "sensitive to social liquidity than are their female counterparts. However, with the exception of gender, no statistical difference was observed for the rest of the variables. Hence, the results of our subsample analysis demonstrate that the extent of myopic or rational addiction to social apps varies across diverse demographic groups and app categories. 4.5. Robustness Checks\n",
              "\n",
              "4.5.1. The Myopic Model of Addiction. Because a future consumption term in Equation (5) is derived from the second term of the ﬁrst-order condition (Equation (A10) in Appendix A), a signiﬁcant value for ˆ 2 indicates that each individual carefully considers the impact of current consumption on future utility and consumption (Becker et al. 1994). However, the myopic model does not incorporate the future utility term in the ﬁrst-order condition; instead, it only includes past consumption. Therefore, the myopic model is likely to overestimate the impact of past consumption. As expected, Table 4 shows that the myopic model of\n",
              "\n",
              "\n",
              "930\n",
              "Table 4 GMM Estimates Using the Myopic Model of Addiction Ct Variables Ci\n",
              "t −1\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "Table 5\n",
              "\n",
              "A Simultaneous Estimation to Validate the Presence of a Common Addictive Stock Ci\n",
              "t\n",
              "\n",
              "SNS Facebook 0 288 0 030 2 030 0 252\n",
              "∗∗∗\n",
              "\n",
              "Social game Anipang 0 372 0 030\n",
              "∗∗∗\n",
              "\n",
              "ˆ1\n",
              "\n",
              "Variables Ci Ci\n",
              "t −1\n",
              "\n",
              "Social networking service Facebook 0 279∗∗∗ 0 037 0 245∗∗∗ 0 021 1 797∗∗∗ 0 248\n",
              "\n",
              "Social game Anipang 0 303∗∗∗ 0 017 0 295∗∗∗ 0 053 3 580∗∗∗ 0 514 0 017 0 018 0 004 0 019 −0 031 0 020 0 027 52,071 1,824 0 824 0 934\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "ˆ1 ˆ2\n",
              "\n",
              "Lt ˆ 3 Observations Number of users\n",
              "\n",
              "∗∗∗\n",
              "\n",
              "4 666 0 445\n",
              "\n",
              "∗∗∗ t +1\n",
              "\n",
              "110,395 2,710\n",
              "\n",
              "53,895 1,824\n",
              "\n",
              "Lt ˆ 3 Ci Ci Ci\n",
              "t −1\n",
              "\n",
              "Note. Robust standard errors are in parentheses. ∗∗∗ p < 0 001.\n",
              "\n",
              "ˆ4\n",
              "\n",
              "−0 009 0 009 0 006 0 011\n",
              "\n",
              "addiction overestimates the effects of past consumption for both SNS (0 288 > 0 244 p-value < 0 05) and social games (0 372 > 0 275 p-value < 0 05). 4.5.2. Simultaneous Estimation. We derived and analyzed an integrated model that simultaneously reﬂects users’ consumption behaviors over SNSs and social games. We extended our model to enable the two addictive apps (i.e., SNS and social game) to share a common addictive stock (Equation (6)). Equation (A8) in Appendix A demonstrates that each app has its own separate addictive stock that is equal to the consumption of the app for the previous period. Equation (6), which is a modiﬁed version of Equation (A8) in Appendix A, indicates that every user has one addictive stock common to SNSs and social games; this stock is assumed equal to the sum of the consumption of both social apps for the previous period Ai t = Ci\n",
              "t −1\n",
              "\n",
              "t\n",
              "\n",
              "ˆ5 ˆ6\n",
              "\n",
              "t +1\n",
              "\n",
              "−0 011 0 010 0 139 107,685 2,710 0 882 0 555\n",
              "\n",
              "Interest rate, r Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "Note. Robust standard errors are in parentheses. ∗∗∗ p < 0 001.\n",
              "\n",
              "+C\n",
              "\n",
              "i t −1\n",
              "\n",
              "(6)\n",
              "\n",
              "On the basis of the procedures described in the paper, we obtained the following demand function: Ci t =\n",
              "0\n",
              "\n",
              "+\n",
              "\n",
              "1 Ci t −1\n",
              "\n",
              "+\n",
              "\n",
              "2 Ci t +1 5C i t\n",
              "\n",
              "+\n",
              "\n",
              "3 Lt\n",
              "\n",
              "+\n",
              "\n",
              "4 C i t −1\n",
              "\n",
              "+\n",
              "\n",
              "+\n",
              "\n",
              "6 C i t +1\n",
              "\n",
              "(7)\n",
              "\n",
              "where 0 = − a1 + a3 / 1 + r / ucc + uAA / 1 + r , 1 = −uCA / ucc + uAA / 1 + r > 0, 2 = 1 / 1 + r > 0, 3 = −uLC / ucc + uAA / 1 + r > 0, 4 = 1 = −uCA / ucc + uAA / 1 + r > 0, 5 = − ucc + uAA / 1 + r / ucc + uAA / 1 + r , 6 = uc A /ucA 4 / 1 + r , and r is the interest rate. We then estimated Equation (7) by using the system GMM estimator. The reﬁnement allowed us to determine that the estimated coefﬁcients of all of the additional variables (i.e., 4 , 5 , and 6 are insigniﬁcant and that the estimated coefﬁcients of the existing variables (i.e., 1 , 2 , and 3 are robust against this speciﬁcation (Table 5). This ﬁnding indicates that SNSs and social games do not share a common addictive stock; rather, each app separately accumulates its addictive stock.\n",
              "\n",
              "4.5.3. Forward-Looking Behavior. Most empirical research on rational addiction relies on the assumption that individuals can forecast future prices accurately. This assumption has been criticized because very few price increases are announced a year in advance (Gruber and K˝ oszegi 2001). To redress this oversight, Gruber and K˝ oszegi (2001) suggested an alternative mechanism for testing forward-looking behaviors using monthly cigarette consumption data. They found that tax increases that are yet to be implemented lead to decreased consumption of cigarettes in the present, which is evidence of forward-looking behaviors. In keeping with Gruber and Köszegi’s (2001) approach, we test whether users of social apps exhibit forward-looking behaviors. We consider the following model for a speciﬁc event for each social app: Ci t = + · EVENT t + · Preannouncet + · Ii + · Tt + (8)\n",
              "\n",
              "where Ci t is the amount of social app consumption by individual i at time t ; EVENT t is the dummy variable indicating that the event is actually launched at time t ; Preannouncet is the dummy variable indicating that the event is preannounced at time t ; and Ii and Tt are full sets of individual and week dummies, respectively. In this scenario, when an event is preannounced but not yet implemented, Preannouncet has a value of 1 and EVENT t has a value of 0. When an event is actually launched, both EVENT t and Preannouncet have a\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "931 similar to those obtained in the main analysis. This alternative measurement scheme for the liquidity construct ensures that our results remain robust under the assumption of homophilious behaviors among app users (i.e., users react more positively to other users with similar traits; McPherson et al. 2001). 4.6. Falsiﬁcation Tests Results Auld and Grootendorst (2004) demonstrated that the rational addiction model tends to yield spurious evidence in favor of the rational addiction hypothesis even for nonaddictive goods, such as milk, eggs, oranges, and apples, especially when aggregate data are used. Note that we do not use aggregate data, but individual user-level data, to estimate the rational addiction model in this study. Nevertheless, to address the issue of the potential spurious relations and disprove alternative explanations (e.g., mobile apps are generally addictive), we conducted falsiﬁcation tests. Speciﬁcally, we considered popular smartphone “utility apps” that are regarded as nonaddictive, such as camera, photo gallery, and address book. If these apps are not germane to rational addiction theory, our results in favor of the rational addiction hypothesis will be further strengthened. Table 8 shows the results of the falsiﬁcation tests. No signiﬁcant impact of future or past consumption is evident (95% conﬁdence level). Apps such as camera, photo gallery, and address book are neither addictive nor rational. Note that the samples used in this falsiﬁcation test are smaller than those used in the main analysis. To alleviate the concern that arises from the difference in sample size, we reduced the main data set into clusters of 8-week periods, which are equivalently sized as the data set used for the falsiﬁcation test. Accordingly, from the 56 weeks of data available in our data set, we extracted 49 combinations of the 8-week long data set (i.e., weeks 1–8, weeks 2–9, weeks 3–10, , and weeks 49–56) and used them to validate the rational addiction model. The estimated coefﬁcients of lagged consumption, Ci t−1 , are significantly positive across all 8-week estimations (100%). In the case of lead consumption Ci t+1 , statistical signiﬁcance was observed for 22 out of the 28 coefﬁcients (79%). These consistent results support our main ﬁndings and demonstrate that the signiﬁcance of the estimated consumption coefﬁcients was not derived by simple correlations of consumptions within extended periods. Consequently, we can successfully reject the falsiﬁcation argument and assert the rigor of our rational addiction framework in this context. We performed an additional veriﬁcation test to distinguish the effects of social apps from those of regular nonsocial apps. We chose the top three nonsocial game apps (Angry Birds, Minecraft, and Temple Run) for the additional falsiﬁcation test. These are standalone game apps that offer minimal or no social\n",
              "\n",
              "Table 6\n",
              "\n",
              "Effect of Preannouncement on App Consumption—Fixed Effects Model Ct\n",
              "\n",
              "Variables\n",
              "\n",
              "SNS Facebook 2 913 0∗∗∗ 167 8 460 6∗∗ 160 3 113,105 2,710\n",
              "\n",
              "Social game Anipang −276 1 449 5 −12 337 0∗∗∗ 303 3 55,719 1,824\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "EVENT t ˆ Preannounce t ˆ Observations Number of users\n",
              "\n",
              "Notes. Standard errors are in parentheses. A complete set of time dummy variables is included to account for ﬁxed time effects. Facebook Home was launched on April 17, 2013, and was preannounced on April 5, 2013, in Korea. Anipang for Sacheonseong was launched on February 19, 2013, and was preannounced on January 10, 2013. ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              "\n",
              "value of 1. For SNSs, we use the launch of “Facebook Home,” which provides new features for the Facebook app, as an event for Facebook. For social games, “Anipang for Sacheonseong,” which is a new game that competes against Anipang, is considered an external event. Table 6 presents the results of Gruber and Köszegi’s (2001) test for forward-looking behavior using these two events. The coefﬁcient of Preannouncet is positive and signiﬁcant for Facebook. Facebook users were found to increase their weekly consumption substantially (i.e., by as much as 461 seconds) when the dominant SNS company preannounced the launch of its Facebook Home app. This ﬁnding provides evidence of forward-looking behaviors, at least for this population. In the case of Anipang, the coefﬁcient of Preannouncet is negative and signiﬁcant. This shows that Anipang users had already reduced their weekly consumption signiﬁcantly (i.e., by as much as 12,337 seconds) when the top social game provider preannounced its future launch of a new competing game. One plausible explanation is that Anipang users forecast that the liquidity of Anipang would decrease when the new competing social game was launched. These results conﬁrm the forward-looking behavior of social app users, which is additional evidence in favor of the rational addiction model. 4.5.4. Alternative Measurement for Liquidity. In the absence of individual user-level liquidity information in our data, we conducted robustness checks by restricting liquidity to users with the same demographic proﬁle. Speciﬁcally, we deﬁne g i as a group of users who share the same demographic proﬁle with user i. As such, Lg i t stands for the liquidity of group g i , which we use in lieu of Lt in our model. As shown in Table 7, for all g i s based on age, gender, education, and income, the results remain qualitatively\n",
              "\n",
              "\n",
              "932\n",
              "Table 7 Robustness Check Results\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "Demographic group Age Variables Ct Panel 1: SNS Ci Ci\n",
              "t −1\n",
              "\n",
              "Gender Ct\n",
              "\n",
              "Education Ct\n",
              "\n",
              "Income Ct\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "ˆ1 ˆ2 ˆ3\n",
              "\n",
              "0 270∗∗∗ 0 035 0 231∗∗∗ 0 023 6 214∗∗∗ 0 896 0 169 107,685 2,710 0 887 0 249\n",
              "\n",
              "0 247∗∗∗ 0 037 0 207∗∗∗ 0 023 1 341∗∗∗ 0 186 0 193 107,685 2,710 0 839 0 738 Panel 2: Social game\n",
              "\n",
              "0 285∗∗∗ 0 032 0 259∗∗∗ 0 019 0 342∗∗∗ 0 095 0 100 107,685 2,710 0 879 0 792\n",
              "\n",
              "0 238∗∗∗ 0 038 0 196∗∗∗ 0 025 1 198+ 0 622 0 214 107,685 2,710 0 818 0 246\n",
              "\n",
              "t +1\n",
              "\n",
              "Lg i\n",
              "\n",
              "t\n",
              "\n",
              "Interest rate, r Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "Ci Ci\n",
              "\n",
              "t −1\n",
              "\n",
              "ˆ1 ˆ2 ˆ3\n",
              "\n",
              "0 305∗∗∗ 0 015 0 287∗∗∗ 0 051 10 500∗∗∗ 1 112 0 063 52,071 1,824 0 815 1 000\n",
              "\n",
              "0 277∗∗∗ 0 018 0 251∗∗∗ 0 059 8 186∗∗∗ 1 187 0 104 52,071 1,824 0 857 0 972\n",
              "\n",
              "0 317∗∗∗ 0 015 0 276∗∗∗ 0 052 1 469∗∗∗ 0 378 0 149 52,071 1,824 0 834 1 000\n",
              "\n",
              "0 296∗∗∗ 0 015 0 267∗∗∗ 0 054 7 269∗∗∗ 0 915 0 109 52,071 1,824 0 852 0 999\n",
              "\n",
              "t +1\n",
              "\n",
              "Lg i\n",
              "\n",
              "t\n",
              "\n",
              "Interest rate, r Observations Number of users Serial correlation (p-value) Instrument validity (p-value) Note. Robust standard errors are in parentheses. + p < 0 1; ∗∗∗ p < 0 001.\n",
              "\n",
              "functions (i.e., exchange of points and competitions). The falsiﬁcation results indicate that the consumption patterns for these nonsocial apps do not ﬁt the\n",
              "Table 8 GMM Estimates of the Rational Addiction Model Using Nonaddictive Apps Ct Variables Ci Ci\n",
              "t −1\n",
              "\n",
              "propositions of the rational addiction model (Table 9). This lends support to our arguments and empirical derivations.\n",
              "\n",
              "Table 9 Address book 0 359 0 247 0 438 0 229 −0 780 4 584 14,942 2,803 0 878 0 483 Variables Ci Ci\n",
              "t −1\n",
              "\n",
              "GMM Estimates of the Rational Addiction Model Using Nonsocial Apps Ct Angry Birds −0 022 0 111 −0 061 0 103 −1 343 5 420 1,190 379 0 555 0 973 Minecraft 0 016 0 121 0 206 0 154 6 690 21 160 377 141 0 677 0 999 Temple Run −0 158∗ 0 077 0 018 0 115 3 429 3 318 1,060 307 0 781 0 943\n",
              "\n",
              "Camera 0 279 0 302 0 022 0 069 1 293∗∗∗ 0 316 12,513 2,590 0 518 0 020a\n",
              "\n",
              "Gallery 0 283 0 177 0 332 0 173 1 140 0 614 12,071 2,451 0 280 0 106\n",
              "\n",
              "ˆ1 ˆ2\n",
              "\n",
              "t +1\n",
              "\n",
              "ˆ1 ˆ2\n",
              "\n",
              "Lt ˆ3 Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "t +1\n",
              "\n",
              "Lt ˆ 3 Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "Notes. Robust standard errors are in parentheses. We collected data between March 5, 2012, and April 29, 2012 (eight weeks), for the falsiﬁcation test. a We cannot reject the Hansen test of overidentiﬁcation at the 5% signiﬁcance level. ∗∗∗ p < 0 001.\n",
              "\n",
              "Notes. Robust standard errors are in parentheses. We collected data between July 23, 2012, and November 4, 2012 (15 weeks), for the additional falsiﬁcation test. ∗ p < 0 05.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "933 and “cold turkey” consumption patterns among rational addicts. In summary, unlike nonaddicts, rational addicts can experience an unstable steady state in which they become heavily addicted and consume a commodity in large quantities. Furthermore, rational addicts often experience more difﬁculty in overcoming addiction than do myopic addicts. Becker and Murphy (1988) assert that rational addicts may deliberately postpone terminating their addiction as they attempt to ﬁnd ways to optimally minimize short-run loss of utility from abruptly ceasing consumption. For example, rational smoking addicts initially endeavor to stop smoking by experimenting with easier aids (e.g., substituting gum chewing or jogging for smoking) until they ﬁnd the “best” method for quitting smoking with minimal shortterm loss of utility; that is, all factors being equal, rational addicts may suffer from more failures when attempting to terminate addiction given that they are more “fastidious” than myopic addicts in determining an optimal quitting method. This tendency means that rational addicts also remain at risk for adverse conditions. The emerging phenomenon of rational addiction to social apps was scrutinized in an empirical analysis of how individual user characteristics inﬂuence addictive behaviors. Most economics-based rational addiction studies, including Becker et al. (1994), attempt to demonstrate that addiction itself is a rational behavior. Such concentration can be explained by the fact that this stream of research is typically conducted at the aggregate level, without scholars referring to individual heterogeneity in terms of extent of rationality. In other words, these studies focus exclusively on the behaviors of “average” users given their reliance on aggregated, macrolevel consumption data (e.g., annual cigarette sales) that do not quite illuminate the speciﬁc behavioral patterns of individuals. At this level, some users are rational, whereas others are not; some addicts exhibit sounder forward-looking behaviors than do other addicts. Aggregate-level studies aimed at conﬁrming the rationality of addiction may only provide a partial insight into individual variations. Our individual-level and comprehensive panel data enabled us to determine that individuals with varying sociodemographic characteristics differ in their utility maximization trajectories and cost–beneﬁt rationalization over time. Addiction researchers, particularly those who study digital dependency, should therefore use individual-level data to disentangle structural differences in the ability of individuals to exercise selfcontrol and rationally manage their consumption over different time horizons. 5.2. Implications for Policymakers Regulatory agencies and lawmakers have become increasingly concerned over the addictive qualities of\n",
              "\n",
              "5.\n",
              "\n",
              "Discussion and Implications\n",
              "\n",
              "5.1. Implications for Addiction Research The rational addiction framework has been applied in distinct contexts to investigate various addictive behaviors. Most empirical studies have nonetheless been limited to physical commodities, such as alcohol, cigarettes, and drugs. Our alterations to and enrichment of the theoretical basis developed by Becker and Murphy (1988) can offer a fresh vantage point from which to study addictive conduct induced by nonphysical, nonmonetary, and highly social commodities. The results derived from the structure of intertemporal consumption demand reveal that individuals become more vulnerable to social app addiction under the following conditions. First, given that positive reinforcement drives the development of an addictive behavior, individuals who are exposed to stronger positive reinforcement and exhibit resilient consumption inertia are more susceptible to social app addiction. Second, the more sensitive an individual is to social liquidity, the greater her vulnerability to social app addiction. More vulnerable addicts will proactively increase their current consumption of apps when they expect future social liquidity or network effects to increase. Third, individuals who heavily discount the exacerbating effects of current consumption on future harmful consequences are more vulnerable to myopic addiction. These addicts tend to appraise the utility gained from consuming social apps as outweighing the discounted reduction in utility arising from adverse consequences. Our ﬁndings lend credence to the notion of rational addiction in the context of mobile social app consumption. Rational addiction to social games and SNSs can be interpreted as an indication that the users of these social goods are in control of their thinking and actions when confronted with consumption decisions. Caution must be used, however, when interpreting these empirical regularities. Rational addiction is still one form of addiction that can result in negative consequences. From a social welfare perspective, nonaddiction should be viewed as more ideal than rational addiction. Being “rational” means attempting to maximize utility over time given constraints and therefore exerts no direct implications for consumption quantity (Becker and Murphy 1988). Becker and Murphy (1988) alert us to the possibility that a small change in environmental factors (e.g., price change) will drive people to drastically change their consumption of addictive goods. They call this condition an unstable steady state and regard it as an important aspect of rational addiction theory. In their study, Becker and Murphy (1988) demonstrate that steady states easily become unstable when goods are highly addictive. According to the authors, the unstable state also explains binge\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "934 SNSs and social games, with such groups treating app usage as a growing social pandemic that affects many lives, especially those of young “vulnerable” people. These social apps may have already become pastimes for adolescents with narcissistic tendencies who, many experts believe, are likely to experience the adverse consequences of the mental escapism, procrastination, preoccupation, and poor time management that often accompany addictive behaviors; suicide may even be one of these ramiﬁcations (Kuss and Grifﬁths 2011). To address the digital addiction issue, government agencies have implemented preventive measures, but thus far only in a regulatory and coercive manner. Korea’s Ministry of Culture, Sports, and Tourism, for example, has been implementing the controversial 2010 law called the “Cinderella law,” which forbids online game providers from offering services to teenagers under 16 between midnight and 6 a.m. Similarly, the Vietnamese government imposes daytime and nighttime curfews, during which access to online games are blocked. The effectiveness of these intimidating directives has been extensively questioned because of their many obvious legal loopholes and evasions. On top of that, regulations and legal controls have achieved only temporary and limited success in curbing addiction (Song 2014). Successful intervention may be less a matter of domination and more an issue of understanding the root and nature of the problem. Why do people deviate from normal, nonaddicted, and prescriptive states and form an unhealthy attachment to substances or services? A simple explanation would be that they are either uninformed about the detrimental consequences of such indulgence or incapable of resisting immediate gratiﬁcation and dependence even with sufﬁcient awareness of potential harm. With this in mind, we offer two mechanisms that could be effective in controlling compulsive behavior and improving public health; that is, addiction to social apps can be mitigated by information-enhancing policies and capacityenhancing policies. The former have been suggested as effective public policies, especially in situations characterized by considerable and persistent heterogeneities among people (Winter 2011). Our ﬁndings show that when consuming social apps, some people are rational and far-sighted, but others are myopic and short-sighted. In addition, degrees of myopia and forward looking, as well as elasticity to social liquidity, statistically vary across groups of diverse demographic characteristics. As articulated earlier, individuals with varying sociodemographic characteristics differ in their utility maximization trajectories and cost–beneﬁt rationalization over time. How do we identify a priori which groups are more vulnerable to myopic addiction and which are more sensitive to social liquidity? Can we capitalize on differences that\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "are predicted on the basis of demographic traits and subsequently design policies that effectively “satisfy” those who need care and those who do not? Many people fall victim to social app addiction because they underestimate its physical and mental health consequences (Lin et al. 2015). Excessive smartphone consumption is signiﬁcantly associated with depression, stress, and sleep disturbance (Thomée et al. 2011). Lin et al. (2015) ﬁnd that the stronger a user’s dependence on a smartphone, the greater their underestimation of adverse health effects. Just as information-enhancing policies successfully curb cigarette addiction (Hammond et al. 2007), they are expected to substantially enhance public awareness of digital addiction hazards, stimulate sustained selfcontrol, and dissuade nonaddicts from compulsive consumption. The effectiveness of these policies lies in the provision of warnings (similar to those on cigarette packages) or the broadcast of documentaries regarding the severity of and vulnerability to a health risk originating from overdependence. Such policies could be useful particularly in curbing social app addiction, which signiﬁcantly varies depending on individual differences. The real strength of information-enhancing policies is that they positively affect addicts who need such information to temper compulsion but do not in any way inﬂuence those already fully informed about the risks of compulsive conduct. This feature means that asymmetric social policies (Winter 2011) cover virtually everyone across the spectrum of social app addiction; that is, no efforts are required to differentiate among social app addicts. Alternatively, capacity-enhancing policies operate under the principle of reinforcing self-disciplinary and rational management abilities. They are a form of management that is particularly suitable for myopic addicts given their limited willpower in sustaining self-regulation. These short-sighted individuals also exhibit consumption preferences that are inconsistent across time (i.e., assigning more weight to current consumption than to future consumption), thereby entangling themselves in a lifestyle of immediate gratiﬁcation and exposing themselves to severe risk. Under capacity enhancement, a user’s ability to exercise selfcontrol and rationally manage intertemporal consumption can be improved through a mechanism: liquidity transparency. Social liquidity plays an important role in the distribution of app consumption across temporal spaces. To reinforce the forward-looking behavior of social app consumers and prevent them from crossing over to myopic addiction, developers or platform providers could design additional usability features that aid users in easily identifying the degree of social liquidity. Social app users may beneﬁt from such signaling addons in that these features help users plan consumption\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "935 the distractions of social media; the signiﬁcant decline in workplace productivity has cost the U.S. economy US$650 billion a year (Shore 2012). This ﬁgure is remarkable given that America allocates US$170 billion and US$151 billion annually to ensuring public welfare and health and addressing productivity losses caused by smoking, respectively (Bach 2012). On average, work is disrupted by social media once every 10.5 minutes; workers require 23 minutes to refocus on their tasks. The public image of and attitudes toward social app platforms and developers may rapidly erode as our health, social lives, and economic welfare are severely jeopardized by excessive dependence on SNSs or social games. Platform providers or developers of social apps, particularly game app creators, may be able to enhance their public appeal by pursuing regulation in the form of informationenhancing or capacity-enhancing policies. Although “ethically” oriented marketing campaigns may reduce short-term proﬁts, these initiatives will eventually elevate the quality with which developers/providers respond to negative spillovers caused by people’s compulsive behaviors. In addition, information-enhancing policies may have strategic appeal to app developers and platform providers, resulting in increases rather than decreases in consumption in certain situations. For example, if social app users overestimate the risks of addiction, they are already consuming too little even when a large number of users are available for social interaction. Correcting users’ information deﬁciency could increase their social app consumption (Winter 2011). Consequently, given the high degree of variations among users in risk perceptions toward social app addiction, social app developers and platform providers may be able to increase app consumption from those who were initially misinformed about the risk of such compulsive behaviors through the provision of correct information about the hazard. Our ﬁndings also provide app developers and platform owners with valuable insights into usage patterns for mobile social apps, which starkly contrast with those for nonsocial apps (e.g., utility apps). Moreover, recognizing the wide-ranging variations in addictiveness and rationality across diverse demographic groups can advance the prediction of consumption intensity over time and the establishment of effective advertising and targeting strategies.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "distribution across social apps, wisely manage usage, and maximize utility. Some social games (e.g., Anipang in Korea), for instance, publish a leader scoreboard, which lists players who earn the highest points among a given circle of friends connected through a platform. To promote social exchange, the scoreboard is constantly updated for a player’s SNS contacts. In addition to scoreboards, usage metrics can be openly disseminated by developers to advise players about how intensively their contacts are participating in a game. Users of social apps exhibit different consumption patterns when new rules are enacted (Claussen et al. 2013). Liquidity posts facilitate the accurate monitoring and prediction of friends’ usage patterns, as well as the sensible management of intertemporal consumption for optimal utility. Making liquidity information publicly available is equally advantageous to app developers and platform providers. Consider Skype’s online status indicator, a feature that allows users to easily identify who is currently available for interaction in their networks. This component reduces the search costs associated with ascertaining availability. The status function also enables users to know how active or inactive friends are over a speciﬁc period (i.e., daytime or weekends). Additionally, the recognition or perception of others’ usage patterns may help users more efﬁciently plan consumption over time. The reduced transaction costs would, in turn, encourage users to more actively and regularly consume (or plan to consume) a service. Implications for Social App Developers and Platform Providers On the surface, social app developers and platform providers have no incentive to embrace regulatory agencies’ efforts to design and implement information-enhancing or capacity-enhancing policies. The reduction in general app consumption targeted by such regulations is viewed as damaging to digital businesses, at least in the short term. Such damage is exempliﬁed by the dilemma confronting industries that produce legalized “sin” products or services (e.g., tobacco, liquor, junk food, and gambling). Because the products are addictive, unhealthy commodities, the companies that manufacture them are facing intense public scrutiny today. Few people would consider SNSs or social games to be as “sinful” as the aforementioned addictive substances, yet research (i.e., Hofmann et al. 2012) reveals that mobile-based SNSs are more addictive and more difﬁcult to resist than cigarettes and alcohol. Parents are increasingly apprehensive about their children’s fascination with SNSs and platform games, which have been known to diminish the quality of family relationships. Another unfavorable effect of addiction to social apps is the economic ramiﬁcations stemming from 5.3.\n",
              "\n",
              "6.\n",
              "\n",
              "Limitations and Future Research\n",
              "\n",
              "Several limitations of this study along with directions for future research need to be noted. Our ﬁndings are derived based primarily on an analysis of the two most representative social apps, and, therefore, we make no attempts to generalize our results to other social apps.\n",
              "\n",
              "\n",
              "936 Furthermore, major SNS sites, such as Facebook, Twitter, and LinkedIn, differ substantially in terms of value propositions, business scope, and target customers, as well as available socialization features and functionalities. These structural heterogeneities inherent to diverse SNS sites may inﬂuence users’ future orientation and app consumption patterns. Future research should be directed toward determining whether these differences inﬂuence the ﬁndings reported in this study. Although several studies published by major academic journals (e.g., Danaher and Dagger 2013, Ma et al. 2014, Sabnis and Grewal 2015) use Nielsen data collected by similar stratiﬁed sampling procedures, we acknowledge that potential sample self-selection bias may reduce the generalizability of our ﬁndings. We believe the most ideal method is to verify whether people who participated in experiments exhibit the same usage behaviors as those who did not. However, because of conﬁdentiality issues, the company does not make such data available to the public. Like other studies utilizing such data, the current research was unable to formally verify self-selection. One prediction is that unbiased measurement could strengthen our results since users may be curtailing their behavior if they know they are being monitored, and addicted users may be unlikely to select into the monitor. We acknowledge that this issue may be one of the limitations of our work. Another caveat is related to the model speciﬁcation. Consistent with Becker et al. (1994), our model prioritizes parsimony and substantive importance over predictive power and policy counterfactuals. Consequently, we may have omitted several control variables from our speciﬁcations. For example, apart from social liquidity, other factors (e.g., platform type, price, reward systems) may affect social app consumption. Although our model competently explains a large portion of variances in observed consumption regularities, we acknowledge the model’s parsimony as a limitation. Future studies could expand the menu of variables to enhance the model’s richness and comprehensiveness. Furthermore, the reduced-form approaches used to implement our model estimation prevents us from examining alternative policies. Gordon and Sun (2015) recently developed a structural model of addiction and stockpiling to examine cigarette consumption. Future research can beneﬁt from the comprehensive empirical insights that may be revealed by the structural estimation approach. Finally, the mobile paradigm is rapidly altering the dynamics of users’ socialization process and communication protocols. Although the empirical scrutiny of a 13-month long, weekly panel data may sufﬁce to understand users’ behaviors, the unprecedented pace of the mobile revolution and the perpetual ﬂux in the\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "demand structures for technology suggest that nothing is stable and permanent, including the way we rationalize and justify our online behaviors. Therefore, a more nuanced and systematic inquiry is clearly needed into the continuous interplay between human rationality and technology evolution.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "7.\n",
              "\n",
              "Conclusion\n",
              "\n",
              "Social apps have been painted as a boon to interpersonal connectivity within or across one’s social boundaries. Yet, the excessive use of and compulsive dependence on such programs have become major social problems around the globe. The beneﬁts acquired from social apps appear to be overshadowed by addiction-related challenges. In illuminating this issue, survey-based inquiries grounded in psychological frameworks are commonplace, but scant attention has been devoted to economics-based empirical works on digital vulnerabilities that are brought forth by addiction to ubiquitous IT artifacts. Drawing on Becker and Murphy’s (1988) rational addiction framework, this study delved into whether the fundamental economic principles of utility maximization and rational behavior direct consumption patterns for highly addictive social apps, such as SNSs and social games. The ﬁndings obtained from comprehensive panel data on weekly app consumption support, at the aggregate level, the propositions of the rational addiction paradigm in relation to the social media context; that is, users of social apps amplify their sensitivity to social liquidity as they build their intertemporal consumption structures and maximize the discount utility derived from them. Note, however, that this aggregated ﬁnding highlights only the behaviors of the average user and does not cover structural differences among individuals. This limitation is compensated by the subgroup analysis, which reveals extensive heterogeneity originating from demography; some users are identiﬁed as myopic addicts, and others are classiﬁed as rational addicts. Additionally, the signiﬁcant variances in the degree of addictiveness and rationality among addicts translate to equally immense differences in exposure to risk and susceptibility to addiction across various demographic groups. The presence of such individual differences prompts the need for asymmetric social policies, namely, information-enhancing and capacityenhancing regulations, which can satisfy the intervention requirements of both addicted and nonaddicted individuals. The advancement and increase in pervasiveness of mobile technology can only be expected to exacerbate the vulnerability to digital addiction posed by SNSs and games—an effect that can potentially take a tremendous toll on public health. Further aggravating\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "937\n",
              "In Equation (A7), each individual app user chooses Ci t to maximize the sum of lifetime utility discounted at the rate r subject to the time budget constraint in Equation (A5) max\n",
              "C t =1\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "the problem is the advent of new user-friendly digital technologies, such as wearables and sensing devices. IT professionals and businesses continue to enhance the on-demand experience of users with technologies. Ubiquity and advancement nevertheless come with a price. Researchers must be mindful of the dynamic interplay between technological innovation and human behaviors given technology’s apparent tendency to exceed our rational capability to manage our consumption of innovations. Acknowledgments\n",
              "This research was partially supported by the Asan Research Foundation of Korea. Wonseok Oh is the corresponding author of this paper.\n",
              "\n",
              "1+r\n",
              "\n",
              "−t\n",
              "\n",
              "u Ci\n",
              "\n",
              "t\n",
              "\n",
              "Ai\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "(A7)\n",
              "\n",
              "where r is an interest rate. Consistent with the rational addiction model, addictive stock is assumed to be equal to the consumption of previous periods (Equation (A8)). In addition, a quadratic utility function in Ci t Ai t , and Lt is employed to derive the empirical demand function (Equation (A9)) Ai t = Ci Ui\n",
              "t t −1\n",
              "\n",
              "(A8)\n",
              "\n",
              "= a1 Ci t + a2 Ai t + a3 Lt + ucc Ci2 t /2 + uAA A2 i t /2 + uLL L2 t /2 + uCA Ci t Ai t + uCL Ci t Lt + uLA Lt Ai\n",
              "t\n",
              "\n",
              "Appendix A\n",
              "We posit that an individual’s utility depends on the factors speciﬁed in Equation (A1) Ui t = u Ci\n",
              "t\n",
              "\n",
              "(A9)\n",
              "\n",
              "Yi\n",
              "\n",
              "t\n",
              "\n",
              "Ai\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "(A1)\n",
              "\n",
              "By substituting Equations (A8) and (A9) into Equation (A7), a utility-maximizing demand function subject to the time constraint can be derived. Equation (A10) represents the associated ﬁrst-order condition u Ci\n",
              "t\n",
              "\n",
              "where Ci t is the amount of social app consumption of individual i at time t . In our context, no direct monetary cost is associated with consuming mobile social apps, but a user incurs an opportunity cost, i.e., the value of what other activities the user could have done during that time. Yi t refers to the time spent on these outside options of i at time t , and Ai t indicates the amount of addictive stock of i at time t . Finally, Lt reﬂects the degree of social liquidity at time t . Furthermore, we assume that the utility function is concave with negative second derivatives, and that consumption of addictive goods does not affect the marginal utility of outside goods consumption UCC < 0 UAA < 0 UY Y < 0 UCY = 0 UAY = 0 (A2)\n",
              "\n",
              "Ai Ci t\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "+\n",
              "\n",
              "1 u Ci 1+r\n",
              "\n",
              "t +1\n",
              "\n",
              "Ai t+1 Lt+1 =0 Ci t\n",
              "\n",
              "(A10)\n",
              "\n",
              "The resulting demand function of social apps represents current consumption, Ci t , as a function of past consumption (Ci t−1 and future consumption (Ci t+1 , as well as the current degree of social liquidity, Lt Ci t =\n",
              "0\n",
              "\n",
              "+\n",
              "\n",
              "1 Ci t −1\n",
              "\n",
              "+\n",
              "\n",
              "2 Ci t +1\n",
              "\n",
              "+\n",
              "\n",
              "3 Lt\n",
              "\n",
              "(A11)\n",
              "\n",
              "where 1 = −uCA / ucc + uAA / 1 + r , 2 = 1 / 1 + r , 3 = −uLC / ucc + uAA / 1 + r , r is an interest rate, and uCA , uCC , and uAA are second-order derivatives of the utility function.\n",
              "\n",
              "As pointed out earlier, there are three characteristics of addictive consumptions—withdrawal, tolerance, and reinforcement—that are represented in the following mathematical forms: UC > 0 UA < 0 UCA > 0 (A3)\n",
              "\n",
              "Appendix B. Difference GMM Estimates of Rational Addiction Model\n",
              "Ci Variables Ci Ci\n",
              "t −1 t\n",
              "\n",
              "In addition, we assume that social liquidity Lt positively affects the marginal utility of app consumption, and that addictive stock does not affect the marginal effect of social liquidity on utility ULC > 0 ULA = 0 (A4)\n",
              "\n",
              "Social networking service Facebook 0.425∗∗∗ (0.0249) 0.378∗∗∗ (0.0132) 0.923∗∗∗ (0.169) 104,975 2,710 0.705 0.912\n",
              "\n",
              "Social game Anipang 0.440∗∗∗ (0.00985) 0.467∗∗∗ (0.0187) 0.735∗∗∗ (0.147) 50,247 1,824 0.990 0.999\n",
              "\n",
              "ˆ1 ˆ2\n",
              "\n",
              "t +1\n",
              "\n",
              "The term W in Equation (A5) represents the total amount of time (i.e., 24 hours per day) allowed to each individual on any given day, denoting a time budget constraint. This constraint is similar to the budget constraint in the Becker and Murphy (1988) model Ci t + Yi t = W (A5)\n",
              "\n",
              "Lt ˆ 3 Observations Number of users Serial correlation (p-value) Instrument validity (p-value)\n",
              "\n",
              "where W is the length of time period t . Because of the time budget constraint, the utility function can be restated as shown in Equation (A6) Ui t = u Ci\n",
              "t\n",
              "\n",
              "Ai\n",
              "\n",
              "t\n",
              "\n",
              "Lt\n",
              "\n",
              "(A6)\n",
              "\n",
              "Note. Robust standard errors are in parentheses. ∗∗∗ p < 0 001.\n",
              "\n",
              "\n",
              "938 References\n",
              "Amato PR, Sobolewski JM (2001) The effects of divorce and marital discord on adult children’s psychological well-being. Amer. Sociol. Rev. 66(6):900–921. Amichai-Hamburger Y, Vinitzky G (2010) Social network use and personality. Comput. Human Behav. 26(6):1289–1295. Antonucci TC, Akiyama H (1987) Social networks in adult life and a preliminary examination of the convoy model. J. Gerontology 42(5):519–527. Arellano M, Bond S (1991) Some tests of speciﬁcation for panel data: Monte Carlo evidence and an application to employment equations. Rev. Econom. Stud. 58(2):277–297. Auld MC, Grootendorst P (2004) An empirical analysis of milk addiction. J. Health Econom. 23(6):1117–1133. Bach L (2012) Toll of tobacco in the United States of America. Campaign for Tobacco-Free Kids (September 13), https://www .tobaccofreekids.org/research/factsheets/pdf/0072.pdf. Baltagi BH, Grifﬁn JM (2002) Rational addiction to alcohol: Panel data analysis of liquor consumption. Health Econom. 11(6): 485–491. Baumeister RF, Leary MR (1995) The need to belong: Desire for interpersonal attachments as a fundamental human motivation. Psych. Bull. 117(3):497–529. Baumeister RF, Bratslavsky E, Muraven M, Tice DM (1998) Ego depletion: Is the active self a limited resource? J. Personality Soc. Psych. 74(5):1252–1264. Becker GS, Murphy KM (1988) A theory of rational addiction. J. Political Econom. 96(4):675–700. Becker GS, Grossman M, Murphy KM (1994) An empirical analysis of cigarette addiction. Amer. Econom. Rev. 84(3):396–418. Bennet S (2015) 28% of time spent online is social networking. SocialTimes (January 27), http://www.adweek.com/ socialtimes/time-spent-online/613474. Blundell R, Bond S (1998) Initial conditions and moment restrictions in dynamic panel data models. J. Econometrics 87(1):115–143. Blundell R, Bond S, Windmeijer F (2001) Estimation in dynamic panel data models: Improving on the performance of the standard GMM estimator. Nonstationary Panels, Panel Cointegration, Dynam. Panels 15(1):53–91. Calvert RL (1985) The value of biased information: A rational choice model of political advice. J. Politics 47(2):530–555. Chaloupka F (1988) An economic analysis of addictive behavior: The case of cigarette smoking. Unpublished doctoral dissertation, City University of New York, New York. Chaloupka F (1991) Rational addictive behavior and cigarette smoking. J. Political Econom. 99(4):722–742. Chen C, Leung L (2015) Are you addicted to candy crush saga? An exploratory study linking psychological factors to mobile social game addiction. Telematics Informatics 33(4):1155–1166. Claussen J, Kretschmer T, Mayrhofer P (2013) The effects of rewarding user engagement: The case of Facebook apps. Inform. Systems Res. 24(1):186–200. Clogg CC, Petkova E, Haritou A (1995) Statistical methods for comparing regression coefﬁcients between models. Amer. J. Sociol. 100(5):1261–1293. Correa T, Hinsley AW, De Zuniga HG (2010) Who interacts on the Web?: The intersection of users’ personality and social media use. Comput. Human Behav. 26(2):247–253. Danaher PJ, Dagger TS (2013) Comparing the relative effectiveness of advertising channels: A case study of a multimedia blitz campaign. J. Marketing Res. 50(4):517–534. Echeburúa E, de Corral P (2010) Addiction to new technologies and to online social networking in young people: A new challenge. Adicciones 22(2):91–95. Ellison NB, Steinﬁeld C, Lampe C (2007) The beneﬁts of Facebook “friends:” Social capital and college students’ use of online social network sites. J. Computer-Mediated Comm. 12(4): 1143–1168. Everitt BJ, Robbins TW (2005) Neural systems of reinforcement for drug addiction: From actions to habits to compulsion. Nature Neuroscience 8(11):1481–1489.\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "Fang X, Hu PJH, Li Z, Tsai W (2013) Predicting adoption probabilities in social networks. Inform. Systems Res. 24(1):128–145. Fields T (2014) Mobile and Social Game Design: Monetization Methods and Mechanics (CRC Press, Boca Raton, FL). Fowler FJ (1992) How unclear terms affect survey data. Public Opinion Quart. 56(2):218–231. Freud S, Bonaparte M, Nathan M (1930) Le mot d’esprit et ses rapports avec l’inconscient (Gallimard Publishing, Paris). Gordon BR, Sun B (2015) A dynamic model of rational addiction: Evaluating cigarette taxes. Marketing Sci. 34(3):452–470. Gould S (2015) It took 75 years for the telephone to reach 100 million users…and it took Candy Crush Saga 15 months. Business Insider (July 28), http://www.businessinsider.com/ it-took-75-years-for-the-telephone-to-reach-100-million-users -and-it-took-candy-crush-15-months. Grifﬁths M (2000) Internet addiction–time to be taken seriously? Addiction Res. Theory 8(5):413–418. Grifﬁths M (2005) A “components” model of addiction within a biopsychosocial framework. J. Substance Use 10(4):191–197. Grossman M, Chaloupka F (1998) The demand for cocaine by young adults: A rational addiction approach. J. Health Econom. 17(4):427–474. Gruber J, Köszegi B (2001) Is addiction “rational”? Theory and evidence. Quart. J. Econom. 116(4):1261–1303. Hammond D, Fong GT, Borland R, Cummings KM, McNeill A, Driezen P (2007) Communicating risk to smokers: The impact of health warnings on cigarette packages. Amer. J. Preventive Medicine 32(3):202–209. Hedges K (2014) Do you have FOMO: Fear of missing out? Forbes (March 27), http://www.forbes.com/sites/work-in-progress/ 2014/03/27/do-you-have-fomo-fear-of-missing-out. Herz A (1997) Endogenous opioid systems and alcohol addiction. Psychopharmacology 129(2):99–111. Hofmann W, Vohs KD, Baumeister RF (2012) What people desire, feel conﬂicted about, and try to resist in everyday life. Psych. Sci. 23(6):582–588. Homans GC (1958) Social behavior as exchange. Amer. J. Sociol. 63(6):597–606. Hribar P, McInnis J (2012) Investor sentiment and analysts’ earnings forecast errors. Management Sci. 58(2):293–307. Iannaccone PM (1984) Long-term effects of exposure to methylnitrosourea on blastocysts following transfer to surrogate female mice. Cancer Res. 44(7):2785–2789. Jiang K, Lepak DP, Hu J, Baer JC (2012) How does human resource management inﬂuence organizational outcomes? A meta-analytic investigation of mediating mechanisms. Acad. Management J. 55(6):1264–1294. Katz ML, Shapiro C (1985) Network externalities, competition, and compatibility. Amer. Econom. Rev. 75(3):424–440. Kemp S (2015) Digital, social, and mobile in 2015. We Are Social (January 21), http://wearesocial.sg/blog/2015/01/digital-socialmobile-2015. Kim EJ, Namkoong K, Ku T, Kim SJ (2008) The relationship between online game addiction and aggression, self-control and narcissistic personality traits. Eur. Psychiatry 23(3):212–218. Kim J, LaRose R, Peng W (2009) Loneliness as the cause and the effect of problematic Internet use: The relationship between Internet use and psychological well-being. CyberPsychology Behav. 12(4):451–455. Kuss DJ, Grifﬁths MD (2011) Online social networking and addiction—A review of the psychological literature. Internat. J. Environ. Res. Public Health 8(9):3528–3552. Lin YH, Lin YC, Lee YH, Lin PH, Lin SH, Chang LR, Tseng HW, Yen LY, Yang CC, Kuo TB (2015) Time distortion associated with smartphone addiction: Identifying smartphone addiction via a mobile application (app). J. Psychiatric Res. 65(1): 139–145. Liu JL, Liu JT, Hammitt JK, Chou SY (1999) The price elasticity of opium in Taiwan, 1914–1942. J. Health Econom. 18(6): 795–810. Ma L, Krishnan R, Montgomery AL (2014) Latent homophily or social inﬂuence? An empirical analysis of purchase within a social network. Management Sci. 61(2):454–473.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kwon et al.: Excessive Dependence on Mobile Social Apps\n",
              "Information Systems Research 27(4), pp. 919–939, © 2016 INFORMS\n",
              "\n",
              "939\n",
              "Sabnis G, Grewal R (2015) Cable news wars on the Internet: Competition and user-generated content. Inform. Systems Res. 26(2):301–319. Shore J (2012) Social media distractions cost U.S. economy $650 billion. MashableAsia (November 3), http://mashable.com/2012/ 11/02/social-media-work-productivity/#CWGGUguEuiqs. Song W (2014) Midnight ban imposed on online games. Korea Herald (April 12), http://www.koreaherald.com/view.php?ud =20100412000752. Steinﬁeld C, Ellison NB, Lampe C (2008) Social capital, self-esteem, and use of online social network sites: A longitudinal analysis. J. Appl. Developmental Psych. 29(6):434–445. Stigler GJ, Becker GS (1977) De gustibus non est disputandum. Amer. Econom. Rev. 67(2):76–90. Susarla A, Oh JH, Tan Y (2012) Social networks and the diffusion of user-generated content: Evidence from YouTube. Inform. Systems Res. 23(1):23–41. Thomée S, Härenstam A, Hagberg M (2011) Mobile phone use and stress, sleep disturbances, and symptoms of depression among young adults—A prospective cohort study. BMC Public Health 11(66). http://bmcpublichealth.biomedcentral.com/ articles/10.1186/1471-2458-11-66. Turel O, Serenko A, Giles P (2011) Integrating technology addiction and use: An empirical investigation of online auction users. MIS Quart. 35(4):1043–1062. Walsh T (2014) Candy crush’s puzzling mathematics. Amer. Scientist 102(6):430–433. Wei PS, Lu HP (2014) Why do people play mobile social games? An examination of network externalities and of uses and gratiﬁcations. Internet Res. 24(3):313–331. Winter H (2011) The Economics of Excess: Addiction, Indulgence, and Social Policy (Stanford University Press, Stanford, CA). Xu Z, Turel O, Yuan Y (2012) Online game addiction among adolescents: Motivation and prevention factors. Eur. J. Inform. Systems 21(3):321–340. Young KS (1998) Caught in the Net: How to Recognize the Signs of Internet Addiction—And a Winning Strategy for Recovery (John Wiley & Sons, New York). Zeng X, Wei L (2013) Social ties and user content generation: evidence from Flickr. Inform. Systems Res. 24(1):71–87. Zhang M (2014) People now spend more time looking at mobile screens than TV. SocialTimes (November 18), http://www .adweek.com/socialtimes/people-now-spend-time-looking-mobile -screens-tv/208110.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 05:55 . For personal use only, all rights reserved.\n",
              "\n",
              "Makuch E (2014) 93 million people play Candy Crush Saga daily– Do you? GameSpot (Feburary 18), http://www.gamespot.com/ articles/93-million-people-play-candy-crush-saga-daily-do-you/ 1100-6417819. McPherson M, Smith-Lovin L, Cook JM (2001) Birds of a feather: Homophily in social networks. Annual Rev. Sociol. 27(1): 415–444. Mobilia P (1993) Gambling as a rational addiction. J. Gambling Stud. 9(2):121–151. Moreno MA, Jelenchick LA, Egan KG, Cox E, Young H, Gannon KE, Becker T (2011) Feeling bad on Facebook: Depression disclosures by college students on a social networking site. Depression Anxiety 28(6):447–455. Morris M, Ogan C (1996) The Internet as mass medium. J. Comm. 46(1):39–50. Morris M, Lundell J, Dishman E (2004) Catalyzing social interaction with ubiquitous computing: A needs assessment of elders coping with cognitive decline. Dykstar-Erickson E, Tscheligi M, eds. CHI’04 Extended Abstracts Human Factors Comput. Systems (ACM, New York), 1151–1154. Naqvi NH, Rudrauf D, Damasio H, Bechara A (2007) Damage to the insula disrupts addiction to cigarette smoking. Science 315(5811):531–534. Ng BD, Wiemer-Hastings P (2005) Addiction to the Internet and online gaming. Cyberpsychology Behav. 8(2):110–113. Olekalns N, Bardsley P (1996) Rational addiction to caffeine: An analysis of coffee consumption. J. Political Econom. 104(5): 1100–1104. Pagano M (1989) Trading volume and asset liquidity. Quart. J. Econom. 104(2):255–274. Peele S (1985) The Meaning of Addiction: Compulsive Experience and Its Interpretation (Lexington Books, Lexington, MA). Pelling EL, White KM (2009) The theory of planned behavior applied to young people’s use of social networking Web sites. CyberPsychology Behav. 12(6):755–759. Pollak RA (1970) Habit formation and dynamic demand functions. J. Political Econom. 78(4):745–763. Pollak RA (1976) Habit formation and long-run utility functions. J. Econom. Theory 13(2):272–297. Roll R, Schwartz E, Subrahmanyam A (2009) Options trading activity and ﬁrm valuation. J. Financial Econom. 94(3): 345–360. Ryder HE Jr, Heal GM (1973) Optimal growth with intertemporally dependent preferences. Rev. Econom. Stud. 40(1):1–31.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "_i-H98bOP3w4",
        "colab_type": "code",
        "outputId": "ac8ceb2e-230c-4385-b75d-f52a2411608b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "significance_level_sents = [sent for sent in doc4.sents if 'significanct at' in sent.string]\n",
        "significance_level_sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "cz8rlysduukn",
        "colab_type": "code",
        "outputId": "270f1771-5176-4855-c2b8-7850cb6e2d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "p_großer_als_Sents = [sent for sent in doc4.sents if 'p >' in sent.string]\n",
        "p_großer_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[In the case of the SNS (panel 1), older users exhibit only myopic usage patterns and no rational addiction behaviors ( ˆ 2 , p > 0 05).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "W_KcAkBFvGri",
        "colab_type": "code",
        "outputId": "8d0152a2-b8d7-4ac0-b17a-2cecab334490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "p_großer_als_Sents = [sent for sent in doc4.sents if 'p >' in sent.string]\n",
        "p_großer_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[In the case of the SNS (panel 1), older users exhibit only myopic usage patterns and no rational addiction behaviors ( ˆ 2 , p > 0 05).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "xNTsmg5Wuud5",
        "colab_type": "code",
        "outputId": "19f1af8e-d28b-40f6-c161-cd3f7779b9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc4.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[p < 0 001.\n",
              " \n",
              " , p < 0 05; ∗∗ p < 0 01; ∗∗∗ p, p < 0 001.\n",
              " , p < 0 001.\n",
              " , ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              " , + p < 0 1; ∗∗∗ p < 0 001.\n",
              " , p < 0 001.\n",
              " \n",
              " Notes., p < 0 05.\n",
              " \n",
              " , p < 0 001.\n",
              " \n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "bAizEE2_GSQa",
        "colab_type": "code",
        "outputId": "63bb7a5a-9bf8-4a82-f5fb-d810deb9ea43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7984
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc5 = nlp(open(u\"MartensD#ProvostF#ClarkJ#de FortunyE_2016_Mining Massive Fine-Grained Behavior Data to Improve Predictive Analytics_MIS Quarterly_4.txt\").read())\n",
        "doc5\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BIG DATA & ANALYTICS IN NETWORKED BUSINESS\n",
              "\n",
              "MINING MASSIVE FINE-GRAINED BEHAVIOR DATA 1 TO IMPROVE PREDICTIVE ANALYTICS\n",
              "David Martens\n",
              "Applied Data Mining Research Group, Department of Engineering Management, University of Antwerp, S.B. 516, 2000 Antwerp BELGIUM {David.Martens@uantwerpen.be}\n",
              "\n",
              "Foster Provost\n",
              "Department of Information, Operations, and Management Sciences, Stern School of Business, New York University, 44 West Fourth Street, New York, NY 10012 U.S.A. {fprovost@stern.nyu.edu}\n",
              "\n",
              "Jessica Clark\n",
              "Department of Information, Operations, and Management Sciences, Stern School of Business, New York University, 44 West Fourth Street, New York, NY 10012 U.S.A. {jclark@stern.nyu.edu}\n",
              "\n",
              "Enric Junqué de Fortuny\n",
              "Department of Marketing Management, Rotterdam School of Business, Erasmus University Rotterdam, Burgemeester Oudlaan 50, 3062 PA Rotterdam, THE NETHERLANDS {junquedefortuny@rsm.nl}\n",
              "\n",
              "Organizations increasingly have access to massive, fine-grained data on consumer behavior. Despite the hype over “big data,” and the success of predictive analytics, only a few organizations have incorporated such finegrained data in a non-aggregated manner into their predictive analytics. This paper examines the use of massive, fine-grained data on consumer behavior—specifically payments to a very large set of particular merchants—to improve predictive models for targeted marketing. The paper details how using this different sort of data can substantially improve predictive performance, even in an application for which predictive analytics has been applied for years. One of the most striking results has important implications for managers considering the value of big data. Using a real-life data set of 21 million transactions by 1.2 million customers, as well as 289 other variables describing these customers, the results show that there is no appreciable improvement from moving to big data when using traditional structured data. However, in contrast, when using fine-grained behavior data, there continues to be substantial value to increasing the data size across the entire range of the analyses. This suggests that larger firms may have substantially more valuable data assets than smaller firms, when using their transaction data for targeted marketing. Keywords: Behavioral similarity, big data, response modeling, banking, payment data, customer analytics\n",
              "\n",
              "Introduction1\n",
              "This paper studies the use of massive fine-grained data when applying predictive analytics. Specifically, the paper focuses\n",
              "\n",
              "1\n",
              "\n",
              "Bart Baesens, Ravi Bapna, James R. Marsden, Jan Vanthienen, and J. Leon Zhao served as the senior editors for this paper.\n",
              "\n",
              "on settings where the consumers’ fine-grained financial transactions can be observed, with our particular application being the identification of prospective customers for marketing offers in banking. Financial firms increasingly are using predictive modeling to target offers to cross- or up-sell to existing customers and for customer retention (Hormozi and Giles 2004; Hu 2005; Van Den Poel and Lariviere 2003). This is not a paper describing a new application area. Predic-\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4, pp. 869-888/December 2016\n",
              "\n",
              "869\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "tive analytics has been used extensively for targeted marketing, and large banks use such methods routinely. Indeed, the bank that is the subject of this study has a sophisticated targeted marketing operation, using data on the customers’ demographics, geographic location, and prior activity with the firm (tenure, lifetime value so far, services used, etc.). As a shorthand, we will call this type of data structured data. Instead this paper examines expanding the data used in the modeling to “big data” and, specifically, to massive finegrained data on consumer transaction behavior, in a nonaggregated manner. Does it add value? If so, can it be done simply and in a scalable manner? In particular, this paper focuses on taking advantage of finegrained data on customer payments to merchants, which banks collect routinely. Such money-transfer data currently are not being used (broadly) for targeted marketing, either because the data are too big or unwieldy for traditional methods to handle or because the modelers are not convinced of the value of changing their methods. However, intuitively, observing that a consumer makes payments to a certain squash center in Brussels, a student restaurant in Leuven, and a high-end fashion store online provides substantial information about the consumer’s interests. As we will demonstrate empirically, such fine-grained data are remarkably predictive of which consumers will be good prospects for particular offers. The data set for this study, described in detail in the “Results” section, contains over 21 million payments made by 1.2 million customers to 3.2 million merchants. The main contributions of this paper are: • The demonstration that incorporating measures of behavioral similarity based on massive fine-grained transaction data indeed can improve predictive analytics in a real application.2 An overview and comparison of different modeling techniques to mine such data in terms of predictive performance and scalability. The careful examination of when exactly fine-grained behavioral similarity adds value, whether it is complementary to existing methods, and whether it is subject to improvement with increasingly bigger training data. The associated demonstration that firms with larger data assets can have a significant advantage when applying predictive analytics.\n",
              "\n",
              "In what follows, we start by reviewing prior work on datadriven targeted marketing. We then introduce a behavioral similarity measure that allows the building of predictive models incorporating massive, fine-grained behavior data. This technique is evaluated empirically on the banking data in the “Results” section, including a comparison with traditional targeting based on structured data. The final section concludes the paper and raises issues for future research.\n",
              "\n",
              "Prior Work\n",
              "Sophisticated marketing modelers use predictive analytics to build models to estimate which potential prospects will be good prospects for product offers, as well as which customers will be likely to abandon the company (attrition or churn prediction). In this paper we focus on the sorts of data used, so it is worthwhile spending some time reviewing current practices and related research. The most sophisticated datadriven marketers use a wide variety of data to create features that summarize consumers’ demographics, geographic location, and related characteristics (see Hill et al. 2006; Hormozi and Giles 2004; Hu 2005; Van Den Poel and Lariviere 2003). In cases where the consumers are (or have been) customers of the firm, to these features are added summaries of the individuals’ prior activity with the firm (tenure, lifetime value so far, services used, etc.). Furthermore, when available, features also can summarize product position and product use. The predictive analytics community has gotten used to targeted marketing and attrition/churn applications as bread-andbutter examples of predictive analytics in action, and even have benchmark data sets representing this sort of data (e.g., the data sets from KDDCUP 1997, 1998, 20093). In some cases, traditional targeted marketing does incorporate data from transaction behavior. However, transaction data traditionally are aggregated into a relatively small set of variables summarizing properties such as the transactions’ recency (e.g., when was the most recent transaction), frequency (e.g., what is the frequency of the transactions), and monetary value (e.g., what is the monetary value of the transactions) (Fader et al. 2005). Many variants of such RFM variables can be engineered, including the average amount of payment, the median, or some other percentile. Such variables are included in our structured data. A main point of this paper is to demonstrate that it also is important to view transaction data as very detailed and fine-grained information on a consumer’s behavior, and that such a view can lead to improving predictive performance. One case where fine-grained transaction data have been taken into account in traditional targeted marketing is in social\n",
              "3\n",
              "\n",
              "•\n",
              "\n",
              "•\n",
              "\n",
              "•\n",
              "\n",
              "2 The behavioral similarity method introduced and assessed in this paper has been used in practice by a leading bank to improve its targeting of customers. The actual production results are proprietary.\n",
              "\n",
              "http://www.kdd.org/kdd-cup.\n",
              "\n",
              "870\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "network-based marketing (Hill et al. 2006; Verbeke et al. 2014). Social network targeting has been justified based on theories of homophily (McPherson et al. 2001) and social influence (Aral et al. 2009). Unfortunately, neither of these theories is sufficient for justifying the use of general behavioral similarity for targeting. Social influence requires an actual social connection between the individuals in order that there be correlations in their preferences, not simply similarity in their behavior. The concept of homophily also is used as theoretical justification for data-driven targeting in a slightly more circuitous manner. Social theory has long held that social connections are more likely to be made between people who are similar, along a wide range of dimensions—and the stronger the similarity the more likely a connection will be made (McPherson et al. 2001). Thus, people who are connected are likely to be similar, which justifies targeting the social network neighbors of people who are observed to exhibit the desired characteristics (such as having purchased the product; Hill et al. 2006). Such targeting can be done by observing fine-grained behaviors indicating a social connection (Hill et al. 2006), but unfortunately as a theoretical justification, as with social influence it requires that there be a social connection. Aral et al. (2009) provide intriguing evidence that a surprising proportion of the observed correlation in product adoption of social-network neighbors is due simply to their similarity rather than to social influence. From a predictive analytics point of view, the fact of being socialnetwork neighbors could be considered a particularly useful similarity measure (as suggested by Hill et al. 2006). However, even the sophisticated targeted marketing study of Hill et al. (2006) did not consider massive fine-grained data on payments or merchant visitation. One area of study that has examined prediction using finegrained behavior data, albeit for an application with important differences, is recommender systems/collaborative filtering, where one tries to predict the utility of items for a particular user based on the items previously purchased or rated by other users (Adomavicius and Tuzhilin 2005). A typical example is predicting which movies a consumer is likely interested in, based on the ratings provided by the user and the set of all other users’ ratings. The main differences with our setting are (1) that recommender systems make predictions within the same domain of behavior as the data used to make the predictions, and (2) recommender systems try to predict for a very large number of products simultaneously. Within our domain, where the features are merchants transacted with, a recommender system approach would tell which other merchants a consumer is likely to transact with. A targeted marketing model will predict the value of one specific target variable for the consumer, outside the domain of merchants. (In this case, for example, would the consumer buy a pension fund product if it were to be offered?)\n",
              "\n",
              "Modeling Techniques for FineGrained Payment Data\n",
              "Payment Data Representation\n",
              "Consider a (possibly anonymized) transaction log containing payments4 from a set of consumers to various entities, such as firms, institutions and other persons, which we will loosely call merchants. We distinguish between three data representations, as shown in Figure 1, each opening up a separate set of applicable prediction techniques. First, we can represent the data as a very large matrix X where element xij indicates whether consumer i made a payment to merchant j (xij = 1) or not (xij = 0). A separate binary vector represents the target variable (i.e., the quantity to be estimated and for which training labels will be provided; Provost and Fawcett 2013). Note that for this paper we limit ourselves to a binary matrix X and binary target variables. Given the encouraging results we present below, the extension to a weighted matrix that represents the frequency, recency, or monetary value of the payments and other types of target variables are interesting future directions. When dealing with behavior data, where people interact with massive sets of entities like merchants (by choice), the data usually are extremely sparse. The intuitive explanation for this is that people can only make a finite number of such choices in a limited amount of time (Junqué de Fortuny et al. 2013). This is amplified in the present application by the fact that these choices involve monetary transactions. In our setting, this translates to the fact that no one consumer will transfer money with a large fraction of possible payment receivers. Therefore, the data set representing behaviors is very large dimensionally yet extremely sparse. Techniques that take advantage of the sparsity can be useful even in settings where analysts are not accustomed to building models from massive data matrices or using specialized big data computing architectures. Specifically, we can exploit the extreme sparsity of the matrix to design a direct similarity comparison that is particularly scalable. Consider that such data also can be represented as a bipartite graph (bigraph). In bigraphs, sometimes also referred to as affiliation or two-mode networks (Borgatti and Everett 1997; Breiger 1974; Latapy et al. 2008), there are two types of nodes with edges only between nodes of different types. In our example, we have consumers as one type of node, merchants as another, and edges defined by the payment transaction data. The matrix representation discussed previously corresponds to the adjacency matrix of the bigraph.\n",
              "4\n",
              "\n",
              "The payment transactions can be viewed broadly, including debit and credit transactions, check payments, etc.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "871\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Note: Payments can be represented as a matrix X, as an attributed bigraph from consumers to merchants, or as a projected, attributed pseudo-social network.\n",
              "\n",
              "Figure 1. Data Representation of a Transaction Log of Payments\n",
              "\n",
              "A Network-Based Behavioral Similarity Score\n",
              "Consider a transaction log containing money transfers, as illustrated for the very simple example in Figure 1, and repeated in Table 1. Let us introduce a behavioral similarity score based on a two-step approach: (1) define a weighted pseudo-social network that represents the similarity in payment behavior between consumers, and (2) based on the resulting network, compute predictive behavioral similarity (BeSim) scores for each consumer for a selected target variable. Defining a Weighted Pseudo-Social Network The similarity network will be constructed such that two consumers are considered similar if they make payments to the same entities, and are more similar the more such connections they share. This leads to the pseudo-social network (PSN), where a data network among consumers is built by linking two consumers if they send a payment to the same merchant. We call the inferred network model among consumers a pseudo-social network because, as in a true social network, strongly connected consumers demonstrate a strong similarity, at the very least in the particular merchants with which they transact. The key underlying assumption is that, as with to a true social network, if two consumers are strongly linked, they will be similar in other ways as well—such as affinity for a marketing offer. It is a pseudo-social network because, by and large, the linked consumers probably have no true social relationship with one another. In the PSN, each link provides some evidence of similarity. For example, in Figure 1, both Adam and Bill made a payment to Little Bookstore, and hence are linked in the PSN.\n",
              "\n",
              "Now the question becomes how to assign a similarity weight to the links, incorporating two aspects: (1) the more merchants the linked consumers share, the higher the weight should be, and (2) the more popular a merchant, the lower the weight should be. The latter aspect is motivated by the fact that there will be companies that many consumers pay, such as telecommunication operators or energy providers. These may provide little information on the similarity between two consumers and could swamp more informative links based on, for example, the fact that two consumers shop at the same small store. Very popular merchants can be omitted or down-weighted. As a simple example of such a “micro-affinity” measure, one could weight a merchant by the inverse of the number of customers: 1/NCj. We will introduce more sophisticated metrics later. The resulting weight between two consumers X and Y is then defined as the sum of the micro-affinity values (in this simple case, the 1/NCj values) of the shared merchants j. This satisfies the desire that having more shared merchants leads to stronger links, and also takes into account the microaffinity.\n",
              "\n",
              "w( X , Y ) =\n",
              "\n",
              "1 NC j ∈[ merchants ( X ) ∩ merchants ( Y ) ] j\n",
              "\n",
              "\n",
              "\n",
              "(1)\n",
              "\n",
              "Predictive BeSim Scores Based on the Labeled PSN Given the weighted PSN with a label for each consumer (having purchased the product in the past or not), we can now\n",
              "\n",
              "872\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Table 1. Example Payment Data to Merchant-Specific Metrics*\n",
              "merchant j LittleBookStore DeliC Amazon EnergyInc Consumers A, B A, D A, B B, C, D NCj 2 2 2 3 NSj 1 0 1 2 1/NCj .5 .5 .5 .33\n",
              "\n",
              "*Number of customers NCj, number of known buyers NSj, and inverse frequency 1/NCj. The known buyers (label 1) among the consumers are denoted in boldface.\n",
              "\n",
              "apply a modification to the standard weighted-vote relational neighbor (wvRN) procedure for predictive inference with network data (Macskassy and Provost 2003, 2007). Let’s call the consumers that are positively labeled (the known buyers) the “positive seed customers.” The BeSim score for each consumer is simply the sum of the weights to the seed customers. For example, the score of David is the sum of the weight of the links in the PSN to Bill and Clyde (these are the only neighbors who are seed customers; see also Figure 2). The differences from the prior work using wvRN are important for processing the massive data: 1. Here only the positive class is considered (thereby avoiding most of the links in the massive network). The weights of the links between consumers are computed as the sum of the micro-affinity values (1/NCj in the example) for all shared merchants. The final scores are not normalized across all neighbors, but instead are simply the sum across the relatively small set of positive neighbors.\n",
              "\n",
              "Note that a normalization factor per data instance can be added equal to the sum of the weights of the links in the pseudo-social network: Z X = 1/ Y w( X , Y ) .\n",
              "\n",
              "(\n",
              "\n",
              ")\n",
              "\n",
              "2.\n",
              "\n",
              "The BeSim calculation thus provides a measure of behavioral similarity between a consumer X and the set of known buyers (seed customers). The time complexity of computing the BeSim measure is O(n · m ¯ ), where n is the number of training points, and m ¯ is the average number of merchants consumers pay (i.e., the average number of nonzero elements per row). This corresponds to one pass over the complete log of payment data, where we count the number of times a certain merchant is paid, by the seed customers versus all consumers. Once the empirical probabilities are all calculated, the actual scoring requires summing just those that correspond to nonzero elements, as given by Eq. (2). The calculation is illustrated further with the simplified example shown in Table 1. The score for each consumer is obtained by summing the 1/NCj scores across each consumer’s set of merchants. In the example, consumer A (Adam) made payments to three merchants: LittleBookStore, DeliC, and Amazon. Hence, the score for A is given by the sum of the scores for these three merchants as calculated in Eq. (4), where each of these scores is determined by the empirical probability NSj/NCj. For consumer D (David), the scores for DeliC and EnergyInc are summed, providing a score of 0.67. We can easily verify these results by applying the modified wvRN on the PSN given in Figure 2. The formulation given by Eq. (2) has the advantage that it scales very well to a huge number of merchants and consumers (as compared to running the actual wvRN on the PSN).\n",
              "\n",
              "3.\n",
              "\n",
              "To understand the BeSim score more deeply, consider the following: using the example definition of the link weights as sums of micro-affinity scores, the terms in the resultant BeSim score can be regrouped algebraically by merchant. For each merchant j, the corresponding BeSim term is the empirical probability Ej—the ratio of the number of seed customers (known buyers) that made a payment to the merchant (NSj) divided by the total number of (unique) consumers making a payment to the merchant (NCj). When we have n consumers and m unique merchants, the (heuristic) score of a consumer Xi is defined as the sum of the empirical probabilities Ej of the merchants to which Xi has made payments (j |xij = 1):\n",
              "\n",
              "S BeSim ( A) = S LittleBookStore + S DeliC + S Amazon 1 0 1 + + 2 2 2 =1 = S BeSim ( D) = 0.67\n",
              "(4)\n",
              "\n",
              "S BeSim ( X i ) =\n",
              "\n",
              "j |xi , j =1\n",
              "\n",
              " Ej\n",
              "\n",
              "(2)\n",
              "\n",
              "Ej =\n",
              "\n",
              "NS j NC j\n",
              "\n",
              "(3)\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "873\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Note: Positively labeled customers are indicated as green (shaded) nodes, and the weights are taken as the sum of the 1/NCj values of the shared merchants.\n",
              "\n",
              "Figure 2. The Weighted PSN of the Running Example (Table 1)\n",
              "\n",
              "Relation to Other Neighbor-Based Methods\n",
              "While we already discussed prior work, we can now provide a deeper comparison based on the technical details presented in the previous section. As just noted, drawing inferences with the BeSim calculation produces a neighbor-based inference method most closely related to wvRN, as applied to the induced pseudo-social network. It is useful to clarify the difference from the most popular neighbor-based inference method, k-nearest-neighbor inference (kNN). Although there are many variants of kNN, the principle is always the same: a similarity metric is chosen between instances, and the inference for a new instance is calculated based on the target values of the k training instances (those with the value of the target variable known) most similar to this new instance. These target values are combined via some combining function, possibly taking the instances’ similarities into account. A traditional kNN classifier, however, using (for example) Euclidean or cosine distance, is not nearly as efficient as the BeSim calculation as the amount of data grows. The time penalty for massive data sets stems from the fact that kNN must calculate the distance between all inference data and the training data. This results in time ¯ ) (with n the number of training complexity O(n · nte · m ¯ the points, nte the number of inference/test points, and m average number of features a vector has). In our specific scenario, kNN would take about 100,000 times longer to compute than BeSim. Within the general neighbor-based prediction framework, another main difference with the present method (as with wvRN) is that the number of neighbors can be different for\n",
              "\n",
              "each instance, and this number is determined implicitly by the data—specifically, by the number of seed customers who share a payment receiver with the focal consumer. Further, BeSim only considers positive examples as candidate neighbors. The similarity function and combining function then comprise the novel BeSim calculation (including the link importance weighting). One contribution of this paper, thus, is the introduction of this scalable similarity computation that can be used for fast inference from other massive fine-grained behavioral data coming from transactions, web monitoring, social networking sites, etc. In theory, the BeSim metric could also be introduced to kNN classifiers, especially when behavior data are used.\n",
              "\n",
              "Alternative Calculations of BeSim\n",
              "So far, to introduce the use of fine-grained behavioral similarity, we presented one particular behavioral similarity calculation. Specifically, we used the sum of supervised components (NS/NC) over all merchants a consumer paid to (Eq. (6), where m is the total number of unique merchants). In the empirical evaluation below we consider several variants. First, we consider adding an additional micro-affinity penalty. We take inspiration from the well-known IDF relevance measure used in information retrieval, where terms occurring in many documents receive low weights and terms occurring in fewer documents receive higher weights. The inverse consumer frequency (ICF), defined by Eq. (5), provides an indication of the inverse popularity of the merchant, as a function of the number of customers that made a payment to\n",
              "\n",
              "874\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "the merchant (NCj), and the total number of consumers in the dataset (n). This additional penalty leads to the heuristic score defined by Eq. (7). We can also remove the micro-affinity weighting altogether, leading to Eq. (9), which scores a merchant based on the absolute number of seed customers only. Taking this even further, Eq. (10) shows the calculation where each merchant receives a binary value indicating whether any seed customer (known buyer) has made a payment to it or not. We can also consider replacing this ICF weighting metric with a more sophisticated alternative. (The reason for further attention on this weighting factor is that it works well, as will be shown later.) Employing ICF is based on the presumption that merchants to which fewer consumers make payments should contribute more influence in the similarity calculation than merchants who receive payments from many consumers. Using ICF, we discount non-informative merchants such as the tax authority. However, ICF might (also) amplify noninformative noise: merchants to which only a very few customers make payments will receive a very high weight, even though this may be simply due to random chance (e.g., due to how the data were sampled). To assess the importance of this concern, we replace the ICF measure with a frequency-weighting based on the Beta distribution, a continuous probability distribution defined by two parameters, α and β, which define its shape. By tuning the parameters, we can determine the empirically optimal shape of the weight distribution as a function of the normalized degree (number of unique consumers that made a payment) of a merchant. For the analyses that follow, these parameters are tuned on a validation set (one third of the training data) with a grid search procedure, separately using the different target evaluation measures, AUC (SB,AUC) and lift (SB,Lift), discussed below. This leads to empirically optimal values α* and β*. Note that more advanced approaches can be considered to learn these hyper-parameters, by optimizing a loss or likelihood function, using a Bayesian, gradient descent or random search approach (Bergstra and Bengio, 2012). The flexibility of the Beta distribution (see Figure 3) gives a learning-based method to penalize low-frequency merchants, based on the degree to which they provide informative signal or mainly noise. As will be described later, the Beta distribution with parameters determined to be optimal empirically on these data does indeed resemble closely the shape of ICF and thereby confirms that the low-frequency merchants contribute more valuable information than deleterious noise (see Figure 4). However, Figure 4 also shows that while the shape of the best Beta distribution conforms to that of the shape of the ICF\n",
              "\n",
              "measure, the resultant weights are significantly different. Thus it makes sense to compare the two empirically to judge their relative generalization performance.\n",
              " n  ICFj = log10   NC    j\n",
              "\n",
              "(5)\n",
              "\n",
              "S NSNC (x) = S ICF (x) =\n",
              "\n",
              "j |x j = 1\n",
              "\n",
              "\n",
              "\n",
              "NS j NC j ⋅ ICF ( j )\n",
              "\n",
              "(6)\n",
              "\n",
              "j |x j =1 NC j\n",
              "\n",
              "\n",
              "\n",
              "NS j\n",
              "\n",
              "(7)\n",
              "\n",
              "S B (x) =\n",
              "\n",
              "j |x j = 1\n",
              "\n",
              "\n",
              "\n",
              "NS j NC j\n",
              "\n",
              "⋅ B α * , β* ( j)\n",
              "\n",
              "(\n",
              "\n",
              ")\n",
              "\n",
              "(8)\n",
              "\n",
              "S NS (x) = S1 (x) =\n",
              "\n",
              "j | x j =1\n",
              "\n",
              " NS\n",
              "NS\n",
              "\n",
              "j\n",
              "\n",
              "(9)\n",
              "\n",
              "j |x j = 1\n",
              "\n",
              " I ( S (x) ≠ 0)\n",
              "\n",
              "(10)\n",
              "\n",
              "Results\n",
              "We now present results comparing different methods for purchase prediction, based on fine-grained payment data from a major international bank.5 The goal of this study is specifically to assess whether predictive modeling based on finegrained payment data holds value, and to compare different methods with an eye toward both predictive and time performance when processing massive data. Later in this section, we will assess whether modeling fine-grained data actually adds value over traditional approaches (using traditional structured data) for predictive modeling for targeted marketing.\n",
              "\n",
              "The Data\n",
              "The data for the analyses are from a period of 11 months, comprising over 21 million (debit) transactions made by 1.2 million of the bank’s customers (anonymized) to a total of 3.2 million unique, anonymized merchants. We built predictive\n",
              "\n",
              "These are data from a European office, which is noteworthy because European consumers and American consumers have different general creditand debit-account habits, with European consumers employing non-card debit transfers substantially more frequently.\n",
              "\n",
              "5\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "875\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Figure 3. Weighting Schemes for a Merchant Based on the Normalized Degree\n",
              "\n",
              "Figure 4. ICF Weighting Versus Beta Distribution Optimal Weighting (optimized for AUC and Lift at 1%)\n",
              "\n",
              "models for two tasks, targeting the purchasing of two different financial products: a pension fund product and a long-term deposit product, with, respectively, 20 percent and 3 percent of the customers having bought the product (the “class priors”). The binary target variable for each product represents whether or not the consumer purchased the product. No targeted campaign had taken place beforehand. Figure 5 shows some characteristics of the data. Figure 5(a) shows a histogram of the number of customers per merchant (the merchant’s degree in the bigraph; bars, left vertical axis). We see that most merchants receive few payments (the\n",
              "\n",
              "average number of customers per merchant is 6.7).6 However, there exist a few merchants to which almost all consumers make payments. These are likely monopoly-like companies such as energy suppliers, large telecommunication operators, or the tax authority. Since for this study the data were anonymized, we are not able to know exactly what they were. The ICF weight for each merchant is given by the black line (right vertical axis), showing how merchants with\n",
              "6 Although the number of customers for a payment receiver goes up to several hundred thousand, the vast majority of the distribution is given in the range shown, up to 20.\n",
              "\n",
              "876\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Notes: Figure 5(a) shows a histogram of the number of customers per merchant (the bars); the corresponding ICF weight is shown by the black line. In Figure 5(b), the distribution of the number of neighbors for consumers in the projected graph (the pseudo-social network) is shown.\n",
              "\n",
              "Figure 5. Banking Data Characteristics\n",
              "\n",
              "many customers are down-weighted more severely than merchants with few customers. Figure 5(b) plots the distribution of the number of neighbors in the projected graph. Most consumers have more than 100,000 neighbors, showing that the pseudo-social network indeed has a different structure than a typical, true social network (people rarely have 100,000 friends, for example). The merchants with very many customers implicitly link very many consumers, leading to this high connectivity. (Note: Since the BeSim calculation is based only on the positive neighbors, the number it must process is much smaller.) In addition to the payment transaction data, 289 traditional variables were obtained from the bank for the consumers in the data set. These are the variables used by the bank for their own targeting. For confidentiality reasons, a complete enumeration of all variables is not possible, but these variables can be categorized as follows: • • • • • • Demographics, such as age and gender Location, such as postal code, province, and main bank office Prior products, such as financial funds, savings accounts, or other products Product usage, such as amount of use for a product Tenure, such as time with the bank RFM, recency, frequency, and monetary value of payments\n",
              "\n",
              "Such data accord with those traditionally used by large banks and other large companies for their customer analytics applications (see Hill et al. 2006; Hormozi and Giles 2004; Hu 2005; Van Den Poel and Lariviere 2003). We will investigate the complementary value of these data later in this section.\n",
              "\n",
              "Predictive (Generalization) Performance: Purchase Prediction Using Payment Data\n",
              "We now present the generalization performance of the different BeSim variants for predicting consumer purchase likelihood, as measured by the area under the ROC curve (AUC) (Fawcett 2006) and the lift over random selection (Linoff and Berry 2011). The AUC is equivalent to the Mann–Whitney–Wilcoxon statistic, and measures how well a predictive model ranks a binary outcome. The lift over random selection is defined for a particular targeting threshold—usually the approximate percentage of consumers who will be targeted, those receiving the highest predicted likelihoods. Given a targeting threshold (e.g., the top 1 percent), the lift is the ratio of the target purchase rate to the average purchase rate (corresponding to random selection). For example, consider the situation where in the population as a whole, we expect 5 percent of the consumers to be purchasers; if for the 1 percent of consumers with the highest model scores the observed purchase rate is 15 percent, then we have obtained a lift of three. AUC and lift are both exam-\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "877\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "ined because together they give a more nuanced assessment of the predictive performance of the model: the AUC measures performance over the entire range of predictions and the lift measures performance for those with the highest predicted purchase likelihoods. In practice, we only really care about the latter, as typically targeting budgets allow one only to target the very top prospects on the ranked list; however, for completeness we would also like to assess the overall generalization performance of the model. Besides the various BeSim measures, we also consider estimating the relative purchase likelihoods via support vector machines (SVMs), which are generally thought to work well on high-dimensional data. We choose a linear kernel, as linear models have been reported to perform as well or even better than nonlinear alternatives for sparse data, since soft linear separability is more likely to occur (Hastie et al. 2001). (Supplementary studies verify this for the data used in this paper.) Equally importantly, using a linear kernel, the resulting models are generally faster to train and evaluate than nonlinear models. We use L1 regularization (on the L2 loss) for several reasons. Ng (2004) observes that L1 regularization has been shown to be better than L2 regularization if the number of input instances is smaller than the number of features, as it is in these data. Also, L1 regularization promotes sparsity in the model (Hastie et al. 2001), which is in line with our goal of finding particular merchants that are informative. Furthermore, based on the guidance of Chapelle and Keerthi (2008), in order to allow for operationally reasonable convergence rates on the massive data during the processingintensive cross-validation phase, we relaxed the 0-stopping criterion level of the quadratic program to be solved in the SVM procedure during cross-validations, since a stringent error tolerance level is not necessary in this phase to reach good final models, especially when dealing with high volumes of data. Based on preliminary analysis, this setup achieved similar (not significantly different) generalization performance at a more than ten-fold speed increase. For the analyses in this section, the data are split into a training set (90 percent) and a (held out) test set (the remaining 10 percent). All consumers in the training set who bought the product are labeled as known buyers; scores are computed using the different methods for the consumers in the test set. As discussed above, we evaluate all of the models in terms of predictive performance using AUC and lift. This process was then repeated 10 times, each with a different random training/test split. The averages and standard deviations of the results over the 10 analyses for each of the techniques and each of the measures are shown in Table 2. The best models in terms of AUC are the variants of BeSim where the Beta distribution is optimized (on a validation\n",
              "\n",
              "portion of the training data) to maximize AUC. Interestingly, as noted above, inspection of the optimal Beta distribution parameters reveals that it has a similar general shape to ICF (Figure 4). The SVM-based methods are not competitive for AUC using these massive, sparse data. In terms of lift, the best AUC model (BeSim with the Beta distribution tuned for AUC) does not perform well. However, as we would hope, BeSim with the Beta distribution trained for each lift threshold performs quite well for the corresponding lift threshold—in four of six cases being the bestperforming method (or tied for that distinction), and in all cases being a close competitor. The main disadvantage of the Beta distribution version of BeSim is that it takes a very long time to optimize since two parameters need to be crossvalidated on a separate part of the huge data set (Table 3). Although it is never the best-performing method, the SVM using the fine-grained behavior data performs much better for lift than it does for AUC, especially for Product 2. However, it is even more computationally expensive than the Betabased BeSim calculations (Table 3). The biggest surprise is that the ICF-based BeSim scores perform comparably to or better than any of the other techniques, including the predictive modeling methods, for topsegment prediction (lift)—the main measure of interest. This is especially remarkable because it is also one of the fastest techniques to run on massive data; it could easily scale up to much larger data sets due to its space efficiency and sparse linear run-time complexity. Seeing that the heuristic SICF approach performs best in terms of predictive performance and scalability, we examine it in more depth next, where we assess the complementarity of the fine-grained payment data and the structured data.\n",
              "\n",
              "Behavioral Similarity Versus Traditional Structured Modeling\n",
              "We will now replicate the procedure employed by this bank for modeling using traditional (structured) data, compare it to the BeSim scoring, and examine combining the two. Note that as part of its normal practice, the bank performs studies of different learning/prediction methods using standard predictive analytics evaluation procedures. Analytical Setup To replicate the bank’s standard practice, we build a linear support vector machine (SVM) (Vapnik 1995) model using the 289 traditional variables on a balanced sample from the\n",
              "\n",
              "878\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Table 2. Results for the Empirical Study Comparing the Different Versions of BeSim and SVM-Based Modeling*\n",
              "(a) Product 1 Method ICF NSNC NS S1 SB,AUC SB,Lift SVM (b) Product 2 Method ICF NSNC NS S1 SB,AUC SB,Lift SVM AUC 62.9 (2.3) 62.9 (2.5) 57.8 (2.7) 58.1 (3.6) 70.9 (2.5) 62.88 (2.6) 50.7 (3.2) AUC 63.5 (4.1) 63.1 (4.1) 51.1 (3.3) 51.7 (2.7) 78.9 (4.2) 63.3 (4.0) 67.1 (4.2) lift1 11.6 (2.5) 11.5 (2.4) 1.25 (0.3) 3.0 (0.9) 1.3 (0.6) 11.0 (2.2) 7.3 (2.8) lift1 19.9 (7.4) 19.2 (7.3) 1.3 (0.3) 3.1 (0.9) 1.0 (0.8) 18.2 (7.1) 18.8 (7.5) lift5 3.5 (0.4) 3.5 (0.4) 1.4 (0.3) 1.9 (0.3) 1.3 (0.2) 3.5 (0.5) 2.0 (0.5) lift5 5.1 (1.6) 5.1 (1.5) 1.1 (0.2) 1.7 (0.4) 1.0 (0.4) 5.1 (1.5) 4.9 (1.7) lift10 2.3 (0.2) 2.3 (0.2) 1.4 (0.2) 1.6 (0.2) 1.3 (0.2) 2.4 (0.3) 1.3 (0.3) lift10 3.2 (0.7) 3.1 (0.7) 1.1 (0.2) 1.4 (0.2) 0.9 (0.3) 3.2 (0.7) 3.2 (0.9)\n",
              "\n",
              "*Each value is an average over the 10 test folds, with standard deviation in parentheses.\n",
              "\n",
              "Table 3. Averaged Time Duration to Learn the Different Models and Score the Test Data*\n",
              "Method ICF NSNC NS S1 Beta SVM Duration (sec.) 55 40 35 34 4097 6919\n",
              "\n",
              "*As the preprocessing time is the same for all models, it is not included.\n",
              "\n",
              "training set, including all the seed customers and just as many randomly selected non-buyer consumers.7 A forward input selection procedure based on the area under the ROC curve (AUC) (Fawcett 2006) was conducted to select a maximum of 30 variables.8 A validation set (chosen as a third of the training set) is held out to determine the optimal number of\n",
              "\n",
              "variables and to optimize the SVM’s regularization parameter using a grid search, using the grid [0.001 0.01 0.1 1 10 100 1000]. The resulting model is called the structured data (SD) model. For further comparison, we also created a new model that (as described next) is a combination of the BeSim and the SD models. This will help us to assess the extent to which the two models incorporate complementary information. Specifically, for this analysis we produce a linear combination of the two models’ scores. The BeSim output score is rescaled to the interval [0,1] by subtracting the minimum and dividing by the range. All positive examples and just as many negative\n",
              "\n",
              "7 This sampling was conducted to enable the large number of analyses with the structured data. A smaller-scale comparison using the full data yielded similar results. 8\n",
              "\n",
              "A 30 variable plateau is visible in terms of AUC, as shown by Figure B1 in Appendix B.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "879\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "examples are chosen to create a balanced sample (for scalability). Since the BeSim score is only available for the test data, we are limited to estimating the combined model on test data. Therefore, we hold out a randomly selected 10 percent of the previously defined test set to estimate the weights that combine the two output scores. The results are then computed over the remaining 90 percent of the test set to evaluate the performance of all models. The model resulting from combining the BeSim and linear SVM models is denoted BeSim + SD. Empirical Comparison For the two products, Tables 4 and 5 report the AUCs and lifts at 1, 5, and 10 percent averaged over 10 randomizations, each time using 80 percent of the data as training data and the remaining 20 percent as test data, following the procedures described above, and using the BeSim model as a benchmark. In the tables, for each of these performance metrics the cell with the best value is shown in boldface. The BeSim model alone seems to perform comparatively quite poorly when viewed in terms of AUC. However, it does extraordinarily well when comparing the lift at 1 percent. BeSim’s lift degrades as the threshold becomes more liberal: it is comparable with the other models at 5 percent, and worse at 10 percent and higher. This pattern is observed for both products. The attentive reader will have noticed a difference in performance between Table 2 and Table 4. This is explained by the fact that in the BeSim-only calculations (Table 2), the validation set was set apart from the training set for all metrics and not used for training. This was done to keep the comparison conservative since one method needed to use a validation set (viz., the Beta function). Using more data yields better results (Tables 4 and 5), to which we will return in the next section. For all comparisons, the BeSim calculation plays a part in the best-performing model. Using a one-sided paired t-test over the 10 randomizations, we find that the combined BeSim + SD model performs significantly better than the individual BeSim and SD models for the AUC, lift at 5 percent and lift at 10 percent (all p-values < 1e-5). For the lift at 1 percent, the BeSim model alone performs best, significantly outperforming the SD model (p-value 7e-6), and the combined method (at a lower significance level, p-value 0.02). For product 2, the same techniques perform best, always significantly outperforming the two others (all p-values < 1e-5). Thus, BeSim does a very good job when predicting which consumers have the highest likelihood of purchasing: the\n",
              "\n",
              "consumers ranked most highly by BeSim are comparatively very likely to buy the product themselves. As mentioned above, given budget limitations, marketing campaigns are often limited to targeting only the high-percentile prospects. Examining the ROC and lift curves shown in Figure 6 illustrates why we see the comparison numbers that we do. The curves show the model generalization performance across all thresholds (for a chosen representative randomization). Indeed, BeSim performs very well at the top of the rankings, but once the high percentiles have been passed, the BeSim model (solid line) performs quite badly. Notice that the ROC curve becomes almost a straight line, indicating that, in this region, BeSim does not discriminate among these consumers at all. Looking even more deeply, the reason for this performance is that BeSim only provides a nontrivial score to a small number of consumers (which seemingly are indeed very likely candidates for the product). In the calculation of BeSim, only the neighbors of existing (seed) customers in the pseudo-social network are provided with a nonzero score; most of the PSN remains unscored (see Appendix A for details). More advanced network inference schemes, including collective inference (Macskassy and Provost 2007), thus may further improve the performance of inference as compared to scores based only on immediate neighbors in the pseudo-social network. We can contrast BeSim’s performance with the performance curve for the SD model (lighter, dotted line). The latter exhibits the ROC curve shape one normally sees for typical predictive models. Notably, it performs worse than the BeSim model at the very high score range and better everywhere else. The combined model (BeSim + SD) performs strikingly well over the entire score range. Thus scoring using similarity based on fine-grained behavior data indeed has complementary predictive power to the traditional structured, data-based scoring. This result is particularly encouraging given the simplicity of the method we used to combine the two different scores. Designing a more sophisticated combining strategy may give additional lift.\n",
              "\n",
              "Big Data and Generalization Performance\n",
              "It is important to consider that these results are generated for a particular data set of a particular size. There is extreme variance in the number of customers patronizing different banks. There is also a large variance in the number of customers for different banking products. Thus, it is interesting to ask whether “data assets” of different sizes confer different improvements in decision-making ability. Further, standard\n",
              "\n",
              "880\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Table 4. Results for Product 1 with 80% Training Data, Showing the Averages and Standard Deviation (in parentheses)\n",
              "AUC BeSim SD BeSim + SD 63.9 (0.6) 75.5 (0.7) 78.2 (0.9) Lift 1% 14.9 (1.0) 4.9 (0.3) 12.7 (3.0) Lift 5% 4.1 (0.2) 3.9 (0.3) 5.5 (0.5) Lift 10% 2.6 (0.1) 3.3 (0.2) 4.0 (0.2)\n",
              "\n",
              "Table 5. Results for Product 2 with 80% Training Data, Showing the Averages and Standard Deviation (in parentheses)\n",
              "AUC BeSim SD BeSim + SD 71.7 (0.7) 86.6 (0.5) 89.0 (0.7) Lift 1% 31.8 (0.6) 10.1 (0.5) 18.2 (4.1) Lift 5% 7.6 (0.1) 7.8 (0.2) 9.7 (0.7) Lift 10% 4.4 (0.1) 6.0 (0.1) 6.7 (0.2)\n",
              "\n",
              "Figure 6. ROC and Lift Curves for the Behavioral Similarity (BeSim) Model, the Model Using Structured Data (SD), and the Combined Model (BeSim + SD)\n",
              "\n",
              "practice in this application (as we are told) generally is to build targeting models from subsamples of the entire customer base, as the analysts believe that learning from a moderatesized sample will confer all of the predictive power they are going to get. For both of these reasons, it is important to examine the relationship between the amount of training data and the generalization performance of the different models. Because of its design, the BeSim scoring should be affected strongly by the amount of data available: larger training data size means more connections among consumers as well as more seed customers becoming available for inference; both should tend to lead to lower estimation variance (and thus lower error) in the scores. Most importantly, with more data,\n",
              "\n",
              "prospects for whom the BeSim score would previously have been zero due to no connection to any prior (seed) buyer would increasingly receive nonzero scores. Thus, it may be that these results are conservative compared with what might be expected across a large bank’s entire customer base (which could be one or two orders of magnitude larger), and especially for products with large sets of prior purchasers (seed customers). We can assess the effect of the data size within the range of our sample by plotting learning curves (Provost and Fawcett 2013), simulating the availability of different amounts of data. In Figure 7 we show the evolution of the performance metrics for the models (BeSim, SD, and BeSim + SD) as we increase\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "881\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Figure 7. Learning Curves: Change in Lift and AUC on the Test Set for Product 1 (left) and Product 2 (right) as the Amount of Training Data is Increased\n",
              "\n",
              "the amount of data available for training.9 The AUCs and lifts for the BeSim model (solid line) indeed increase as we increase the amount of training data, and do so relatively constantly across the entire range. As noted above, as more seed customers become available, more consumers in the network will receive a nontrivial score. This improvement trend is not observed for the SD (traditional) model for any of the performance metrics, for either product. As is typically observed in predictive modeling applications (see Perlich et al. 2003), after a certain point the marginal performance improvements obtained by adding more training data become small. This indicates that we should expect further model improvements for the BeSim model (and the combined BeSim + SD model) with larger data sets (with more seed customers), even\n",
              "9 For this analysis, for each training size we use all remaining data as testing data. Note that we are unable to assess the performance with 100% of the data used for training, as no more test data would be available to score.\n",
              "\n",
              "though here we already have data on a fairly large number of customers. If this trend were to continue for orders of magnitude more data, it would argue that the largest banks have a remarkable data asset from which they could get significant competitive advantage over banks with smaller customer bases. Considering these big data arguments, it is important to assess whether computational cost would prohibit the practical use of the new techniques. In fact, inference using the BeSimbased techniques is quite fast (an analysis of the computational requirements of the BeSim calculation was provided earlier). For all of the analyses in this paper, inference over the entire data set took about a minute (detailed measures are provided in Table 3). All analyses were conducted on an Intel Core i5-2400 CPU @ 3.10Ghz machine with 4Gb RAM. The BeSim procedure is implemented in Matlab, so the run time could likely be improved substantially. The linear SVM models were built using the LIBLINEAR package (Fan et al. 2008).\n",
              "\n",
              "882\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "As the data size grows, expected BeSim inference time shows a linear increase. Most time is spent on the initial preprocessing of the data, going from payment transaction data to the lists of customers for each merchant (as previously shown in Table 1). For our analyses, this required about a day of computation to incrementally read in and process the transactions.10 Note that the BeSim model can be built incrementally; as new data become available the model can be updated in one fast operation.\n",
              "\n",
              "The scalability and ease of interpretation and implementation of our method are of importance in the deployment of new techniques for targeting consumers. As described by Michael Wexler, Director of Digital Insights and Marketing Effectiveness at Citibank (Wexler 2014): Because of (perceived regulatory restrictions), for many decisions touching consumers or capital allocation, banks keep preferring to use models that are well understood in the community and are well supported out-of-the-box in the software they use (SAS, primarily), and are able to be easily examined around a) specific and clear impact of each input variable and b) the model consistency and stability. This works fine with regressions and other generalized linear models…but there is little guidance from the regulators on how to similarly review and judge machine learning models, many which have complex nonlinear impacts from predictors, and may be continuously changing (and therefore seen as non-stable). Even well regarded and documented models…are kept in the R&D wing, and are usually not part of mainstream bank decision processes around marketing, offer selection, or other optimizations for consumers. Some exceptions include the fraud-detection groups, who are given more leeway to experiment, and some of the specialized quant trading groups, who have less liquid markets requiring more advanced math to manage. At the end of the day, there is acceptable risk in the use of any model, and, while banks don’t avoid all risk, they do tend to prefer measurable risk, and this is considered easier to do with traditional models. A somewhat surprising aspect of behavioral similarity targeting is that it can be remarkably privacy friendly, in contrast to how it may seem prima facie. For the BeSim component, the only data required are 1. An anonymized transaction log: a list of anonymized payment transactions, denoting for each transaction the following attributes: • Consumers (anonymized, but reversible for targeting) • Nonconsumer merchants (anonymized; no need to be reversible) The target values for a set of anonymized consumers for training\n",
              "\n",
              "Expert Feedback\n",
              "The bank with which we worked for this study was particularly happy with the well-performing BeSim + SD model for the following reason: From the point of view of scoring consumers, we can consider the BeSim score to be just another variable. Then the combined BeSim+SD model is itself simply a linear model, where each component variable has a simple explanation. The structured variables were the variables already in use. The BeSim score has the intuitively satisfying interpretation as the similarity of the consumer prospect to the prior customers of the product in question. The fact that the variable had a calculation behind it was not problematic, as the other variables in use also do (e.g., RFM variables). Furthermore, the calculation is easy to explain intuitively. To automate the individual explanations of why the BeSim measure classified a particular consumer as being a good prospect, the method introduced by Martens and Provost (2014) can be applied, where an explanation would be defined as the set of merchants that a consumer paid, such that removing these payments would lead to the consumer not being predicted to be a good prospect. An example explanation for a consumer that is predicted to be interested in a student loan might be “if this consumer had not made a payment to online_course_XYZ, then the predicted class would change from interested to not interested in a student loan.” As anecdotal support for this line of work, BeSim-based targeting indeed was deployed by the bank. The production results are proprietary, but the bank claims that their own “A/B” evaluations support our conclusions: the prospects identified by the BeSim-based models actually purchased the product significantly more frequently than the prospects identified by the traditional targeting models.\n",
              "\n",
              "2.\n",
              "10 Presumably, a large bank with one or two orders of magnitude more data would not be running Matlab on a desktop PC. Moreover, modern big data architectures could speed up this sort of processing substantially.\n",
              "\n",
              "Each consumer as well as each merchant in the network can have her identity encrypted, not needing any name or account\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "883\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "number. In the case of the consumers, the encryption would be reversible in order actually to target a subset of them. However, the decryption could be limited to a protected, taskspecific environment. This privacy friendliness is a very attractive feature in a banking setting as it does not allow modelers and analysts to view customers’ names and payment profiles. In addition, the data would be useless to almost any recipient in the case of a data breach. As we have seen, additional data on consumer characteristics, as in the SD data, can improve predictive performance. These also could be encrypted for modeling. Another operational advantage of this type of data concerns data quality (Moges et al. 2012): typically this is a major challenge when working with structured data. With payment data, however, no such issues arose: a simple “dump” from the transaction log is all that is needed for the BeSim method to be applied. The account number of the customer or the merchant was never missing or invalid. This might seem obvious and of little importance, but this has tremendous implications for the time saved on data preprocessing. Before continuing to the conclusion, it is worth briefly discussing the proprietary nature of such behavior data and its implications for scientific research. The research area comprising data science and big data for business is highly dependent on close collaboration with industry, because understanding effectiveness depends on the actual characteristics and distributions of real data. When analyzing behavior data, as we do, privacy is always a major issue; such data contain information of consumers’ everyday actions. Many studies (including this one) only receive such data in an encrypted form. For example, the data we received contained identifying information neither on the individual nor on the merchant. This limits the degree to which we can dig into the results to understand them more deeply. A separate but related issue is that such sensitive company data rarely if ever can be shared with other researchers, changing the possibilities for replicability and follow-up studies. Thus, replicability is limited to other researchers applying the methods to similar data sets from the same or other organizations. One might argue that this is a more interesting sort of replicability than simply making sure that mistakes were not made in running the code on a particular data set, since replicating the results on similar data sets would test generality. Nevertheless, as a scientific community, we may want to elevate the discussion of the tradeoffs between being able to do certain sorts of studies at all and limitations based on restrictions to sharing data. Such an examination might elevate a little-trodden research avenue: (how) can we create synthetic behavioral data sets\n",
              "\n",
              "that mimic the true data sufficiently to satisfy research needs, yet guarantee the confidentiality of the original data to satisfy our organizational partners? Related to this issue is the fact that, for our study, we relied on the bank for the preprocessing of the structured data (the 289 traditional variables) and we are not allowed to list the exact meaning of each of them. Therefore, we rely on the bank to have used correct data science practices, which again limits the reproducibility of our findings.\n",
              "\n",
              "Conclusion\n",
              "This paper provides an in-depth study of the use of a particular sort of big data—massive, fine-grained data on consumer behavior—to improve targeted marketing. Specifically, we examined the use of behavioral similarity for predictive modeling using fine-grained data on payments to specific individual merchants, in the context of targeting product offers to banking customers. We first isolated the computation of similarity from massive, fine-grained data by defining direct measures of behavioral similarity (BeSim). For two different banking products, the results show that the BeSim method is substantially better at placing consumers who purchase at “the top of the list” (i.e., score them as the highest-ranked individuals) than a traditional targeting model. Further, the BeSim model and the traditional model comprise complementary information. Combining the two produces a very robust model that gives better lift for almost any targeting budget. (The pure BeSim model still does better for the smallest selections of candidates.) The BeSim calculation identifies those consumers most similar to key individuals of interest. The payment behavior data allow this similarity to be broadly based on the tastes, interests, and latent socioeconomic constraints represented by shared payment recipients and sources of money transfers. In our banking applications, the individuals of interest were prior customers of the products, so the BeSim found other consumers who were very similar along these dimensions to the existing customers. However, the BeSim design is not specific to targeting marketing offers. If the individuals of interest were chosen to be different tranches of total credit exposure, the BeSim method may be helpful for predicting wallet share. If the individuals of interest instead were chosen to be particularly good (or bad) known credit risks, the BeSim models ought to find other very similar consumers. Thus, behavior similarity could improve another very common modeling application: estimating creditworthiness. The analyses also show a striking result of particular relevance to the current discussions of using big data for\n",
              "\n",
              "884\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "improving business decision making (Provost and Fawcett 2013). In this application, predictive modeling using traditional structured data does not seem to be enhanced by increasing the amount of data to a massive scale. In contrast, targeting based on fine-grained behavioral similarity is indeed enhanced substantially by increasing the data size to a massive scale. This suggests that the already telling results presented in this paper may underestimate—possibly by a lot—the potential lift achievable by calculating behavioral similarity from all the data available to a huge bank. This provides one of the clearest illustrations (of which we are aware) of how large institutions have an important asset in the data they have collected, an asset from which they can get substantial competitive advantage over institutions without as much data—in this example, smaller banks. The analyses showing that the alternative BeSim calculations perform very well also provide broader evidence that direct behavior similarity calculations indeed should be considered for research and practice when massive fine-grained behavioral data are available. The point of this paper was not to design the best method for this application or this sort of data; it seems likely that future work will show how to achieve even larger performance improvements from such data. (The alternative BeSim calculations based on the Beta distribution calculation are not suitable to large data on a desktop platform, but may be implemented feasibly using big data architectures.) Nonetheless, the basic BeSim calculation is quite simple to calculate and to implement, which is not a trivial factor when trying to deploy the model. A first avenue for future research is defining and testing a weighted input matrix or bigraph, using RFM indicators of the payments. The timing of payments, both in terms of the time horizon to use, as well as the specific day and time of day of payment, might also be interesting further improvements to examine. Using different time horizons would also allow one to investigate the dynamics of the resulting network, for example, using the network with data of different quarters, years, etc. to create the models. Analyzing the resulting models, both in the workings of the models (e.g., the coefficients of the linear model) and the performance thereof could lead to additional insights into the domain and when to apply which technique. From a technical point of view, other avenues for future research include defining extended heuristics, applying other large-scale classification techniques, and performing dimensionality reduction on the input matrix using singular value decomposition (Clark and Provost 2016). Within a banking setting, other applications of our methodology include churn prediction, fraud detection, and default\n",
              "\n",
              "prediction, where for the latter we assume that the payments of consumers are likely also predictive for their creditworthiness. Note that such use is also applicable for credit card companies. Finally, behavioral similarity for predictive modeling is applicable well beyond the banking setting. Other companies (telecommunications companies, Amazon.com, online advertising companies, payment processors such as VISA and Paypal) have data on the specific merchants with which consumers transact. In this age of increased analysis of massive data, we hope that this paper can add to the new line of thinking into how firms best can use their data assets for consumer analytics in banking and beyond.\n",
              "\n",
              "Acknowledgments\n",
              "We are very grateful to the bank for providing us the data and working with us to achieve these results. We thank our reviewers for their constructive feedback. We give a special thanks to the editors of this special issue and the authors of the other articles in this issue, who provided us with valuable and constructive feedback during the workshop that took place in the summer of 2015. Foster Provost thanks NEC and Andre Meyer for Faculty Fellowships.\n",
              "\n",
              "References\n",
              "Adomavicius, G., and Tuzhilin, A. 2005. “Toward the next Generation of Recommender Systems: A Survey of the State-ofthe-Art and Possible Extensions,” IEEE Transactions on Knowledge and Data Engineering (17:6), pp. 734-749. Aral, S., Muchnik, L., and Sundararajan, A. 2009. “Distinguishing Influence-Based Contagion from Homophily-Driven Diffusion in Dynamic Networks,” Proceedings of the National Academy of Sciences (106:51), pp. 21.544-21-21.549. Bergstra, J., and Bengio, Y. 2012. “Random Search for HyperParameter Optimization,” Journal of Machine Learning Research (13), pp. 281-305. Borgatti, S. P., and Everett, M. G. 1997. “Network Analysis of 2Mode Data,” Social Networks (19:3), pp. 243-269. Breiger, R. L. 1974. “The Duality of Persons and Groups,” Social Forces (53:2), pp. 181-190. Chapelle, O., and Keerthi, S. 2008. “Large Scale Support Vector Machines,” presentation at the ICML Workshop on Large Scale Learning, July 9. Clark, J., and Provost, F. 2016. “Matrix-Factorization-Based Dimensionality Reduction in the Predictive Modeling Process: A Design Science Perspective,” Working Paper No. CBA-16-01, Stern School of Business, New York University. Fader, P. S., Hardie, B. G. S., and Ka, L. L. 2005. “RFM and CLV: Using Iso-Value Curves for Customer Base Analysis,” Journal of Marketing Research (42:4), pp. 415-430. Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., and Lin, C. J. 2008. “LIBLINEAR: A Library for Large Linear Classification,” Journal of Machine Learning Research (9), pp. 1871-1874\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "885\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Fawcett, T. 2006. “An Introduction to ROC Analysis,” Pattern Recognition Letters (27:8), pp. 861-874. Hastie, T., Tibshirani, R., and Friedman, J. 2001. The Elements of Statistical Learning, Data Mining, Inference, and Prediction, New York: Springer. Hill, S., Provost, F., and Volinsky, C. 2006. “Network-Based Marketing: Identifying Likely Adopters Via Consumer Networks,” Statistical Science (22), pp. 256-276. Hormozi, A.M., and Giles, S. 2004. “Data Mining: A Competitive Weapon for Banking and Retail Industries,” Information Systems Management (21:2), pp. 62-71. Hu, X. 2005. “A Data Mining Approach for Retailing Bank Customer Attrition Analysis,” Applied Intelligence (22:1), pp. 47-60. Junqué de Fortuny, E., Martens, D., and Provost, F. 2013. “Predictive Modeling with Big Data: Is Bigger Really Better?,” Big Data (1:4), pp. 215-226. Latapy, M., Magnien, C., and Vecchio, N. 2008. “Social Networks: Basic Notions for the Analysis of Large Two-Mode Networks,” Social Networks (30:1), pp. 31-48. Linoff, G. S., and Berry, M. J. 2011. Data Mining Techniques: For Marketing, Sales, and Customer Relationship Management, Hoboken, NJ: Wiley Computer Publishing. Macskassy, S. A., and Provost, F. 2003. “A Simple Relational Classifier,” in Proceedings of the Second Workshop on MultiRelational Data Mining, Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, pp. 64-76. Macskassy, S.A., and Provost, F. 2007. “Classification in Networked Data: A Toolkit and a Univariate Case Study,” Journal of Machine Learning Research (8), pp. 935-983. Martens, D., and Provost, F. 2014. “Explaining Data-Driven Document Classifications,” MIS Quarterly (38:1), pp. 73-99. McPherson, M., Smith-Lovin, L., and Cook, J.M. 2001. “Birds of a Feather: Homophily in Social Networks,” Annual Review of Sociology (27:1), pp. 415-444. Moges, H., Dejaeger, K., Lemahieu, W., and Baesens, B. 2012. “A Total Data Quality Management for Credit Risk: New Insights and Challenges,” International Journal of Information Quality (3:1) (http://dx.doi.org/10.1504/IJIQ.2012.050036). Ng, A. Y. 2004. “Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance,” in Proceedings of the 21st International Conference on Machine Learning, Banff, Canada. Perlich, C., Provost, F., and Simonoff, J. S. 2003. “Tree Induction vs. Logistic Regression: A Learning-Curve Analysis.,” Journal of Machine Learning Research (4), pp. 211-255. Provost, F., and Fawcett, T. 2013. Data Science and its Relationship to Big Data and Data-Driven Decision Making, Sebastapol, CA: O’Reilly Media.\n",
              "\n",
              "Van Den Poel, D., and Lariviere, B. 2003. “Customer Attrition Analysis for Financial Services Using Proportional Hazard Models,” Journal of Operational Research (157), pp. 196-217. Vapnik, V. N. 1995. The Nature of Statistical Learning Theory, New York: Springer-Verlag. Verbeke, W., Martens, D., and Baesens, B. 2014. “Social Network Analysis for Customer Churn Prediction,” Applied Soft Computing (14:Part C), pp. 431-446. Wexler, M. 2014. Private communication.\n",
              "\n",
              "About the Authors\n",
              "David Martens is a Professor at the University of Antwerp, where he heads the Applied Data Mining research group. His research focuses on the development and application of data mining techniques that lead to improved understanding of human behavior, and the use thereof in marketing and finance. In 2014, David won the “Best EJOR Application Paper Award” (European Journal of Operational Research), and in 2008 was a finalist for the prestigious international KDD doctoral dissertation award. Foster Provost is Professor of Information Systems and Data Science, and Andre Meyer Faculty Fellow at New York University’s Stern School of Business. He is coauthor of the best-selling data science book, Data Science for Business. His research has won many awards, including best paper awards at KDD, and the INFORMS Design Science Award. It also formed the basis for several data-science-oriented companies. Foster previously was editor-in-chief of the journal Machine Learning. His latest album, Mean Reversion, will be released in 2016. Jessica Clark is a doctoral candidate in the Information, Operations, and Management Sciences department at New York University’s Stern School of Business, concentrating in Information Systems. Her research focuses on applied data science, particularly in the domain of advertising. Enric Junqué de Fortuny is an Assistant Professor at the Rotterdam School of Management, Erasmus University Rotterdam (The Netherlands). He holds a Master’s degree in Computer Science Engineering from the University of Ghent (Belgium), a Ph.D. in Applied Economics from the University of Antwerp (Belgium), and was previously a Senior Research Fellow at INSEAD’s eLab for Big Data (France/Singapore). His research interests include the development of machine learning algorithms with a specific focus on predicting human behavior and improving comprehensibility in data science.\n",
              "\n",
              "886\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Appendix A\n",
              "Output Score\n",
              "\n",
              "Figure A1. Output Score of BeSim Model for Product 1, with the Consumers Ranked According to the Output Score (Similarity for Product 2). Most consumers receive a (near-) zero score while a few receive a high score.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "887\n",
              "\n",
              "\n",
              "Martens et al./Mining Massive Fine-Grained Behavior Data\n",
              "\n",
              "Appendix B\n",
              "Feature Selection\n",
              "\n",
              "Figure B1. AUC (Y-axis) for an Increasing Number of Features (X-axis). We chose to use a maximum of 30 features, as a plateau is reached at that point for both products (marked with the dotted line).\n",
              "\n",
              "888\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Copyright of MIS Quarterly is the property of MIS Quarterly and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "UQVYQowfKJCD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RH-ppIHeGzrR",
        "colab_type": "code",
        "outputId": "0c840829-65af-4167-b55d-0b72a29841af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "significantly_Sents = [sent for sent in doc5.sents if 'significantly' in sent.string]\n",
        "significantly_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[However, Figure 4 also shows that while the shape of the best Beta distribution conforms to that of the shape of the ICF\n",
              " \n",
              " measure, the resultant weights are significantly different.,\n",
              " Based on preliminary analysis, this setup achieved similar (not significantly different) generalization performance at a more than ten-fold speed increase.,\n",
              " Using a one-sided paired t-test over the 10 randomizations, we find that the combined BeSim + SD model performs significantly better than the individual BeSim and SD models for the AUC, lift at 5 percent and lift at 10 percent (all p-values < 1e-5).,\n",
              " For the lift at 1 percent, the BeSim model alone performs best, significantly outperforming the SD model (p-value 7e-6), and the combined method (at a lower significance level, p-value 0.02).,\n",
              " For product 2, the same techniques perform best, always significantly outperforming the two others (all p-values < 1e-5).,\n",
              " The production results are proprietary, but the bank claims that their own “A/B” evaluations support our conclusions: the prospects identified by the BeSim-based models actually purchased the product significantly more frequently than the prospects identified by the traditional targeting models.\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "uVeOWmS5xqUd",
        "colab_type": "code",
        "outputId": "7066bde3-dd3c-4fe3-8963-a468e5e3bddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "significantly_Sents[2:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Using a one-sided paired t-test over the 10 randomizations, we find that the combined BeSim + SD model performs significantly better than the individual BeSim and SD models for the AUC, lift at 5 percent and lift at 10 percent (all p-values < 1e-5).,\n",
              " For the lift at 1 percent, the BeSim model alone performs best, significantly outperforming the SD model (p-value 7e-6), and the combined method (at a lower significance level, p-value 0.02).,\n",
              " For product 2, the same techniques perform best, always significantly outperforming the two others (all p-values < 1e-5).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "wbQ3QWQlGWXf",
        "colab_type": "code",
        "outputId": "fb6cdab8-df21-4cc4-f16d-6778561c0b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8908
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc6 = nlp(open(u\"SteinbartP#KeithM#BabbJ_2016_Examining the Continuance of Secure Behavior - A Longitudinal Field Study of Mobile Device Authentication_Information Systems Research_2.txt\").read())\n",
        "doc6\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This article was downloaded by: [130.89.97.167] On: 17 February 2017, At: 08:09 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA\n",
              "\n",
              "Information Systems Research\n",
              "Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org\n",
              "\n",
              "Examining the Continuance of Secure Behavior: A Longitudinal Field Study of Mobile Device Authentication\n",
              "Paul John Steinbart, Mark J. Keith, Jeffry Babb\n",
              "\n",
              "To cite this article: Paul John Steinbart, Mark J. Keith, Jeffry Babb (2016) Examining the Continuance of Secure Behavior: A Longitudinal Field Study of Mobile Device Authentication. Information Systems Research 27(2):219-239. http://dx.doi.org/10.1287/ isre.2016.0634 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages\n",
              "\n",
              "INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org\n",
              "\n",
              "\n",
              "Information Systems Research\n",
              "Vol. 27, No. 2, June 2016, pp. 219–239 ISSN 1047-7047 (print) ISSN 1526-5536 (online) http://dx.doi.org/10.1287/isre.2016.0634 © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Examining the Continuance of Secure Behavior: A Longitudinal Field Study of Mobile Device Authentication\n",
              "Paul John Steinbart\n",
              "Department of Information Systems, Arizona State University, Tempe, Arizona 85287, paul.steinbart@asu.edu\n",
              "\n",
              "Mark J. Keith\n",
              "Information Systems Department, Brigham Young University, Provo, Utah 84602, mark.keith@gmail.com\n",
              "\n",
              "Jeffry Babb\n",
              "Department of Computer Information Systems and Decision Management, West Texas A&M University, Canyon, Texas 79016, jbabb@wtamu.edu\n",
              "\n",
              "t is not enough to get information technology (IT) users to adopt a secure behavior. They must also continue to behave securely. Positive outcomes of secure behavior may encourage the continuance of that behavior, whereas negative outcomes may lead users to adopt less-secure behaviors. For example, in the context of authentication, login success rates may determine whether users continue to use a strong credential or switch to less secure behaviors (e.g., storing a credential or changing to a weaker, albeit easier to successfully enter, credential). Authentication is a particularly interesting security behavior for information systems researchers to study because it is affected by an IT artifact (the design of the user interface). Laptops and desktop computers use full-size physical keyboards. However, users are increasingly adopting mobile devices, which provide either miniature physical keypads or touchscreens for entering authentication credentials. The difference in interface design affects the ease of correctly entering authentication credentials. Thus, the move to use of mobile devices to access systems provides an opportunity to study the effects of the user interface on authentication behaviors. We extend existing process models of secure behaviors to explain what inﬂuences their (dis)continuance. We conduct a longitudinal ﬁeld experiment to test our predictions and ﬁnd that the user interface does affect login success rates. In turn, poor performance (login failures) leads to discontinuance of a secure behavior and the adoption of less-secure behaviors. In summary, we ﬁnd that a process model reveals important insights about how the IT artifact leads people to (dis)continue secure behaviors. Keywords : continuance of security behavior; security behaviors; authentication; password; passphrase; mobile computing; smartphone; usability; user interface; longitudinal research; ﬁeld experiment History : Radhika Santhanam, Senior Editor; Alessandro Acquisti, Associate Editor. This paper was received on March 19, 2014, and was with the authors 8 months for 3 revisions. Published online in Articles in Advance May 19, 2016.\n",
              "\n",
              "I\n",
              "\n",
              "1.\n",
              "\n",
              "Introduction\n",
              "\n",
              "Although a large and growing body of research (Anderson and Agarwal 2010, Dinev and Hu 2007, Huang et al. 2011, Lee and Larsen 2009, Liang and Xue 2010) has shed light on the factors that inﬂuence a user’s initial adoption of secure behaviors, there has been little research about the long-term continuance of secure behaviors. This gap is important because persuading people to adopt a desirable security practice is of limited value if they subsequently discontinue it and revert to less-secure behaviors. Research has shown that initial use decisions are different from decisions to continue using that system (Agarwal and Karahanna 2000, Bhattacherjee 2001, Karahanna et al. 1999, Kim et al. 2007, Taylor and\n",
              "219\n",
              "\n",
              "Todd 1995, Venkatesh and Bala 2008). In particular, actual experience using a system moderates the inﬂuence of intentions and attitudes on subsequent behavior (Hong et al. 2008; Limayem et al. 2007; Venkatesh et al. 2008, 2012). For example, people tend to disable or stop using security features that are inconvenient or difﬁcult to perform (Adams and Sasse 1999). Authentication controls are a critical part of an information security program because their objective is to limit system access to only authorized individuals. Three types of authentication credentials are commonly used: something you know (e.g., a PIN, password, or passphrase), something you have (e.g., a smart card or USB token), or something you are (e.g., biometric identiﬁers such as ﬁngerprints, voice recognition, etc.). Of these three types of credentials,\n",
              "\n",
              "\n",
              "220\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "the ﬁrst is ubiquitous. Indeed, in many systems, a user name and a password (or equivalent) is the only authentication credential used. Even when systems require multiple credentials, a practice referred to as multifactor authentication, passwords usually are one of those factors. Thus, password usability remains an important security topic. A well-documented challenge in the use of password-based authentication is the trade-off between security and ease of use (Brown et al. 2004; Huang et al. 2011; Ives et al. 2004; Keith et al. 2007, 2009; Yan et al. 2004; Zviran and Haga 1999). For example, people tend to create passwords that are easy to remember, which often means that they are also easy to guess. The importance of ease of use means that features of the information technology (IT) artifact, such as the nature of the user interface (UI), are likely to be key determinants of the decision to continue a secure authentication behavior. The potential effect of the design and implementation of the UI on secure authentication behavior is particularly relevant in light of the ever-increasing use of mobile devices (e.g., phones, tablets, etc.) to access and store sensitive information in ﬁnancial systems and social networks. Whereas desktop and laptop computers provide full-size physical keyboards for entering authentication credentials, mobile devices possess miniature keyboards or, increasingly, touchscreens, which represent ﬂat facsimiles of a traditional keyboard. These differences in the UI make data entry both slower and more error prone when using mobile devices rather than desktop or laptop computers (Bao et al. 2011, Jakobsson and Akavipat 2012, Lee and Zhai 2009, Park et al. 2008). Consequently, if people experience difﬁculty in entering strong authentication on mobile devices, they may be tempted to discontinue a secure authentication behavior (e.g., the use of a strong authentication credential) and adopt a less-secure, but easier to successfully perform, alternative. For example, they may store the password on their device and conﬁgure it to automatically submit it whenever accessing a remote system. Such behavior seriously weakens authentication security—particularly in light of the millions of reported incidents of mobile device loss or theft each year (ConsumerReports 2014). People also sometimes “loan” their phone to others (Ben-Asher et al. 2011, Karlson et al. 2009). Whether lost, stolen, or loaned, the result is that an unauthorized person has physical possession of the device. Therefore, if the device was conﬁgured to automatically submit authentication credentials, the risk of unauthorized access to systems that contain sensitive information is high. This study makes several important contributions to the security literature. First, we examine the factors that inﬂuence the continuation of secure behaviors. We use the results of psychology research on\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "memory structures and the modiﬁcation of decision strategies to extend existing information systems (IS) process models of security behavior (Liang and Xue 2009). Speciﬁcally, we characterize individuals’ attempts to behave securely as part of a cybernetic loop that inﬂuences subsequent behavior (Carver and Scheier 1982, Wiener 1948). Our second contribution is methodological. Two of the greatest difﬁculties in information privacy and security research are (1) collecting valid measures of real user behaviors and (2) monitoring these behaviors over time (Belanger and Crossler 2011, Smith et al. 2011). We address those problems by designing a longitudinal ﬁeld experiment that allows us to observe people’s authentication behaviors in a natural setting, rather than in a controlled laboratory experiment. This increases the likelihood that the behaviors we observe are representative of those likely to occur in practice. In particular, we investigate how the UI, speciﬁcally the means (touchscreen versus fullsize physical keyboard) used to enter authentication credentials to obtain access to a remote system, affects continuance of authentication behaviors. Our results suggest that despite mechanisms designed to improve the usability of mobile keyboards (e.g., displaying the last character typed), mobile interfaces clearly hinder the continuance of secure authentication behaviors. Consequently, the mobile interface may be the catalyst that ﬁnally shifts—indeed, is currently shifting—the security paradigm away from relying primarily (and in many cases, solely) on textentry-based authentication and toward the use of multifactor approaches that include other types of credentials (e.g., biometrics).\n",
              "\n",
              "2.\n",
              "\n",
              "Literature Review and Theory\n",
              "\n",
              "Voluntary security behaviors in nonworkplace settings have received increased research attention over the past decade. For example, studies have examined the use of antimalware to protect computers (Dinev and Hu 2007, Johnston and Warkentin 2010, Lee and Larsen 2009, Liang and Xue 2010), the use of ﬁrewalls to control access to home wireless networks (Woon et al. 2005), and intent to engage in protective behaviors to respond to computer security threats in general (Anderson and Agarwal 2010). This stream of research has primarily drawn on protection motivation theory (Rogers 1975, Tanner et al. 1991) to explain how and why people choose to behave securely. Liang and Xue (2009) developed an IS-speciﬁc variant, which they called Technology Threat Avoidance Theory (TTAT), designed to speciﬁcally focus on computer security. According to TTAT, secure behaviors represent a coping response to recognized threats. The two most important antecedents\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "221\n",
              "\n",
              "Figure 1\n",
              "\n",
              "(Color online) Cybernetic Loop and TTAT Process Model\n",
              "\n",
              "Cybernetic loop (Carver and Scheier 1982) Goal\n",
              "\n",
              "TTAT process model portion (Liang and Xue 2009) Goal (avoid malicious IT) Comparator (threat appraisal)\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Comparator Input function (perception) Impact on environment Output function (behavior) Input function (perception)\n",
              "\n",
              "Output function (coping behavior) Impact on environment (improved protection)\n",
              "\n",
              "Source. Adapted from Liang and Xue (2009).\n",
              "\n",
              "of secure behaviors are (1) perceptions that a threat is serious (i.e., has a high likelihood of occurring and results in severe consequences) and (2) perceptions that the threat is avoidable because there exist effective countermeasures that can be performed without excessive cost or effort. This variance model has been validated in a number of studies (e.g., Johnston and Warkentin 2010, Liang and Xue 2010). However, TTAT also includes a process model explaining the continuance of security behaviors that has received signiﬁcantly less attention from researchers. The process model is based on a cybernetic feedback loop (Carver and Scheier 1982). One of the core concepts of cybernetic theory (Wiener 1948) is that human beings constantly adjust their behavior toward an end goal via cybernetic feedback loops (Carver and Scheier 1982). A cybernetic loop consists of a goal, comparator, input function, environmental impacts, and output function (see Figure 1). The cybernetic loop begins when there is a disturbance to either the environment (e.g., the user’s password is cracked) or the goal (e.g., IT manager dictates a stronger password). Consider, for example, that someone has purchased a new mobile device and would like to use it for ﬁnancial transactions such as online banking or investments. However, the ﬁnancial institution likely has a security policy that requires use of an authentication credential that meets certain guidelines. Or, the user voluntarily chooses to create a strong authentication credential. Either way, the result is a new goal: to create and use a strong credential to access the remote system. The user compares their present state (weak or no authentication credential) to their desired state (strong credential). The result of that comparison triggers the output function. The output function represents the behavior intended to eliminate the discrepancy—in this case, the creation of a stronger password. The impact on the environment is that the user’s credential is now more resistant to\n",
              "\n",
              "password guessing and cracking. If the credential is not sufﬁciently strong, the input function senses the noncompliance, compares it to a reference point (comparator), which may be either an internal standard or an explicit requirement embedded in the remote system, and the loop repeats until the goal is achieved. Thus, the task of creating a strong credential can be explained as an iterative process in which the user continually reﬁnes the credential until it becomes satisfactory based on system feedback. However, if there is a disturbance in the environment, the cybernetic loop may continue even after a credential is created. After creating a strong credential, another important factor emerges—the ease of remembering and typing that credential to authenticate to the system. Strong credentials may satisfy the goal of providing more security, yet be difﬁcult to use (Brown et al. 2004; Keith et al. 2007, 2009). If a strong authentication credential results in more failed login attempts, a new goal surfaces: to make the credential more usable. The output function represents the behavior(s) designed to achieve this new goal, including (1) becoming skilled at remembering and typing the credential correctly, (2) changing the credential to something that is still compliant with policy, but easier to use, or (3) storing the credential so that it does not have to be continuously reentered. The latter two coping mechanisms may weaken security. Figure 2 depicts the cybernetic loop of TTAT in a sequential (rather than looped) model (as depicted in Figure 1) beginning with an initial output function behavior and including a subsequent output function. In summary, following the example of TTAT, we adopt the cybernetic feedback loop (Carver and Scheier 1982, Liang and Xue 2009, Wiener 1948) as our core theory to explain the continuance of secure authentication behaviors. We extend that basic model by using relevant reference disciplines to explain steps B through E in Figure 2 in more detail. In\n",
              "\n",
              "\n",
              "222\n",
              "Figure 2\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "(Color online) Two Cycles Through the Cybernetic Feedback Loop\n",
              "\n",
              "a. Output function (behavior)\n",
              "\n",
              "b. Impact on environment\n",
              "\n",
              "c. Input function (perception)\n",
              "\n",
              "d. Comparator\n",
              "\n",
              "e. Output function (behavior)\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Coping behavior (initial credential strength)\n",
              "\n",
              "Experience (login success)\n",
              "\n",
              "Comprehension and interpretation of usability and login success\n",
              "\n",
              "Comparison of the results to the initial goal\n",
              "\n",
              "+ –\n",
              "\n",
              "Coping behavior (continued use) Coping behavior (storing and changing credential)\n",
              "\n",
              "User interface (mobile vs. full keyboard)\n",
              "\n",
              "Previously explained by TTAT (Liang and Sue 2009) and protection motivation theory (Johnston and Warkentin 2010)\n",
              "\n",
              "Explained here in the authentication context by memory systems theory (Squire 2004, Ullman 2004)\n",
              "\n",
              "Supported here by theory on cognitive load (Todd and Benbasat 1991, 1992, 1999), IS continuance (Bhattacherjee and Premkumar 2004, Brown et al. 2012, Ortize de Guinea and Webster 2013), and psychology theory on emotion (Bargh and Ferguson 2000, Rydell et al. 2008)\n",
              "\n",
              "particular, we draw on research on memory systems (Ullman 2004) to explain how the user interface inﬂuences (dis)continuance of a secure authentication behavior (step B of Figure 2). We draw from theory on IS continuance and psychology theory concerning the inﬂuence of perceived effort and emotions on decision strategies to explain how user perceptions and reactions to performing the security behavior (step C of Figure 2) inﬂuence the decision to either continue to engage in that behavior or to switch to an alternative designed to bring the user’s existing state and goal state in line (steps D and E of Figure 2). Environmental Impacts: Memory Systems and the User Interface Many security behaviors require both recall of knowledge and skill in applying that knowledge. For example, successful authentication to a system requires both remembering the credential linked to that system and then correctly entering it. Similarly, encrypting sensitive information requires recognizing the need to encrypt something and knowing how to do so. Sometimes the objective is to learn what not to do (e.g., reduce the risk of malware by not clicking on URL links embedded in email) and then practicing such restraint. Research on memory structure, particularly the distinction between declarative and procedural memory (Squire 1986), provides an explanation of how these two components of security behavior tasks (recall and procedures) jointly interact to affect task success. 2.1.1. Role of Human Memory Systems. Authentication credentials should be easy to remember and use yet also be resistant to guessing and brute-force 2.1.\n",
              "\n",
              "enumeration attacks. The nature of human memory makes it difﬁcult to satisfy both objectives. New information (e.g., authentication credentials) is initially stored in short-term memory until it can be transferred to long-term memory (Ullman 2004). This process is aided by rehearsal and “chunking” the information held in short-term memory (Baddeley 1994). Chunking refers to the ability to group information together (e.g., letters into words, words into phrases) to aid retention. Meaningful information is easier to remember (Ebbinghaus 1913). Words with speech sounds similar to previously learned words are also easier to store (see the phonological similarity effect; Baddeley 2012), which explains why mnemonic passwords are easier to remember than random ones (Yan et al. 2004). This phenomenon also likely accounts for the widespread use of common words and personal information as passwords (Johansson and Riley 2005). However, “simple” authentication credentials comprised of common words found in dictionaries, such as ﬂuffy or nonetheless, are weak because they are easily guessed or cracked through brute-force enumeration techniques. Credentials that add one or more numbers either before or after a common word, such as 579rhyme or ready123, or that replace letters with symbols, such as p@$$w0rd, are also “simple” because they, too, are weak and easily cracked. Therefore, most organizations have password policies that prohibit the use of common words as part of an authentication credential and instead require users to create credentials that include a mix of uppercase and lowercase letters, numbers, and special characters. Credentials that satisfy those constraints, such as Tq7#P@m9, are considered “complex” and are stronger and more resistant to attack. However, such complex credentials are also quite unlike\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "223\n",
              "\n",
              "existing words and phrases already stored in memory. Novel words and phrases are more difﬁcult to store and recall (Squire 2004). Thus, we would expect that it should be harder to remember complex than simple passwords. However, it is also possible to use a passphrase as a credential. Because a credential’s resistance to brute-force attacks is affected more by its length than by the size of the character set, there are arguments that long passphrases consisting solely of uppercase and lowercase letters are stronger than shorter passwords that include nonletter characters (Keith et al. 2007, 2009). Moreover, because the entire phrase (e.g., IwentsnorkelingintheNationalParkatKeyBiscayne) is used, rather than using only speciﬁc characters from the phrase (e.g., using the ﬁrst letter of each word in the preceding phrase to create IwsitNPaKB) to create a shorter password, it should be easier to correctly recall passphrases than complex passwords. Indeed, evidence indicates that passphrases result in fewer memory-based errors in recall (Jakobsson and Akavipat 2012) and fewer actual login failures based on memory failures (Keith et al. 2007, 2009) than passwords. By contrast, passwords comprised of 12 characters that do not constitute a meaningful phrase are hard to remember (Paul et al. 2011). Thus far, we have discussed the process of creating and initially learning a new authentication credential. However, to retain knowledge, it must be transferred from short-term into long-term memory (Baddeley 2012). Long-term memory can be divided into two primary types: declarative and procedural. The declarative memory system underlies the learning and retention of facts and events (Ullman 2004)— it is the type referred to as “memory” in everyday language (Squire 2004). Declarative memory is almost entirely explicit, meaning it is available at a conscious level. Although authentication credentials may be initially stored as episodic declarative memory—meaning they are tied closely to the cues and details relevant to the event of creating it—they are eventually stored as facts, making them semantic declarative memories. Whereas episodic memories are very event speciﬁc and context speciﬁc—making them less transferrable from one situational recall to another—semantic memories are relatively easier to recall across contexts (e.g., entering the same password on multiple computers). The preceding discussion explains how declarative memory affects credential recall. However, authentication credentials must not only be recalled correctly but also entered correctly into a login prompt. This is the role of procedural memory (Ullman 2013). The procedural memory system represents the motor skills learned from performing a given action or sequence of actions and rules repeatedly\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "(Squire 2004). Procedural memory represents our nonconscious implicit motor skills such as riding a bicycle or entering a password or passphrase via a physical keyboard or touch screen. In summary, authentication credentials are learned in a process that begins with short-term memory, where the credential is rehearsed long enough to transfer it to long-term memory. During initial use of the credential, the user relies primarily on declarative memory to recall it. Then, with practice in entering the credential, it becomes stored in procedural memory. Eventually, because declarative memory tends to decay quickly (Ullman 2004), a user may rely primarily on procedural memory to correctly enter the credential. Credentials that are either adequately rehearsed, well “chunked,” based on meaningful information, or comprised of known words and phrases (or some combination of those factors) will be more easily stored in short-term memory and transferred to declarative memory. Thus, simple passwords and passphrases should be easier to learn and recall than complex passwords comprised of randomly scrambled different types of characters. Furthermore, because complex credentials violate normal typing rules they are more difﬁcult to learn how to enter correctly and, therefore, to store in procedural memory than are either simple passwords or “wordprocessing-compatible” passphrases (i.e., those that use natural words, follow normal rules for capitalization and punctuation, and incorporate numbers as part of dates). Difﬁculty in retrieving the credential from declarative memory increases the likelihood of login failures due to memory problems; poor procedural memory increases the likelihood of login failures due to typographical errors. Thus, the preceding discussion leads to our ﬁrst hypothesis: Hypothesis 1 (H1). Authentication credential complexity is positively related to login failures. 2.1.2. Effect of the User Interface. The authentication task environment, including the UI1 of any IT artifacts employed, may affect both recall of information from declarative memory and the use of procedural knowledge. The UI can affect recall of information from declarative memory by altering the salience of cues. Cues that facilitate recall can be verbal, written, or graphical (Tulving and Pearlstone 1966). In particular, images and text that are present when new information is stored in working memory and transferred\n",
              "1\n",
              "\n",
              "It should be noted that there are many elements of the UI including graphical design of the form, text boxes, buttons, lists, data input controls, etc. (Shneiderman 1986). The elements most relevant to the authentication context include the layout of the keyboard (i.e., where the buttons reside), the total number of key presses required to enter a credential, the properties of the password input box, and the graphical- and text-based cues included on the authentication form screen.\n",
              "\n",
              "\n",
              "224\n",
              "Figure 3\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "(Color online) Example of Graphical- and Text-Based Cue Differences for Desktop and Mobile Interfaces\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "to long-term memory are effective cues that aid in retrieving that information from long-term memory at a later time (Ericsson and Kintsch 1995). For example, the images incorporated into advertising campaigns are remarkably effective at helping consumers recall brand names (Keller 1987). To understand the role of retrieval cues in credential recall, consider the recommendation to use a different password for each information system (Ives et al. 2004). Since different passwords are used across systems, the text and graphical content of the UI serves as a cue to help a person recall the correct password. Consequently, changing the UI, for example, by switching from a desktop to mobile (MacKay et al. 2004, Wiedenbeck et al. 2005), changes a website’s content and, therefore, may disrupt declarative recall, thereby increasing login failures. As an example, Figure 3 depicts both the desktop and mobile versions of the login screen for JPMorgan Chase. The desktop version contains a variety of images and text that are not present in the mobile version. Indeed, the textboxes and button that allow authentication comprise only a portion of the desktop screen, whereas the mobile version is entirely occupied by the authentication interface. As a result, the user cannot use any of the desktop cues to aid recall on the mobile UI. The nature of the UI can also affect the ability to use procedural knowledge. Full-size physical keyboards clearly display all possible characters and require at most the use of two keys simultaneously to capitalize a letter or enter a special character. By contrast, most touchscreens on mobile devices require considerable effort to change between lowercase and uppercase letters, or to select numbers and special characters. For example, Jakobsson and Akavipat (2012) point out\n",
              "\n",
              "that it takes 21 actions to enter the 12-character credential “ﬂY2theM0On!” on a touchscreen, but requires only 15 key presses if using a physical keyboard. It also requires interrupting the data entry process to decide whether a particular character is on the current screen (Sears and Zha 2003). Consequently, typing on touchscreens is inherently more error prone than typing on keyboards (Lee and Zhai 2009, Park et al. 2008). Indeed, studies have found that it takes two to three times longer to enter a typical complex password on a mobile device with a touchscreen or miniature keyboard than when entering the same credential via a full-sized physical keyboard (Bao et al. 2011). As a result, the touchscreen UI is clearly a unique context from traditional keyboards. As discussed above, procedural memory is difﬁcult to transfer from one context to another (Ullman 2013). Thus, the UI can affect login success by making it harder to both (1) recognize the cues used to recall the correct authentication credential from declarative memory and (2) by restricting the user’s ability to draw from procedural memory. This leads to our second hypothesis: Hypothesis 2 (H2). Login failures will be greater when entering authentication credentials using a miniature keyboard or touchscreen than when using a normal-sized physical keyboard. In addition to directly affecting login success, there is reason to believe that the UI will also moderate the effect of strong credentials on login success. Because the declarative and procedural memory systems interact cooperatively in human learning and processing, negative effects in one system may lead to negative effects on the other (Ullman 2004, 2013). This interdependent relationship was identiﬁed speciﬁcally in\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "225\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "the context of a word recall experiment. After disrupting the procedural memory system, participants performed lower on a word recall task than the control group (Brown and Robertson 2007). This interrelationship between the declarative and procedural memory systems suggests how the design of the UI can affect performance of a security task. Most people have developed procedural knowledge for typing their authentication credentials on the full-sized physical keyboards found on laptop and desktop computers, and can do so with a high rate of success. The different UI found on a mobile device, however, precludes drawing on that previously acquired procedural memory to enter a password with minimal conscious effort. Instead, users must consciously search for whether each character in their credential is visible on the screen or requires pressing a key to access a different touchscreen, while retaining the entire credential in memory. This heightened cognitive burden increases the difﬁculty of learning the new procedural knowledge (Keisler and Shadmehr 2010), thereby increasing the likelihood of login failures due to typing errors. If repeated, such failures may cause users to question whether they are recalling the correct information (i.e., authentication credential) for the task. As a result, the person may then try to resolve the problem by recalling and using different information. Thus, an initial error in applying procedural knowledge (e.g., unsuccessfully entering an authentication credential because of a typographical mistake) may cause a subsequent error in the declarative memory system (e.g., recalling the wrong credential). Thus, the preceding discussion suggests that the difference between the UI provided on mobile devices and that found on laptops and desktop computers is likely to exacerbate the inherent difﬁculty of using strong passwords and leads to our third hypothesis: Hypothesis 3 (H3). The effect of credential strength on login failures will be greater when using a miniature keyboard or touchscreen than when using a normal-sized physical keyboard. 2.2. The Comparator and the Outcome Function: How Experience Inﬂuences (Dis)Continuance Based on the cybernetic feedback loop (Carver and Scheier 1982), after the environment has been disrupted (e.g., via switching to use of a mobile UI for authentication), the user must perceive this disruption (input function), and make a comparison between the new state and the desired state (comparator) that leads to new behavior (next output function) designed to bring the two states into congruence. In the context of authentication, the comparison between expectations and experience (step D in Figure 2) is binary: the user either successfully accesses the system or fails to\n",
              "\n",
              "do so. These two outcomes determine what happens in step E in Figure 2 (selection of a coping behavior). According to cybernetic theory, success requires no change in behavior, because the user has obtained the desired goal. The desired state is what the user expects. Thus, when experience matches the desired state, it represents a conﬁrmation of expectations. IS research on system use has shown that conﬁrmation of expectations (and positive disconﬁrmation, i.e., ﬁnding that a system exceeds expectations) encourages continuance (Bhattacherjee and Premkumar 2004, Hong et al. 2006, Limayem et al. 2007). Hence, in the context of authentication, the login success should encourage continued use of the authentication credential. By contrast, if the comparator (step D in Figure 2) evaluates current experience as not matching the desired state (e.g., in the context of authentication, a login failure), the user takes action (coping behavior, step E in Figure 2) to resolve the discrepancy. As shown in Figure 2, there are three possible responses to a failure to authenticate when using a particular credential. One is to keep using the same credential, repeating the process until achieving success via practice. Another alternative is to store the credential so that it is automatically submitted when authenticating. This solution eliminates login failures, regardless of whether caused by forgetting the credential or difﬁculty in correctly entering it. A third possible solution that also ﬁxes both causes of login failures is to change to a different credential that is easier to remember and easier to type. The ﬁrst alternative (continued use of a credential) is desirable because it maintains security at a given level, whereas the other two alternatives solve the problem, but do so at the expense of weakening security. Prior research in both IS and psychology suggests several reasons why people may discontinue using an authentication credential that results in login failures and switch to an alternative behavior instead. Continued practice using an authentication credential will reduce login failures (Keith et al. 2007, 2009) while maintaining security, but requires time and effort. By contrast, the other possible alternatives (storing the credential for automatic submission or switching to a weaker credential that is easy to enter) quickly resolve the problem of login failures with minimal effort, but do so by weakening security. Research in psychology indicates that decision makers seek to maximize quality (accuracy) while simultaneously minimizing total effort (Payne 1982, Payne et al. 1993). IS research on decision aids has found that people’s perceptions of required effort play a bigger role in their choice of which decision strategy to adopt than does consideration of the relative quality of those options (Todd and Benbasat 1991,\n",
              "\n",
              "\n",
              "226\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "1992). In other words, people tend to trade off accuracy for effort. Moreover, this tendency persists even in the presence of explicit incentives related to accuracy (Todd and Benbasat 1999). This effort-accuracy trade-off may explain why people tend to discontinue performing security behaviors that are difﬁcult to perform successfully (Adams and Sasse 1999). Therefore, it is reasonable to expect that people will respond to login failures when using a strong credential by discontinuing a difﬁcult to perform behavior (manual entry of a strong authentication credential) and switching to an alternative behavior (e.g., storing their credential so that they do not have to manually enter it or switching to a weaker credential that is easier to enter) that requires less effort to perform successfully. Whereas conﬁrmation of expectations encourages continuance, negative disconﬁrmation of expectations (i.e., experiencing difﬁculties that were either unexpected or greater than anticipated) creates intentions to discontinue use (Bhattacherjee and Premkumar 2004, Brown et al. 2012, Ortiz de Guinea and Webster 2013). Moreover, those studies indicate that negative disconﬁrmation of expectations affects subsequent behavior both directly and, by creating dissatisfaction with the existing state of affairs, indirectly. The direct effect represents the role of cognitive perceptions on behavior, in that a discrepancy between the desired and actual states results in changing behavior to eliminate that discrepancy. The indirect effect, through dissatisfaction, represents the complementary role that emotions play on the decision to (dis)continuance (Ortiz de Guinea and Markus 2009). Dissatisfaction is a negative emotion. There is evidence that “people are motivated to change or alter their environment when in negative moods and to leave well enough alone when in positive moods” (Bargh and Ferguson 2000, p. 932). Furthermore, negative emotions encourage risky behavior (Rydell et al. 2008). In the context of authentication, login failures are likely to be perceived negatively because they deny anticipated access. Therefore, the resulting dissatisfaction may encourage discontinuing the behavior (manual entry of a strong authentication credential) that causes the problem (login failure) and changing to behaviors (e.g., storing the credential for autosubmission or replacing it with a simpler one that is easier to enter) that solve the problem, but do so by increasing risk. In summary, the preceding discussion suggests several complementary reasons why people are likely to respond to login failures (step E in the cybernetic loop depicted in Figure 2, coping behaviors) by discontinuing an existing authentication behavior and replacing it with a less secure behavior. Hence we offer the following multipart hypothesis:\n",
              "\n",
              "Hypothesis 4A (H4A). Login failures will increase the likelihood of storing an authentication credential and having it automatically submitted when attempting to access a remote system. Hypothesis 4B (H4B). Login failures will increase the likelihood of changing an authentication credential to one that is easier to use (shorter or less complex). 2.3. Effect of the User Interface on Coping Response In addition to the effect of login failures, there is also reason to expect that the UI will directly inﬂuence authentication behaviors. As shown in Figure 2, the decision to (dis)continue a behavior is affected by comparing a person’s experience in executing that behavior with the desired/expected state. When the desired and actual states match (i.e., when expectations are conﬁrmed) people are likely to continue the behavior; but when those expectations are disconﬁrmed, they are likely to respond by discontinuing the behavior. The nature and design of the UI plays an important role on that process by affecting how easy (hard) it is to successfully execute a given behavior. Indeed, people’s choice of a decision strategy can be inﬂuenced by changing the design of a decision aid so that it makes one strategy easier than another (Todd and Benbasat 1999). Moreover, ease of use should increase satisfaction and, thereby, the desire to continue using that system or performing a given behavior. By contrast, difﬁculties in use are likely to cause frustration and dissatisfaction, leading to discontinuance. In short, the UI can be either an enabler or inhibitor (Cenfetelli 2004) to the continuance of a security behavior. As discussed earlier, the touchscreen interface provided on smartphones and other mobile devices makes it more difﬁcult to enter long and complex authentication credentials, thereby increasing the likelihood of login failures due to typing errors. In addition, typing difﬁculties may also cause people to question whether they are entering the proper credential, which could, in turn, lead to login failures due to memory errors. Consequently, it is not surprising that many people report that they do not like using such an interface to enter authentication credentials (Trewin et al. 2012). Indeed, there is evidence that some people so dislike the process for entering passwords on touchscreens that they actively seek ways to avoid having to do so (Bao et al. 2011, Jakobsson and Akavipat 2012). This leads to the following multipart hypothesis: Hypothesis 5A (H5A). Ceteris paribus, users will be more likely to store their authentication credential when using a mobile UI.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "227\n",
              "\n",
              "Figure 4\n",
              "\n",
              "Theoretical Model and Hypotheses\n",
              "\n",
              "Credential strength\n",
              "\n",
              "H1 Login failures\n",
              "\n",
              "H4A H4B\n",
              "\n",
              "Coping behavior 1. Storing credential 2. Changing credential\n",
              "\n",
              "H3\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Mobile UI\n",
              "\n",
              "H2\n",
              "\n",
              "Mobile UI practice H5A, B\n",
              "\n",
              "Hypothesis 5B (H5B). Ceteris paribus, users will be more likely to change their authentication credential to a simpler one when using a mobile UI. 2.4. Effects of Practice and Learning Through repeated practice, people develop procedural memory about how to perform a task (Ullman 2004). As a result, performance tends to improve over time. For example, Keith et al. (2007, 2009) found that over a three-month period, participants in their experiments experienced fewer login failures due to either typographical or memory errors when entering passphrases via full-size physical keyboards. Thus, it is reasonable to expect that any effects associated with using a mobile device to authenticate should decrease over time. Consequently, we explore whether practice using a mobile device to authenticate moderates the effects of the mobile device UI on login failures and coping behaviors. In summary, Figure 4 presents our research model hypotheses, and research questions.\n",
              "\n",
              "found. Game points were summarized on the website leaderboard (see Figure 6). Participants had to authenticate through the website to view the leaderboard, photos of themselves and others, and details of each clue found (e.g., time span, location on a map, photo, points earned). To encourage participants to return often to the game website (and attempt to authenticate), several other features were implemented. First, participants could create an online proﬁle with a variety of personal and demographic information. Second, an online social network was incorporated into the game that allowed players to follow and track the progress of their friends, refer other players, and exchange messages through the website. Participants could also earn game points for completing their proﬁle, following other players, and referring friends. These features were designed to inspire website interactivity, boost authentication attempts, and create a natural sense of realism and actual privacy risk. To make the game points relevant and desirable to participants, we provided weekly and end-of-game rewards. Each week, we awarded ﬁve to 15 $10 gift cards to the participants who either (1) were ﬁrst to ﬁnd all of that week’s locations, or (2) were randomly selected based on a point-weighted virtual “drawing.” At the end of the game, the two participants with the most total points, and one more based on a point-weighted random drawing, won a new tablet computer. 3.1. Ensuring Experimental Validity Five hundred and sixty-eight undergraduates at a large private university in the western United States participated in the experiment. To generate valid and realistic information disclosure behaviors, participants needed to perceive actual personal risk and fear of disclosing information. This was accomplished in multiple ways. First, we obtained IRB2 approval to not require participants’ informed consent because informed consent automatically elevates participants’ awareness of risk and the artiﬁcial nature of data collection. Rather, participants were recruited under the false pretense that a local mobile app business wanted to pilot test a new geo-caching app at their university. As a result, there was no priming effect on participants and they were less susceptible to social desirability bias (Richman et al. 1999). Moreover, they were told that the friends they referred to the game did not have to be university students or employees.\n",
              "2\n",
              "\n",
              "3.\n",
              "\n",
              "Method\n",
              "\n",
              "We created a mobile app with an accompanying website to conduct a ﬁeld experiment to test the hypotheses. The mobile app was a game that allowed social interactions among players. This game required participants to login to the website frequently, using either personal computers (laptops or desktops) or a mobile device. The mobile app (called “ﬁndamine” or “ﬁnd.a.mine” in the Apple App Store and Google Play) is a modiﬁed geo-caching game visualized in Figure 5. Each week (for 12 weeks), three new clues were delivered to the participant’s mobile device (either tablet or smartphone). They earned points by deciphering the clue and travelling to the location. If the participant was close enough (GPS-veriﬁed) to the location, they could click a “Found it!” button, which would prompt them to take a picture of themselves at the location through the mobile app. If the participant could not decipher the clue, the app provided a closeness meter (see Figure 5(d) and 5(e)) that indicated how geographically close they were to the target clue. This indicator updated in real time allowing the participant to ﬁnd any clue as they travelled around. Participants earned game points for each clue\n",
              "\n",
              "Universities in the U.S. require research involving human subjects to be approved by an Institutional Review Board (IRB) to ensure that participants do not suffer physical or psychological harm. Normally, this involves fully explaining the nature of the experimental treatments and obtaining informed consent.\n",
              "\n",
              "\n",
              "228\n",
              "Figure 5 (Color online) Mobile App Screenshots\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "(a) Splash screen\n",
              "\n",
              "(b) Location data disclosure\n",
              "\n",
              "(c) Mobile app login\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "(d) Clue begins\n",
              "\n",
              "(e) Mobile app clue found\n",
              "\n",
              "(f) Desktop website login Web part\n",
              "\n",
              "Second, the context of the app was chosen to replicate several relevant forms of information privacy and encourage consistent disclosure. For example, by choosing an app design with weekly incentives, participants were motivated to play for more than just extra credit. Because it was a geo-caching app, there was a clear need to collect location data, which presents personal safety risks (Baum et al. 2009). The social network aspect of the app created both additional enjoyment as well as creating vertical and horizontal personal information privacy risks (Posey et al. 2010). Thus, participants’ personal information could legitimately be made publicly available—unless they set their privacy settings to restrict their data to “friends only” or “nobody.” This is critical because fear is essential to motivating users to create strong passwords (Vance et al. 2013).\n",
              "\n",
              "Third, the ﬁndamine app architecture needed to match those that are most potentially dangerous. In particular, the game was made possible by a native mobile app, a cross-platform website, and Web services that connected the mobile app to the external database. This common architecture allows mobile apps to easily send data to external thirdparty servers. When the app was introduced to participants, they were given a brief explanation of how the mobile app and website worked together with the same data. Consequently, participants were aware that the mobile app was capable of sending personal information to remote servers. 3.2. Experimental Manipulation The study’s objective is to investigate the effect of the UI on people’s behavior when using authentication\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "229\n",
              "\n",
              "Figure 6\n",
              "\n",
              "(Color online) Game Leaderboard (Desktop Website View)\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "credentials of different types and strength. Prior research has found that people do not voluntarily create long passphrases (Keith et al. 2007, 2009). Therefore, we manipulated the instructions in an attempt to encourage some participants to create and use passphrases. Participants were randomly assigned to one of three conditions when they opened the website.3 1. Control group: no instructions or requirements for credential generation. 2. Passphrase group: passphrases encouraged, but not required through technical veriﬁcation (to estimate real voluntary use). 3. Passphrase + beneﬁt group: passphrases encouraged and users told they would not be required to change their credential as often. 3.3. Measures Because of our research design, we were able to capture a variety of valid and objective measures for\n",
              "3\n",
              "\n",
              "credential characteristics, coping behaviors, and task success. 3.3.1. Credential Strength. The appendix describes several possible measures of credential strength and the reasons that led us to select experts’ subjective assessment to test our hypotheses. Consistent with prior research (Keith et al. 2007, 2009), we measured credential strength by providing the actual authentication credentials to two coders4 who were unaware of the study’s hypotheses and asked them to make a subjective judgment as to whether the credential was “simple,” “moderate,” or “complex” in terms of strength against cracking attempts. Because complex passwords do not follow traditional spellings and language patterns, they are also more resistant to guessing and “cracking.” As a result, the concept of complexity is essentially synonymous with credential strength (Adams et al. 1997, Keith et al. 2009) in the password context. Therefore, the coders were instructed to interpret these terms as being indicative of the likelihood that the credential could be\n",
              "4\n",
              "\n",
              "It should be noted that the purpose of this study is not to test the efﬁcacy of credential instructions. The purpose of the instructions was to encourage the creation of different types of credentials to facilitate testing our hypotheses.\n",
              "\n",
              "One coder was a master’s student in IS with an emphasis in security. The other coder is a vice president of operations at a large consumer information privacy company.\n",
              "\n",
              "\n",
              "230\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "guessed by a human or program. To further guide their subjective assessments, they were briefed on the objective experimental ﬁndings from prior research (Keith et al. 2007, 2009) on the proven characteristics of strong credentials (e.g., length and character variance). These coders agreed on 81% of the credential ratings K = 0 813 p < 0 001 . When the coders disagreed, we deferred to the practitioner. We used this three-level (simple–moderate–complex) subjective assessment to represent the strength of a participant’s initial authentication credential.5 3.3.2. Login Failures. Actual login failures were measured to represent the environmental impacts in Figure 2. Every login attempt was captured in the database and coded to reﬂect the outcome of that attempt (success or failure). 3.3.3. Longitudinal Authentication Behaviors. We captured two types of longitudinal authentication behaviors. First, for each login attempt we recorded whether the user entered their credential manually or avoided entry by using the “remember me” feature. Higher scores represent a greater percentage of all authentication attempts were performed with a saved credential—thus indicating less secure authentication behavior. Second, we stored every credential a user ever created and had the two judges rate both the initial and changed credentials. We calculated the change in credential strength by subtracting the rating of the old credential from the rating of the new. Both ratings were on three-point scales (simple, moderate, complex), so the change score could range from −2 to +2.6 Thus, negative scores reﬂect changing to a weaker credential, and positive scores reﬂect changing to a stronger credential. 3.3.4. Mobile Interface and Mobile Practice. To test the effects of the UI and practice we collected data about the client browser and operating system information at each login attempt to determine whether the login was from a mobile device or a traditional laptop or desktop computer. We created a variable called mobile interface by calculating the percentage of all login attempts made using a mobile device. However, this does not distinguish between a user who only made two login attempts total with one being from a mobile device and a user who made 20 login attempts, with 10 from a mobile device. Therefore, to differentiate between these two types of users,\n",
              "5\n",
              "\n",
              "Table 1\n",
              "\n",
              "Descriptive Statistics Male n = 402 Female n = 166 ¯ = 20 91 x ¯ = 1 425 x 17 (23.6%) ¯ = 0 30) 25 (x ¯ = 4 79 Total x ¯ = 1 43 Mobile x\n",
              "\n",
              "Age (years) Points accumulated Weekly prizes won Friends recruited Number of website sessions\n",
              "\n",
              "¯ = 23 46 x ¯ = 1 569 x 55 (76.4%) ¯ = 0 61) 162 (x ¯ = 9 90 Total x ¯ = 3 90 Mobile x\n",
              "\n",
              "Table 2 Treatment\n",
              "\n",
              "Type of Credential Created Password 91 81 85 277 141 Passphrase 9 18 15 52 (16%) 24 (15%)\n",
              "\n",
              "1. No prompt (control) (%) 2. Passphrase prompt (%) 3. Passphrase + Beneﬁt (%) Gender (of those who disclosed) Male Female\n",
              "\n",
              "we also calculated the raw count of attempts over a mobile device as a measure of overall mobile practice. 3.3.5. Control Variables. Three control variables were included in the analysis. First, the total number of login attempts was used to control in predicting the total number of login failures and the number of successful login attempts based on a stored password. Second, two demographic variables—age and gender—were collected from the ﬁndamine.mobi proﬁles as used as controls for login failures, storing credentials, and changing credentials.\n",
              "\n",
              "4.\n",
              "\n",
              "Result\n",
              "\n",
              "Reanalysis of our data using several composite measures of credential strength yielded substantially the same results. Therefore, in the interest of simplicity, we report analysis based on experts’ judgments of credential strength.\n",
              "6\n",
              "\n",
              "For example, changing from a complex to a simple credential would yield a change score of −2 1 − 3 , whereas changing from a complex to a moderate strength credential would yield a change score of −1 2 − 3 .\n",
              "\n",
              "4.1. Descriptive Statistics Table 1 presents descriptive statistics of the players (demographic data were recorded from the player’s game proﬁle) and their gameplay. About two-thirds (68%) of participants were male. Although participants could refer any friend to play the game to earn points, men comprise a larger portion of the electronic gaming population (ESA 2013) and even more so of geocachers (Schneider et al. 2011). Table 2 indicates the number of passwords versus passphrases created by group manipulation. An ANOVA including contrast estimates indicate that treatment 2 p = 0 03 was successful because players in that condition were more likely to create a passphrase than were players who were not prompted to consider doing so. Interestingly, treatment 3 resulted in fewer passphrases created than treatment 2. There was no gender difference in credential selection. Table 3 shows descriptive statistics about the length, character set, complexity, and strength of participants’ authentication credentials. Strong credentials\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "231\n",
              "PLS Path Loadings Not Depicted in Figure 7 Coefﬁcient −0 054 −0 011 −0 016 0 040 −0 037 −0 019 0 279 0 632 t -stat 1 28 0 53 0 58 0 92 1 36 0 84 1 26 2 16∗\n",
              "\n",
              "Table 3\n",
              "\n",
              "Credential Strength Details Simple Moderate 60 83 22 23 11 54 13 40 48 Complex 79 73 13 73 13 72 14 42 96 Overall 54 4 54 72 10 14 11 07 568\n",
              "\n",
              "Table 5 Path\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Average entropy (bits) Percent cracked Key presses required for traditional UI Key presses required for mobile UI Count\n",
              "\n",
              "47 87 66 59 9 17 10 05 424\n",
              "\n",
              "Gender → Login failures Gender → Storing credential Gender → Changing credential Age → Login failures Age → Storing credential Age → Changing credential Total login attempts → Login failures Total login attempts → Storing credential\n",
              "†\n",
              "\n",
              "had higher theoretical entropy, were more difﬁcult to crack, and required a greater number of key presses than either simple or moderate strength credentials (see the appendix for details). However, most credentials were simple and quickly cracked. Table 4 summarizes descriptive statistics about credential use. Participants attempted to log in from a laptop or desktop more often than from a mobile device (1,612 versus 1,022 attempts). However, 194 (34%) of participants logged into the website via their mobile device at least once, with almost 39% of all website access occurring via mobile device. Login failures were more likely when using a mobile device. Two hundred and eighteen (38%) participants stored their credentials, with storage being more likely when using mobile devices. Seven percent of participants changed their credentials a total of 45 times. Overall, the tendency was to change to a weaker credential: eight participants changed from a complex to a simple credential (change score of −2) and 16 changed from either complex to moderate or moderate to simple (change scores of −1), but only ﬁve people switched to a stronger credential. There was no signiﬁcant difference between passwords (36.2%) and passphrases (35.7%) in terms of storing the credential or changing to a credential that was either weaker or of the same strength (6.5% each). 4.2. Tests of Hypotheses We analyzed a path model with the partial least squares (PLS) structural equation modeling (SEM) technique using SmartPLS 3.0 (Ringle et al. 2014) to test our hypotheses. The use of PLS is appropriate because (1) we need to test multiple paths in the same model, (2) most of our measures are\n",
              "Table 4 Credential Use Behaviors Simple Total mobile login attempts Total traditional login attempts Mobile login failure rate (%) Traditional login failure rate (%) Mobile remembered login rate (%) Traditional remembered login rate (%) Count of changes to each type Count of changes away from each type 744 1,074 18.54 18.62 44.89 31.28 n = 32 n = 15\n",
              "\n",
              "p < 0 10; ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              "\n",
              "not interval based, and (3) several of our measures do not exhibit a normal distribution (Chin et al. 2003, Fornell and Bookstein 1982). Figure 7 shows the PLS model we used to test our hypotheses. Interaction effects were tested using the product-indicator approach (Chin et al. 2003). Table 5 lists the path coefﬁcients for covariates that were tested in our model, but which are omitted from Figure 7 so that it focuses on our hypotheses. The R2 values indicate the variance explained in that construct. H1 predicted that stronger authentication credentials would increase login failures due to typing errors and memory failures. Figure 7 shows that, after controlling for login attempts, the path from credential strength to login errors is signiﬁcant and positive = 0 254 p = 0 001 . Thus, H1 is supported. H2 predicted that login failures would be higher when using a mobile UI than when using a full-size physical keyboard. The path from the use of a mobile device to login failures is positive and signiﬁcant = 0 305 p = 0 041 . Thus, H2 is supported. H3 predicted that the effect of credential strength on login failures would be greater when using a mobile UI than when using a full-size physical keyboard. Figure 7 also shows that the use of a mobile device had a signiﬁcant, positive moderating effect on the effect of credential strength on login errors = 0 319 p = 0 027 . Thus, H3 is supported. H4A and H4B predicted that login failures would lead to the adoption of less secure authentication behavior, either by storing the authentication credential or changing to a weaker one. As shown in Figure 7, the path from login failures to storage of\n",
              "\n",
              "Moderate 139 361 42.27 25.76 40.72 26.87 n = 12 n = 20\n",
              "\n",
              "Complex 84 177 33.33 23.73 42.86 48.59 n=1 n = 10\n",
              "\n",
              "Total 1,022 1,612 24.31 20.78 43.85 32.20 n = 45 n = 45\n",
              "\n",
              "\n",
              "232\n",
              "Figure 7 (Color online) PLS Path Coefﬁcients\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "n.s.\n",
              "\n",
              "Mobile practice 0.30* – 0.29*\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Mobile interface 0.65*** 0.32* 0.31* 0.23*** Initial credential strength Login failures R2 = 36.7% – 0.22** Changing credential R2 = 13.6% Storing credential R2 = 38.7% – 0.17*\n",
              "\n",
              "0.25***\n",
              "\n",
              "Notes. Gender and age are not depicted for simplicity. ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              "\n",
              "the credential is signiﬁcant and positive = 0 226, p < 0 001 , indicating that login failures increased the tendency to store authentication credentials. The path from login failures to changing credentials is negative and signiﬁcant = −0 222 p = 0 009 , indicating that login failures result in changing to a weaker credential. Thus, both H4A and H4B are supported. Overall, 230 (40%) participants either stored their credential or changed it. H5A and H5B predicted that use of a mobile UI would increase the likelihood of less-secure authentication behaviors by either storing their authentication credentials or switching to a credential that is simpler to use but weaker. As shown in Figure 7, the path from mobile device use to storage of credentials is positive and signiﬁcant = 0 654, p = 0 001 , indicating that as the proportion of access attempts via mobile devices increases, so does the tendency to store the authentication credential. Figure 7 also shows that the path from mobile device use to entropy of the new credential is negative and signiﬁcant = −0 166 p = 0 024 , indicating that increased use of mobile devices increases the tendency to switch to a weaker credential. Thus, H5A and H5B are supported. We also explored whether practice ameliorates the impact of using a mobile UI on login failures and coping behaviors. Figure 7 shows that practice does not signiﬁcantly moderate the path between mobile device use and login failures = −0 093, p = 0 248 , but does affect both the likelihood of storing authentication credentials = 0 295 p = 0 033 and the tendency of mobile device users to change to a weaker authentication credential = −0 291, p = 0 050 . However, the signs of the coefﬁcients are mixed, indicating that practice affects the two coping behaviors in different ways. The sign of the coefﬁcient on the path representing H5A is positive, indicating\n",
              "\n",
              "that increased practice exacerbates, rather than mitigates, the tendency to store authentication credentials when using mobile devices. By contrast, the sign of the coefﬁcient on the path representing H5B is negative, indicating that practice ameliorates the tendency to switch to a simpler credential. Finally, we also examined demographic factors. Neither gender nor age affected login failures, the likelihood of storing credentials, or the likelihood of switching to a weaker credential.\n",
              "\n",
              "5.\n",
              "\n",
              "Discussion\n",
              "\n",
              "This study extends research on voluntary security behaviors by investigating the factors that inﬂuence their (dis)continuance. Drawing on the process portion of the TTAT proposed by Liang and Xue (2009), we ﬁnd that the decision to (dis)continue a security behavior emerges from a cybernetic loop that reﬂects one’s experience in performing that behavior. In other words, although users may initially adopt a secure behavior, they will modify or drop that behavior if it requires too much effort to perform successfully. Speciﬁcally, we found that when a strong authentication credential results in login failures, people tend to either store their authentication credential for autosubmission or to change to a credential that is easier to enter correctly. Both solutions solve the problem of login failures, but do so at the cost of weakening security. Thus, we show that the IT artifact (the nature of the user interface for entering authentication credentials) affects both the success in executing a security behavior and the decision to (dis)continue that behavior. Our ﬁndings have important implications for both research and practice. 5.1. Implications for Research Overall, our results show that a longitudinal and process-oriented perspective is essential to understanding how the IT artifact, speciﬁcally the nature\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "233\n",
              "\n",
              "of the user interface, affects user security behaviors. We found a clear cycle of behavior based on temporal introduction of different stimuli. Initially, users create authentication credentials that reﬂect their desire for a given level of security. Subsequently, after users practice a security behavior (by attempting to login through a prompt), their behaviors reﬂect a desire for usability: if their initial authentication credential is too difﬁcult to correctly enter via the UI of a mobile device they either conﬁgure the device to store and automatically submit the credential or they change to a simpler, but weaker, one. From a theoretical perspective, these ﬁndings support cybernetic loop theory (Carver and Scheier 1982) as a process model of security behavior continuance. We thus extend the process version of Liang and Xue’s (2009) TTAT to show that cybernetic processes not only explain how people respond to a threat but also account for how the IT artifact affects their security behaviors. We further contribute to this theory by drawing from research on memory systems (Ullman 2004), IS research on the effect of cognitive load on decision strategy (Todd and Benbasat 1991, 1992, 1999), and psychology research on the role of emotions in decision making (Bargh and Ferguson 2000, Bargh et al. 2001, Chen and Bargh 1999) to explain how the cybernetic loop process model will unfold in the context of authentication security goals and behavior. In particular, two user goals independently and jointly inﬂuence the continuance of secure behaviors over time and cause people to enter additional cycles of the cybernetic feedback loop. The ﬁrst goal is the user’s desire for security, which is currently well explained by variance models such as TTAT (Liang and Xue 2009) and adapted protection motivation-based theories (Johnston and Warkentin 2010, Herath and Rao 2009). The second goal is the user’s desire for usability, which is affected by the nature of the IT artifact (UI) used to perform the task. However, these two goals are not simultaneously considered (e.g., in a cost/beneﬁt trade-off calculation) each time a user must make a decision about their security behaviors. Rather, consideration of the security goal is stimulated by events such as security training and awareness programs (Bulgurcu et al. 2010) or personal experience with security breaches (Herath and Rao 2009), whereas the usability goal is stimulated by difﬁcult experiences with maintaining the security behavior as a person interacts with the IT artifact. Therefore, a theoretical process model that accounts for behavioral adjustments over time best explains behavior. Our ﬁndings also underscore the importance of acknowledging human factors issues, speciﬁcally ease of use, as an inhibitor of secure behavior. Just as people are often willing to trade off accuracy for effort when making decisions (Todd and Benbasat 1991,\n",
              "\n",
              "1992, 1999), our results show that they are willing to trade off security for ease of use. Thus, in our experiment, when people experience repeated login failures when using mobile devices, they respond by switching from a more secure authentication behavior (i.e., manual entry of a strong credential) to less secure, but easier to perform alternatives (use of a simpler credential or storage and autosubmission of the credential). Clearly, there is a need for further theory development and research that focuses on usability when using mobile devices in the authentication process. Prior research on authentication credentials when using physical keyboards provides an example of the kind of approach that is needed. Keith et al. (2007) found that long passphrases which did not reﬂect well-learned typing skills resulted in login failures. Building off that ﬁnding, Keith et al. (2009) hypothesized and found that simple instructions to create passphrases that used basic rules about typing (e.g., capitalization of initial letters of words, use of numbers as parts of dates, etc.) would make such credentials easier to use. In this study, we found that stronger authentication credentials increased login failures. An important topic for future research is to investigate how to design credentials that are strong, yet easy to correctly enter when using mobile devices with virtual touchscreens. Our results also suggest that improving user memory systems (i.e., practice) is not a panacea for the limitations of mobile interface. Although practice did decrease the propensity to switch to weaker credentials when using a mobile device to authenticate, it did not reduce the login failure rate when using a mobile device. Moreover, contrary to expectations, continued practice with the mobile UI to authenticate actually increased the propensity to store those credentials. We propose that these three ﬁndings are interrelated. The ﬁnding that practice did not reduce login failures suggests that entering a password over a mobile UI never became as easy as when using a traditional keyboard and full-sized screen. As explained before, the mobile UI still requires many more keystrokes for the same text (Jakobsson and Akavipat 2012) even if every keystroke is correct. Therefore, the repeated practice of authentication over a mobile UI may have been simply a stark reminder of the extra effort required to enter a strong credential with such a UI, and the concomitant increase in the likelihood of making mistakes. If we are correct that the problem is caused by the nature of the UI, it is logical that users would respond by adopting the coping behavior that resolves the problem by eliminating the need to manually enter the credential—thus, mobile UI practice leading to an increased likelihood of storing the password—rather than the coping behavior\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "234\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "that still required manual entry of the credential. However, although plausible, we cannot support this explanation with our data and, therefore, suggest that it is an important topic for future research. 5.2. Implications for Practice Although our study investigates voluntary security behavior, our ﬁndings are relevant to the workplace. Employers are increasingly allowing employees to use their own personal mobile devices to access the corporate network, a practice referred to as “bring your own device” (BYOD). Over time and through repeated experience, people develop habits on how they use technology (Limayem et al. 2007). If employees get habituated to acting in a certain way when using mobile devices for personal use, those habits may carry over when using that same personal mobile device for work. On the surface, the move to use mobile devices to authenticate to remote systems appears likely to improve security because it involves multifactor authentication using a combination of something you have (your mobile device) and something you know (a PIN, password, or passphrase). However, our results suggest that the inherent design features of the mobile UI may actually decrease security because users are likely either to change from a strong, but hard-to-enter, credential to a weaker, but easier to enter, one or to store their credential on the device and conﬁgure it for autosubmission (thus changing what appears to be multifactor authentication to multimodal authentication using two things that a person has: their mobile device and the stored credential). However, mobile devices are susceptible to being lost or stolen; over three million smartphones were stolen in 2013 (ConsumerReports 2014). In addition, people sometimes “loan” their phone to others (Ben-Asher et al. 2011, Karlson et al. 2009). In either case, this increases the risk that whoever has obtained physical possession of the device can attempt to gain unauthorized access to the corporate system. This risk is further increased by the fact that survey data indicate that a majority of companies that currently permit BYOD rely solely on passwords for authentication (Johnson and DeLaGrange 2012). One potential solution to this problem is for people to conﬁgure their mobile devices to require authentication to turn it on. Disturbingly, survey results ﬁnd that many people do not conﬁgure their phones to require any form of authentication (Clarke and Furnell 2005, ConsumerReports 2014, Jones and Heinrichs 2012). Moreover, the majority of people who do conﬁgure a password or PIN to access their phone report that they never change it (Barn et al. 2014). The trend to incorporate biometrics to initially logon to a mobile device (e.g., ﬁngerprints or facial\n",
              "\n",
              "recognition) mitigates the risk that an unauthorized person can use a lost or stolen device. However, the threat is not totally eliminated because (1) it is still possible for people to conﬁgure their device to bypass such controls; (2) the device could be left unattended and stolen after the owner used the biometric to turn it on; and (3) the owner would deﬁnitely have to have authenticated prior to “loaning” the device to another person. Thus, managers need to be concerned about how BYOD might affect employees’ choice and use of authentication credentials. Of course, employers will probably require employees to conﬁgure their mobile devices to require authentication as a condition of permitting BYOD. Employers will also create and enforce policies regarding the need to periodically change passwords. Consequently, the important question is the extent to which employees truly comply with those policies or attempt to circumvent them, for example by storing their credential and conﬁguring their mobile device to automatically submit it whenever they want to login to the corporate system, to make life easier. Our results suggest that this is indeed a potential problem. In summary, our results have implications for both managers and users. Managers need to reconsider how they formulate security policies and deploy new IT artifacts. It is not enough to focus only on how a new technological development or a proposed policy, if properly implemented and complied with, improves security. Managers must also consider how changes in the IT artifact (e.g., replacing a full-size physical keyboard with a virtual touchscreen) and policy requirements (e.g., password credential composition rules) interact with one another in the context in which they will be used. In particular, our results suggest that (1) secure authentication policies that are effective in the desktop computing paradigm will not work in the mobile paradigm, and (2) voluntary secure authentication behaviors that are not adequately usable will be discontinued. Therefore, managers must ensure that a secure authentication policy is sufﬁciently usable or at least ﬁnd ways to force users to comply with it. Otherwise, they risk having their employees develop “creative workarounds” that make it easier to “comply” with policies, but in a manner that actually reduces security. Similarly, users should be wary of our natural tendency to place usability before security and exercise safe judgment in spite of mobile UI limitations. 5.3. Limitations This study is subject to several limitations inherent with the use of student subjects in a controlled experiment. One issue concerns the extent to which the results generalize to other populations of interest.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "235\n",
              "\n",
              "It is possible that age and experience with mobile devices may affect our results. However, as noted earlier, we did test for and failed to ﬁnd evidence that repeated practice in using a mobile device to authenticate ameliorated the login failure rate or the tendency to switch to a simpler credential. Moreover, if it is true, as it is sometimes argued, that younger people have fewer problems using IT than do older people, then the use of students as subjects may actually understate the magnitude of the problems associated with using complex authentication credentials to log in via mobile devices. Another issue is that controlled experiments may cause participants to behave differently than they would in real situations. As we explained when describing our research design, we took several steps to mitigate that risk, including obtaining permission to not inform participants that this was an experiment. In addition, we designed the experiment so that participants believed that they were playing a game. Gamiﬁcation has been shown to cause people to become absorbed in the immediate task (i.e., playing the game) and, thereby, to reveal more realistic behaviors (Agarwal and Karahanna 2000, Deterding 2012, Hoffman and Novak 1996, Singh 2012). Furthermore, the game data stored in their ﬁndamine proﬁle had strategic value in the context of the game itself: a player who accessed other players’ accounts could view their clues about the target locations. Therefore, much like a poker player needs to keep their cards private, ﬁndamine players needed to keep their clues private, which should have motivated them to restrict access to those data via use of a strong authentication credential. There was also an element of personal risk associated with unauthorized access to their personal information. Although ﬁndamine did not include credit card or bank account information, it did record and share their GPS location, social network connections, and demographic proﬁle data similar to that found in major online social networks. Disclosure of GPS information enables criminals to stalk victims, which can lead to robberies when the victim is not home (Johanson 2013) or serious physical crimes such as assault, rape, or murder (Baum et al. 2009, HufﬁngtonPost 2012). In summary, we believe that our participants were motivated to participate in a variety of ways for an assortment of reasons. One potential source of variance not accounted for in our study is that mobile users may employ an external keyboard when using their device.7 It was not possible to capture this nuance in the client variables we recorded in each login attempt. However, such behavior by participants in our study would only bias against ﬁnding any difference in login failure rates due to different UI.\n",
              "7\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "The design of our experiment did not permit us to collect data to determine whether the reason that participants discontinued a strong authentication behavior was primarily cognitive (i.e., a conscious attempt to reduce effort) or emotional (dissatisfaction or annoyance). However, both explanations are consistent with predictions based on cybernetic loop theory that people will respond to repeated login failures by discontinuing a behavior that leads to task failure. We leave exploration of this issue as a topic for future research. Finally, we drew on existing knowledge about memory systems to derive implications concerning the effect of the UI on the continuance of secure behavior. However, our understanding of how memory systems function is continually evolving (Baddeley 2012). Therefore, IS security researchers need to monitor new insights from psychology research on memory because those ﬁndings may suggest even more effective ways to design the UI to best support secure behaviors.\n",
              "\n",
              "6.\n",
              "\n",
              "Conclusions\n",
              "\n",
              "This paper reports the results of a ﬁeld experiment that investigated the effect of an IT artifact (the nature of the UI) on authentication behaviors. Our most important ﬁnding is that user authentication behaviors differ depending on whether they are using traditional computers or mobile devices for remote access to a network. We demonstrate that those differences are due to differences in the UI of the two types of devices. Thus, our study underscores the need for security researchers and practitioners to consider how the IT artifact interacts with task characteristics (e.g., requirements about authentication credential composition) when considering the effects of adopting new technologies or changing security requirements. As Adams and Sasse (1999, p. 40) argue, “users are not the enemy” but only create problems because of mismatches between human capabilities and the requirements of secure behavior. Acknowledgments\n",
              "The authors would like to sincerely thank the review team for the time and effort they gave to greatly improve the quality of this paper. The authors would also like to thank the reviewers and participants of the 2013 Dewald Roode Information Security and Privacy Workshop IFIP WG8.11/WG11.13 for their constructive comments and suggestions. Any remaining mistakes are the sole responsibility of the authors.\n",
              "\n",
              "Appendix. Details of Credential Strength Measures\n",
              "Credential composition and strength was measured in a variety of ways. First, the theoretical entropy was calculated: Entropy = log2 N L , where N is the size of the character set and L is the length (Bialynickibirula\n",
              "\n",
              "We thank an anonymous reviewer for raising this issue.\n",
              "\n",
              "\n",
              "236\n",
              "Table A.1 Pearson Correlation Table Entropy Cracked Expert judgment\n",
              "∗∗∗\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "Table A.2 Cracked −0 32∗∗∗ Credential\n",
              "\n",
              "Example of Credential Strength Results Entropy 40.00 40.00 88.56 81.95 40.00 73.55 Expert 1 2 2 3 2 2 Cracked Yes No Yes No No Yes\n",
              "\n",
              "−0 39 0 54∗∗∗\n",
              "\n",
              "∗∗∗\n",
              "\n",
              "Signiﬁcant at the p < 0 001 level.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "and Mycielski 1975). Entropy represents the difﬁculty of attempting a brute-force approach to guessing a credential, with higher levels of entropy indicating a “stronger” credential. Entropy is considered a more accurate measure of true credential strength against cracking than merely calculating the total number of possible password combinations based on a given length and character set, N L (Johansson and Riley 2005) because the length of a password, in bits, plus the size of the character set, creates a doubling of the number of guesses required for brute-force cracking for each bit added to the password information. However, credential entropy is still only a hypothetical measure of strength. The actual words and word permutations commonly used in credentials, and also found in rainbow tables,8 is constantly changing based on user behavior. Therefore, we created NT hashes9 of each password used and then used two separate programs to see which credentials could be cracked. The ﬁrst program was an opensource program called Ophcrack. All of the free rainbow tables that came with Ophcrack were used. The other program was a private paid application called Hash Suite with more advanced rainbow tables. The second measure of credential strength (i.e., initial coping behavior) was whether the user’s credential could be cracked by either of these programs. Both entropy and credential crack rate are summarized by credential type in Table 3. Because both of the objective measures listed above contain known potential measurement and error issues, we also considered experts’ assessment of credential strength, a subjective measure used in prior research (Keith et al. 2007, 2009). We recruited two independent expert judges— who had no part or stake in the outcome of the research— to rate each credential’s complexity and strength as “simple,” “moderate,” or “strong.” These coders agreed on 81% of the credential ratings K = 0 813 p < 0 001 . When the coders disagreed, we chose to use the decision by the expert practitioner. Table A.1 shows that all three measures were correlated with one another. Both entropy and expert judgment are negatively correlated with cracked. Thus, as expected, credentials with higher entropy are less likely to be cracked. Similarly, credentials judged to be complex are less likely to be cracked than credentials that experts assessed as being simple. Table A.1 also shows that entropy and expert judgment are positively correlated, but apparently also measure different aspects of credential strength. Therefore, we\n",
              "8\n",
              "\n",
              "password wrosapsd icecreamsandwhich Extra8iscuit$ icanﬁnd Basketball11\n",
              "\n",
              "Notes. Expert scores: 1 = simple; 2 = moderate; 3 = complex.\n",
              "\n",
              "investigated how well each measure related to whether a particular credential was likely to be cracked. Table A.2 lists several example passwords10 and their subsequent entropy scores, expert ratings, and whether or not it was successfully cracked. Note that the ﬁrst two passwords (password and wrosapsd), have the exact same entropy scores because they are of the same length and are derived from the same character base (lowercase letters). However, the expert judged the latter to be moderately complex (2) but considered the former to be simple (1), and those judgments were consistent with the fact that “wrosapsd” was not cracked, whereas “password” was. Now consider the second two passwords: “icecreamsandwhich” has a higher entropy score than does “Extra8iscuit$” because of its length, but “Extra8iscuit$” comes from a larger character base that includes both uppercase and lowercase letters, numbers, and characters. Once again, the expert correctly judged “Extra8iscuit$” to be more complex: it was not cracked, but “icecreamsandwhich” was. The last two passwords represent a situation where the password cracking software was a poor indicator of complexity because it was able to crack the higher entropy password of “Basketball11” but not the lower entropy password of “icanﬁnd.” This is because the success of the cracking software depends on the quality of the rainbow tables. If the rainbow table is poor, then the cracking results will also be poor. In this case, the expert judged both passwords to be moderately complex (2): “icanﬁnd” because it was a phrase and not a single word; “Basketball11” because it contained multiple types of characters and was longer even though it was a single word. These three examples suggest that expert judgment is likely a better indicator of true credential strength than entropy scores. We think that one reason for this difference is that the chance of login failures increases with the number of keystrokes required to enter a credential, but the entropy scores for two credentials may not be related to the number of keypresses required to enter that credential. For example, refer back to the second pair of credentials in Table A.2. The ﬁrst, “icecreamsandwhich” has an entropy\n",
              "10\n",
              "\n",
              "A rainbow table is a precomputed list of password hashes usually used for recovering/cracking plain-text passwords.\n",
              "9\n",
              "\n",
              "An NT hash refers to the algorithm used by Windows operating systems to generate hashed versions of user passwords.\n",
              "\n",
              "Other than the password “password,” all other credentials in this table were modiﬁed slightly to comply with IRB restrictions. However, the examples were modiﬁed to reﬂect the same entropy score as the original credential (i.e., the nonword “wrosapsd” replaces an actual credential that was a nonword comprised of eight lowercase alphabetic characters). Moreover, the expert rating and cracked status are based on the actual credential.\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "237\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "score of 88.56 and requires 17 keypresses to enter; the second, “Extra8iscuit$” has a lower entropy score of 81.95 but also requires 17 keypresses to enter on a virtual touchscreen interface. Entropy scores also do not take into account the nature of keypresses. The 17 keypresses required to enter “icecreamsandwhich” each only require pressing one ﬁnger at a time on a physical keyboard. They also require pressing only one key at a time on a virtual touchscreen, without the need to ever shift displays. By contrast, entering the credential “Extra8iscuit$” on a physical keyboard requires pressing two keys simultaneously twice (shift plus e to yield E, and shift plus 4 to yield $). On a virtual touchscreen, it requires changing the display ﬁve times (once to shift to capital letters, once to return to lowercase, once to change to numbers, once to return to lowercase, and once to display special symbols including the $). We believe that the need to either press multiple keys simultaneously on a physical keyboard or the need to continually press a key to change the display on a virtual touchscreen increases the probability of a typing mistake. Therefore, based on the preceding discussion, and to be consistent with prior research on the use of authentication credentials (Keith et al. 2007, 2009), we chose to use expert judgment as our measure of credential strength.\n",
              "\n",
              "References\n",
              "Adams A, Sasse MA (1999) Users are not the enemy. Comm. ACM 42(12):40–46. Adams A, Sasse MA, Lunt P (1997) Making passwords secure and usable. Thimbleby H, O’Conaill B, Thomas PJ, eds. People and Computers XII (Springer, London), 1–19. Agarwal R, Karahanna E (2000) Time ﬂies when you’re having fun: Cognitive absorption and beliefs about information technology usage. MIS Quart. 24(4):665–694. Anderson CL, Agarwal R (2010) Practicing safe computing: A multimedia empirical examination of home computer user security behavioral intentions. MIS Quart. 34(3):613–643. Baddeley A (1994) The magical number seven: Still magic after all these years? Psych. Rev. 101(2):353–356. Baddeley A (2012) Working memory: Theories, models, and controversies. Annual Rev. Psych. 63:1–29. Bao P, Pierce J, Whittaker S, Zhai S (2011) Smart phone use by non-mobile business users. Bylund M, Juhlin O, Fernaues Y, eds. Proc. 13th Internat. Conf. Human Comput. Interaction Mobile Devices Services (ACM, New York), 445–464. Bargh JA, Ferguson MJ (2000) Beyond behaviorism: On the automaticity of higher mental processes. Psych. Bull. 126(6):925–945. Bargh JA, Gollwitzer PM, Lee-Chai A, Barndollar K, Trötschel R (2001) The automated will: Nonconscious activation and pursuit of behavioral goals. J. Personality Soc. Psych. 81(6): 1014–1027. Barn BS, Barn R, Tan J-P (2014) Young people and smart phones: An empirical study on information security. Sprague RH Jr, ed. Proc. 47th Hawaii Internat. Conf. System Sci. HICSS (IEEE, Los Alamitos, CA), 4504–4514. Baum K, Catalano S, Rand M (2009) National Crime Victimization Survey: Stalking Victimization in the United States–Revised NCJ 224527, Bureau of Justice Statistics Special Report, U.S. Department of Justice, Ofﬁce of Justice Programs, Washington, DC. http://www.bjs.gov/content/pub/pdf/svus_rev.pdf. Belanger F, Crossler RE (2011) Privacy in the digital age: A review of information privacy research in information systems. MIS Quart. 35(4):1017–1041.\n",
              "\n",
              "Ben-Asher N, Kirschnick N, Sieger H, Meyer J, Ben-Oved A, Möller S (2011) On the need for different security methods on mobile phones. Bylund M, Juhlin O, Fernaues Y, eds. Proc. 13th Internat. Conf. Human Comput. Interaction Mobile Devices Services (ACM, New York), 465–473. Bhattacherjee A (2001) Understanding information systems continuance: An expectation-conﬁrmation model. MIS Quart. 25(3):351–370. Bhattacherjee A, Premkumar G (2004) Understanding changes in belief and attitude toward information technology usage: A theoretical model and longitudinal test. MIS Quart. 28(2): 229–254. Bialynickibirula I, Mycielski J (1975) Uncertainty relations for information entropy in wave mechanics. Comm. Math. Phys. 44(2):129–132. Brown AS, Bracken E, Zoccoli S, Douglas K (2004) Generating and remembering passwords. Appl. Cognitive Psych. 18(6):641–651. Brown RM, Robertson EM (2007) Off-line processing: Reciprocal interactions between declarative and procedural memories. J. Neuroscience 27(39):10468–10475. Brown SA, Venkatesh V, Goyal S (2012) Expectation conﬁrmation in technology use. Inform. Systems Res. 23(2):474–487. Bulgurcu B, Cavusoglu H, Benbasat I (2010) Information security policy compliance: An empirical study of rationalitybased beliefs and information security awareness. MIS Quart. 34(3):523–548. Carver CS, Scheier MF (1982) Control theory: A useful conceptual framework for personality–social, clinical, and health psychology. Psych. Bull. 92(1):111–135. Cenfetelli RT (2004) Inhibitors and enablers as dual factor concepts in technology usage. J. Assoc. Inform. Systems 5(11):472–492. Chen M, Bargh JA (1999) Consequences of automatic evaluation: Immediate behavioral predispositions to approach or avoid the stimulus. Personality Soc. Psych. Bull. 25(2):215–224. Chin WW, Marcolin BL, Newsted PR (2003) A partial least squares latent variable modeling approach for measuring interaction effects: Results from a Monte Carlo simulation study and an electronic-mail emotion/adoption study. Inform. Systems Res. 14(2):189–217. Clarke NL, Furnell SM (2005) Authentication of users on mobile telephones—A survey of attitudes and practices. Comput. Security 24(7):519–527. ConsumerReports (2014) Smart phone thefts rose to 3.1 million last year, consumer reports ﬁnds. (May 28), http://www .consumerreports.org/cro/news/2014/04/smart-phone-thefts -rose-to-3-1-million-last-year/index.htm. Deterding S (2012) Gamiﬁcation: Designing for motivation. Interactions 19(4):14–17. Dinev T, Hu Q (2007) The centrality of awareness in the formation of user behavioral intention toward protective information technologies. J. Assoc. Inform. Systems 8(7):386–408. Ebbinghaus H (1913) Memory: A Contribution to Experimental Psychology (Teachers College, Columbia University, New York). Ericsson KA, Kintsch W (1995) Long-term working memory. Psych. Rev. 102(2):211–245. ESA—Entertainment Software Association (2013) 2013 sales, demographic and usage data: Essential facts about the computer and video game industry. http://www.theesa.com/facts/pdfs/ESA _EF_2013.pdf. Fornell C, Bookstein FL (1982) Two structural equation models: LISREL and PLS applied to consumer exit-voice theory. J. Marketing Res. 19(4):440–452. Herath T, Rao HR (2009) Protection motivation and deterrence: A framework for security policy compliance in organisations. Eur. J. Inform. Systems 18(2):106–125. Hoffman DL, Novak TP (1996) Marketing in hypermedia computermediated environments: Conceptual foundations. J. Marketing 60(3):50–68. Hong S, Kim J, Lee H (2008) Antecedents of user-continuance in information systems: Toward an integrative view. J. Comput. Inform. Systems 48(3):61–73.\n",
              "\n",
              "\n",
              "238\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "Hong S, Thong JY, Tam KY (2006) Understanding continued information technology usage behavior: A comparison of three models in the context of mobile Internet. Decision Support Systems 42(3):1819–1834. Huang D-L, Patrick Rau P-L, Salvendy G, Gao F, Zhou J (2011) Factors affecting perception of information security and their impacts on IT adoption and security practices. Internat. J. Human-Comput. Stud. 69(12):870–883. HufﬁngtonPost (2012) Jenn Gibbons returns to Chicago, completing charity trip in spite of assault. (August 14), http://www .hufﬁngtonpost.com/2012/08/14/jenn-gibbons-returns-to-c_n _1776169.html. Ives B, Walsh KR, Schneider H (2004) The domino effect of password reuse. Comm. ACM 47(4):75–78. Jakobsson M, Akavipat R (2012) Rethinking passwords to adapt to constrained keyboards. Proc. Mobile Security Technologies, IEEE Comput. Soc. Security Privacy Workshop, San Francisco. Johanson M (2013) How burglars use Facebook to target vacationing homeowners. IBT (July 11), http://www.ibtimes.com/ how-burglars-use-facebook-target-vacationing-homeowners-13 41325. Johansson J, Riley S (2005) Protect Your Windows Network: From Perimeter to Data (Addison-Wesley, Upper Saddle River, NJ). Johnson K, DeLaGrange T (2012) Sans survey on mobility/BYOD security policies and practices. http://www.sans .org/reading-room/whitepapers/analyst/survey-mobility-byod -security-policies-practices-35175. Johnston AC, Warkentin M (2010) Fear appeals and information security behaviors: An empirical study. MIS Quart. 34(3): 549–566. Jones BH, Heinrichs LR (2012) Do business students practice smartphone security? J. Comput. Inform. Systems 53(2):22–30. Karahanna E, Straub DW, Chervany NL (1999) Information technology adoption across time: A cross-sectional comparison of pre-adoption and post-adoption beliefs. MIS Quart. 23(2): 183–213. Karlson AK, Brush AJ, Schechter S (2009) Can I borrow your phone?: Understanding concerns when sharing mobile phones. Greenberg S, Hudson SE, Hinckley K, Morris ME, Olsen DR Jr, eds. Proc. SIGCHI Conf. Human Factors Comput. Systems (ACM, New York), 1647–1650. Keisler A, Shadmehr R (2010) A shared resource between declarative memory and motor memory. J. Neuroscience 30(44): 14817–14823. Keith MJ, Shao B, Steinbart PJ (2007) The usability of passphrases for authentication: An empirical ﬁeld study. Internat. J. HumanComput. Stud. 65(1):17–28. Keith MJ, Shao B, Steinbart PJ (2009) A behavioral analysis of passphrase design and effectiveness. J. Assoc. Inform. Systems 10(2):63–89. Keller KL (1987) Memory factors in advertising: The effect of advertising retrieval cues on brand evaluations. J. Consumer Res. 14(3):316–333. Kim H-W, Chan HC, Chan YP (2007) A balanced thinking— Feelings model of information systems continuance. Internat. J. Human-Comput. Stud. 65(6):511–525. Lee S, Zhai S (2009) The performance of touch screen soft buttons. Greenberg S, Hudson SE, Hinckley K, Morris ME, Olsen DR Jr, eds. Proc. SIGCHI Conf. Human Factors Comput. Systems (ACM, New York), 309–318. Lee Y, Larsen KR (2009) Threat or coping appraisal: Determinants of SMB executives’ decision to adopt anti-malware software. Eur. J. Inform. Systems 18(2):177–187. Liang H, Xue Y (2009) Avoidance of information technology threats: A theoretical perspective. MIS Quart. 33(1):71–90. Liang H, Xue Y (2010) Understanding security behaviors in personal computer usage: A threat avoidance perspective. J. Assoc. Inform. Systems 11(7):394–413. Limayem M, Hirt SG, Cheung CM (2007) How habit limits the predictive power of intention: The case of information systems continuance. MIS Quart. 31(4):705–737.\n",
              "\n",
              "MacKay B, Watters C, Duffy J (2004) Web page transformation when switching devices. Brewster S, Dunlop M, eds. Mobile Human-Computer Interaction-MobileHCI 2004, Lecture Notes Comput. Sci., Vol. 3160 (Springer-Verlag, Berlin Heidelberg), 228–239. Ortiz de Guinea A, Markus ML (2009) Why break the habit of a lifetime? Rethinking the roles of intention, habit, and emotion in continuing information technology use. MIS Quart. 33(3): 433–444. Ortiz de Guinea A, Webster J (2013) An investigation of information systems use patterns: Technological events as triggers, the effect of time, and consequences for performance. MIS Quart. 37(4):1165–1188. Park YS, Han SH, Park J, Cho Y (2008) Touch key design for target selection on a mobile phone. ter Hofte H, Mulder I, eds. Proc. 10th Internat. Conf. Human Comput. Interaction Mobile Devices Services (ACM, New York), 423–426. Paul CL, Morse E, Zhang A, Choong Y-Y, Theofanos M (2011) A ﬁeld study of user behavior and perceptions in smartcard authentication. Campos P, Graham N, Jorge J, Nunes N, Palanque P, Winckler M, eds. Human-Computer Interaction— Interact 2011, Lecture Notes Comput. Sci., Vol. 6949 (SpringerVerlag, Berlin Heidelberg), 1–17. Payne JW (1982) Contingent decision behavior. Psych. Bull. 92(2):382–402. Payne JW, Bettman JR, Johnson EJ (1993) The Adaptive Decision Maker (Cambridge University Press, Cambridge, UK). Posey C, Lowry PB, Roberts TL, Ellis TS (2010) Proposing the online community self-disclosure model: The case of working professionals in France and the UK who use online communities. Eur. J. Inform. Systems 19(2):181–195. Richman WL, Kiesler S, Weisband S, Drasgow F (1999) A metaanalytic study of social desirability distortion in computeradministered questionnaires, traditional questionnaires, and interviews. J. Appl. Psych. 84(5):754–775. Ringle C, Wende S, Will A (2014) Smartpls 3.0. http://www .smartpls.de. Rogers RW (1975) A protection motivation theory of fear appeals and attitude change. J. Psych. 91(1):93–114. Rydell RJ, Mackie DM, Maitner AT, Claypool HM, Ryan MJ, Smith ER (2008) Arousal, processing, and risk taking: Consequences of intergroup anger. Personality Soc. Psych. Bull. 34(8):1141–1152. Schneider IE, Silverberg KE, Chavez D (2011) Geocachers: Beneﬁts sought and environmental attitudes. Cyber J. Appl. Leisure Recreation Res. 14(1):1–11. Sears A, Zha Y (2003) Data entry for mobile devices using soft keyboards: Understanding the effects of keyboard size and user tasks. Internat. J. Human-Comput. Interaction 16(2):163–184. Shneiderman B (1986) Designing the User Interface-Strategies for Effective Human-Computer Interaction (Pearson Education India, Boston). Singh S (2012) Gamiﬁcation: A strategic tool for organizational effectiveness. Internat. J. Management 1(1):108–113. Smith HJ, Dinev T, Xu H (2011) Information privacy research: An interdisciplinary review. MIS Quart. 35(4):989–1015. Squire LR (1986) Mechanisms of memory. Science 232(4758): 1612–1619. Squire LR (2004) Memory systems of the brain: A brief history and current perspective. Neurobiology Learn. Memory 82(3):171–177. Tanner JF Jr, Hunt JB, Eppright DR (1991) The protection motivation model: A normative model of fear appeals. J. Marketing 55(3):36–45. Taylor S, Todd PA (1995) Understanding information technology usage: A test of competing models. Inform. Systems Res. 6(2):144–176. Todd P, Benbasat I (1991) An experimental investigation of the impact of computer based decision aids on decision making strategies. Inform. Systems Res. 2(2):87–115. Todd P, Benbasat I (1992) The use of information in decision making: An experimental investigation of the impact of computerbased decision aids. MIS Quart. 16(3):373–393. Todd P, Benbasat I (1999) Evaluating the impact of DSS, cognitive effort, and incentives on strategy selection. Inform. Systems Res. 10(4):356–374.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Steinbart, Keith, and Babb: Examining the Continuance of Secure Behavior\n",
              "Information Systems Research 27(2), pp. 219–239, © 2016 INFORMS\n",
              "\n",
              "239\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Trewin S, Swart C, Koved L, Martino J, Singh K, Ben-David S (2012) Biometric authentication on a mobile device: A study of user effort, error and task disruption. Zakon RH, ed. Proc. 28th Annual Comput. Security Appl. Conf. (ACM, New York), 159–168. Tulving E, Pearlstone Z (1966) Availability versus accessibility of information in memory for words. J. Verbal Learn. Verbal Behav. 5(4):381–391. Ullman MT (2004) Contributions of memory circuits to language: The declarative/procedural model. Cognition 92(1):231–270. Ullman MT (2013) The declarative/procedural model of language. Pashler H, ed. Encyclopedia of the Mind (Sage Publications, Los Angeles), 224–226. Vance A, Eargle D, Ouimet K, Straub D (2013) Enhancing password security through interactive fear appeals: A web-based ﬁeld experiment. Sprague RH Jr, ed. Proc. 46th Hawaii Internat. Conf. System Sciences (HICSS) (IEEE, Los Alamitos, CA), 2988–2997. Venkatesh V, Bala H (2008) Technology acceptance model 3 and a research agenda on interventions. Decision Sci. 39(2): 273–315.\n",
              "\n",
              "Venkatesh V, Thong JYL, Xu X (2012) Consumer acceptance and use of information technology: Extending the uniﬁed theory of acceptance and use of technology. MIS Quart. 36(1):157–178. Venkatesh V, Brown SA, Maruping LM, Bala H (2008) Predicting different conceptualizations of system use: The competing roles of behavioral intention, facilitating conditions, and behavioral expectation. MIS Quart. 32(3):483–502. Wiedenbeck S, Waters J, Birget JC, Brodskiy A, Memon N (2005) PassPoints: Design and longitudinal evaluation of a graphical password system. Internat. J. Human-Comput. Stud. 63(1): 102–127. Wiener N (1948) Cybernetics: Or Control and Communication in the Animal and the Machine (Wiley, New York). Woon I, Tan G-W, Low R (2005) A protection motivation theory approach to home wireless security. Proc. Internat. Conf. Inform. Systems, Las Vegas, NV. Yan J, Blackwell A, Anderson R, Grant A (2004) Password memorability and security: Empirical results. IEEE Security Privacy 2(5):25–31. Zviran M, Haga WJ (1999) Password security: An empirical study. J. Management Inform. Systems 15(4):161–185.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "QXtigreZG1Bf",
        "colab_type": "code",
        "outputId": "aef2ded2-cf22-4924-d4d9-47b6fafbc917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc6.sents if 'p =' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[An ANOVA including contrast estimates indicate that treatment 2 p = 0 03 was successful because players in that condition were more likely to create a passphrase than were players who were not prompted to consider doing so.,\n",
              " 254 p = 0 001 .,\n",
              " 305 p = 0 041 .,\n",
              " 319 p = 0 027 .,\n",
              " The path from login failures to changing credentials is negative and signiﬁcant = −0 222 p = 0 009 , indicating that login failures result in changing to a weaker credential.,\n",
              " 654, p = 0 001 , indicating that as the proportion of access attempts via mobile devices increases, so does the tendency to store the authentication credential.,\n",
              " −0 166 p = 0 024 , indicating that increased use of mobile devices increases the tendency to switch to a weaker credential.,\n",
              " Figure 7 shows that practice does not signiﬁcantly moderate the path between mobile device use and login failures = −0 093, p = 0 248 , but does affect both the likelihood of storing authentication credentials = 0,\n",
              " 295 p = 0 033 and the tendency of mobile device users to change to a weaker authentication credential = −0 291, p = 0 050 .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "e0w8feMWzLZr",
        "colab_type": "code",
        "outputId": "67b14937-cee0-4917-c2d3-fc53f8a13a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc6.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[813 p < 0 001 .,\n",
              " Traditional remembered login rate (%) Count of changes to each type Count of changes away from each type 744 1,074 18.54 18.62 44.89 31.28 n = 32 n = 15\n",
              " \n",
              " p < 0 10; ∗ p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              " ,\n",
              " p < 0 05; ∗∗ p < 0 01; ∗∗∗ p < 0 001.\n",
              " ,\n",
              " 226, p < 0 001 , indicating that login failures increased the tendency to store authentication credentials.,\n",
              " Signiﬁcant at the p < 0 001 level.\n",
              " ,\n",
              " 813 p < 0 001 .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "metadata": {
        "id": "XBww3MTkGWHf",
        "colab_type": "code",
        "outputId": "1c4d9ab0-5d51-439c-860a-19db83c7710f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12517
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc7 = nlp(open(u\"OhH#AnimeshA#PinsonneaultA_2016_Free Versus for-a-Fee - The Impact of a Paywall on the Pattern and Effectiveness of Word-of-Mouth Via Social Media_MIS Quarterly_1.txt\").read())\n",
        "doc7\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RESEARCH ARTICLE\n",
              "\n",
              "FREE VERSUS FOR-A-FEE: THE IMPACT OF A PAYWALL ON THE PATTERN AND EFFECTIVENESS OF WORD-OF-MOUTH VIA SOCIAL MEDIA1\n",
              "Hyelim Oh\n",
              "School of Computing, National University of Singapore, 13 Computing Drive, Singapore 117417 SINGAPORE {hyelim.oh@nus.edu.sg}\n",
              "\n",
              "Animesh Animesh and Alain Pinsonneault\n",
              "Desautels Faculty of Management, McGill University, Montréal, Québec, H3A 1G5 CANADA {animesh.animesh@mcgill.ca} {alain.pinsonneault@mcgill.ca}\n",
              "\n",
              "Information goods providers such as print newspapers are experimenting with different pricing models for their online content. Despite research on the topic, it is still not clear how information pricing strategy influences word-of-mouth (WOM) via social media, which has become a dominant channel for raising awareness about a newspaper’s articles and attracting new visitors to its website. Using The New York Times’ paywall rollout as a natural experiment, our study examines how the implementation of paywall by a firm (i.e., a shift from “free” to “for-a-fee”) influences the pattern and effectiveness of online WOM in social media. Our results indicate that implementing a paywall (i.e., charging for content that was earlier available for free) has a disproportionate impact on WOM for popular and niche articles, creating a longer tail in the WOM (i.e., content sharing) distribution. Further, we find that the impact of WOM on the NYT’s website traffic weakens significantly after the introduction of a paywall. These results show that a paywall has implications for product and promotion strategies. The study offers novel and important implications for the theory and practice of strategic use of social media and paywall. Keywords: Information goods, information pricing, paywall, social media, long tail, word-of-mouth, website traffic\n",
              "\n",
              "Introduction1\n",
              "Digital technologies and the Internet have disrupted traditional business models in many industries, such as music and travel. The newspaper publishing industry, the core product of which is information goods (i.e., news content), is also on the verge of disruption because of online distribution of news\n",
              "1\n",
              "\n",
              "Ravi Bapna was the accepting senior editor for this paper. Gal OestreicherSinger served as the associate editor. The appendices for this paper are located in the “Online Supplements” section of the MIS Quarterly’s website (http://www.misq.org).\n",
              "\n",
              "content. Specifically, declining circulation of print newspapers (Vanacore 2010) and an increasing trend toward digital news consumption by consumers have driven traditional print newspapers to adopt the Internet as the medium to offer digital content. However, given the intense competition in the online news market and almost zero marginal cost for providing news online, newspapers find it difficult to charge a fee for accessing their online content (Chyi 2005). Therefore, most online newspapers have been providing content free of charge while making money from online advertising. However, as more consumers switch from print to online news consumption, the online advertising revenue (which is significantly lower than print ad revenue) does not counter-\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1, pp. 31-56/March 2016\n",
              "\n",
              "31\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "balance the loss of revenue from print newspaper subscribers (Peters 2011). Given these realities, it is not surprising that print news publishers have been debating the tradeoff of charging a fee versus providing the content for free. Charging a fee can increase the total revenue that a publisher receives from online consumers. However, it is likely to significantly decrease online readership (Chyi 2005), which in turn, will lower the total revenue that comes from online advertisements (Dewan et al. 2003). The New York Times (NYT) is one such newspaper experimenting with different pricing models for their online content. Learning from its 2005 failure (when the NYT charged for access to content from noted columnists but could not get significant subscribers for such paid content), NYT rolled out a new paywall strategy in March 2011. Since both subscriptions and ad revenues are a function of the newspaper’s readership, it is important for NYT to ensure that the traffic to its website does not drop significantly. Recognizing the importance of retaining the current NYT website visitors with possibly low willingness to pay (WTP), NYT has implemented a generous access policy allowing non-subscribers to read 20 articles for free (Peters 2011). However, merely retaining current customers is not enough, and like other business, a newspaper publisher needs to rely on advertising and word-of-mouth (WOM) to increase awareness and to acquire new customers (subscribers as well as non-subscribers). In the context of online newspapers, WOM via social media is a dominant channel for raising awareness about a newspaper’s articles and acquiring new visitors to newspaper’s website. The WOM about news content is generated as individuals share the content in the social media by posting links to the news article. It is important to note that since content sharing is implicit WOM about an article’s importance or relevance, we refer to content sharing as WOM in this paper. Recognizing the importance of social media such as Twitter and Facebook in creating WOM about online content and a resulting increase in awareness and traffic,2 the NYT designed its paywall with a special access policy for social media users. Specifically, NYT allowed individuals to bypass the paywall when they access NYT content by following NYT webpage links shared in social media. Industry experts and market researchers suggest that the introduction of a subscription fee model would lower the NYT’s website traffic (Regan 2011). However, it is not clear how this new paywall strategy will impact the WOM dynamics of\n",
              "\n",
              "the NYT’s content in online social media.3 Given the importance of online social media for the sustainability and growth of content providers such as NYT, our study examines the impact of a paywall on the pattern and effectiveness of such WOM in social media. Specifically, using NYT’s recent adoption of a digital subscription model as a natural experiment, we examine how a paywall influences the patterns of WOM on Twitter (i.e., NYT news article links shared in tweets).4 We further analyze how a paywall affects the effectiveness of WOM. Specifically, we examine the relationship between the volume of WOM on Twitter and NYT’s website traffic and how this dynamic changes after implementation of the paywall. The results show that the implementation of a paywall decreased the volume of WOM, and had a disproportionate impact on WOM distribution. Specifically, the results suggest that the WOM for popular content dropped significantly, resulting in less concentration of popular content in WOM distribution. Our analysis to test the plausibility of a mechansim based on exposure theory as the underlying driver of the long tail5 suggests that light users who consume few articles have a tendency to consume popular articles. After the paywall was introduced, light users had a higher likelihood of reducing NYT content consumption due to their low WTP for content. The relatively larger reduction of NYT content consumption by light users who prefer popular content reshaped the WOM distribution. To examine the effectiveness of WOM, we combined Twitter content sharing data with NYT’s website traffic data. Our results show that the volume of social media WOM had a\n",
              "\n",
              "3 At the time the paywall was introduced, NYT was charging an average of $4.00 per week. This annual fee ($455) was relatively high, compared to the rates charged by other players in the industry, such as The Wall Street Journal ($207), The Economist ($110), Netflix/Hulu ($96), and Pandora ($36) (DeGusta 2011). There were criticisms about NYT’s pricing strategy (McAthy 2013). However, NYT gained about 390,000 user conversions to paid digital subscriptions by the end of 2011. Although the new revenue stream generated by the paywall surpassed ad losses by $19.2 million (3.4%) in the first year (Lee 2012), the decrease in advertising revenue suggests that NYT needs to invest in maintaining and increasing website traffic (Ives 2011; Myers 2012; Yackanich 2013). 4\n",
              "\n",
              "As described in detail later, the link to an article shared by a user in social media is a type of WOM where the user implicitly or explicitly gives a positive endorsement about the importance or relevance of the article being shared. The long tail, coined by Anderson (2006), describes a shift of demand distribution as niche products grow to become a larger share of total sales. Prior work focusing on the long tail has examined changes of sales concentration from offline to online channels (Brynjolffson et al. 2011; Brynjolfsson et al. 2003; Zentner et al. 2013).\n",
              "\n",
              "5 2 According to Alexa (as of March 31, 2011), Twitter and Facebook account for 2.43% and 8.63% of upstream and downstream NYT website traffic, respectively.\n",
              "\n",
              "32\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "positive impact on website traffic. However, the contribution of social media WOM to website traffic weakened after introduction of the paywall. This study makes substantial contributions to the literature. First, we complement the extant research in the information goods pricing literature by proposing and empirically showing the significant interplay between a firm’s pricing strategy (i.e., paywall) and WOM pattern and effectiveness. The unique natural experiment created by NYT’s paywall rollout helps to control for confounding factors. Second, our study contributes to the WOM literature that measures the impact of online WOM on various performance measures by highlighting the role of WOM on newspaper website traffic and by proposing a moderating role of a paywall on the relationship between WOM and performance outcome (i.e., website traffic). Third, we contribute to the theory of exposure and variety seeking by indirectly validating the theory by employing a finite mixture model. Last, we extend the literature examining various aspects of long tail phenomenon, especially in the online context, by examining the impactof a paywall on the long tail in the distribution of online WOM for information goods such as newspaper articles. Our findings also have significant implications to firms, such as newspaper companies, who are experimenting with news content (i.e., a type of information good) pricing. First, our study suggests that before implementing a paywall, a firm should consider the impact of such a pricing decision on WOM dynamics, which may indirectly affect future revenues. Second, our results highlight the need to measure and manage the impact of a firm’s strategy on WOM. Finally, the shift in the distribution of online WOM after a paywall is introduced has implications for product and news content pricing strategies. The rest of this paper is organized as follows: First, we discuss the related literature. Second, we develop research questions highlighting the impact of a paywall on WOM patterns and effectiveness. Third, we describe the data collection and variable construction process. Fourth, we present the empirical model specifications and the results. Next, we discuss theoretical and managerial implications, the limitations of our study, and future research directions. Finally, we present the conclusions.\n",
              "\n",
              "unusual cost structure with high fixed costs, but with almost zero variable costs for production and distribution (Shapiro and Varian 1999). Researchers have primarily employed analytical modeling approaches to analyze optimal pricing strategies for information goods under different contexts (Chellappa and Shivendu 2005; Khouja and Park 2007; Sundararajan 2004) and to study the interaction between the information goods pricing strategy and a firm’s product strategies such as bundling (Bakos and Brynjolfsson 1999; Wu et al. 2008), versioning (Chen and Seshadri 2007), and differentiation (Choudhary 2010). Extending this stream of literature, we employ an empirical approach that examines the interaction between an information goods pricing strategy and WOM dynamics. Recently, given the intense competition in the information goods markets such as online news and consumers’ low WTP for online content due to the existence of free online and offline alternatives, the freemium pricing strategy is gaining popularity. A firm employing the freemium pricing strategy offers free content (i.e., product) supported by online advertising revenue and offers premium content to paying subscribers (Anderson 2009). Although recent empirical research has found that community participation (Oestreicher-Singer and Zalmanson 2013) and social contagion (Bapna and Umyarov 2015) may have a positive impact on the success of a firm’s freemium strategy, moving from free ad-supported content to the freemium model is challenging. Given the low WTP for online news (Chyi 2005), one can expect that the online readership of a newspaper may decline after the introduction of a paywall. However, it is not evident how the transition from “free” to “for-a-fee” would impact the WOM about the newspaper’s content. Therefore, our study draws upon insights from the existing literature to investigate the impact of charging a fee for news content on WOM dynamics related to news content in social media.\n",
              "\n",
              "Online WOM\n",
              "Word-of-mouth (WOM) is defined as an interpersonal communication, about an organization or its products that is independent from the organization’s marketing activities (Bone 1995). Online WOM is a type of WOM where the communication is mediated by Internet technologies. Further, WOM in social media about information goods (such as newspaper articles) is a special case of online WOM. Unlike WOM for a product/ service where users share their opinions and recommendations about the product/service, WOM in social media for information goods such as news content involves users implicitly/ explicitly communicating their belief about the importance/ relevance of news content by sharing links to the news content in online social media.\n",
              "\n",
              "Related Literature\n",
              "Information Goods Pricing\n",
              "Information goods (i.e., products that can be digitized) raise interesting pricing opportunities and challenges due to their\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "33\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "In a social media WOM process, individuals transmit articles they evaluate to be worth sharing. For example, a user’s sharing of a tweet6 or retweet,7 containing the link to a newspaper article, can be considered as implicit WOM (e.g., recommendation to read the article) about an article’s importance. Therefore, content sharing is considered as WOM in this paper. Such WOM not only acts as a signal about the importance or relevance of the content of the article (in the view of the sender) but it also allows dissemination of the content itself within the social network. Thus, WOM about newspaper articles in social media also affords an opportunity for receivers to consume the content.8 A significant body of WOM research has examined a wide variety of issues related to WOM. Of particular interest to our study is the research on patterns and effectiveness of WOM. Researchers have identified various interesting distribution patterns of WOM. Focusing on the temporal pattern of the WOM, researchers have examined how the average valence of WOM varies over time. The empirical evidence suggests that the average valence tends to decrease as time progresses because earlier consumers of a product (who are also likely to be early reviewers) tend to be more zealous about it (Hu et al. 2007; Li and Hitt, 2008). Examining the distribution of WOM in terms of valence, Hu et al. (2007) found that online WOM about products is skewed toward positive valence, creating an asymmetric bimodal (J-shaped) distribution. They suggest that the J-shaped distribution results from two sources of self-selection biases: purchasing and underreporting. Recently, researchers have examined the distribution of WOM for hit/niche products and found a U-shaped relationship between WOM volume and popularity of product (Dellarocas et al. 2010). These results suggest that consumers are more likely to generate WOM for extreme products— niche products or hit products—in terms of popularity. Similar to Dellarocas et al. (2010), our study also examines the distribution of WOM for hit/niche products. However, our focus is on examining the change in the distribution of WOM for hit/niche content as a result of implementation of a paywall. Effectiveness of WOM has also been studied recently. Research suggests that online WOM impacts various perforA tweet is a short message sent by a sender to its recipients on Twitter, a social media platform.\n",
              "7 6\n",
              "\n",
              "mance measures such as product adoption and sales (Chevalier and Mayzlin 2006; Godes and Maylin 2004; Liu 2006). It has been argued that awareness and persuasion are significant underlying cognitive processes of WOM that impact consumer behavior and consequent outcomes (Duan et al. 2008; Liu 2006). Volume and valence are among the most important WOM attributes that have been examined to understand the effectiveness of WOM (e.g., Liu 2006). Volume measures the total amount of WOM interactions, while valence captures the nature of WOM messages (i.e., positive or negative). Research suggests that both volume and valence of WOM have a significant impact on product sales (Chevalier and Mayzlin 2006), while some prior empirical studies indicate that the volume of WOM is more effective than the valence due to the relative dominance of awareness effect vis-à-vis persuasion effect (Liu 2006). In our context, volume is more relevant than valence. Given that users in social media are generating WOM about news content to communicate the importance or relevance of news articles to others, articles that are below a certain threshold in terms of importance or relevance (in the opinion of the WOM sender) will not be shared whereas articles that exceed this threshold will be shared. As a result, all of the news articles that are shared will have high valence. Moreover, the valence is unobserved in the present context. Therefore, we focus on the impact of volume of WOM on performance outcome. Specifically, since website traffic is an important proxy for subscription and advertising revenue, we examine the impact of WOM volume on newspaper website traffic. Further, we extend this research stream by highlighting the role of information pricing strategy in moderating the impact of WOM on performance outcome (i.e., website traffic).\n",
              "\n",
              "Theoretical Framework\n",
              "In this section we outline the main empirical questions addressed in the paper and discuss the theoretical rationale underlying each question.\n",
              "\n",
              "The Impact of a Paywall on WOM Pattern\n",
              "Volume of WOM: Research has suggested the difficulty of charging a fee for content because of consumers’ low WTP (Chyi 2005; Picard 2000). Consequently, a paywall is expected to lower the online readership of NYT. To the extent that a significant proportion of readers who stop accessing NYT (or reduce their consumption of NYT content) were actively sharing NYT articles earlier, the seeds (i.e., initial\n",
              "\n",
              "A retweet is the retransmission of a tweet received by an user on Twitter.\n",
              "\n",
              "8\n",
              "\n",
              "A recent study measuring information sharing on Twitter shows that 18% of the messages shared (i.e., 1.8 million messages/tweets) in July 2009 had links (i.e., URLs) to websites (Singh 2009), demonstrating the role of social media in spreading WOM about online content.\n",
              "\n",
              "34\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "sharing) for NYT content will be lowered. Further, given that the volume of WOM in a social media also depends on the viral spread of the content through relationships among members of the social network, the lack of seeds that can be retransmitted by other members in the social media will lower the amount of retweets, thus weakening the potential viral effect. As a result, a paywall will lead to lower WOM by decreasing both the amount of links to news content shared through seeds and retransmission of seeds. Further, retransmission volume may also be indirectly affected by a paywall due to the possible change in the characteristics of the content being tweeted. Given that a seed tweet containing a link (i.e., URL) to niche content has a lower likelihood of being clicked/read (which generally is a precondition for retweeting) than popular content, if a paywall increases (decreases) the proportion of popular articles that are tweeted vis-à-vis niche articles, the retweet volume may increase (decrease) exponentially due to network effects. Finally, if a sizable proportion of readers decide to change their sharing behavior and increase their propensity to tweet and/or retweet to help their subscribers/followers on social media bypass9 the NYT paywall, then it is even possible to observe an increase in the amount of NYT WOM. Given that the net result of a paywall on WOM volume would depend on the relative strength of these forces, we empirically examine the impact of a paywall on WOM volume. Distribution of WOM: To the extent that a paywall leads to a reduction of the volume of WOM due to a significant proportion of users who have low WTP for content (Chyi 2005), causing them not to adopt the subscription model and discontinue or reduce their consumption of NYT content, the paywall is also likely to shift the distribution of the WOM. Readers of an online newspaper exhibit heterogeneity in terms of WTP for accessing the digital content. Although WTP may depend on a variety of factors (Chyi 2005), we focus on the usage intensity (i.e., the number of articles read and/or shared), which is one of the important factors that influence the value of the product/service in the context of a subscription for accessing content. Consequently, usage intensity is expected to determine whether a consumer will subscribe to the content at a given price or not (Danaher 2002). The NYT’s paywall pricing strategy essentially creates two versions of the product: a free version that allows access to fewer than 20 articles in a month and a subscription version that allows unlimited access to all content. Consumers who read/share few articles (referred to as light users) are very\n",
              "9\n",
              "\n",
              "sensitive to price (Danaher 2002) and therefore are more likely to select the free version or switch to alternative news sources because of their low WTP. In such a freemium model, light users are further classified into two groups: (1) consumers who read less than the free article limit and (2) consumers who read more than the free article limit but read less than the threshold to convert to subscribers. The first group of light users does not necessarily leave the NYT and will not affect the WOM distribution after introductoin of the paywall.10 Light users who are in the second group can make two possible subscription decisions: (1) Reduce their consumption to fewer than the 20 free articles limit (2) Discontinue their consumption of the NYT news content and switch to alternative news sources Both of these cases influence the net effect of the NYT’s paywall on its WOM distribution. Consumers who are heavy users (i.e., read/share more articles) will be more willing to subscribe and select the paid version because of their high willingness to pay. As a result, after the paywall implementation, the distribution of users in terms of their usage will shift toward heavy users as the paywall will disproportionately affect light users who will have a higher reduction in their consumption of NYT content (which in extreme cases may even lead to their discontinuation of NYT content consumption, i.e., attrition) due to the paywall restriction. The definition of usage intensity construct, its operationalization, and literature supporting the operationalization is presented in Table 1. Next, we argue that this disproportionate reduction in the consumption of NYT content by light users, after the paywall, will affect the distribution of the NYT articles read and shared. Recognizing that consumers differ in terms of their preference for popular versus niche products, prior research suggests that the frequency of usage is associated with con-\n",
              "\n",
              "Recall that NYT excluded the visits to its website that originated through a social media link from counting toward the free article limit.\n",
              "\n",
              "In an extreme case, it is possible that if a free article limit is too high, none of the users have to reduce or discontinue their consumption of the NYT articles after the paywall. However, such a case is not realistic because if this cutoff of free articles is too high, there is no incentive that one would convert to a subscription. In fact, in our study period in 2011, the free article limit of the NYT paywall was 20 articles per month, but the NYT reduced this allowance to 10 articles per month later in 2012. Further, it is important to note that the larger the percentage of such users who read less articles in a month than the monthly free articles limit set by NYT paywall, the more difficult it would be to find a significant change in the volume or pattern of NYT WOM that can be attributed to the paywall. Therefore, our results showing significant change in volume and pattern of NYT WOM suggest that this group of users is relatively small.\n",
              "\n",
              "10\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "35\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 1. Definition of Key Constructs\n",
              "Construct Usage intensity Definition Reader classification based on the quantity of news article consumption level (i.e., number of news articles read) Content type classified based on content demand Operationalization Heavy versus light readers: use an individual’s volume of NYT link sharing on Twitter as a proxy of his/her NYT content consumption Popular versus niche content: relative content popularity based on WOM rank of content Supporting Literature The pattern of online WOM about TV shows has explanatory power in TV ratings (Godes and Mayzlin 2004)\n",
              "\n",
              "Content popularity\n",
              "\n",
              "Sales rank-demand (i.e., sales) relationship has been established in the long-tail literature in various contexts such as Amazon’s book (Brinjolfsson et al. 2003), DVD rental (Zentner et al. 2013), and mobile app markets (Garg and Telang 2013)\n",
              "\n",
              "Figure 1. The Inverse Relationship Between Content Popularity and Consumer Usage Intensity\n",
              "\n",
              "sumption patterns in terms of content popularity. According to the theory of exposure (Elberse 2008; McPhee 1963), popular products monopolize the consumption of light consumers, whereas heavy consumers choose a mix of hit and niche products. The exposure theory suggests that consumers who choose niche products tend to be familiar with many alternatives while those who know of few alternatives tend to stick with popular products. Likewise, the variety-seeking literature suggests that users’ preference for niche products is positively associated with their quantity of consumption. Simonson (1990), for instance, finds that those customers who buy larger quantities per purchase tend to select a greater variety of items. Elberse (2008) also finds evidence that\n",
              "\n",
              "customers with a higher frequency of usage have a tendency to consume niche products in the tail of the sales concentration distribution. The definition of the content (i.e., product) popularity construct, its operationalization and literature supporting the operationalization is presented in Table 1. To summarize, as illustrated in Figure 1, we expect that the light user segment is more likely to consume popular articles (i.e., articles that appear in the head of the content popularity distribution) whereas the heavy user segment is more likely to consume a mix of niche articles (i.e., articles that appear in the tail of the content popularity distribution) and popular articles. Juxtaposing this statement with the earlier observa-\n",
              "\n",
              "36\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "tion that light user segments are more likely to reduce (or even discontinue, in extreme cases) their consumption of NYT content after the paywall implementation, we expect that the consumption and sharing of popular articles will exhibit a disproportionate reduction vis-à-vis niche articles as light users who read more popular articles will exhibit a stronger reduction in their NYT content consumption vis-à-vis heavy users who read a mix of niche and popular articles. As a consequence of the reduced consumption of light users who are expected to disproportionately consume more popular articles, there would also be a change in the distribution of the WOM about NYT’s content in social media after the paywall implementation. Specifically, as the number of consumers generating WOM about popular content (and/or the amount of popular content that a consumer consumes) will decrease disproportionately, the distribution of WOM will shift toward the articles in the tail of the content popularity distribution (i.e., a decrease in the volume of WOM at the head of the content popularity distribution will be stronger vis-à-vis a decrease in the volume of WOM at the tail of the content popularity distribution), thus leading to long-tail distribution of NYT WOM. On the other hand, if the users with low WTP (who reduce their consumption of NYT content after introduction of the paywall) are not systematically different—in terms of the preference for popular versus niche content—than those with high WTP (who do not alter their consumption after introduction of the paywall), then, although the volume of NYT WOM may decrease, the WOM pattern would not change after introduction of the paywall as the reduced consumption will also exhibit the same proportion of popular and niche content. Thus, we would not observe any significant change in the NYT WOM distribution after introduction of the paywall. In other words, our argument relies on exposure theory to posit that the paywall will lead to long-tail distribution of NYT WOM content. However, relying on exposure theory is not the only plausible mechanism that may lead to long-tail distribution. An alternative mechanism that may also lead to long-tail effect relies on the user’s strategic behavior when faced with the decision to curtail their consumption.11 Up to now, we have not made\n",
              "We also examined the plausibility of another alternative mechanism that is based on “resentment logic” to explain the impact of the paywall on NYT’s WOM pattern. In this mechanism, we assume that a proportion of NYT readers may boycott the NYT and “stop reading NYT” due to a negative emotional response, such as resentment, toward the NYT’s adoption of a paywall (Campbell 1999; Xia et al. 2004). This assumption would explain the decrease in the volume of NYT WOM after introduction of the paywall, but to explain the long tail effect we would have to make an additional\n",
              "11\n",
              "\n",
              "any assumption about the strategy of users who are forced to reduce their NYT content consumption due to imposition of a paywall monthly limit and their low WTP. In other words, we have treated users as non-strategic if they do not systematically change their consumption behavior in terms of what articles to read on the NYT website when they are faced with a monthly article limit set by the paywall and they have to curtail their content consumption. To the extent that the users are strategic in their decision to choose whether to curtail popular content consumption or to curtail niche content consumption, given that they have to reduce their overall content consumption, we can apply information economics and search cost theory to understand the behavior of such strategic users and its implication on the long-tail effect.12 Such strategic users would compare the search cost for finding the content they have to forego on the NYT website on an alternative news or similar website and curtail the consumption of the content type that requires lower search cost to find on an alternative website.13 Given that popular content is available at most of the websites whereas niche content (in most cases) is exclusive to a specific website, it is easier to find popular content that appears on the NYT website on alternative websites. In other words, ceteris paribus, the\n",
              "\n",
              "assumption. Specifically, we will have to argue that these users, who drop out due to resentment, are systematically different from other users in terms of their consumption of popular and niche content. In other words, we will have to assume that those users who are more likely to read popular content, vis-à-vis those who read both popular and niche content, are also more likely to feel resentment after introduction of the paywall and thus would stop reading NYT. However, in the absence of a convincing rationale or theory to support such an assumption, we rule out this mechanism and focus on other mechanisms that may generate the long-tail effect. Follow-up research may measure users’ emotional response to a paywall using survey data collected from users who discontinued reading NYT and relate it to their content preference and WTP. Further, there may be other alternative theories to explain the long-tail result that we demonstrate in this paper and follow-up research may be needed to identify and isolate any such plausible mechanisms that may also lead to a long-tail result.\n",
              "12 We would like to thank an anonymous reviewer for suggesting this alternative mechanism. 13 Another possibile strategic behavior is when a segment of heavy users decide to curtail their consumption of NYT, in order to avoid paying the subscription fee, by strategically reducing their consumption of popular content on NYT (as the search cost for finding such content on an alternative website is low). To the extent that there is a significant number of users in such a heavy user segment, we are likely to see a long-tail effect due to the reduction in popular content consumption/sharing; however, unlike the arguments based on WTP, this reduction would be due to the change in the behavior of heavy users rather than light users.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "37\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "search cost for finding popular content is lower than the search cost for finding niche content. Therefore, users who have lower WTP are likely to curtail their NYT consumption of popular content due to the paywall. To the extent that there is a significant number of light users who strategically take advantage of the NYT’s free article limit and read/share the NYT’s niche articles while consuming popular news articles available in (free) alternative news sources, the proportion of NYT’s popular content consumed and shared will decrease disproportionately vis-à-vis NYT’s niche content, leading to the long-tail effect (i.e., less concentrated WOM distribution). Although this alternative mechanism, which builds upon search cost theory, does not rely on exposure theory, it complements and extends the exposure theory-based mechanism. Specifically, if low WTP users, who are likely to consume a larger proportion of popular products, are also likely to strategically curtail their NYT content consumption such that they selectively reduce a larger proportion of content with lower search costs (as per search cost theory), then the long-tail effect would be stronger as more popular content will be curtailed vis-à-vis niche content. Given the abovementioned mechanisms that suggest the possibility of the NYT paywall leading to a long-tail effect, we empirically test whether the introduction of paywall leads to a longer tail (less concentrated) WOM distribution (i.e., the volume of WOM for popular content will drop more than the volume of WOM for niche content).\n",
              "\n",
              "the seeders are regular visitors to NYT’s website. To the extent the WOM by these initial seeders reaches those network members (either to their direct followers and/or other network members who are exposed to the WOM spread through retransmission decisions by followers) who are not frequent visitors to the NYT website and a significant proportion of them decide to visit the website, the increase in WOM would lead to higher website traffic. To summarize, in aggregate, the WOM about a firm’s content in online social networks is likely to bring new readers (who directly or indirectly follow the seed reader) to the firm’s websites. Although intuitive, we empirically measure the strength of the positive relationship between WOM volume (i.e., social media buzz) and website traffic so that we may examine how this association changes after a paywall implementation. We now turn to an examination of how a paywall moderates this positive relationship between WOM volume and website traffic. There are two competing effects explaining the impact of a paywall implementation on the effectiveness of WOM. In this highly connected digital age, firms are aware of the value of social media and actively engage it in their business strategy. For example, NYT allows visitors who come from links on social media to bypass its paywall. If this bypass effect (i.e., an increase in the likelihood of nonsubscribers’ clicking on NYT content available through social media as they attempt to maximize the number of articles that can be accessed without paying a subscription fee) is dominant in website traffic generation, the relative strength of the relationship between social media WOM and website traffic may increase after a paywall implementation. However, there is another argument suggesting a negative effect on the relationship between the volume of WOM and website traffic after a paywall implemention. Research suggests that content characteristics play a significant role in determining the virality of online content (Berger and Milkman 2012). As we argued in the previous section, a paywall may lead to a disproportionate decrease in the WOM about popular content. Given that popular articles contain content for which there is a greater demand from a larger audience (Zentner et al. 2013), we expect that a decrease in the proportion of WOM about popular content will in turn lower the average clicks per link shared through social WOM. Therefore, the virality effect works in the opposite direction to the bypass effect mentoined above. We expect that the bypass effect will not be able to mitigate the negative effect of changes in WOM distribution because the virality effect can be exponential through online social networks (Shi et al.\n",
              "\n",
              "The Impact of Paywall on WOM Effectiveness\n",
              "Next we focus on WOM effectiveness. In our context, WOM effectiveness refers to the ability of the WOM to increase the number of individuals who consume the information good. Given that the consumption of newspaper articles occurs at the newspaper’s website, an increase in consumption is synonymous with an increase in website traffic. Therefore, we examine the association between the volume of online WOM (also referred to as social media buzz) and website traffic. The WOM generated as a result of content spreading over an online social network through transmission, consumption, and retransmission of information (Berger and Milkman 2011; Stephen and Lehmann 2012) creates awareness about the content and increases the set of potential users who can consume the content. The initial group of users who post a message containing a link to an article, also referred to as seeders, broadcast their opinion about the importance or relevance of an article to their followers. We can assume that\n",
              "\n",
              "38\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "2013). As such, we expect that the paywall will weaken the strength of the relationship between social media WOM and website traffic due to the changes in the distribution of WOM after the paywall implementation. However, it is an empirical question as to which effect is dominant, and we examine the net impact of the paywall on the relationship between the volume of WOM and website traffic.14 Hence, we examine whether the positive relationship between WOM volume (i.e., social media buzz) and website traffic is weakened after a paywall introduction. A summary of alternative plausible mechanisms that may drive the impact of a paywall on the pattern and effectiveness of WOM, a brief rationale and assumptions underlying the mechanisms, and theoretical as well as empirical support for or against these mechanisms is provided in Appendix A.\n",
              "\n",
              "versa. In order to isolate this time trend effect and the influence of other extraneous factors, we employ a difference-indifference approach and use Los Angeles Times (LAT) as a control group. LAT is a major national newspaper in a different geographic region.15 We collected tweets that contain newspaper link sharing for NYT and LAT articles between February 26, 2011, and March 18, 2011, and between April 4, 2011, and April 24, 2011, respectively. Because Twitter APIs do not allow us to collect population sized historical tweets, we obtained the exhaustive dataset containing all public NYT and LAT link sharing on Twitter through a Twitter-licensed data reseller company. The keywords used in data collection include nyti.ms (lat.ms), nytimes.com (latimes.com) and @nytimes (@latimes).16 Our final data sets contain 1,287,570 tweets containing NYT links and 226,911 tweets containing LAT links. The population size of the data allows us to create and analyze the distributions of WOM in terms of content popularity, which is elaborated below. We were also able to obtain daily website traffic data for the two newspapers in the same time period from HitWise, a marketing research firm that collects aggregate clickstream data17 from a geographically diverse range of ISP networks and opt-in panels, representing all types of Internet usage, including home, work, educational, and public access. In order to support our use of Twitter WOM for a proxy of article reading, we conducted an online survey using Google consumer survey panels. Out of 613 survey respondents residing in the U.S., 103 participants responded that they share news articles on Facebook and/or Twitter. We asked the approximate numbers of their online newspaper reading (mean: 38.4, SD: 69.4), sharing on Facebook (mean: 5.7, SD: 15.15), and sharing on Twitter (mean: 5, SD: 19.18) in\n",
              "\n",
              "Data\n",
              "The approach used to test the research questions presented above is based on a natural experiment. This allows us to analyze whether WOM pattern on Twitter and WOM dynamics related to website traffic significantly differ before and after NYT’s paywall rollout on March 28, 2011. To test this question, we combine data from two sources: NYT link sharing on Twitter and website traffic for 21 days before and after NYT’s paywall rollout. Although there is no rule of thumb regarding the appropriate length of a time window, we chose a 21 day study period before and after the paywall based on the recent empirical studies on content diffusion using large-scale Twitter data. Empirical studies note that content diffusion on Twitter occurs relatively fast (Goel et al. 2012; Kwak et al. 2010), reporting that half of retweeting occurs within an hour and 75 percent in under a day and that the timescale of diffusion on Twitter lasts only a few days in most cases. One potential concern is whether the treatment effect of a paywall may be biased because of a greater proportion of interesting news events in the pre-paywall period, or vice\n",
              "\n",
              "According to Audit Bureau Circulation, the average daily circulation of Los Angeles Times ranked fifth among daily newspapers in the U.S. in 2011, while NYT ranked third. The other three among the top five newspapers are The Wall Street Journal, USA Today, and New York Daily News. Detailed information is available at the following link: http://www.poynter.org/ latest-news/mediawire/151696/wall-street-journal-usa-today-new-yorktimes-top-latest-circulation-report/. It is still possible that NYT WOM shared in generic URL shorteners such as bit.ly is not included. However, a small-scale test using random users sampling shows that NYT article links shared on bit.ly is a relatively small fraction. Moreover, this fraction is not likely to be systematically different before and after introduction of the paywall.\n",
              "17 Hitwise data is based on an extensive sample size (25 million people worldwide, including 10 million in the U.S.), which is weighted to ensure that the data is accurate and representative. 16\n",
              "\n",
              "15\n",
              "\n",
              "It is plausible that there are other mechanisms through which the paywall may affect the effectiveness of WOM. For example, to the extent a large segment of users show resentment toward NYT due to its introduction of a paywall and hence systematically does not click on and/or share a NYT link on social media, the effectiveness of WOM would weaken after the paywall. Since our paper does not measure user resentment, we can neither support nor rule out the resentment-based explanation for the impact of a paywall on NYT WOM effectiveness. Although this resentment-based mechanism may be an alternative to the virality-based logic suggesting negative impact of a paywall on WOM effectiveness, the mechanisms may coexist.\n",
              "\n",
              "14\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "39\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "a normal week. The correlation between the number of online news reading and sharing on Twitter in a normal week is positive and significant (ρ = 0.35, p-value < 0.01). We also found that the correlation between reading and sharing on Facebook is positive and significant (ρ = 0.21, p-value < 0.05). The survey items are provided in Appendix B.\n",
              "\n",
              "Measures for WOM Pattern Analysis\n",
              "To test the impact of a paywall on the pattern of WOM (i.e., news link sharing), we conduct a content-level (i.e., news article) analysis. We compute the counts of NYT WOM and LAT WOM, which are used as the dependent variables for examining the impact of paywall on the volume of WOM. We examine the distribution of WOM by computing WOM Rank of Content from both the NYT and LAT data sets. Consistent with prior empirical studies (Brynjolfsson et al. 2011; Brynjolfsson et al. 2003), the NYT Content with highest WOM is assigned the lowest value for WOM Rank of Contenti. We follow prior long tail studies (Brynjolffson et al. 2011; Brynjolffson et al. 2003; Zentner et al. 2013) and measure popularity of content in a relative sense using an ordinal ranking of the content based on frequency of content sharing. In our sample, a significant proportion of tweets contained NYT and LAT links to past articles, which may create positive inflation of the paywall effect because the articles in the pre-paywall block simply had a longer duration in our sample, allowing greater opportunity for these articles to be shared. To prevent this bias, we use a 1-day horizon (i.e., only focus on the WOM for an article that was generated on the same day the article was published) in measuring the dependent variables. Table 2 presents the summary statistics of our data sets.\n",
              "\n",
              "As our data shows that news link sharing has a significant variation depending on the day of the week, we create two weekend dummy variables. Initial investigation showed that the independent variable, tweets, was not normally distributed. As suggested by Gelman and Hill (2007), we take the logarithm on Tweets volume to control for its left-skewed nature. We denote the log-transformed variables by adding “ln” to the variable name. Since the inclusion of the paywall dummy variable and the interaction term of paywall and the lagged variable of Tweets volume may lead to high multicollinearity, which, if uncorrected, may lead to inflated standard errors and even inconsistent or unstable estimates, we mean-centered these variables before generating their interaction term.\n",
              "\n",
              "Evaluation of Quasi-Experiment Design: Treatment and Control Groups\n",
              "In this section we provide evidence in support of our difference-in-difference (DID) setup using multiple empirical approaches. First, we test the validity of the LAT control group on dimensions of user demographics, and time trends of website visits and content. We checked the key demographics of NYT and LAT website visitors that can influence their news content consumption and sharing behavior: household income, age, and gender. Table C1 in Appendix C indicates that the two groups are qualitatively similar in both demographic variables and that selection issues in demographics are not severe. Then, following DID research (Bertrand et al. 2004),18 we look at whether the NYT and LAT website visits have similar time trends in the absence of the intervention (i.e., NYT paywall introduction). In order to conduct a statistical test for a pre-treatment parallel time trend check, we conduct a newspaper group (i.e., NYT) fixed effects panel regression with a set of pre-treatment dummies using the website visit data. We utilize a longer time frame (10 weeks from January 8, 2011, to March 18, 2011) for weekly time dummies. Our approach is similar to the panel data DID model with newspaper group fixed effects, and we estimated the following model specification:\n",
              "18 Regarding the validity of the control group, we want to emphasize that we need to find the similarity of trend before the treatment, not the similarity of the magnitude, as noted in Bertrand et al. (p. 251),\n",
              "\n",
              "Measures for WOM Effectiveness Analysis\n",
              "To examine the impact of a paywall on WOM effectiveness, we conduct a website-level analysis. We created the measures of daily WOM volume (aggregated over all of the articles in a day) and website traffic. The website traffic metric is widely used in online advertising to determine where and how to advertise. Website visitst is defined as the number of unique visitors to a website in a day. In addition, we counted the total number of tweets that contain either a NYT or LAT link shared in day t. This variable represents the daily WOM volume. Table 3 presents the summary statistics of our site-level data sets.\n",
              "\n",
              "This specification is a common generalization of the most basic DD setup (with two periods and two groups), which is valid only under the very restrictive assumption that changes in the outcome variable over time would have been exactly the same in both treatment and control groups in the absence of the intervention.\n",
              "\n",
              "40\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 2. Summary Statistics: Content-Level Data\n",
              "Mean 55.20 10.84 Before Paywall Roll-Out SD Min 265.87 1 N = 10604 33.66 1 N = 4618 Max 8563 1563 Mean 47.41 11.00 After Paywall Roll-Out SD Min 140.66 1 N = 10578 23.57 1 N = 4512 Max 4517 586\n",
              "\n",
              "NYT WOM LAT WOM\n",
              "\n",
              "Table 3. Summary Statistics: Site-Level Data\n",
              "Mean 1,709,428 33,773 294,845 5,308 Before Paywall Rollout SD Min 247,586 1,436,089 9,595 20.260 73,610 224,487 1,584 2,800 Max 2,268,483 54,100 536,144 9,817 Mean 1,428,147 27.540 263,799 5,497 After Paywall Rollout SD Min 106,698 1,257,954 5,313 16,934 17,725 231,990 1,103 2,752 Max 1,577,606 34,513 295,065 7,052\n",
              "\n",
              "NYT Website Visits NYT WOM LAT Website Visits LAT WOM\n",
              "\n",
              "Website visitsit = α i + l = week 1 βl ,t Dl ,t\n",
              "10\n",
              "\n",
              "+ l = week 1 γ l ,t Dl ,t × NYTi + εit\n",
              "10\n",
              "\n",
              "(1)\n",
              "\n",
              "where Website visitit is the gross website visits of newspaper i on day t, NYTi is an indicator variable equal to one if an observation occurs in newspaper i, Dl,t is a vector of weekly dummies, and αi is a vector of NYT fixed effect. We do not include a main effect in this equation for NYT dummy as it would be subsumed entirely by the NYT fixed effects. The coefficient of the interaction term, γl,t, captures the differences in the weekly time trends between NYT and LAT. If LAT is an adequate control for NYT, then we would expect γl,t = 0. Note that the variable of interest in our control group validation is γ because it captures the difference in time trends in website traffic between NYT and LAT over the common time trends, which are captured by β. Table 4 reports the estimates of the Equation (1). The eight estimates of the interaction terms (γ1 – γ8 ), except for one week (γ9),are statistically insignificant, showing that changes in the volume of NYT and LAT website visits exhibit mostly parallel trends in the pre-paywall treatment period. Finally, we compare the news events covered in the top 200 popular NYT and LAT news articles based on their WOM counts before and after paywall rollout. Among these 400 NYT and LAT news articles, we identified 235 articles (i.e., 116 NYT and 119 LAT articles) that covered the same news events and ranked among the top 400 articles in our NYT and LAT WOM distributions, implying that LAT content shared\n",
              "\n",
              "is qualitatively similar to NYT content shared in terms of content-specific time trends. In addition to the major news events such as breaking news and top stories summarized in Table C2 in Appendix C, we found significant overlaps of miscellaneous news events, such as “obituary of the last WW1 veteran”19 and “announcement of recent research findings on sugar-food addiction.”20 Alhough LAT shows greater local news coverage than NYT, the different local news coverage between NYT and LAT (e.g., NY Region or LA Now) would not have a serious impact on our long-tail results because the greater proportions of local news in LAT’s top 200 popular article sample are found consistently in both pre- and post-paywall samples. Another possible concern associated with the validity of our quasi-experimental design is the exogeneity of NYT paywall treatment. In order to attribute changes in the outcomes to the paywall, we exclude alternative explanations that factors other than the paywall may affect the outcomes in the post-paywall period. First, considering the possibility of NYT reporters’ promotional efforts after the paywall, we counted the WOM volume of 56 NYT-related Twitter accounts and conducted a paired two-sample t-test before and after the paywall. The results show that there is no statistically significant difference\n",
              "\n",
              "19 NY Times: “Frank Buckles, Last World War I Doughboy, Is Dead at 110” on March 1, 2011; LA Times: “Frank Buckles dies at 110; last American veteran of World War I” on March 1, 2011.\n",
              "\n",
              "NYT: “Is Sugar Toxic?” on April 13, 2011; LAT: “If food addiction exists, blame the brain—not the cookies” on April 5, 2011.\n",
              "\n",
              "20\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "41\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 4. Pretreatment Parallel Trend Checks of Daily Website Visits\n",
              "γ1 γ2 γ3 γ4 γ5 γ6 γ7 γ8 γ9 Constant Observations R-squared Website Visits 5,655 (72,883) -80,325 (72,883) -106,874 (72,883) -107,719 (72,883) -53,306 (72,883) 35,189 (72,883) 60,128 (72,883) 74,985 (72,883) 354,533*** (72,883) 914,877*** (25,768) 140 0.608\n",
              "\n",
              "Notes: Because of page limits, we only report the estimates of the interaction between weekly dummy and NYT dummy at each week (γ1 – γ9) and do not report the coefficients of weekly dummies (β1 – β9), which are not statistically significant. Standard errors in parentheses. ***p < 0.01, **p < 0.05, *p < 0.1\n",
              "\n",
              "in WOM volume (pre-paywall mean = 149.4, post-paywall mean = 141.0, t = 1.11, p = 0.271). The results reduce the concern of confounding factors associated with NYT reporters’ (including NYT columnists Nick Bilton and Paul Krugman and NYT’s Twitter accounts such as NYT Business) promotional social media efforts. Qualitatively, we examined whether there is a change in news event coverage in the post-paywall period using LAT as a natural basis for comparison of NYT’s news supply. Table C1 shows that NYT’s news event coverage after the paywall remains similar to the LAT control group. Our observation is consistent with the Harvard Business School case (Kumar et al. 2012) on the NYT paywall that does not describe any NYT management team’s plan to change its content strategy.\n",
              "\n",
              "and LAT content (i.e., the control group). Because the dependent variable, WOMi, is the count of link sharing for either NYT or LAT content i on Twitter, we employ a count model. Based on the results of the over-dispersion test, we estimate a negative binomial regression (NBD). For the sake of exposition, in Equation 2, we present the specification of our difference-in-differences(DID) model for content i in a simple linear form. However, the NBD model has a nonlinear functional form, with the expected count of the dependent variable modeled as E(yi|Xi) = μi = exp(Xiβ + gi).\n",
              "\n",
              "WOM i = β0 + β1 Paywalli + β2 NYTi +\n",
              "\n",
              "β3 Paywalli × NYTi + εi , εi ~ N (0, σ 2 )\n",
              "\n",
              "(2)\n",
              "\n",
              "Model and Empirical Findings\n",
              "We now describe the empirical testing approach and the findings for our research questions, and report robustness checks.\n",
              "\n",
              "Impact of a Paywall on WOM Pattern\n",
              "Volume of WOM: Difference-in-Differences Approach We compare the WOM for 21 days before and after the paywall rollout for NYT content (i.e., the treatment group)\n",
              "\n",
              "where Paywalli is a dummy that equals one if the time period is after the paywall rollout period (see Table 5 for the variable description). The paywall dummy captures the aggregate factors that would cause changes in the dependent variable in both the treatment and control groups. The dummy variable, NYTi, captures the possible differences between the treatment and control groups prior to the paywall rollout. The coefficient of interest, β3, is an estimate of the interaction term, Paywalli × NYTi, which equals one if an observation is in the treatment group in the post-paywall period. The estimates for Equation (2) are presented in column (4) of Table 6. Consistent with our prior expectation, Paywall is negative and significant in Model 1 (β1 = -0.154; p < 0.01). The β3 on the interaction term is negative and significant in column (4). The interaction plot in Figure 2 indicates that\n",
              "\n",
              "42\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 5. Variable Definitions\n",
              "Site-Level Variables Paywallt Website visitst WOMt Saturdayt Sundayt Paywalli NYTi NYT WOMi LAT WOMi WOM Rank of Contenti A dummy variable indicating 1 if day t is in the post-paywall period Total volume of daily website traffic in day t Total volume of NYT link sharing on Twitter in day t A dummy variable coded as 1 if the day is Saturday A dummy variable coded as 1 if the day is Sunday Content-level Variables A dummy variable coded as 0 if an observation is prior to the paywall rollout, otherwise 1 A dummy variable coded as 1 if an observation is a NYT link, otherwise 0 1-day diffusion count of NYT article link sharing on Twitter 1-day diffusion count of LAT article link sharing on Twitter Rank of an article after sorting articles (in descending order) based on the volume of WOM for that article (i.e., the highest shared articles get the lowest rank)\n",
              "\n",
              "Table 6. WOM Volume: Difference-in-Differences Analysis\n",
              "(1) Dependent Variable Paywalli NYTi Paywalli × NYTi Saturdayi Sundayi Constant No. of observations R-squared 0.160*** (0.0305) 0.192*** (0.0323) 3.334*** (0.0152) 20,892 0.0004 0.229*** (0.0419) 0.249*** (0.0437) 3.678*** (0.0199) 12,504 0.0014 0.132*** (0.032) 0.167*** (0.034) 3.974*** (0.016) 21,182 0.0004 NYT Original WOM -0.0453** (0.0201) (2) NYT Retweet WOM -0.266*** (0.0266) (3) NYT WOM -0.154*** (0.021) (4) NYT & LAT WOM 0.0133 (0.0311) 1.619*** (0.0261) -0.166*** (0.0370) 0.0602** (0.0268) 0.115*** (0.0275) 2.370*** (0.0221) 30,312 0.0199\n",
              "\n",
              "Standard errors in parentheses. ***p < 0.01, **p < 0.05, *p < 0.1.\n",
              "\n",
              "after the paywall implementation, NYT content sharing decreases, whereas the change of the rivals’ content sharing shows the opposite trend to the NYT treatment group. With regard to our control variables (i.e., Saturday, Sunday), we find negative and significant effects in both the NYT and the LAT sample, which suggests that users share less news content on weekends. In addition to the estimation of the NYT main effects in column (3) and DID coefficient in column (4), we conducted a split sample analyses of original tweet (column 1)\n",
              "\n",
              "and retweet only (column 2) samples.21 Interestingly, the absolute value of the negative coefficient of paywall (β1 = -0.266, p < 0.01) in column (2) is larger, whereas the absolute value of the negative coefficient of paywall in the original tweet sample in column (1) is significantly lower than the coefficients of the full sample that combines both tweets and retweets. Given that retweeting NYT links shared by other NYT readers is not directly affected by the\n",
              "We thank an anonymous reviewer for suggesting this split sample analysis.\n",
              "21\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "43\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Figure 2. Interaction Between NYT Paywall and NYT’s Versus LAT’s WOM\n",
              "\n",
              "paywall, the results indicate that the decrease in the volume of retweet WOM may be associated with the change in the proportion of popular articles being tweeted, which supports our argument that the paywall may affect the virality of the content behind a paywall thus exacerbating the negative effect of changes in WOM distribution. Distribution of WOM: DDD Analysis To test whether the introduction of a paywall leads to a longer tail of WOM distribution (i.e., the volume of WOM for popular content will drop more than the volume of WOM for niche content), we employ two long-tail measures. First, we calculated the Gini coefficients of the WOM distribution based on the Lorenz curves (Brynsolfsson et al. 2011). The results indicate that the Gini coefficient for WOM for NYT articles in the pre-paywall period (0.816) is greater than the coefficient in the post-paywall period (0.762). On the other hand, the Gini coefficients of LAT do not exhibit differences before or after NYT’s paywall rollout (0.653 and 0.630, respectively). This suggests that the WOM for popular articles has decreased in the case of NYT after the paywall. Second, since the Gini coefficient analysis does not allow tests to assess whether these differences are statistically significant or not, we employ a log-linear regression specification to examine how a paywall reshapes the WOM distribution with respect to content popularity (Brynjolfsson et al. 2011; Brynjolfsson et al. 2003). Specifically, we fit WOM and WOM Rank of Content to log-linear regressions that model the power-law distributions of WOM at the content level. Hence, all non-categorical variables in our long tail analyses are log transformed, thereby allowing us to identify percentage changes in effect. Then, we create a Paywall dummy (defined as 0 if an observation is in the pre-paywall period, otherwise 1) and interact it with ln(WOM Rank of Content). Finally, in line with our prior DID formulation, our model incorporates LAT as a control group. Paywalli × ln(WOM Rank of Contenti) × NYTi is the three way interaction between\n",
              "\n",
              "the dummies of the treatment and control groups and content popularity rank, an indicator of changes in the WOM for popular and niche content. The difference-in-difference-indifferences (DDD) model specification for content i is\n",
              "ln(WOM i ) = β0 + β1 ln(WOM Rank of Contenti ) + β2 Paywalli + β3 NYTi + β4 Paywalli × ln(WOM Rank of Contenti ) + β5 Paywalli × NYTi + β6 NYTi × ln(WOM Rank of Contenti ) + β7 Paywalli × ln(WOM Rank of Contenti ) × NYTi + εi\n",
              "\n",
              "(3)\n",
              "\n",
              "Columns (1) and (2) in Table 7 present the results of the Pareto curve estimation. We focus on the results of the DDD model in Column (2). The β7 coefficient on the three-way interaction of the paywall, NYT and WOM rank terms is statistically significant. The interaction plots in Figure 3 illustrate the two-way interactions between paywall and WOM rank. As shown in Figure 3a, the absolute value of the slope coefficient for NYT decreases after the paywall implementation. The results suggest that the paywall rollout leads to a shift of NYT’s WOM distribution such that the decrease in WOM for articles with higher popularity (i.e., lower WOM rank) is much larger than the decrease in WOM for articles with lower popularity (i.e., higher WOM rank). It is noteworthy that the slope coefficient for LAT does not exhibit any significant changes before and after NYT’s paywall rollout, suggesting that the shift of NYT’s content sharing distribution is not caused by extraneous factors such as time trends. Therefore, we conclude that NYT’s content sharing distribution becomes less concentrated (a longer tail) after the paywall rollout. Potential Mechanisms for Long-Tail Effect Although our paper focuses on empirically examining the net impact of paywall on WOM pattern and does not isolate the impact of various alternative mechanisms that may lead to this net effect, given the data availability, we are able to perform\n",
              "\n",
              "44\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 7. WOM Pattern: Difference-in-Difference-in-Differences Analysis\n",
              "Dependent Variable Paywalli NYTi Paywalli × NYTi ln(WOM Rank of Contenti) NYTi × ln(WOM Rank of Contenti) Paywalli × ln(WOM Rank of Contenti) Paywalli × NYTi × ln(WOM Rank of Contenti) Saturdayi Sundayi Constant No. of observations R-squared -0.0380*** (0.0136) -0.0575*** (0.0144) 16.50*** (0.0566) 21,182 0.855 -0.0707*** (0.00919) -1.586*** (0.00627) (1) ln(NYT WOM) 0.702*** (0.0825) (2) ln(WOM) 0.0636*** (0.00729) 1.827*** (0.0117) 0.0834*** (0.0167) -1.443*** (0.00476) -0.475*** (0.0105) -0.0512*** (0.00690) -0.0649*** (0.0151) -0.0301*** (0.0107) -0.0512*** (0.0110) 2.204*** (0.00543) 30,312 0.866\n",
              "\n",
              "Notes: Because of a concern regarding multicollinearity, ln(Content Popularity) and NYT are mean-centered in Model 4. After such standardization, the VIF scores for all variables are lower than 2.7, and the mean VIF score is 1.94. Standard errors in parentheses. ***p < 0.01, **p < 0.05, *p < 0.1.\n",
              "\n",
              "(a) New York Times\n",
              "\n",
              "(b) Los Angeles Times\n",
              "\n",
              "Figure 3. Interaction Between NYT Paywall and Popularity Rank of Content (in Terms of WOM)\n",
              "\n",
              "a post hoc analysis to test the plausibility of the assumptions of one of the mechanisms (i.e., the exposure theory-based explanation for the long tail effect). First, we test for the presence of inverse relationships between consumers’ content sharing intensity and the rank of the content (in terms of WOM popularity) shared by them. Then, we\n",
              "\n",
              "identify two distinct segments of users in terms of content sharing intensity and the rank of the content shared. Finally, we relate these user segments to the likelihood of reduced consumption (and attrition in extreme cases) after the paywall. Note that the observations in our sample are at the tweet message level, and the measures can be created by aggregating either at the content or user level as per the context of\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "45\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "the analysis. We incorporate variables based on both kinds of aggregation in this analysis. Based on WOM Rank of Contenti, we create a user’s Average Rank of Content Shared at the user level. Average Rank of Content Sharedj is created by aggregating all of the articles shared by a user and then taking an average of the WOM Rank of Content measure for each article. The metric captures a user’s tendency to consume and share popular versus niche content. A lower mean reflects a user’s preference for sharing popular content. Then, we create User Rank for user j by sorting users based on the count of the total articles shared by each user in descending order (i.e., the highest sharer gets the lowest rank). As per the exposure theory-based conjecture, the paywall treatment effect is caused by reduced WOM behavior (or, in extreme cases, attrition) of a certain user segment that is more likely to consume popular content. In order to take into account the differential impact of a paywall with respect to user heterogeneity, we conduct a latent user segment analysis using finite mixture modeling. The purpose of our latent segment analysis is to identify latent user segments in terms of content sharing level and preference for content popularity.22 These segments can then be compared for difference in content consumption as well as attrition likelihoods after the paywall is introduced. The finite mixture of a normal regression model captures the differences between light and heavy user segments in the consumption of head and tail content in the content sharing distribution. The S-segment finite mixture model is specified as follows:\n",
              "\n",
              "Table 8 reports the parameter estimates for the two-segment model.23 Based on the estimated coefficients and segment weights in Table 8, we conducted post-estimation to determine each subject’s segment membership. Tables 9 indicates that segment 1 is comprised of the light users who read less and are more likely to read popular content, whereas segment 2 is composed of the heavy users who read more and less popular (a mix of niche and popular) content. This finding supports the underlying assumption about the relationship between a user’s content consumption intensity and preference for popular versus niche content. Another assumption that exposure theory-based mechanism relies on is that light user segments (who can be argued to have low WTP) are more likely to reduce (or even discontinue, in extreme cases) their consumption of NYT content after the paywall implementation. Although a paired t-test analysis shows that NYT users reduce the volume of their reading/sharing after introduction of the paywall (pre-paywall mean: 4.70, post-paywall mean: 4.09, t = 11.51, p-value < 0.01), and while there is no statistically significant difference in LAT users’ reading/sharing volume (pre-paywall mean: 4.60, post-paywall mean: 4.52, t = 1.19, p-value = 0.23), it is difficult to cleanly attribute any reduction in consumption by the light user segment to the paywall due to the nature of the data. Specifically, the result are likely to be confounded due to the noisy measurement of variable capturing the change in the content consumption by light users after the paywall as the light user segment has a more dispersed sharing pattern (due to their low overall sharing) and therefore any change in their behavior may not be captured accurately within a short time period. We acknowledge the data limitation and concern about the reliability of the inferences based on such data; however, given that the purpose of this post hoc test is not to establish causality but to provide preliminary support for the plausibility of the assumptions underlying one of the proposed alternative mechanisms, we conduct an analysis to examine if these two segments differ in terms of their content consumption (i.e., reading/sharing) after the paywall is introduced. To reduce the impact of noise in the data, we create a categorical dependent variable, rather than continuous variable, to measure the reduction in consumption. This variable, referred to as ReducedConsumptionj , is created by comparing users in the pre- and post-paywall samples and is coded as 1 if a user shared content before the paywall rollout and did not share content after the paywall rollout. For those users who shared content both before and after the paywall rollout, the ReducedConsumptionj variable is coded as 0. Then we conduct a two-sample proportion test to examine which segment\n",
              "\n",
              "f y j | X ;θ1 ,θ2 . ,θ S ; π 1 , π 2 , , π S =\n",
              "\n",
              " k =1 π k f k ( y j | X ;θk ),0 < θS < 1 and  k =1 π k\n",
              "S S\n",
              "\n",
              "(\n",
              "\n",
              ")\n",
              "\n",
              "(4)\n",
              "\n",
              "where yj denotes average content popularity as a dependent variable, vector θk = (1, Xj(1), …, Xj(k)) contains all of the parameters of k predictor variables that could explain the user content dimensions, and πS is the mixing proportions (weights) of the predictor variables Xj. fk(y|X; θS) are the segment densities of the mixture. To select the number of segments, we computed model fit indices including the Bayesian Information Criteria (BIC) (see Table D1 in Appendix D) (McLachlan and Peel 2000). For the sake of brevity, we choose the two-segment solution.\n",
              "22 In order to reduce the computational burden of estimating a finite mixture model with large-scale data, we took a subsample approach and used 15% of our NYT data. In this data set, we identified that 23,310 (63.7%) users in the pre-paywall sample discontinued NYT content sharing after the paywall rollout.\n",
              "\n",
              "The estimation results for the three-segment model (reported in Appendix D) are qualitatively similar to the two-segment model.\n",
              "\n",
              "23\n",
              "\n",
              "46\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 8. Parameter Estimates of Finite Mixture Models\n",
              "Dependent Variable: ln(Average Rank of Content Sharedj) Segment 1 Intercept ln(User Rankj) Proportion 7.608 (.144)*** -0.652 (.015)*** 42.60% Segment 2 4.946 (.098)*** 0.016 (.011)*** 57.40%\n",
              "\n",
              "Standard errors are in parentheses. *p < 0.1; **p < 0.05; ***p < 0.01.\n",
              "\n",
              "Table 9. Post-Estimation of Finite Mixture Models\n",
              "Segment 1 Mean ln(Average Rank of Content Sharedj) ln(User Rankj) 1.670 (.008) 9.070 (.005) [95% Confidence Interval] [1.633, 1.687] [9.059, 9.081] Mean 5.166 (.007) 8.971 (.006) Segment 2 [95% Confidence Interval] [5.152, 5.181] [8.958, 8.984]\n",
              "\n",
              "Notes: Standard errors are in parentheses. Segment 1 and Segment 2 represent Light & Popular (read less and popular articles) and Heavy & Mix of Niche and Popular (read more and niche articles) user segments, respectively.\n",
              "\n",
              "Table 10. Two-Sample Test of Proportion\n",
              "Segment Light &Popular Heavy & Mix of Niche and Popular Mean 0.688 0.589 [95% of conf. interval] [0.681, 0.695] [0.583, 0.596] Test Statistic z = 19.4303***\n",
              "\n",
              "Notes: The attrition likelihood in Segment 1: Light & Popular (0.688) is significantly greater than the attribution likelihood in Segment 2: Heavy & Mix of Niche and Popular (0.589) at p < 0.01.\n",
              "\n",
              "Table11. Average Usage of Users Who Reduced Consumption After Paywall Versus Others (i.e., ReducedConsumption = 1 Versus ReducedConsumption = 0)\n",
              "Comparisons of Average Usage in Pre-Paywall Week 1* vs. Week 2* ReducedConsumption = 1 ReducedConsumption = 0\n",
              "* **\n",
              "\n",
              "Comparisons of Average Usage in Pre- Versus Post-Paywall Week 1* vs. Week 2** 2.59 19.32 Week 1* vs. Week 3** 2.73 19.81\n",
              "\n",
              "Week 1* vs. Week 3* 2.06 15.52\n",
              "\n",
              "2.09 15.76\n",
              "\n",
              "Refers to the pre-paywall period. Refers to the post-paywall period.\n",
              "\n",
              "of users is more likely to reduce or discontinue their content consumption. The results, presented in Table 10, show that users who have lower consumption levels and higher tendency to share popular content are more likely to have higher\n",
              "\n",
              "likelihood to reduce NYT content consumption after paywall implementation than users in the other segment, thus providing indirect support for the plausibility of the assumption based on the WTP argument.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "47\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "To provide additional support for this assumption, we also examine the descriptive data about weekly sharing pattern. While the total number of unique users decreases from 258,261 in the pre-paywall period to 179,062 in the postpaywall period, Table 11 shows that the average sharing by the users who reduced consumption after introduction of the paywall is significantly lower than the average sharing by other users. In other words, these descriptive statistics support that users classified as reducing consumption after introduction of the paywall are more likely to be light users. Finally, while we provide indirect support for the assumptions underlying the exposure theory-based mechanism, due to lack of data, we cannot rule out the search theory-based mechanism identified in the theory section. Follow-up research may employ survey methodology to measure the search cost and the presence of strategic behavior among the users to find support for or against the search theory-based mechanism.\n",
              "\n",
              "additional benefits for interpretation of the coefficients. We use The Washington Post’s daily website visits as a rival newspaper’s website traffic. Retweet ratiot represents the proportion of retweets among daily gross tweets messages. ln(Rival website visitst) and Retweet ratiot are the excluded exogenous variables that help us estimate the equations. The dummy variables, Saturdayt and Sundayt, are included in all equations to control for variations in the content and user reading behavior during weekends. Table 12 presents the estimation results for the 3SLS model. We find that the coefficients of ln(WOMt) (α1 = 0.3724; p < 0.01) and ln(WOMt-1) (α2 = 3.80e-06; p < 0.1) are positive and significant in the website traffic equation, suggesting that there is a positive relationship between the volume of online WOM (i.e., social media buzz) and website traffic. We now turn to an examination of the impact of a paywall on such interdependence. The results also show that both Paywall (α4 = !0.0807; p < 0.01) and the interaction of Paywall and ln(WOMt) (α5 = !0.129; p < 0.1) are negative and statistically significant. We plot the interaction between paywall and WOM volume. Figure 4 shows that the magnitude of WOM’s effect on website traffic generation decreases after NYT’s paywall, suggesting that the association of the volume of online WOM (i.e., social media buzz) with website traffic is weakened after a paywall introduction. The results provide evidence that social WOM is a significant predictor of website traffic; however, a firm’s information pricing changes these dynamics. We now turn to a discussion of the effect of a paywall on the counterfactual in our control group (i.e., LAT), which we used to alleviate any confounding factors associated with the timing of the NYT paywall implementation. As expected, our results for the same analysis using the LAT sample show that the interaction term between paywall and WOM volume has no significant effect on LAT’s website traffic. Furthermore, consistent with our main results from the NYT sample, the volume of WOM on Twitter has a positive and significant effect on website traffic generation (α1 = 0.589; p < 0.01), providing additional empirical support for our prior result that online WOM volume is positively related to website traffic. Economic Significance We examine the economic impact of WOM on website traffic and the impact of a paywall on this relationship using their marginal effects. As reported in Table 3, the average Website visits is 1,709,428 in the pre-paywall period. A one-standard deviation increase in WOM volume in the previous period increases the volume of website traffic in the current period by 3.4 percent when evaluated at the mean before paywall\n",
              "\n",
              "The Impact of Paywall on WOM Effectiveness\n",
              "Simultaneous Model of Paywall, WOM Diffusion, and Website Traffic In the second part of our analysis, we examine the relationship between the volume of daily social WOM and daily website traffic and the dynamics of this relationship after a paywall introduction. Given the interdependence and endogenous relationships between WOM volume and website traffic, we develop the following three-stage least-square (3SLS) model at the site level: one equation with daily website visits as the dependent variable (the website traffic equation) and one with daily WOM volume as the dependent variable (the WOM equation).\n",
              "ln(Website visitst ) = α0 + α1 ln(WOM t ) + α2 ln(WOM t −1 ) + ln(WOM t ) + α6 ln( Rival website visitst ) + α7 Saturdayt +\n",
              "\n",
              "α3 ln(Website visitst −1 ) + α4 Paywallt + α5 Paywallt × α7 Sundayt + ut\n",
              "ln(WOM t ) = β0 + β1 ln(Website visitst ) +\n",
              "\n",
              "(5)\n",
              "\n",
              "β2 ln(Website visitst −1 ) + β3 ln(WOM t −1 ) + β4 Paywallt + β5 Retweet ratiot + β6 Saturdayt + β7 Sundayt + vt\n",
              "\n",
              "(6)\n",
              "\n",
              "For the website traffic equation, ln(Website visitst) denotes the daily gross website traffic at day t, and its one-day lagged variable is defined as ln(Website visitst-1). Similarly, ln(WOMt) represents the total number of tweets that contain a NYT link at day t. The log-linear transformation provides\n",
              "\n",
              "48\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Table 12. Estimation Results for WOM and Website Traffic (3SLS Model)\n",
              "NYT Dependent Variable ln(WOMt) ln(WOMt-1) ln(Website visitst-1) Paywallt Paywallt × ln(WOMt) ln(Rival website visitst) Saturdayt Sundayt Constant R-squared Dependent Variable ln(Website visitst) ln(Website visitst-1) ln(WOMt-1) Paywallt Retweet ratiot Saturdayt Sundayt Constant R-squared ln(Website visitst) 0.372*** (0.0703) 3.80e-06* (0.000) 0.0867 (0.135) -0.0807*** (0.0223) -0.129* (0.0785) 0.108* (0.0621) 0.0320 (0.0339) 0.0732** (0.0326) 11.59*** (1.969) 0.868 ln(WOMt) -2.352 (3.974) 0.143 (0.558) 1.58e-05 (0.000) -0.224 (0.326) 6.136 (4.890) -0.681 (0.470) -0.537** (0.268) 29.69 (50.50) 0.573 LAT ln(Website visitst) 0.589*** (0.218) 9.65e-06 (0.000) 1.20e-06*** (0.000) -0.0981*** (0.0352) -0.168 (0.108) 0.0680 (0.0878) 0.112 (0.113) 0.150 (0.0960) 11.28*** (1.190) 0.835 ln(WOMt) -1.256 (4.464) .45e-06 (0.000) 6.06e-05 (0.000) 0.178* (0.106) 3.835 (6.685) -0.667 (0.687) -0.526 (0.401) 13.32 (51.14) 0.058\n",
              "\n",
              "Notes. To avoid a concern of multicollinearity (i.e., the collinearity between the main effect of paywall and the interaction effect of paywall and WOM volume), ln(WOMt) is mean-centered for all the relevant variables. Standard errors in parentheses. ***p < 0.01, **p < 0.05, *p < 0.1.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "49\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Figure 4. Interaction Between NYT Paywall and Website Traffic (3SLS)\n",
              "\n",
              "rollout. However, after NYT’s paywall implementation, the same one-standard deviation increase in WOM volume in the previous period increase of website traffic in the current period by only 2.3 percent. Robustness Tests To address any concern about the validity of the time series instrument and the order of lagged variables, we conduct the subsample analysis using a vector autoregressive model with exogenous variables (VARX) and compare the contribution of WOM on website traffic generation before and after NYT’s paywall rollout. VARX is well suited to capture the dynamic interactions between WOM and website traffic over time (Luo et al. 2013). We find that the optimal lag length of the VARX model is 1, according to the Schwartz’s Bayesian Information Criterion (SBIC) and Akaike Information Criterion (AIC). Similar to 3SLS results, the results for VARX show that there is a positive and statistically significant relationship between WOM volume and website traffic before NYT’s paywall rollout; however, this interdependence becomes insignificant after the paywall rollout. The detailed model specification and results are reported in Appendix E. Potential Mechanisms for Impact of Paywall on WOM Effectiveness In the theory section, we identified three potential mechanisms (i.e., bypass, virality, and user resentment) that may affect WOM effectiveness after paywall. Given the results that show a decrease in the strength of the positive relationship between social media WOM and website traffic after a paywall implementation, we can conclude that the role of the bypass mechanism (and other such mechanisms), which should theoretically have a positive impact on WOM effec-\n",
              "\n",
              "tiveness is either nonexistent or relatively weaker than the impact of mechanisms such as virality effect and user resentment effect, which have a negative impact on WOM effectiveness. We indirectly find support for the virality effect as a plausible mechanism affecting WOM effectiveness. First, the results are consistent with the prediction of virality mechanism. Second, we find support for the assumptions of the mechanism that require disproportionate decrease in WOM about popular content, after the paywall, as our results suggest that the paywall creates a long tail of content distribution such that popular content sharing drops significantly. Further, as discussed in the “Volume of WOM: Differences-in-Differences Approach” section, the split sample analyses of original tweet and retweet only samples (Table 6) show lowered retweeting behavior, indirectly supporting the reduced effectiveness of WOM after introduction of the paywall. Finally, recall we suggested that a proportion of NYT readers may boycott the NYT and “stop clicking on NYT links in social media” due to negative emotional response, such as resentment toward the paywall. If this were the case, we would expect the lowered effectiveness of NYT WOM after introduction of the paywall. In the present study, we cannot directly either support or rule out the resentment-based explanation for the impact of the paywall on NYT WOM effectiveness because we did not explicitly measure user resentment in our study. In summary, we find support for the virality mechanism; however, our paper cannot isolate the relative impact of virality versus resentment mechanism on the relationship between social media WOM and website traffic. As mentioned earlier, this calls for follow-up research examining the plausibility of the resentment mechanism.\n",
              "\n",
              "50\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Discussion\n",
              "Using NYT’s paywall rollout as a natural experiment, our study finds that the introduction of the paywall influences the pattern and effectiveness of WOM on Twitter. Analyzing the pattern of WOM, we find that the volume of WOM for NYT’s content drops significantly after the paywall is introduced, but the paywall (i.e., charging for content that was earlier available for free) has a disproportionate impact on volume of WOM for content that differs in terms of content popularity, creating a longer tail in the WOM (i.e., content sharing distribution). Specifically, the volume of WOM about popular content decreases more significantly than the volume of WOM about niche content. Although we identify multiple mechanisms that may explain the long-tail effect due to the paywall, given the data availability, we are able to test the assumptions underlying only one of the mechanisms. Specifically, we examine the plausibility of the exposure theory-based mechanism by conducting a latent segment analysis. The results show that because of low WTP for content, light users consuming popular content have higher likelihoods of reduced WOM behavior than heavy users consuming a mix of niche and popular articles, thus reshaping the WOM distribution. The results suggest that the exposure theory-based mechanism is one of the plausible mechanisms that create the long tail effect after the paywall is introduced. To ensure that these results are not affected by extraneous confounding factors, we employed a difference-in-difference approach by comparing the WOM pattern for another newspaper (i.e., LAT) before and after NYT implemented its paywall. The absence of any significant change in the content pattern for LAT provides support for the impact of a paywall on the long-tail distribution of the WOM for NYT content within the Twitter platform. Given that we use the secondary data of WOM on Twitter (i.e., content sharing), we conducted an online user survey to support our use of content sharing as a proxy for actual content consumption and find a positive and significant correlation between reading and sharing on Twitter. The long-tail results combined with the online survey help us understand how the introduction of a paywall affects the content demand patterns among news articles. We chose Twitter as our empirical context because of the feasibility of collecting population-sized news article link sharing. However, we also find a positive and significant correlation between reading and sharing on Facebook, supporting the fact that news article sharing behavior on Twitter would be qualitatively similar to content sharing behavior on Facebook or other social media platforms.\n",
              "\n",
              "Further, our analysis, examining the impact of a paywall on the effectiveness of online WOM in increasing the newspaper’s website traffic, shows that paywalls have a negative impact on WOM effectiveness. First, our findings show that WOM within Twitter leads to website traffic. The role of WOM on website traffic generation is a long-time speculation. However, our study is one of few that empirically demonstrate this relationship. Further, we find that this positive relationship between WOM volume and website traffic weakens significantly after the introduction of NYT’s paywall, whereas the strength of the relationship between WOM and website traffic for LAT, which served as the counterfactual, did not vary during the same time period. These results show that a paywall negatively moderates the relationship between WOM and website traffic, providing implications for online advertising revenue. The findings of this study offer novel and important implications for the literature on strategic use of social media and paywall.\n",
              "\n",
              "Theoretical Implications\n",
              "To the best of our knowledge, this is the first study that examines how a firm’s decision about a paywall implementation affects the distributional pattern of WOM about information content (such as news) consumption and its further impact on WOM effectiveness. This paper makes four important theoretical contributions. First, our study informs the information goods pricing literature. Specifically, we argue and empirically validate that shifting from free to for-a-fee pricing may weaken the impact of a WOM promotion strategy. We complement the extant research in the information goods pricing literature by proposing and empirically showing the significant interplay between a paywall strategy and its WOM pattern as well as WOM effectiveness. Prior work on information pricing has primarily examined, mostly analytically, the interaction between information pricing and product strategies (Bakos and Brynjolfsson 1999; Chen and Seshadri 2007; Choudhary 2010; Wu et al. 2008). We conduct an empirical study that shows the impact of a paywall on WOM pattern, which is part of a firm’s promotion strategy and has implications for the firm’s product strategy. Another implication of these results is that researchers developing analytical models to understand alternative pricing strategies for information goods need to take into account the impact of such pricing on the online WOM, which in turn determines the potential demand for content and revenue. Incorporating the effects of different\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "51\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "pricing policies on word-of-mouth pattern and effectiveness in analytical models will provide a more realistic assessment of the impact of pricing policies on firm performance. Second, our study contributes to the WOM literature by examining both the pattern and effectiveness of WOM via social media. Given that the WOM dynamics can be affected by firms’ strategies in other domains such as information pricing, researchers studying WOM pattern, diffusion, and effectiveness should pay attention to such forces by identifying and controlling them in their empirical studies. Our study also extends the WOM literature by highlighting that the distribution of WOM for hit/niche content may change as a result of a change in paywall strategy. Further, complementing the studies that measure the impact of online WOM on various measures related to financial performance such as product adoption and sales (Chevalier and Mayzlin, 2006; Godes and Maylin, 2004; Liu 2006), we suggest that WOM has a positive impact on the newspaper’s website traffic, which is a critical factor influencing financial performance. We also extend this research stream by highlighting the role of a paywall strategy in moderating the impact of WOM on performance outcome (i.e., website traffic). Third, we contribute to the theory of exposure and variety seeking. The results from the finite mixture model analysis provide indirect empirical support for the notion that those users who have high consumption levels seek more variety and are more likely to consume products that are less popular vis-à-vis those users who have low consumption levels. Fourth, our study extends the literature examining various aspects of long-tail phenomenon, especially in the online context. Prior literature has focused on the extent of the long tail for physical products (Brynjolfsson et al. 2010) and studied the impact of information technology on the long tail in the distribution of product sales (Brynjolfsson et al. 2011; Brynjolfsson et al. 2003). However, an important but underinvestigated topic is the long tail effect in the context of information goods (digital content) where Internet and digitization technologies tend to have significant impacts on consumers’ consumption patterns (e.g., subscription-based online music services, Elberse 2008). Furthermore, to our knowledge, little research has examined non-technology drivers, such as firm strategies, on the long tail consumption patterns of information. Our study fills this research gap by examining the impact of a paywall on the long tail in the distribution of online WOM for information goods such as newspaper articles. This study is in line with recent literature on the online long tail effect that examines, for instance, the impact of social media buzz on music consumption patterns (Dewan and Ramaprasad 2012) and the role of product popularity on consumer choice and sales distribution (Elberse\n",
              "\n",
              "2008). Our work enriches this stream of research by highlighting that a firm’s introduction of a paywall can be a demand side driver of long-tail outcomes (Brynjolfsson et al. 2010).\n",
              "\n",
              "Managerial Implications\n",
              "Our findings not only have the potential to enrich the stream of research on strategic use of social media, but also provide managerial implications for firms’ product and promotion strategies. This study provides several managerial implications for content providers in general and online newspaper publishers in particular. First, our results suggest that a content publisher that has implemented a paywall should be aware of its impact on changes in content consumption and sharing distribution. A firm should not only look at the direct cost–benefits of different pricing alternatives but also examine the indirect effects of pricing on revenue through the changes in the pattern and effectiveness of WOM. Second, our results provide support to practitioners’ belief that online WOM plays an important role in the firm’s performance. Through a set of rigorous empirical tests, we show that online WOM does matter in terms of increasing a firm’s website traffic. We also find support for common intuition that moving from a free to a for-a-fee pricing model will lead to a decrease in WOM as well as website traffic (due to a drop in the number of existing customers). Of greater importance, we also suggest that, in spite of a company’s strategy to encourage WOM (e.g., excluding the content accessed through WOM from the NYT’s paywall), the introduction of a paywall may not only lower the WOM but also weaken its power to bring more website traffic. These results suggest that companies need to measure and manage the impact of a firm’s strategy on WOM. To achieve this objective, a company may need to invest in information systems that capture, monitor, and analyze the clickstream data from their webservers as well as WOM data from social media platforms such as Twitter. Further, managers may need to actively manage the WOM dynamics, especially during periods of shift in firm strategies. Third, a content provider should note the implications of our long-tail results (i.e., a paywall is more likely to reduce popular content consumption/sharing) in developing its product and promotion strategies. After paywall implementation by a firm, social transmission of its content may become less viral, thus lowering the effectiveness of WOM in generating website traffic. This negative influence of a paywall on the firm’s website traffic in turn lowers the online advertising revenue, which is a function of website traffic.\n",
              "\n",
              "52\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "The observation that the introduction of a paywall might shift the distribution in the consumption and sharing of information content implies that a company’s product strategy may also need to be reexamined when implementing a new paywall strategy. Specifically, our results suggest that a shift from free to for-a-fee pricing should be followed by a concurrent shift in the company’s product assortment strategy such that the company focuses more on producing and promoting niche content vis-à-vis popular content. However, if the company considers it important to have popular content in its product mix for strategic reasons (e.g., NYT may want to ensure its brand image as an opinion leader for both popular as well as niche content), then the company may experiment with unbundling its content and keep the popular content free while charging for niche content. For example, a content provider may offer “most e-mailed articles” for free to increase website traffic generated through social media, while gaining subscription revenue from loyal consumers who consume more niche content. Such a pricing strategy would lower the detrimental impact of pricing on WOM for popular content and help mitigate a concern regarding the sustainability of NYT’s image as an opinion leader and an influential mainstream newspaper even after the paywall implementation. Finally, our long-tail results show that if a company starts charging fees for its information content, product variety and niche content becomes more important. Companies that provide mostly popular information content need to be cautious in their content pricing decision because the negative influence of a paywall on the firm’s website traffic can significantly lower online advertising revenue.\n",
              "\n",
              "Second, caution should be taken in generalizing our findings because of the fact that NYT was among the first to implement a paywall. It is possible that consumers may become accustomed to paid subscription models over time. Thus, the behavior of consumers in response to similar price strategies by late movers may be different, leading to different WOM dynamics. However, we believe that this first mover effect can only impact the magnitude of our findings. Based on our theory, if consumers become more comfortable with the idea of paying for information goods, the proportion of light users that drop after the paywall is introduced will be smaller and consequently the drop in volume of WOM for popular content will also be relatively low, weakening the long-tail pattern of WOM that we observed in our NYT data. In such a case, we expect that our theoretical explanation of user preference for product variety and the resulting long tail-pattern would hold, although the magnitude of effects will be smaller. Future research on later adoption of a paywall would provide more salient implications for firms considering a paywall strategy. Third, although our empirical study is based on news content, which is a type of information good, we believe that the underlying theory and observed results would hold for other information goods as well. However, the magnitude of the effect and importance as well as the relevance of online WOM in social media may differ across different types of information goods such as music, movies, software, etc. It may be a fruitful avenue for future research to replicate this study for different information goods. Finally, in order to make fair comparisons of WOM volume among the articles that have longer (less) duration and thus have greater (lower) chances to be shared, we used 1-day diffusion window in our analysis. Given the fast diffusion of news articles on Twitter (Goel et al. 2012, Kwark et al. 2010), this measure would be representative. However, an examination of whether there is disturbance to the natural decay in WOM for NYT compared to the counterfactual of the LAT control group would provide interesting additional insights. Therefore, future research could develop a method to estimate a decay function of the shelf life of a news article to compare WOM dynamics for old and current news articles (e.g., drivers of immediate and ongoing WOM, Berger and Schwartz 2011).\n",
              "\n",
              "Limitations and Suggestions for Future Research\n",
              "Given the nature of empirical research, this study also has a few limitations that could be addressed in future research. First, NYT’s paywall, used as the treatment of our natural experiment, was implemented as a single pricing strategy. As such, depending on the design of information pricing policies (e.g., NYT’s single pricing strategy versus online The Wall Street Journal’s online content-based pricing), the magnitude of the paywall effect may vary; however, the implications of the paywall adoption on WOM diffusion patterns would provide insights to understand different pricing schemes of digital goods. Given that NYT and other content providers are currently introducing new pricing strategies (e.g., plans for iPhone and iPad apps), future research could provide interesting insights by comparing our results with the impact of such changes.\n",
              "\n",
              "Conclusion\n",
              "New business models enabled by emerging technologies are driving traditional companies, especially those dealing with information goods, to rethink their pricing strategies. At the same time, exponential growth in social media facilitated by\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "53\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "information technology makes it critical for companies to manage online WOM as part of their promotion strategy. Our results suggest that information pricing and online promotion strategy should not be decided in isolation. The emergence of social media and online WOM has provided opportunities to access rich data sets to address a significant set of behaviorally and managerially relevant questions. In this study, we leverage WOM data collected from social media platforms and complement it with aggregated clickthough data to understand how a firm’s shift in information pricing, such as introduction of a paywall, affects its WOM dynamics. We hope that our work will contribute to and promote future research that enhances our understanding of consumer behavior and digital markets regarding paid content.\n",
              "\n",
              "Acknowledgments\n",
              "The authors would like to thank the senior editor, the associate editor, and the anonymous reviewers for their constructive suggestions and comments on earlier versions of this manuscript. This research was partially funded by Fonds de Recherchesur la Société et la Culture (FQRSC) du Québec and the Social Sciences and Humanities Research Council of Canada.\n",
              "\n",
              "References\n",
              "Anderson, C. 2006. The Long Tail: Why the Future of Business Is Selling Less of More, New York: Hyperion. Anderson, C. 2009. Free: The Future of a Radical Price: The Economics of Abundance and Why Zero Pricing Is Changing the Face of Business, New York: Random House. Bakos, Y., and Brynjolfsson, E. 1999. “Bundling Information Goods: Pricing, Profits, and Efficiency,” Management Science (45:12), pp. 1613-1630. Bapna, R., and Umyarov, A. 2015. “Do Your Online Friends Make You Pay? A Randomized Field Experiment in an Online Music Social Network,” Management Science (61:8), pp. 1902-1920. Berger, J., and Milkman, K. L. 2012. “What Makes Online Content Viral?,” Journal of Marketing Research (49:2), pp. 192-205. Berger,J., and Schwartz, E. 2011. “What Drives Immediate and Ongoing Word-of-Mouth?,” Journal of Marketing Research (48:5), pp. 869-880. Bertrand, M., Duflo, E., and Mullainathan, S. 2004. “How Much Should We Trust Difference-in-Differences Estimates?,” Quarterly Journal of Economics (119:1), pp. 249-275. Bone, P. F. 1995. “Word-of-Mouth Effects on Short-Term and Long-Term Product Judgments,” Journal of Business Research (32:3), pp. 213-223. Brynjolfsson, E., Hu, Y. J., and Simester, D. 2011. “Goodbye Pareto Principle, Hello Long Tail: The Effect of Search Costs on the Concentration of Product Sales,” Management Science (57:8), pp. 1373-1386.\n",
              "\n",
              "Brynjolfsson, E., Hu, Y. J., and Smith, M. D. 2003. “Consumer Surplus in the Digital Economy: Estimating the Value of Increased Product Variety at Online Booksellers,” Management Science (49:11), pp. 1580-1596. Brynjolfsson, E., Hu, Y. J., and Smith, M. D. 2010. “Research Commentary—Long Tails vs. Superstars: The Effect of Information Technology on Product Variety and Sales Concentration Patterns,” Information Systems Research (21:4), pp. 736-747. Campbell, C. C. 1999. “Perceptions of Price Unfairness: Antecedents and Consequences,” Journal of Marketing Research (36:2), pp. 187-199. Chellappa, R. K., and Shivendu, S. 2005. “Managing Piracy: Pricing and Sampling Strategies for Digital Experience Goods in Vertically Segmented Markets,” Information Systems Research (16:4), pp. 400-417. Chen, Y.-J., and Seshadri, S. 2007. “Product Development and Pricing Strategy for Information Goods under Heterogeneous Outside Opportunities,”Information Systems Research (18:2), pp. 150-172. Chevalier, J. A., and Mayzlin, D. 2006. “The Effect of Word of Mouth on Sales: Online Book Reviews,” Journal of Marketing Research (43), pp. 345-54. Choudhary, V. 2010. “Use of Pricing Schemes for Differentiating Information Goods,” Information Systems Research (21:1), pp. 78-92. Chyi, H. I. 2005. “Willingness to Pay for Online News: An Empirical Study on the Viability of the Subscription Model,” Journal of Media Economics (18:2), pp. 131-142. Danaher, P. J. 2002. “Optimal Picing of New Subscription Services: Analysis of a Market Experiment,” Marketing Science (21:2), pp. 119-138. DeGusta, M. 2011. “Digital Subscription Prices Visualized,” The Understatement (available at http://theunderstatement.com/post/ 4019228737/digital-subscription-prices-visualized-aka-the-new). Dellarocas, C., Gao, G., and Narayan, R. 2010. “Are Consumers More Likely to Contribute Online Reviews for Hit or Niche Products?,” Journal of Management Information Systems (27:2), pp. 127-158. Dewan, R. M., Freimer, M. L., and Zhang, J. 2003. “Management and Valuation of Advertisement-supported Web Sites,” Journal of Management Information Systems (19:3), pp. 87-98. Dewan, S., and Ramaprasad, J. 2012. “Research Note—Music Blogging, Online Sampling, and the Long Tail,” Information Systems Research(23:3-Part-2), pp. 1056-1067. Duan, W., Gu, B., and Whinston, A. B. 2008. “Do Online Reviews Matter? An Empirical Investigation of Panel Data,” Decision Support Systems (45:4), pp. 1007-1016. Elberse, A. 2008. “Should You Invest in the Long Tail?,” Harvard Business Review (86:7/8), p 88. Garg, R., and Telang, R. 2013. “Inferring App Demand from Publicly Available Data,” MIS Quarterly (37:4), pp. 1253-1264. Gelman, A., and Hill, J. 2007. Data Analysis Using Regression and Multilevel/HierarchicalModels, New York: Cambridge University Press. Godes, D., and Mayzlin, D. 2004. “Using Online Conversations to Study Word-of-Mouth Communication,” Marketing Science (23:4), pp. 545-560.\n",
              "\n",
              "54\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Goel, S., Watts, D., and Goldstein, D. G. 2012. “The Structure of Online Diffusion Networks,” in Proceedings of the 13th ACM Conference on Electronic Commerce, Valencia, Spain, June 4-8, pp. 623-638. Hu, N., Pavlou, P. A., and Zhang, J. 2007. “Why Do Online Product Reviews Have a J-Shaped Distribution? Overcoming Biases in Online Word-of-Mouth Communication,” available at SSRN (http://ssrn.com/abstract=2380298). Khouja, M., and Park, S. 2007. “Optimal Pricing of Digital Experience Goods under Piracy,” Journal of Management Information Systems (24:3), pp. 109-141. Kumar, V., Anand, B., Gupta, S., Oberholzer-Gee, F. 2012. “The New York Times Paywall,” Harvard Business School Case 512-077, February 2012 (revised January 2013). Kwak, H., Lee, C., Park, H., Moon, S. 2010. “What Is Twitter, a Social Network or a News Media?,” in Proceedings of the 19th International Conference on World Wide Web, Raleigh, NC, pp. 591-600. Ives, N. 2011. “New York Times’ Share of Newspaper Sites’ Traffic Hits 12-Month Low,” Advertising Age, May 11 (available at http://adage.com/article/media/ny-times-sharenewspaper-traffic-hits-12-month-low/227495/). Lee, E. 2012. “The New York Times Paywall Is Working Better Than Anyone Had Guessed,” Bloomberg Tech Blog, December 20 (available at http://go.bloomberg.com/tech-blog/2012-12-20the-new-york-times-paywall-is-working-better-than-anyonehad-guessed/). Li, X., and Hitt, L. M. 2008. “Self-Selection and Information Role of Online Product Reviews,” Information Systems Research (19:4), pp. 456-474. Liu, Y. 2006. “Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue,” Journal of Marketing (70), pp. 74-89. Luo, X., Zhang, J., and Duan, W. 2013. “Social Media and Firm Equity Value,” Information Systems Research (24:1), pp. 146-163. McAthy, R. 2013. “Two Years In: Reflections on the New York Times Paywall,” Journalism.co.uk, March 28 (available at http://www.journalism.co.uk/news/two-years-of-the-new-yorktimes-paywall/s2/a552534/). McLachlan, G., and Peel, D. 2004. Finite Mixture Models, New York: Wiley. McPhee, W. N. 1963. Formal Theories of Mass Behavior, Glencoe, NY: Free Press of Glencoe. Myers, S. 2012. “Latest Numbers Indicate New York Times Traffic Is Flat Since Paywall,” Poynter@40, January 25 (available at http://www.poynter.org/latest-news/mediawire/ 60780/ new-york-times-traffic-flat-since-paywall/). Oestreicher-Singer, G., and Zalmanson, G. 2013. “Content or Community? A Digital Business Strategy for Content Providers in the Social Age,” MIS Quarterly (37:2), pp. 591-616. Peters, J. 2011. “The Times Announces Digital Subscription Plan,” The New York Times, Media Section, March 17 (available at http://www.nytimes.com/2011/03/18/business/media/18times. html). Picard, R. G. 2000. “Changing Business Models of Online Content Services: Their Implications for Multimedia and Other Content\n",
              "\n",
              "Producers,” International Journal on Media Management (2:2), pp. 60-68. Regan, S. 2011. “The NYTimes.com Paywall Causes Traffic to Drop,” The Tech Herald, April 13 (available at http://www. thetechherald.com/articles/The-NYTimes-com-paywall-causestraffic-to-drop). Shapiro, C., and Varian, H. 1998. Information Rules, Boston: Harvard Business School Press. Shi, Z., Rui, H., and Whinston, A. B. 2013. “Content Sharing in a Social Broadcasting Environment: Evidence from Twitter,” MIS Quarterly (38:1), pp. 123-142. Simonson, I. 1990. “The Effect of Purchase Quantity and Timing on Variety-seeking Behavior,” Journal of Marketing Research (27:2), pp. 150-162. Singh, V. 2009. “Some Stats about Twitter’s Content,” Vik’s BLog, October 12 (available at http://zooie.wordpress.com/2009/ 10/12/some-stats-about-twitters-content/). Stephen, A., and Lehmann, D. 2012. “Using Incentives to Encourage Word-of-Mouth Transmissions that Lead to Fast Information Diffusion,” unpublished working paper, University of Pittsburgh. Sundararajan, A. 2004. “Nonlinear Pricing of Information Goods,” Management Science(50:12), pp. 1660-1673. Wu, S.-Y., Hitt, L. M., Chen, P.-Y., and Anandalingam, G. 2008. “Customized Bundle Pricing for Information Goods: A Nonlinear Mixed-Integer Programming Approach,” Management Science (54:3), pp. 608-622. Vanacore. 2010. “US Newspaper Circulation Falls 8.7 Percent,” Associated Press, April 26. Xia, L., Monroe, K. B., and Cox, J. L. 2004. “The Price Is Unfair! A Conceptual Framework of Price Fairness Perceptions,” Journal of Marketing (68:4), pp. 1-15. Yackanich, M. 2013. “Paywalls: It’s Time to Redefine Choice Online,” Innovation Insights, September 30 (available at http://insights.wired.com/profiles/blogs/paywalls-it-s-time-toredefine-choice-online#ixzz33JuG5XzW). Zentner, A., Smith, M., and Kaya, C. 2013. “How Video Rental Patterns Change as Consumers Move Online,” Management Science (59:11), pp. 2622-2634.\n",
              "\n",
              "About the Authors\n",
              "Hyelim Oh is an assistant professor of Information Systems at the School of Computing, National University of Singapore. Hyelim has a Ph.D. from McGill University. Her research interests include social media, crowdsourcing, open innovation, social networks, text mining, and econometric modeling. Her research has appeared in MIS Quarterly and won a Best Paper Award at the International Conference on Information Systems. AnimeshAnimesh is an associate professor in the Information Systems area of the Desautels Faculty of Management, McGill University. Animesh has a Ph.D. from the University of Maryland,\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "55\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "a Master’s in Information Systems Management from Carnegie Mellon University, and a Bachelor’s degree in Business Studies from Delhi University. He studies the adoption, design, and impact of Internet technologies and electronic commerce. His research has been published in top journals, including Information Systems Research, MIS Quarterly, and Marketing Science. Alain Pinsonneault, Fellow-Royal Society of Canada and Fellow of the Association for Information Systems, is a James McGill Professor and the Imasco Chair of information systems in the Desautels Faculty of Management at McGill University. His current\n",
              "\n",
              "research interests include the organizational and individual impacts of information technology, user adaptation, social networks, ERP implementation, e-health, e-integration, strategic alignment of IT, and the business value of IT. His research has appeared in numerous journals, including Management Science, MIS Quarterly, Information Systems Research, Journal of MIS, Decision Support Systems, and Organization Science. He recently completed a 10-year term on the editorial board of Organization Science. He currently serves or has served on the editorial boards of several other journals including MIS Quarterly, Information Systems Research, and Journal of MIS.\n",
              "\n",
              "56\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1/March 2016\n",
              "\n",
              "\n",
              "RESEARCH ARTICLE\n",
              "\n",
              "FREE VERSUS FOR-A-FEE: THE IMPACT OF A PAYWALL ON THE PATTERN AND EFFECTIVENESS OF WORD-OF-MOUTH VIA SOCIAL MEDIA\n",
              "Hyelim Oh\n",
              "School of Computing, National University of Singapore, 13 Computing Drive, Singapore 117417 SINGAPORE {hyelim.oh@nus.edu.sg}\n",
              "\n",
              "Animesh Animesh and Alain Pinsonneault\n",
              "Desautels Faculty of Management, McGill University, Montréal, Québec, H3A 1G5 CANADA {animesh.animesh@mcgill.ca} {alain.pinsonneault@mcgill.ca}\n",
              "\n",
              "Appendix A\n",
              "Summary of Underlying Theoretical Mechanisms\n",
              "A. Impact of Paywall on WOM Pattern Alternative Mechanisms A1. Based on WTP and Exposure Theory A1a Rationale/Arguments/ Assumptions Light user segments (who are likely to have low WTP) are more likely to reduce (or even discontinue in extreme cases) their consumption of NYT content after the paywall implementation. Light user segments are more likely to consume popular articles whereas the heavy user segment is more likely to consume a mix of niche articles and popular content. Light user segments (who are likely to have low WTP) are more likely to reduce (or even discontinue in extreme cases) their consumption of NYT content after the paywall implementation. Search cost for finding popular content is lower than the search cost for finding niche content. Connected Literature/Theory Utility theory — WTP (Danaher 2002) Resulting (Possible) Effect Juxtaposing A1a and A1b leads to long tail effect due to the disproportionate reduction of popular content consumption (as a results of reduction of content consumption by light users). Support for/Against A1a. Supported (see the descriptive statistics in Table 11). A1b. Supported (see results from the postestimation of finite mixture model in Table 9) Since the resulting effects as well as both the assumptions (A1a and A1b) are supported, we suggest that there is support for this mechanism.\n",
              "\n",
              "A1b\n",
              "\n",
              "Exposure theory (McPhee 1963)\n",
              "\n",
              "A2. Strategic User Behavior (by users with low WTP)\n",
              "\n",
              "A2a\n",
              "\n",
              "Utility theory — WTP (Danaher 2002)\n",
              "\n",
              "A2b\n",
              "\n",
              "Search Cost Theory (differential search cost for popular and niche content on alternative website)\n",
              "\n",
              "Juxtaposing A2a and A2b, users who have lower WTP and are forced to curtail their NYT consumption due to paywall are more likely to curtail their consumption of popular content on NYT. This leads to long tail effect due to disproportionate reduction of popular content consumption (as a result of strategic reduction of popular content consumed by light users).\n",
              "\n",
              "A2a is supported (as it is the same as A1a). A2b was not empirically tested in this paper but can be argued theoretically. To the extent that A2b is true, it is possible that this mechanism may also be playing a role in creating the long tail effect after the paywall.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1—Appendices/March 2016\n",
              "\n",
              "A1\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "A3. Strategic User Behavior (by users with high WTP)\n",
              "\n",
              "A3a\n",
              "\n",
              "A segment of heavy users decide to curtail their consumption of NYT in order to avoid paying subscription fee. Search cost for finding popular content is lower than the search cost for finding niche content. Search Cost Theory (differential search cost for popular and niche content on alternative website) Campbell (1999); Xia et al. (2004)\n",
              "\n",
              "A3b\n",
              "\n",
              "Leads to long tail effect due to the disproportionate reduction of popular content consumption (as a result of strategic reduction of popular content consumption by heavy users). This mechanism can coexist with exposure mechanism as well as the strategic user behavior by low WTP users. Leads to long tail effect due to disproportionate reduction of popular content consumption (as a result of dropping out of users who feel resentment).\n",
              "\n",
              "Similar to the mechanism based on the strategic behavior of light users, assumption A3b was not empirically tested. However, our finding that the light users who read mostly popular content are more likely to reduce their content consumption after the paywall (see Table 11) suggests that this mechanism would have weak impact, if any. This mechanism does not suggest that there will be a differential impact of paywall on the content consumption of light and heavy users. Though we do not test assumption A4b, however, we find that consistent with exposure based mechanism light users reduce their content consumption more than heavy users after paywall (see Table 10), which cannot be explained by this mechanism. So we infer that this mechanism may have relatively weak role, if any.\n",
              "\n",
              "A4. User Resentment (systematically related to reading behavior)\n",
              "\n",
              "A4a\n",
              "\n",
              "Due to negative emotional response toward the paywall, a proportion of NYT readers may boycott the NYT.\n",
              "\n",
              "A4b\n",
              "\n",
              "Users who feel resentment are more likely to read popular than niche content\n",
              "\n",
              "No theory (suggested by an anonymous reviewer)\n",
              "\n",
              "B. Impact of Paywall on WOM Effectiveness Alternative Mechanisms B1. Bypass effect B1a Rationale/Arguments/ Assumptions NYT allows visitors who come from links on social media to bypass its paywall. This bypass effect (i.e., increase in NYT non-subscribers’ likelihood to click on a NYT content available through social media as they attempt to maximize the number of articles that can be accessed without paying subscription fee) may be dominant in website traffic generation. Given that popular articles are the content that has a greater demand from a larger audience (Zentner et al. 2013), we expect that a decrease in the proportion of WOM about popular content will in turn lower the average clicks per link shared through social WOM. Due to negative emotional response toward the paywall, a proportion of NYT readers may boycott the NYT and systematically do not click on and/or share NYT link on social media. Underlying Theory/Rationale Related to the Design of the Paywall mechanism Resulting (Possible) Effect The relative strength of the positive relationship between social media WOM and website traffic may increase after a paywall implementation. Support for/Against Our study does not directly measure the bypass effect but given the results that show decrease in the strength of the positive relationship between social media WOM and website trafficafter a paywall implementation, we can assume that role of such mechanism, if any, is small.\n",
              "\n",
              "B2. Virality effect\n",
              "\n",
              "B2a\n",
              "\n",
              "Content characteristics play a significant role in determining the virality of online content (Berger and Milkman 2012)\n",
              "\n",
              "The relative strength of the positive relationship between social media WOM and website traffic may decrease after a paywall implementation.\n",
              "\n",
              "Results show that the paywall may lead to a disproportionate decrease in the WOM about popular content. Results also show decrease in the strength of the positive relationship between social media WOM and website traffic after a paywall implementation. Our paper does not measure user resentment and cannot isolate the relative impact of virality vs. resentment mechanism on the relationship between social media WOM and website traffic. However, we believe that such a segment of users is likely to be relatively small.\n",
              "\n",
              "B3. User Resentment\n",
              "\n",
              "B3a\n",
              "\n",
              "No theory (suggested by an anonymous reviewer)\n",
              "\n",
              "The relative strength of the positive relationship between social media WOM and website traffic may decrease after a paywall implementation. Though this “resentment” based mechanism may be an alternative to the “virality “based logic suggesting negative impact of paywall on WOM effectiveness, both the mechanisms may coexist.\n",
              "\n",
              "A2\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1—Appendices/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Appendix B\n",
              "Online Survey Questionnaires\n",
              "1. 2. 3. 4. 5. 6. Do you share online news articles (URLs) on social media (e.g., Facebook, Twitter)? How many online newspaper articles (e.g., The New York Times, The Los Angeles Times) do you READ per week on average? Please specify an approximate number of online news articles you READ in a normal week? How many online newspaper articles (URL links) do you SHARE on social media (e.g., Facebook, Twitter)? Please specify an approximate number of news articles (URL links) you SHARE on Facebook in a normal week. Please specify an approximate number of news articles (URL links) you SHARE on Twitter in a normal week.\n",
              "\n",
              "Appendix C\n",
              "Supplementary Information of Difference-in-Difference Setup\n",
              "Table C1. Key Demographics of NYT and LTAT Website Visitors\n",
              "Household income < $30,000 $30,000 – $59,000 $60,000 – $99,999 $100,000 – $149,999 > $150,000 18–24 25–34 35–44 45–54 55+ Female Male NYT 16.32% 27.21% 23.60% 17.22% 15.65% 8.35% 12.54% 14.59% 15.62% 49.51% 43.56% 56.44% LAT 16.09% 26.97% 25.50% 21.20% 10.24% 8.62% 11.49% 18.82% 21.50% 37.13% 34.18% 65.82%\n",
              "\n",
              "Age\n",
              "\n",
              "Gender\n",
              "\n",
              "Appendix C2. Comparison of NYT and LAT News Event Coverage\n",
              "Before Paywall Rollout: Top 200 Most-Shared News Articles NYT LAT # of News Articles # of Sharing # of News Articles Overlapped news events† 116 105,904 119 Region-specific local news events 5 2,019 26 Before Paywall Rollout: Top 200 Most-Shared News Articles NYT LAT # of News Articles # of Sharing # of News Articles Overlapped news events‡ 89 50,567 109 Region-specific local news events 8 3,166 39\n",
              "†\n",
              "\n",
              "# of Sharing 11,439 2,340\n",
              "\n",
              "# of Sharing 4,765 3,681\n",
              "\n",
              "Major news events covered across NYT and LAT in pre-paywall sample include Japan tsunami & nuclear accident, Libya rebels & military actions, Bahrain & Arab protests, Wisconsin battle on union, U.S. government budget debates, and U.S. pacific tsunami. ‡ Major news events covered across NYT and LAT in post-paywall sample include U.S. government shutdown & budget deficit, Japan nuclear disaster, Libya rebels & military actions, and election campaigns.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1—Appendices/March 2016\n",
              "\n",
              "A3\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Appendix D\n",
              "Supplementary Information of Finite Mixture Model Estimation\n",
              "Table D1. Model Fit for Alternative Numbers of Segments\n",
              "Number of Latent Segments 1 2 3 4 LL -78369.01 -75868.29 -75069.08 -75030.70 AIC 156744.03 151750.59 150160.16 150091.40 BIC 156768.58 151810.21 150253.84 150219.14 R² 0.0072 0.6602 0.7887 0.7930\n",
              "\n",
              "Note: In terms of the BIC criterion, the models with greater numbers of segments improve model performance. However, an interpretation with a two-segment model is more suitable for our hypothesis testing, and adding an additional segment marginally improves the model fit indices after the three-segment model. We therefore opt to report the model estimates for both the two- and three-segment models.\n",
              "\n",
              "Table D2. Parameter Estimates of Finite Mixture Models (Three-Segment Model)\n",
              "Dependent Variable: ln(Average Rank of Content Shared) Segment 1 Segment 2 Segment 3 7.548 5.041 2.845 (.139) (.096) (.186) -.624 .004 .478 (0.15) (.001) (.020) 46% 51% 2%\n",
              "\n",
              "Intercept ln(User rankj) Proportion\n",
              "\n",
              "Note: Standard errors in parentheses. *p < 0.1, **p < 005, ***p < 0.01.\n",
              "\n",
              "Table D3. Postestimation of Finite Mixture Model (Three-Segment Model)\n",
              "Segment 1 Mean ln(Avg. Rank of Content Sharedj) ln(User rankj)\n",
              "Note. Standard errors are in parentheses.\n",
              "\n",
              "Segment 2 Mean 5.161 (.006) 8.952 (.007) [95% conf. interval] [5.148, 5.174] [8.938, 8.966] Mean\n",
              "\n",
              "Segment 3 [95% conf. interval] [7.360 ,7.391] [9.351, 9.402]\n",
              "\n",
              "[95% conf. interval] [1.765, 1.799] [9.053, 9.075]\n",
              "\n",
              "1.782 (.008) 9.064 (.005)\n",
              "\n",
              "7.376 (.007) 9.377 (.013)\n",
              "\n",
              "A4\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1—Appendices/March 2016\n",
              "\n",
              "\n",
              "Oh et al./Impact of a Paywall on Word-of-Mouth via Social Media\n",
              "\n",
              "Appendix E\n",
              "Supplementary Information on Robustness Checks\n",
              "We estimate the following VARX model:\n",
              "\n",
              " ln (Website visit t )  α p   ϕ11 ϕ12   ln (Website visitst −1 )  β p X t   + εt   =   +   +  ln (Tweetst )   α w   ϕ21 ϕ22   ln (Tweetst −1 )   β2 X t \n",
              "where ln(Website visitst) denotes the daily gross site traffic at day t, and its one-day lagged variable is defined as Website visitst-1. Similarly, ln(Tweetst) represents the total number of tweets that contain the NYT link at day t. The dummy variables, Saturdayt and Sundayt, are included in all equations to control for variations due to differences in the type of news articles published on the weekends as well as the differences in the reading habits of consumers during the weekend. We first conducted the unit root tests. The Dickey-Fuller test results confirm that the variables are stationary rather than evolving in 95% confidence intervals. The results of the VARX model are reported in Table E1.\n",
              "\n",
              "Table E1. Estimation Results for WOM and Website Traffic (VARX Model)\n",
              "Before Paywall Site Traffic Equation: ln(Website visitst) ln(Tweetst-1) ln(Website visitst-1) Saturdayt Sundayt Constant Tweets Equation: ln(Tweetst) ln(Website visitst-1) ln(Tweetst-1) Saturdayt Sundayt Constant 0.2847 (0.1399*) 0.2283 (0.2648) -0.1648 (0.0548) 0.251 (0.0548) 8.1302 (2.6188)*** 0.7253 (0.5866) 0.1905 (0.3100) -0.4563 (0.1203)*** -0.1504 (0.1214) -1.9016 (5.800)*** After Paywall 0.022 (0.0848) 0.2343 (0.2025) -0.1176 (0.037)*** -0.1176 (0.037)*** 10.6532 (2.4953)*** -0.0430 (0.3063) 0.1914 (0.1284) -0.3166 (0.0475)*** -0.4899 (0.0560)*** 10.7258 (3.7767)***\n",
              "\n",
              "Standard errors are in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 1—Appendices/March 2016\n",
              "\n",
              "A5\n",
              "\n",
              "\n",
              "Copyright of MIS Quarterly is the property of MIS Quarterly and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "metadata": {
        "id": "V3B0h_qWG2PB",
        "colab_type": "code",
        "outputId": "e5c1e9ef-6d7c-4b77-b57d-678629a391c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc7.sents if 'p =' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[t = 1.11, p = 0.271).]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "metadata": {
        "id": "f6wxSRlW2QZR",
        "colab_type": "code",
        "outputId": "3094b341-b6fc-430c-8c02-04f36b14d29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc7.sents if 'p <' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[**p < 0.01, **p < 0.05, *p < 0.1\n",
              " \n",
              " in WOM volume (pre-paywall mean = 149.4, post-paywall mean = 141.0,,\n",
              " Consistent with our prior expectation, Paywall is negative and significant in Model 1 (β1 = -0.154; p < 0.01).,\n",
              " **p < 0.01, **p < 0.05, *p < 0.1.\n",
              " \n",
              " after the paywall implementation,,\n",
              " Interestingly, the absolute value of the negative coefficient of paywall (β1 = -0.266, p < 0.01) in column (2) is larger, whereas the absolute value of the negative coefficient of paywall in the original tweet sample in column (1) is significantly lower than the coefficients of the full sample that combines both tweets and retweets.,\n",
              " **p < 0.01, **p < 0.05, *p < 0.1.\n",
              " ,\n",
              " *p < 0.1; **p < 0.05; ***p < 0.01.\n",
              " ,\n",
              " The attrition likelihood in Segment 1: Light & Popular (0.688) is significantly greater than the attribution likelihood in Segment 2: Heavy & Mix of Niche and Popular (0.589) at p < 0.01.\n",
              " ,\n",
              " We find that the coefficients of ln(WOMt) (α1 = 0.3724; p < 0.01) and ln(WOMt-1) (α2 = 3.80e-06; p < 0.1) are positive and significant in the website traffic equation, suggesting that there is a positive relationship between the volume of online WOM (i.e., social media buzz) and website traffic.,\n",
              " 0.0807; p < 0.01) and the interaction of Paywall and ln(WOMt),\n",
              " 0.129; p < 0.1) are negative and statistically significant.,\n",
              " Furthermore, consistent with our main results from the NYT sample, the volume of WOM on Twitter has a positive and significant effect on website traffic generation (α1 = 0.589; p < 0.01), providing additional empirical support for our prior result that online WOM volume is positively related to website traffic.,\n",
              " **p < 0.01, **p < 0.05, *p < 0.1.\n",
              " ,\n",
              " *p < 0.1, **p < 005, ***p < 0.01.\n",
              " ,\n",
              " *p < 0.1, **p < 0.05, *,\n",
              " **p < 0.01.\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "metadata": {
        "id": "7otbLwVkGV2Q",
        "colab_type": "code",
        "outputId": "e03dbc0e-eae5-49b3-a54c-8a5ca7ab3adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24641
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc8 = nlp(open(u\"VenkateshV#BrownS#SullivanY_2016_Guidelines for Conducting Mixed-methods Research - An Extension and Illustration_Journal of the Association for Information Systems_7.txt\").read())\n",
        "doc8\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "J\n",
              "Research Paper\n",
              "\n",
              "ournal of the\n",
              "\n",
              "A\n",
              "\n",
              "ssociation for\n",
              "\n",
              "I\n",
              "\n",
              "nformation\n",
              "\n",
              "S\n",
              "\n",
              "ystems\n",
              "ISSN: 1536-9323\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "Viswanath Venkatesh\n",
              "Department of Information Systems, Walton College of Business, University of Arkansas vvenkatesh@vvenkatesh.us\n",
              "\n",
              "Susan A. Brown\n",
              "Department of Management Information Systems, Eller College of Management, University of Arizona suebrown@eller.arizona.edu\n",
              "\n",
              "Yulia W. Sullivan\n",
              "School of Management, Binghamton University, State University of New York ysulliva@binghamton.edu\n",
              "\n",
              "Abstract:\n",
              "In this paper, we extend the guidelines of Venkatesh et al. (2013) for mixed-methods research by identifying and integrating variations in mixed-methods research. By considering 14 properties of mixed-methods research (e.g., purposes, research questions, epistemological assumptions), our guidelines demonstrate how researchers can flexibly identify the existing variations in mixed-methods research and proceed accordingly with a study design that suits their needs. To make the guidelines actionable for various situations and issues that researchers could encounter, we develop a decision tree to map the flow and relationship among the design strategies. We also illustrate one possible type of mixed-methods research in information systems in depth and discuss how to develop and validate metainferences as the outcomes of such a study. Keywords: Mixed-methods Research, Meta-inferences, Research Design, Qualitative, Quantitative.\n",
              "Suprateek Sarker was the accepting senior editor. This article was submitted on April 23, 2014 and went through 3 revisions.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "pp. 435 – 495\n",
              "\n",
              "July\n",
              "\n",
              "2016\n",
              "\n",
              "\n",
              "436\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "1\n",
              "\n",
              "Introduction\n",
              "\n",
              "Mixed-methods research 1 (i.e., research that combines elements of qualitative and quantitative research approaches) has gained popularity as a method of choice for studying phenomena in information systems (IS) research (e.g., Bhattacherjee & Premkumar, 2004; Keil & Tiwana, 2006; Koh, Ang, & Straub, 2004). Mixed-methods research provides an opportunity to develop novel theoretical perspectives by combining the strengths of quantitative and qualitative methods. Thus, it provides rich insights by overcoming limitations associated with either method alone and results in “meta-inferences”—an integrative view of findings from qualitative and quantitative strands of mixed-methods research (Creswell, 2009; Teddlie & Tashakkori, 2003; Venkatesh, Brown, & Bala, 2013). However, the application of this method in the IS field has been quite limited (see Venkatesh et al., 2013). The different paradigms underlying the knowledge about research methodology have constrained IS scholars’ contributions to understanding business phenomena using mixed-methods research (Greene & Caracelli, 2003; Petter & Gallivan, 2004; Teddlie & Tashakkori, 2003; Venkatesh et al., 2013). Venkatesh et al. (2013) suggest that IS researchers could collaborate to leverage different paradigmatic views and, at the same time, conduct rigorous mixed-methods research because, with it, one can embrace diverse methodological approaches and, thus, reduce the tension between different paradigms (Ågerfalk, 2013). Despite a need for IS research to bridge the gap between different paradigms and/or methods, IS researchers have provided no real mixed-methods guidelines in the emerging paradigms in the IS field. In response to this need, Venkatesh et al. (2013) developed a set of guidelines for conducting mixed-methods research and illustrated the applicability of these guidelines using two published IS papers. Although their guidelines focus on the different types of mixed-methods research by identifying possible combinations of qualitative and quantitative methods, they discuss only the time ordering of the qualitative and quantitative methods in a single research inquiry and focus less on how to design different types of mixed-methods studies based on various criteria (e.g., priority, stage of integration, epistemological perspective). Early approaches to mixed-methods designs (e.g., Creswell, 2003; Greene, Caracelli, & Graham, 1989; Tashakkori & Teddlie, 2003b) have been primarily typological (Maxwell & Loomis, 2003). For example, Creswell (2003) identify two basic types of mixed-methods designs: concurrent and sequential. Although a typological approach of mixed-methods research could help researchers select a particular design for their study (Teddlie & Tashakkori, 2003), mixed-methods studies have a far greater diversity than any single typology can actually capture (Caracelli & Greene, 1997; Guest, 2012; Maxwell & Loomis, 2003; Tashakkori & Teddlie, 2003b). In particular, the existence of more than two paradigms (e.g., positivist, critical realist, postpositivist), the diversity of qualitative and quantitative approaches that one can employ, the wide range of purposes of mixed-methods research, and differences with respect to time orientation have made actually using a mixed-methods design far more complicated than simply fitting it in a typology framework (Maxwell & Loomis, 2003). Consistent with Maxwell and Loomis (2003), we believe that one can use a more flexible approach to mixed-methods research designs to address the limitations of the typology approach. Thus, rather than categorizing mixed-methods designs into a typology framework, we view the design of a study as comprising several different dimensions (from many different typologies) that researchers can flexibly integrate to meet their studies’ purposes. Against this backdrop, we augment the mixed-methods guidelines that Venkatesh et al. (2013) propose by leveraging variations in mixed-methods research. Instead of focusing on one typology or framework, we approach mixed-methods designs by identifying different properties or typologies of mixed-methods research. We provide guidelines that are flexible enough to accommodate different types of mixed-methods research. By considering different properties of mixed-methods research (e.g., purposes, research questions, epistemological assumptions), our guidelines demonstrate how researchers can flexibly identify the existing variations in mixed-methods research and proceed accordingly with a study design that suits their needs (see Maxwell, 1996; Maxwell & Loomis, 2003; Nastasi, Hitchcock, & Brown, 2010). In addition,\n",
              "1\n",
              "\n",
              "Although researchers have used the terms mixed methods and multimethod interchangeably in social and behavioral science, the two do differ conceptually (Venkatesh et al., 2013). Teddlie and Tashakkori (2003) identify two major types of multiple methods research: 1) mixed-methods research and 2) multi-method research. In mixed-methods research, one uses quantitative and qualitative data-collection procedures (e.g., survey and focus group interviews) or research methods (e.g., ethnography and field experiment) to answer the research questions (Tashakkori & Teddlie, 2003a). In contrast, in multi-method research, one addresses the research questions by using two or more quantitative data-collection procedures or research methods (e.g., survey and experiment) or two or more qualitative data-collection procedures or research methods (e.g., ethnography and case study) (Teddlie & Tashakkori, 2003). Mixed-methods research requires a combination of qualitative and quantitative procedures, whereas multimethod research requires a combination of qualitative or quantitative procedures.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "437\n",
              "\n",
              "we comprehensively illustrate how to apply a mixed-methods approach based on different properties of mixed-methods research. We also discuss how to develop and validate meta-inferences as the outcomes of a mixed-methods research project. Bryman (2006), as cited in Harrison and Reilly (2011), found that scholars have had a difficult time in identifying exemplary mixed-methods research due to the absence of best practice templates from which to draw on when it comes to triangulating the findings. By illustrating how to develop and validate meta-inferences, we highlight a key advantage of mixed-methods research over a single method design. This paper proceeds as follows. In Section 2, we summarize mixed-methods research and overview the guidelines for mixed-methods research that Venkatesh et al. (2013) propose. In Section 3, we discuss the variations in mixed-methods research, leverage them to extend Venkatesh et al.’s (2013) guidelines, and present a decision tree to map the flow and relationship among the design strategies. In Section 4, we offer an illustrative study of one possible type of mixed-methods research and concomitant meta-inferences. Finally, in Section 5, we conclude the paper with implications and suggestions for future research.\n",
              "\n",
              "2\n",
              "\n",
              "Overview of Mixed-methods Research\n",
              "\n",
              "In general, one can categorize research in the social sciences into three groups: 1) qualitative research (i.e., research dominated by, but not exclusively based on, constructive paradigms and focused on analyzing narrative data) (Tashakkori & Teddlie, 2003b); 2) quantitative research (i.e., research dominated by positivist paradigms and focused on analyzing numerical data) (Orlikowski & Baroudi, 1991); and 3) mixed-methods research (i.e., research dominated by other paradigms, such as pragmatism, critical realism, and transformative-emancipatory and focused on analyzing both narrative and numerical data) (Teddlie & Tashakkori, 2003). Scholars have defined the concept of mixed-methods research in several ways. In an effort to precisely define mixed-methods research, Johnson, Onwuegbuzie, and Turner (2007) review various definitions for the term. Based on their review, they define mixed-methods research as: the type of research in which a researcher or team of researchers combines elements of qualitative and quantitative research approaches (e.g., use of qualitative and quantitative viewpoints, data collection, analysis, inference techniques) for the broad purposes of breadth and depth of understanding and corroboration. (p. 123) This definition suggests that mixed-methods research can involve mixing two or more different methods “within a single study” or “within a program of research” and that “mixing [methods] might occur across a closely related set of studies” (Johnson et al., 2007, p. 123). Researchers have identified three advantages of mixed-methods research: 1) it enables researchers to simultaneously address confirmatory and explanatory research questions and, therefore, evaluate and generate theory at the same time; 2) it enables researchers to provide stronger inferences than a single method or worldview; and 3) it provides an opportunity for researchers to produce a greater assortment of divergent and/or complementary views (see Venkatesh et al., 2013). When used in combination, quantitative and qualitative methods complement each other and allow for a more robust analysis (Ivankova, Creswell, & Stick, 2006; Tashakkori & Teddlie, 2008). However, mixed-methods research does not replace either a quantitative or a qualitative approach but rather draws from the strengths and minimizes the weaknesses of both methods (Creswell, 2003; Jick, 1979; Johnson & Onwuegbuzie, 2004; Venkatesh et al., 2013). Venkatesh et al. (2013) have proposed the most recent guidelines for conducting mixed-methods research. They divide their guidelines into two major areas: 1) general guidelines (i.e., appropriateness of mixedmethods research and meta-inferences) (Steps 1 to 4) and 2) validation (Steps 5 to 6). We summarize the guidelines next.\n",
              "\n",
              "2.1\n",
              "\n",
              "Step 1: Decide on the Appropriateness of a Mixed-methods Approach\n",
              "\n",
              "At the initial stage of their study, researchers should carefully think about their research questions, purposes, paradigmatic views, and contexts to decide on the appropriateness of a mixed-methods approach. In mixedmethods research, research questions (or research objectives) drive the methods used in the study and set boundaries on the research project. Researchers should employ a mixed-methods design only when they intend to holistically explain a phenomenon for which extant research is fragmented, inconclusive, and/or equivocal.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "438\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "2.2\n",
              "\n",
              "Step 2: Develop Strategies for Mixed-methods Research Designs\n",
              "\n",
              "Once one has determined the research questions, rationale, and objectives, one should next identify the research design strategies. Although mixed-methods researchers have suggested several design strategies, the guidelines focus on two of the most widely used mixed-methods research designs: concurrent and sequential. Researchers should develop a design strategy that best fits their research questions and objectives.\n",
              "\n",
              "2.3\n",
              "\n",
              "Step 3: Develop Strategies for Collecting and Analyzing Mixed-methods Data\n",
              "\n",
              "Researchers can employ multiple modes of data collection and proceed with a mixed-methods data-analysis approach. Researchers may find it beneficial to develop a strategy for mixed-methods data analysis in which “both quantitative and qualitative data are analyzed rigorously so that useful and credible inferences can be made from these individual analyses” (Venkatesh et al., 2013, p. 38).\n",
              "\n",
              "2.4\n",
              "\n",
              "Step 4: Draw Meta-inferences from Mixed-methods Results\n",
              "\n",
              "The term meta-inference describes “the theoretical statements, narratives, or a study inferred from an integration of findings from quantitative and qualitative strands of mixed methods research” (Venkatesh et al., 2013, p. 29). A strong inference is only possible if one has a well-implemented design that is appropriate for the research question. Thus, researchers must determine which research design is most suitable to address their research question(s) and derive their studies’ meta-inferences or conclusions based on the design they select.\n",
              "\n",
              "2.5\n",
              "\n",
              "Step 5: Assessing the Quality of Meta-inferences\n",
              "\n",
              "Teddlie and Tashakkori (2003) propose the term inference quality to refer to issues associated with validity in the context of mixed-methods research. According to Teddlie and Tashakkori, a mixed-methods nomenclature for validation can help differentiate mixed-methods validation from quantitative and qualitative validation (Venkatesh et al., 2013). Thus, consistent with Teddlie and Tashakkori, we use the umbrella term inference quality to refer to validity in mixed-methods research. Venkatesh et al. (2013) propose four stages of assessing the quality of meta-inferences: 1) discuss quality criteria in quantitative and qualitative research, 2) use mixed-methods research nomenclature when discussing inference quality, 3) discuss quality of mixed-methods findings and/or meta-inferences (i.e., explanatory quality), and 4) discuss quality from a research design point of view (i.e., design quality). To assess the quality of inferences, one should assess each component of the study using criteria appropriate for its methodology. Only after one has done this step can one apply the quality assessment of the mixed-methods study to evaluate the quality of meta-inferences.\n",
              "\n",
              "2.6\n",
              "\n",
              "Step 6: Discuss Potential Threats and Remedies\n",
              "\n",
              "Finally, researchers should discuss the potential threats to quality that may arise during the data-collection and analysis phases. Because any serious threats will compromise the quality of inferences, researchers should also discuss the potential remedies to overcome or minimize the threats.\n",
              "\n",
              "3\n",
              "\n",
              "Variations in Mixed-methods Research: An Extension\n",
              "\n",
              "Although Venkatesh et al.’s (2013) guidelines discuss several properties of mixed-methods research (i.e., paradigmatic assumptions, purposes of mixed-methods research, time orientation, and quality of metainferences), the guidelines do not discuss other properties that one can use to develop strategies for conducting mixed-methods research. Further, although some researchers have previously attempted to integrate different properties of mixed-methods research (e.g., Maxwell & Loomis, 2003; Nastasi et al., 2010), existing mixed methods do not elaborate on different design variations and the relationships among them. Thus, we extend Venkatesh et al.’s (2013) guidelines by integrating different properties of mixedmethods research into the guidelines. Identifying how different properties are related and determining how one design decision may lead to another decision will help researchers develop a high-quality mixedmethods study (Guest, 2012; Tashakkori & Teddlie, 2003b). To integrate the design variations that encompass the existing typologies, we reviewed the literature in depth and discussed different variations of mixed-methods research based on the existing typologies in mixed-methods research. From the review, we identified 14 important properties of mixed-methods research\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "439\n",
              "\n",
              "(see Table 1) (Appendix A presents the literature review in more detail) 2. Table 1 lists the 14 properties of mixed-methods research and the possible dimensions that researchers can use to design their studies. We organize these properties into three categories: 1) foundations of design decisions (i.e., preliminary decisions used to guide the research design), 2) primary design strategy decisions (i.e., decisions related to the strands/phases of research and process of designing research), and 3) inference decisions (i.e., decisions related to the development of meta-inferences, data interpretation, and inference quality). Table 1 also provides a list of questions to help researchers select mixed-methods designs that might be the best fit for their study. Table 2 maps the 14 properties to Venkatesh et al.’s (2013) guidelines.\n",
              "Table 1. Variations in the Properties of Mixed-methods Research 3 Property of mixedmethods research Design question addressed by the property Foundations of design decisions • Rhetorical style—format: questions, aims, and/or hypotheses • Rhetorical style—level of integration How will the researcher write the • The relationship of questions to other questions: research questions? independent or dependent • The relationship of questions to the research process: predetermined or emergent • • • Which of the following purposes • does the research design serve? • • • Does the study involve one paradigm or multiple paradigm stances? What paradigmatic perspective will guide the research design? Complementarity Completeness Developmental Expansion Corroboration/confirmation Compensation Diversity Possible dimensions\n",
              "\n",
              "Research questions\n",
              "\n",
              "Purposes of mixedmethods research\n",
              "\n",
              "Epistemological perspectives\n",
              "\n",
              "• Single paradigm stance • Multiple paradigm stance • • • • Pragmatism Critical realism Dialectical Other major paradigmatic perspectives (e.g., postpositivism)\n",
              "\n",
              "Paradigmatic assumptions\n",
              "\n",
              "Primary design strategies Design-investigation strategies Strands/phases of research Does the study develop or test a • Exploratory investigation theory? • Confirmatory investigation Does the study involve one or multiple phases? • Single phase (or single study) or monostrand design • Multiple phases (or research program) or multistrand design\n",
              "\n",
              "Mixing strategies\n",
              "\n",
              "Does the design involve using both qualitative and quantitative • Fully mixed methods research across all components • Partially mixed methods of a study?\n",
              "\n",
              "2\n",
              "\n",
              "3\n",
              "\n",
              "Although typologies that integrate two or more properties of mixed-methods research exist (e.g., Johnson & Onwuegbuzie, 2004; Tashakkori & Teddlie, 1998), we exclude these typologies from our review because we do not study a mixed-methods research design as a choice from a fixed set of possible arrangements. Instead, we discuss the basic typologies of mixed-methods research that are flexible enough to accommodate different types of mixed-methods designs. Among these properties, Venkatesh et al. (2013) cover the purposes of mixed-methods research (i.e., complementarity, completeness, developmental, expansion, corroboration/confirmation, compensation, and diversity), paradigmatic assumptions (i.e., pragmatism, transformative-emancipatory, and critical realism), time orientation (i.e., concurrent and sequential), and inference quality (design quality and explanation quality). The guidelines also discuss (albeit briefly) the types of reasoning in mixed-methods research. In our current guidelines, we discuss the 14 properties listed in Table 1 in more detail.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "440\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table 1. Variations in the Properties of Mixed-methods Research 3 Property of mixedmethods research Time orientation Priority of methodological approach Sampling design strategies Data-collection strategies Design question addressed by the property Do the quantitative and qualitative data collection occur sequentially or concurrently? Does the qualitative or quantitative component have priority or are they equally important? Possible dimensions • Sequential designs • Concurrent designs • Equivalent status design • Dominant-less dominant design (i.e., qualitative dominant or quantitative dominant)\n",
              "\n",
              "• Basic mixed-methods sampling strategies Which of the following sampling • Sequential mixed-methods sampling designs does the researcher use • Concurrent mixed-methods sampling in the data-collection stage? • Multiple mixed-methods sampling strategies What are the best strategies to collect the quantitative and qualitative data? • Multiple modes of data collection (both quantitative and qualitative data collection techniques) • Concurrent mixed analysis • Sequential qualitative-quantitative analysis • Sequential quantitative-qualitative analysis • • • • • • • • • • • • • Inductive theoretical reasoning Deductive theoretical reasoning Inductive and deductive theoretical reasoning Abductive theoretical reasoning Design and explanatory quality Sample integration Inside-outside Weakness minimization Conversion Paradigmatic mixing Commensurability Multiple validities Political\n",
              "\n",
              "How does the researcher Data-analysis strategies analyze the qualitative and quantitative data?\n",
              "\n",
              "Inference decisions Types of reasoning Will a particular theoretical perspective drive the design?\n",
              "\n",
              "Inference quality\n",
              "\n",
              "Which quality issues does the researcher address in the study?\n",
              "\n",
              "Table 2. Guidelines to Properties Mapping Guidelines (Venkatesh et al. 2013) 1) Decide on the appropriateness of a mixed-methods approach. Properties of mixed-methods research Foundations of design decisions: • Research questions • Purposes of mixed-methods research • Epistemological perspectives • Paradigmatic assumptions Primary design strategies: • Design investigation strategies • Strands/phases of research • Mixing strategies • Time orientation • Priority of methodological approach\n",
              "\n",
              "2) Develop strategies for mixed-methods research designs.\n",
              "\n",
              "• Sampling design strategies 3) Develop strategies for collecting and analyzing mixed• Data-collection strategies methods data. • Data-analysis strategies 4) Draw meta-inferences from mixed-methods results. 5) Assess the quality of meta-inferences. Inference decisions: • Types of reasoning • Inference quality\n",
              "\n",
              ") 6) Discuss potential threats and remedies.\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "441\n",
              "\n",
              "In Sections 3.1 to 3.6, we discuss how the steps of our procedure for conducting mixed-methods research integrate the 14 properties.\n",
              "\n",
              "3.1\n",
              "\n",
              "Step 1: Decide on the Appropriateness of a Mixed-methods Approach\n",
              "\n",
              "When determining whether mixed-methods research suits one’s research, one needs to make decisions associated with 1) research questions, 2) research purposes, 3) selection of theoretical perspectives/worldviews or paradigms, and 4) epistemological perspectives. These four mixed-methods research properties make up the foundations of design decisions researchers need to make to determine which approach they will take to establish the boundary assumptions to guide their research project (Creswell, 2003).\n",
              "\n",
              "3.1.1\n",
              "\n",
              "Research Questions\n",
              "\n",
              "Mixed-methods research questions differ from those of qualitative and quantitative research questions. Quantitative research questions tend to be specific in nature (Onwuegbuzie & Leech, 2006). Most quantitative research questions are descriptive (i.e., they simply call for quantifying responses to one or more variables; for example, what is the perception of ease of use of PCs?), comparative (i.e., they call for comparing two or more groups on some outcome variables) (e.g., what is the difference in purchase behaviors between adopters and non-adopters?), or associative (i.e., they deal with trends between (or among) two (or more) variables; for example, what is the nature of the relationship between the intention to adopt and subsequent purchase behavior?) (Onwuegbuzie & Leech, 2006). In contrast, qualitative research questions are more “open-ended, evolving, and non-directional” (Onwuegbuzie & Leech, 2006, p. 482). Good qualitative questions are broad but specific enough to focus on the issues most relevant to the individuals under investigation (Plano Clark & Badiee, 2010). Qualitative questions generally tend to seek, discover, and explore a process or to describe experiences (Onwuegbuzie & Leech, 2006). Referencing Onwuegbuzie and Leech (2006), Creswell (1998) argues that qualitative research questions can either represent broad questions (e.g., how have new adopters’ attitudes toward technology or personal computers evolved as they used the technology every day?) or specific subquestions that address major concerns and complexities that one seeks to resolve (e.g., what does it mean to nonadopters to change their attitudes toward the technology?). The major difference between quantitative and qualitative research questions is that one generally develops quantitative research questions before the study begins; in contrast, one generally develops qualitative questions at the beginning of the study or they emerge at some point throughout the study (Onwuegbuzie & Leech, 2006). Unlike qualitative or quantitative research questions, mixed-methods research questions are “questions that embed both a quantitative research question and a qualitative research question within the same question” (Onwuegbuzie & Leech, 2006, p. 483). Mixed-methods questions determine one’s primary design strategies, including whether one should collect and analyze qualitative data and quantitative data concurrently, sequentially, or iteratively before addressing the questions (Tashakkori & Creswell, 2007). Plano Clark and Badiee (2010) identify four dimensions that describe how researchers can write research questions in the context of their mixed-methods studies: 1) rhetorical style—question format, 2) rhetorical style—level of integration, 3) the relationship of questions to other questions, and 4) the relationships of questions to the research process. One can state a research question based on the first dimension (i.e., rhetorical style—question format) in three different formats: 1) question (researchers write an interrogative sentence complete with a question mark), 2) aim (researchers write a declarative sentence as an expression of research objectives), and 3) hypothesis (researchers write a statement that predicts an outcome for a research question) (Plano Clark & Badiee, 2010). Based on the second dimension (i.e., rhetorical style—level of integration), one can write research questions in a mixed-methods study as described by Creswell (2009) in three ways. First, one can independently write quantitative questions and qualitative questions. For example, in a study of online friendship, a quantitative question might be “what is the relationship between online friendship and happiness?” and a qualitative question might be “what factors play a role in meaningful online friendship?” (Plano Clark & Badiee, 2010). Second, researchers can write separate quantitative questions and/or qualitative questions and supplement them with mixed-methods questions. For example, one qualitative question is “what theory explains adolescents’ process of using social media?”, one quantitative question is “how are the identified factors related?”, and one mixedmethods research question is “how do adolescents use social media?” (Plano Clark & Badiee, 2010). Third,\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "442\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "researchers can write only mixed-methods questions that reflect the procedures or the research content; for example: “how is an effective online community developed and tested?”. If researchers attempt to address more than one research question, they should address the third dimension (i.e., the relationship of questions to other questions) (Plano Clark & Badiee, 2010). The relationship among the questions shapes a study’s overall design and informs the relationship between its quantitative and qualitative components (Plano Clark & Badiee, 2010). Plano Clark and Badiee (2010) suggest two relationship alternatives: 1) research questions may be independent of each other and 2) one research question may depend on the results of other questions (Plano Clark & Badiee, 2010). The last dimension focuses on the relationship of questions to the research process. Research questions in mixed-methods studies may be either predetermined or emergent (Plano Clark & Badiee, 2010). A research question is predetermined when it appears at the beginning of the study based on researchers’ understanding of the literature and practice or disciplinary considerations. In contrast, one forms emergent questions during the design, data-collection, data-analysis, and/or interpretation phases of the research process (Plano Clark & Badiee, 2010).\n",
              "\n",
              "3.1.2\n",
              "\n",
              "Purposes of Mixed-methods Research\n",
              "\n",
              "Based on several resources, including Greene et al. (1989), Creswell (2003) and Tashakkori and Teddlie (2003b), we can summarize the purposes of mixed-methods research into seven categories: 1) complementarity (i.e., to gain complementary views about the same phenomena or relationships), 2) completeness (i.e., to gain a complete picture of phenomena), 3) developmental (i.e., to ensure the questions from one strand emerge from the inference of a previous one or one strand is used to develop hypotheses the researcher will test in the next one), 4) expansion (i.e., to explain or expand on the understanding obtained in a previous strand of a study), 5) corroboration/confirmation or triangulation (i.e., to assess the credibility of inferences obtained from one approach), 6) compensation (i.e., to eliminate potential design weaknesses of one approach by using the other), and 7) diversity (i.e., to obtain divergent views of the same phenomenon) (see Venkatesh et al., 2013).\n",
              "\n",
              "3.1.3\n",
              "\n",
              "Epistemological Perspectives\n",
              "\n",
              "From an epistemological perspective, one can conduct mixed-methods research using a single paradigm or multiple paradigms. A single paradigm perspective proposes that one can accommodate both quantitative and qualitative research under the same paradigm (e.g., positivist, realist) (Tashakkori & Teddlie, 1998). A multiple paradigm perspective claims that alternative paradigms are compatible and can be used in one research project (Teddlie & Tashakkori, 2003). One can view combining multiple paradigms and methodological practices as a strategy that adds rigor, breadth complexity, richness, and depth to a research inquiry (Denzin, 2012). Under this multiple paradigm perspective, researchers have to decide which paradigms best fit their study given they choose to use a particular mixed-methods design (Creswell, Plano Clark, Gutmann, & Hanson, 2003).\n",
              "\n",
              "3.1.4\n",
              "\n",
              "Paradigmatic Assumptions\n",
              "\n",
              "Although specific paradigms are commonly associated with specific methods, one may use both qualitative and quantitative methods appropriately with any research paradigm (Guba & Lincoln, 1994; Tashakkori & Teddlie, 1998). Researchers have proposed several paradigms for mixed-methods research, such as the purist stance (i.e., because the assumptions of different paradigms are incompatible, it is not possible to mix paradigms in the same study), aparadigmatic stance (i.e., driven by research questions and/or purposes), substantive theory stance (i.e., emergent paradigms may be embedded in or intertwined with substantive theories) (Greene, 2007; Venkatesh et al., 2013), complementary strengths stance (i.e., the assumptions of different paradigms are not fundamentally compatible but are different in important ways), dialectic stance (i.e., important paradigm differences should be respectfully and intentionally used together to engage meaningfully with difference), and alternative paradigms stance (i.e., the initiation of a new paradigm that actively embraces and promotes the mixing of methods) (Greene, 2007). From our review, we found that mixed-methods researchers have mostly used the dialectic, alternative paradigms (i.e., pragmatism and critical realism) and complementary strengths stances (i.e., the use of multiple paradigms). The dialectic paradigm stance generally allows one to use more than one paradigmatic tradition in the same research project or research program because it assumes that using multiple paradigms contributes to better understanding the phenomenon under study (Greene & Hall, 2010; Teddlie & Tashakkori, 2003). This\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "443\n",
              "\n",
              "stance recognizes the legitimacy of multiple social inquiry theories and practices because they represent different ways of seeing and understanding the social world (Greene, 2005, 2007; Greene & Hall, 2010). A mixed-methods way of thinking under the dialectic paradigm offers researchers opportunities to meaningfully engage with difference as they encounter it in their studies (Greene & Hall, 2010). The alternative paradigms stance includes pragmatism and critical realism. One of the central ideas in pragmatism is that “engagement in philosophical activity should be done to address problems, not to build systems” (Biesta, 2010, p. 97). Pragmatism supports using both qualitative and quantitative research methods in the same research study or in multistage research programs (Teddlie & Tashakkori, 2003). Because a pragmatist perspective considers practical consequences to be a crucial component of meaning and truth (Venkatesh et al., 2013), researchers need to articulate a purpose for their mixed-methods study to establish the rationale for why they need to mix quantitative and qualitative methods in the first place (Creswell, 2003). Critical realists believe that an objective reality exists but that we can understand it only imperfectly and probabilistically (Tashakkori & Teddlie, 1998). They deny that we have any objective knowledge of the world and accept the possibility of alternative valid accounts of any phenomenon (Maxwell & Mittapalli, 2010). Critical realism “embraces various methodological approaches from different philosophical positions by taking a critical stance towards the necessity and validity of current social arrangements without following the extant paradigms’ assumptions at face value” (Zachariadis, Scott, & Barret, 2013, p. 856). Thus, critical realism is an ideal paradigm for mixed-methods research because its philosophical stance is compatible with the methodological characteristics of both quantitative and qualitative research (Maxwell & Mittapalli, 2010). Finally, according to the complementary strengths stance, one can combine and use other major paradigms used in the social and behavioral sciences (e.g., constructivism/interpretivism, positivism, postpositivism) to support mixed-methods research. Constructivism/interpretivism believes that people construct their own understanding and subjective knowledge as they interact with the world around them (Tashakkori & Teddlie, 2003b). Thus, researchers who embrace this paradigm try to understand phenomena by accessing the meanings participants assign to them (Orlikowski & Baroudi, 1991). Phenomenological sociology, hermeneutics, and ethnography exemplify the constructivist approach (Lee, 1991). In contrast, positivism is premised on the existence of a priori fixed hypotheses or relationships among constructs that one typically investigates with structured instrumentation (Lee, 1991; Orlikowski & Baroudi, 1991). Whereas positivists believe that the researcher and the object of inquiry are independent of each other, postpositivists accept that theories and researchers’ backgrounds, knowledge, and values can influence the study (Trochim & Donnelly, 2007). One can conduct mixed-methods research by combining these paradigmatic approaches (Creswell et al., 2003; Lee, 1991). For example, researchers might use an ethnographic method to study system analysts and end users (Lee, 1991). Based on the results, researchers might use a positivist approach to formulate a formal, general theory that explains, for instance, end user resistance to systems analysis (Lee, 1991). In terms of conducting empirical mixed-methods studies, researchers should consider what the alternative paradigmatic positions are and determine which of the alternative positions best suits their studies (Tashakkori & Teddlie, 2010). When developing a mixed-methods study, one should begin by identifying paradigmatic assumptions, including their philosophical assumptions and theoretical framework, as research foundations that intertwine with the research questions and purposes of mixed-methods research.\n",
              "\n",
              "3.2\n",
              "\n",
              "Step 2: Develop Strategies for Mixed-methods Research Designs\n",
              "\n",
              "After one has established the appropriateness of mixed-methods research, one has to make the primary design decisions associated with strands/phases of research, priority of methodological approach, designinvestigation strategies, mixing strategies, and time orientation. Although these decisions relate to each other, they can be independent and vary as the study evolves.\n",
              "\n",
              "3.2.1\n",
              "\n",
              "Strands/Phases 4 of Research\n",
              "\n",
              "Based on the strands/phases of research, we can classify mixed-methods designs into two types: mixedmethods monostrand designs and mixed-methods multistrand designs (Tashakkori & Teddlie, 2003b; Teddlie & Tashakkori, 2006). Teddlie and Tashakkori (2009) define strand or phase as encompassing three stages:\n",
              "4\n",
              "\n",
              "Strands can also refer to distinctions with regard to a single study (i.e., monostrand) versus multiple studies in a broader research program (i.e., multistrand) (Nastasi et al., 2007, 2010).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "444\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "1) conceptualization (i.e., theoretical foundations, purpose, and research methods), 2) experiential (i.e., data collection and analysis), and 3) inferential (i.e., data interpretation and application). A monostrand study involves only a single phase of the conceptualization-experiential-inferential process, yet it consists of both qualitative and quantitative components (Nastasi et al., 2010; Teddlie & Tashakkori, 2006). In contrast, mixedmethods multistrand designs contain at least two research strands (Bryman, 2006; Teddlie & Tashakkori, 2006). In these designs, one can mix the quantitative and qualitative components in or across all stages (i.e., conceptualization-experiential-inferential process) of the study (Teddlie & Tashakkori, 2006). Mixed-methods multistrand designs often involve multiple phases in a broader research program, with each phase encompassing all of the stages from conceptualization through inference (Teddlie & Tashakkori, 2009). The decision related to strands/phases of research is important because it influences researchers’ decisions associated with other design strategies, such as the priority of methodological approach, mixing strategies, and time orientation. Naturally, monostrand designs have their constraints (Teddlie & Tashakkori, 2006). In contrast, one can implement mixed-methods multistrand designs using parallel, sequential, conversion, or multilevel mixed designs (Teddlie & Tashakkori, 2009).\n",
              "\n",
              "3.2.2\n",
              "\n",
              "Priority of Methodological Approach\n",
              "\n",
              "Based on the priority of the methodological approach, one can categorize mixed-methods research into equivalent-status designs and dominant-less dominant status designs. In equivalent-status designs, researchers generally conduct a study using both qualitative and quantitative approaches about equally to understand the phenomena of interest (Tashakkori & Teddlie, 1998). In dominant-less dominant status designs, researchers usually conduct a study in a single dominant paradigm with a small component of the overall research project drawn from an alternative design (Tashakkori & Teddlie, 1998). One can divide the dominant-less dominant status designs into two categories: qualitative-dominant mixedmethods research and quantitative-dominant mixed-methods research (Johnson et al., 2007). Qualitativedominant mixed-methods research refers to “the type of mixed research in which one relies on a qualitative, constructivist-poststructuralist-critical view of the research process, while concurrently recognizing that the addition of quantitative data and approaches are likely to benefit most research projects” (Johnson et al., 2007, p. 124). In contrast, quantitative-dominant mixed-methods research is “the type of mixed research in which one relies on a quantitative, postpositivist view of the research process, while concurrently recognizing that the addition of qualitative data and approaches are likely to benefit most research projects” (Johnson et al., 2007, p. 124). Although determining the priority of methodological approach is important, researchers can modify their priority decision after the study is complete (Teddlie & Tashakkori, 2009). For example, a quantitativedominant mixed-methods study may become a qualitative dominant study if the qualitative data become more important in understanding the phenomenon under study and vice versa (Teddlie & Tashakkori, 2009). Despite this flexibility, we encourage researchers to refer to their research questions and purposes when deciding whether one component has significantly higher priority than does the other component.\n",
              "\n",
              "3.2.3\n",
              "\n",
              "Design Investigation Strategies\n",
              "\n",
              "The choice of design investigation strategies essentially influences the process of developing inferences through theoretical reasoning techniques (Tashakkori & Teddlie, 1998). Using Patton’s (1990) typology of design dimensions, Tashakkori and Teddlie (1998) identify two different types of investigations in mixedmethods research: exploratory and confirmatory. In exploratory investigations, one conducts the study to develop or generate a new theory. These designs include qualitative case studies, experimental designs, and non-experimental studies. In contrast, in confirmatory investigations, one conducts the study to test an existing theory using hypotheses established a priori. These designs include naturalistic inquiry and quantitative explanatory studies, such as surveys (Tashakkori & Teddlie, 1998).\n",
              "\n",
              "3.2.4\n",
              "\n",
              "Mixing Strategies\n",
              "\n",
              "Mixing or integrating methods and data is the core value of mixed-methods research because, by doing so, one can gain insights from multiple methods (Fielding, 2012). Further, one should consider the decisions regarding what types of data one integrates and how one integrates those data when designing a mixedmethods study. Teddlie and Tashakkori (2009) propose two dimensions of mixing strategies: fully mixed methods and partially mixed methods. A fully mixed-methods design involves using both qualitative and quantitative research across all components of a study (e.g., objective, type of data and operations, type of\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "445\n",
              "\n",
              "analysis, type of inference) (Leech & Onwuegbuzie, 2009). A fully mixed-methods design (also known as a mixed-model design) represents the highest degree of mixing paradigms in which one mixes the qualitative and quantitative paradigms at all or many steps of the study (Tashakkori & Teddlie, 1998). In contrast, a partially mixed-methods design involves conducting a study in which one mixes the quantitative and qualitative portions of the study at specific stages, such as at the sampling, data-collection, data-analysis, or data-inference stages (Teddlie & Tashakkori, 2009). In this design, one could mix their study’s quantitative and qualitative portions in a parallel manner, across chronological phases of the study, or across multiple levels of analysis (Teddlie & Tashakkori, 2009).\n",
              "\n",
              "3.2.5\n",
              "\n",
              "Time Orientation\n",
              "\n",
              "Based on its time orientation, one can categorize mixed-methods research into two types: sequential and concurrent. In sequential mixed-methods designs, researchers typically conduct one strand of the study (e.g., qualitative) first and then the other strand of the study (e.g., quantitative) (Creswell, 2003). The sequence depends on the objective of the study and the research questions. Creswell et al. (2003) propose three types of sequential mixed-methods designs: 1) sequential explanatory (i.e., this design is characterized by conducting the study’s quantitative phase followed by its qualitative phase), 2) sequential exploratory (i.e., this design is characterized by conducting the study’s qualitative phase followed by its quantitative phase), and 3) sequential transformative (i.e., one may prioritize either the quantitative or the qualitative phase and one will generally use a theoretical lens as an overarching perspective in the design that contains both quantitative and qualitative components to guide the study). A concurrent mixed-methods design is characterized by conducting the study’s qualitative and quantitative components during the same stage (Castro, Kellison, Boyd, & Kopak, 2010). This design uses both qualitative and quantitative data and analyses in independent strands to answer the research questions (Teddlie & Tashakkori, 2006). Creswell et al. (2003) identify three types of concurrent mixed-methods designs: 1) concurrent triangulation (i.e., using both qualitative and quantitative data to accurately define relationships among variables of interest), 2) concurrent nested (i.e., a type of design in which one collects both qualitative and quantitative data concurrently but still gives one type of data weight over the other), and 3) concurrent transformative design (i.e., a type of design used to provide support for various perspectives in the context of social change or advocacy). One’s research questions and purposes for conducting mixedmethods research influence the decision associated with time orientation. For example, if one conducts a study to understand a phenomenon as it occurs, one should employ a concurrent mixed-methods design (Venkatesh et al., 2013). In contrast, if one conducts a study to identify and test theoretical constructs in a new context, one should employ a qualitative study followed by a quantitative study (Venkatesh et al., 2013).\n",
              "\n",
              "3.3\n",
              "\n",
              "Step 3: Develop Strategies for Collecting and Analyzing Mixed-methods Data\n",
              "\n",
              "After researchers have made the primary design decisions associated with strands/phases of research, design investigation strategies, priority of methodological approach, mixing strategies, and time orientation, they need to develop a set of strategies for collecting and analyzing mixed-methods data. Before collecting data for their study, researchers should decide on the strategy to select the participants and the number of participants (i.e., sampling design strategies) (Collins, 2010).\n",
              "\n",
              "3.3.1\n",
              "\n",
              "Sampling Design Strategies\n",
              "\n",
              "Sampling is an important step in a research process because it helps determine the inference quality that researchers make and influences the degree to which one can generalize the findings to other individuals, groups, or contexts (Collins, Onwuegbuzie, & Jiao, 2007). In mixed-methods investigations, researchers must make sampling decisions for both the qualitative and quantitative components of the study. Teddlie and Yu (2007) propose five different types of mixed-methods sampling strategies: 1) basic, 2) sequential, 3) concurrent, 4) multilevel, and 5) multiple. To the same end, Onwuegbuzie and Collins (2007) develop a framework for formulating sampling decisions in mixed-methods research based on 1) the time orientation of the component (i.e., simultaneous or sequential) and 2) the relationship between the qualitative and quantitative samples (i.e., identical versus parallel versus nested versus multilevel). Onwuegbuzie and Collin’s framework is similar to Teddlie and Yu’s strategies to the degree that one can categorize them into either sequential or concurrent mixed methods. We discuss four types of mixed-methods sampling designs by integrating these two typologies: basic, sequential, concurrent, and multiple sampling designs. Basic mixed-methods sampling strategies typically include probability sampling (i.e., researchers randomly select the sampling units that are representative of the population) (Collins, 2010), stratified purposive\n",
              "Volume 17 Issue 7\n",
              "\n",
              "\n",
              "446\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "sampling (i.e., researchers first divide the group of interest into strata and then select a small number of cases to study intensively in each strata using a purposive sampling technique), and purposive random sampling (i.e., researchers take a random sample of a small number of units from a much larger target population) (Teddlie & Yu, 2007). Probabilistic sampling designs are generally associated with quantitative studies, whereas purposive sampling designs are associated with qualitative studies (Collins, 2010), and one can use both probabilistic and purposive sampling in quantitative and qualitative studies (Onwuegbuzie & Collins, 2007). Sequential sampling strategies typically involve using methodology and results from the first strand to inform the methodology employed in the second strand (Teddlie & Yu, 2007). According to Onwuegbuzie and Collins (2007), one can categorize sequential mixed-methods sampling designs based on their sampling strategies: 1) identical samples—the same sample members participate in both the qualitative and qualitative phases of the investigation, 2) parallel samples—the samples for the quantitative and qualitative components of the study are different but drawn from the same underlying population, 3) nested samples— the sample members selected for one phase of the study represent a subset of those participants chosen for the other component of the study, and 4) multilevel samples design—involves using two or more sets of samples obtained from different levels of the study (Collins et al., 2007). Concurrent sampling strategies allow researchers to triangulate the results from the separate quantitative and qualitative components of their research (Teddlie & Yu, 2007) and confirm, cross-validate, or corroborate their findings in a single study (Creswell et al., 2003). Like the sequential sampling designs, one can categorize the concurrent mixed-methods sampling strategies into four types of designs (see previous paragraph). Finally, multiple sampling strategies generally involve using more than one sampling technique, such as integrating a stratified purposive sampling with concurrent mixed-methods sampling (Teddlie & Yu, 2007).\n",
              "\n",
              "3.3.2\n",
              "\n",
              "Data-collection Strategies\n",
              "\n",
              "One can categorize data-collection strategies in mixed-methods research based on their degree of predetermined nature, their use of closed- (e.g., a set of questions about users’ attitude toward a particular technology) and open-ended questions (e.g., conducting an interview in which individuals can talk openly about a topic), and their focus for numeric versus non-numeric data analysis (Creswell, 2003). Mixedmethods data-collection strategies can be either quantitative (involves relatively planned “instruments” or predetermined questions for collecting data) or qualitative (mostly unstructured methods of collecting data for measurement or observation) (Tashakkori & Teddlie, 1998). Also, the type of data may be numeric or text, audio recording of participants’ voice, or written notes (Creswell, 2003). In a mixed-methods study, one must recognize that those data-collection strategies have their limitations and their strengths (Johnson & Turner, 2003). Therefore, researchers can use the strengths of one method to overcome the weaknesses of another method by using both in a research study (Johnson & Onwuegbuzie, 2004).\n",
              "\n",
              "3.3.3\n",
              "\n",
              "Data-analysis Strategies\n",
              "\n",
              "Based on the order of data analysis, one can use three strategies to analyze data in mixed-methods research: 1) concurrent mixed analysis (one analyzes both qualitative and quantitative data simultaneously), 2) sequential qualitative-quantitative data analysis (one analyzes qualitative data then quantitative data), and 3) sequential quantitative-qualitative data analysis (one analyzes quantitative data then qualitative data) (Tashakkori & Teddlie, 1998). One can use several analysis tools or methods for analyzing mixed-methods data (e.g., data reduction, data transformation, data correlation) (see Johnson & Onwuegbuzie, 2004). One of the most common dataanalysis practices is data conversion or transformation (i.e., one converts qualitative data into numerical codes that one can represent statistically (quantized), or one converts quantitative data into narrative data that one can analyze qualitatively (qualitized)) (Teddlie & Tashakkori, 2009). One can quantize qualitative data to integrate them with quantitative data to “answer research questions or test hypotheses addressing relationships between independent variables and dependent variables” (Fielding, 2012, p. 126). The quantizing practice also provides useful information by obtaining the numerical values of observations in addition to researchers’ narrative descriptions (Onwuegbuzie & Johnson, 2006; Sandelowski, 2000).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "447\n",
              "\n",
              "In contrast, one can adopt qualitizing techniques if one seeks to extract more information from quantitative data or to confirm interpretations of those data (Sandelowski, 2000). We have fewer examples of qualitizing data than those of quantizing data (Teddlie & Tashakkori, 2009). As Creswell and Plano Clark (2007, p. 188) in Teddlie and Tashakorri (2009) note: “More work needs to be done to expand the techniques for quantifying qualitative data and to develop the analysis options for such transformed data. Writers have written even less about transforming quantitative data into qualitative data. This area is ripe for researcher innovation and future research.”. One possible qualitizing technique is to take a distribution of numeric data on a single variable and then generate separate narrative categories based on the ranges of values in that distribution (i.e., cluster analysis) (Teddlie & Tashakkori, 2009). Joseph, Ng, Koh, and Ang (2012) in their study on IS career histories is one example of IS research that has used a qualitizing technique. The researchers used quantitative cluster analysis to identify distinct career paths in their quantitative data. Their analysis yielded three clusters of IS career paths: information technology, professional labor market, and secondary labor market career. This type of qualitizing is called narrative profile formation because it involves constructing qualitative profiles from quantitative data (Teddlie & Tashakkori, 2009). In general, researchers may plan a decision to transform data before conducting their study, but they generally do it after collecting data (Teddlie & Tashakkori, 2006). For example, in monostrand mixedmethods designs, researchers usually plan data transformation prior to the study because they generally collect only one type of data (either qualitative or quantitative data) and convert that type of data into the other and analyze them accordingly (Teddlie & Tashakkori, 2006). Researchers can also do data transformation in multistrand designs depending on their methodological approach and/or the findings from each phase of their study. For example, if researchers prioritize collecting and analyzing qualitative data, they should perform a quantizing technique to help explain the qualitative results (Creswell, Fetters, & Ivankova, 2004). However, if one believes that the results of each strand of research are sufficient (based on the theoretical concepts), transforming the data might not significantly contribute to the findings. In most cases, data transformation occurs serendipitously (Teddlie & Tashakkori, 2009). For example, researchers may determine that their interview data reveal emerging patterns that they can convert into numerical forms and analyze quantitatively. This practice allows researchers to more thoroughly analyze the data and, thereby, strengthen the inference quality (Teddlie & Tashakkori, 2006). Whereas transforming data in mixed-methods research has several benefits, it also has several limitations and challenges. First, although qualitizing techniques can help researchers gain more insights from their quantitative data, one should use qualitizing techniques cautiously because such techniques might represent an overgeneralization of the observed numeric data (Teddlie & Tashakkori, 2009). It is also possible that profiles emerging from qualitizing techniques yield an unrealistic representation (Sandelowski, 2000). Second, data transformation might cause one to lose depth and flexibility of data interpretation (Driscoll, ApiahYeboah, Salib, & Rupert, 2007). Qualitative data are generally multidimensional (i.e., they can provide insights into a host of interrelated conceptual themes during analysis). These themes are also flexible (i.e., researchers can revisit them during analysis in an iterative analytical process to help them recognize emergent patterns) (Bazeley, 2004). However, quantized data are usually fixed and unidimensional—they comprise a single set of responses that represent a conceptual category determined prior to data collection (Driscoll et al., 2007). To overcome this limitation, researchers have to be able to switch back and forth from a qualitative lens to a quantitative lens by revisiting qualitative data components associated with significant statistical findings (Driscoll et al., 2007). Further, researchers should always assess the conversion legitimation when their data analysis and designs involve data transformation (Onwuegbuzie & Johnson, 2006). The third challenge of data transformation comes from a quantitative research perspective. Quantitative researchers argue that quantized data are vulnerable to the problem of multicollinearity, wherein response categories are themselves linked to one another as a result of the coding strategy (Driscoll et al., 2007). Further, the need to collect and analyze qualitative data can force researchers to reduce their sample size, which can limit the kinds of statistical procedures that they can use to analyze data (Driscoll et al., 2007). To overcome the collinearity issue, researchers can use available statistical remedies (e.g., separating dichotomized codes derived from a single open-ended question in subsequent statistical analysis) (Driscoll et al., 2007). Moreover, if researchers cannot collect a sufficient sample size for accurate estimation, they should avoid doing data transformation.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "448\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "3.4\n",
              "\n",
              "Step 4: Draw Meta-inferences from Mixed-methods Results\n",
              "\n",
              "Developing high-quality meta-inferences depends on the quality of the data analysis in a study’s qualitative and quantitative components (Venkatesh et al., 2013). Given that meta-inferences are generally theoretical statements about a phenomenon, including its interrelated components and boundary conditions, the process of developing inferences is conceptually similar to the process of developing theory from observation (Venkatesh et al., 2013). Thus, one can develop inferences inductively, deductively, or abductively depending on the existence of theoretical foundations or conceptual frameworks underlying the study (Morse, 2010).\n",
              "\n",
              "3.4.1\n",
              "\n",
              "Theoretical Reasoning\n",
              "\n",
              "When researchers use a mixed-methods approach to examine their research questions, they generally switch between different modes of generalizability (Tashakkori & Teddlie, 1998). One can categorize these differences in generalizability concerns into four modes: inductive reasoning, deductive reasoning (Tashakkori & Teddlie, 1998), the combination of inductive and deductive reasoning (Tashakkori & Teddlie, 1998), and abductive reasoning (Van de Ven, 2007). In inductive reasoning, researchers generally gather data from specific instances to build up a theory. Thus, inductive reasoning involves generalizing a theory confirmed in one specific setting to another context as the theory evolves (Tashakkori & Teddlie, 1998). In general, one uses inductive theoretical reasoning in qualitative studies (Merriam, 1998). However, although qualitative studies mostly adopt inductive reasoning, some adopt deductive reasoning processes (Creswell, 2003). In deductive reasoning, researchers generally predict outcomes that are supposed to occur in a theoretical population. Thus, deductive reasoning involves making generalizations from a specific sample that one uses for that theoretical population (Tashakkori & Teddlie, 1998). Although a particular study may adopt either deductive or inductive theoretical reasoning, one will likely use both types of theoretical reasoning simultaneously in developing meta-inferences (Miller, 2003; Tashakkori & Teddlie, 1998). According to pragmatism, mixed-methods researchers can select both the inductive and deductive logic and use them simultaneously in the course of conducting research that focuses on addressing research questions (Tashakkori & Teddlie, 1998). Finally, in abductive reasoning, (Van de Ven, 2007), researchers make a logical connection “between data and theory” and often use it to theorize “about a surprising event” (Feilzer, 2010, p. 10). In this reasoning, researchers move back and forth between theories and data: they “first convert observations into theories and then assess those theories through action” (Morgan, 2007, p. 71). This type of reasoning requires using different approaches to theory and data and offers great opportunity to triangulate inferences developed from qualitative and quantitative research (Feilzer, 2010; Morgan, 2007). Developing meta-inferences depends on research questions, specific methods employed, and empirical domains under investigation (Erzberger & Kelle, 2003). Erzberger and Kelle (2003) suggest that researchers should always look for sufficient empirical evidence for their theoretical statements and avoid any additional assumptions that they cannot examine with the help of empirical data. Given that the most important step in mixed-methods research is triangulating the results (i.e., findings, inferences) from the qualitative and quantitative studies into a coherent conceptual framework that provides an effective answer to one’s research questions, one needs to properly develop good inferences in each strand of the study. In qualitative research, a good inference should “capture the meaning of the phenomenon under consideration for study participants” (Teddlie & Tashakkori, 2009, p. 295). A good qualitative inference is a credible inference; that is, “there is a correspondence between the way respondents actually perceive social constructs and the way researchers portray their overviews” (Mertens, 2005, p. 254). Venkatesh et al. (2013) summarize a variety of techniques for evaluating and enhancing the quality of inferences in qualitative research (i.e., design validity, analytical validity, and inferential validity). We discuss more details about these types of quantitative validities in Section 4. In quantitative research, a good inference has the following characteristics: 1) it establishes relations between variables and provides reasonable certainty that such relationships do not happen by chance; 2) its intensity matches the demonstrated magnitude of the relationship between variables, which the results of analyzing the data support; and 3) it is free of systematic bias in interpreting the results (Teddlie & Tashakkori, 2009). One can use some validity criteria, such as statistical conclusion validity, internal validity, construct validity and external validity, to evaluate the quality of quantitative inferences (Venkatesh et al., 2013). We discuss more details about these types of quantitative validity in Section 4.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "449\n",
              "\n",
              "Findings from mixed-methods research have three possible patterns: divergence, convergence, and complementarity (Erzberger & Kelle, 2003). If the qualitative and quantitative methods applied in the study lead to divergent results (i.e., the qualitative and quantitative results contradict each other), two possible explanations exist: either the divergence is the result of methodological mistakes or the initial theoretical assumptions are incorrect (Erzberger & Kelle, 2003). One should modify and revise theoretical assumptions as a consequence of divergent findings carefully. Researchers have to formulate ad-hoc hypotheses based on already-collected empirical data that may lead them to retain their initial theories and formulate “farreaching speculations that lack a sound empirical basis” (Erzberger & Kelle, 2003, p. 483). These newly developed hypotheses must increase the empirical content of the initial theoretical assumptions without diminishing their consistency, or these hypotheses must improve the consistency of the initial theory without losing empirical content. One also needs to empirically test the newly developed hypotheses using new data, and the newly developed hypotheses should be adaptable to other well-established theories about the phenomena under investigation (Erzberger & Kelle, 2003). If the divergence results from methodological mistakes, researchers must engage in a re-examination process to assess whether the divergent findings are associated with the quality issues in one or more of the methods used or if they suggest a greater complexity inherent in the phenomenon under study (da Costa & Remedios, 2014). If the quantitative and qualitative methods lead to convergent results (i.e., the qualitative and quantitative methods lead to the same results), then the integration may provide good arguments for the quality of the inferences and strengthen the initial theoretical assumptions (Erzberger & Kelle, 2003). Finally, if a mixedmethods approach leads to complementary results (i.e., the qualitative and quantitative results relate to different objects or phenomena but may complement each other), then the integration provides a more complete picture of the empirical domain under study (Erzberger & Kelle, 2003).\n",
              "\n",
              "3.5\n",
              "\n",
              "Step 5: Assess the Quality of Meta-inferences\n",
              "\n",
              "To maximize the quality of meta-inferences drawn from the qualitative and quantitative components, one must examine inference quality, including design quality, explanatory quality, and other legitimation criteria.\n",
              "\n",
              "3.5.1\n",
              "\n",
              "Inference Quality\n",
              "\n",
              "One assesses the quality of meta-inferences by simultaneously examining the design quality (i.e., the degree to which a researcher has selected the most appropriate procedures for answering the research questions) and the explanatory quality (i.e., the degree to which one has made credible interpretations based on the obtained results) (see Tashakkori & Teddlie, 2010; Teddlie & Tashakkori, 2003; Venkatesh et al., 2013). Appendix B defines the different types of inference quality. In addition to design and explanatory quality, Onwuegbuzie and Johnson (2006) propose a typology including nine mixed-methods legitimation types: 1) sample integration, 2) inside-outside, 3) weakness minimization, 4) sequential, 5) conversion, 6) paradigmatic mixing, 7) commensurability, 8) multiple validities, and 9) political legitimation. Whereas Tashakkori and Teddlie’s (2010) quality framework assumes legitimation as an outcome that revolves around inference quality, Onwuegbuzie and Johnson’s typology views legitimation as a continuous process that one should evaluate at each stage of the mixed-research process. By bringing together Tashakkori and Teddlie’s (2010) concept of inference quality and Onwuegbuzie and Johnson’s nine aspects of legitimation, one can extensively assess the quality of a mixed-methods study by not only using the appropriate qualitative and quantitative quality standards but also applying the quality criteria that address the entire mixed-methods study. Sample integration legitimation applies to situations in which researchers aim to make statistical generalizations from a sample population to a larger population (Onwuegbuzie & Johnson, 2006). Insideoutside legitimation refers to “the extent to which the researcher accurately presents and appropriately utilizes the insider’s view and the observer’s views for purposes, such as description and explanation” (Onwuegbuzie & Johnson, 2006, p. 57). Weakness minimization legitimation refers to “the extent to which the weakness from one approach is compensated by the strengths from the other approach” (Onwuegbuzie & Johnson, 2006, p. 57). Sequential legitimation refers to “the extent to which one has minimized the potential problem wherein the meta-inferences could be affected by revising the sequence of the quantitative and qualitative phases” (Onwuegbuzie & Johnson, 2006, p. 57). To assess sequential legitimation, researchers can change the sequential design to a multiple wave design (i.e., one collects and analyzes the qualitative and quantitative data multiple times) (Onwuegbuzie & Johnson, 2006; Sandelowski, 2003). Conversion refers to the extent to which quantizing and qualitizing lead to interpretable data and high inference quality. Paradigmatic mixing legitimation refers to the extent to which researchers successfully\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "450\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "combine and blend their paradigmatic assumptions underlying the qualitative and quantitative approaches “into a usable package” (Onwuegbuzie & Johnson, 2006, p. 57). To meet commensurability legitimation, mixed-methods researchers need to be able to make Gestalt switches (i.e., to switch back and forth from a qualitative lens to a quantitative lens). This iterative process can create a viewpoint separate from and goes beyond what either a qualitative or quantitative viewpoint alone provides. Multiple validities legitimation refers to the extent to which one uses all relevant research strategies and the study meets multiple relevant validity criteria. Political legitimation, the last legitimation type, refers to “the extent to which consumers of mixed methods research value the meta-inferences stemming from both the qualitative and quantitative components of a study” (Onwuegbuzie & Johnson, 2006, p. 57). One of the strategies to achieve this legitimation is to use multiple perspectives and to generate practical theories or results that consumers will value because the results answer important questions and provide practical solutions (Onwuegbuzie & Johnson, 2006). Based on our discussion regarding the development and validation of inferences in mixed-methods research in steps 4 and 5, we summarize the general guidelines for developing high-quality meta-inferences in mixedmethods research in Table 3.\n",
              "\n",
              "3.6\n",
              "\n",
              "Step 6: Discuss Potential Threats and Remedies\n",
              "\n",
              "One can use the legitimation framework that Onwuegbuzie and Johnson (2006) propose and that we discuss previously to identify the quality threats that may potentially compromise the credibility of metainferences 5. Given that threats to inference quality may vary depending on the types of design decisions one uses, we discuss more details about these threats in Section 4.\n",
              "\n",
              "3.7\n",
              "\n",
              "Model of Decision Choice for Conducting Mixed-methods Research\n",
              "\n",
              "To provide guidance for mixed-methods researchers in selecting the most suitable designs for their studies, we develop a decision tree to map the flow and relationship among the design strategies. Figures 1-4 present the decision tree depicting various design decisions that mixed-methods researchers have to make. The rectangles represent basic steps or process and design options in a research project, the diamonds indicate design decisions that researchers need to make, the arrows represent relationships between design decisions and/or processes, and the numbers inside the boxes represent the steps in conducting mixedmethods research as Table 2 describes. Our decision tree also shows that, although mixed-methods research always starts with one or more research questions, one can approach the other decisions in any order (i.e., one need not address them linearly or unidirectionally), and sometimes one can revise questions and/or purposes when needed (Johnson & Onwuegbuzie, 2004). Further, a decision at an earlier stage may or may not influence a decision at a later stage of research. For example, the decision associated with strands or phases of research influences the decision related to data collection and analysis; however, mixing strategies do not necessarily influence the decision associated with time orientation.\n",
              "\n",
              "5\n",
              "\n",
              "Onwuegbuzie (2003) identifies 22 threats to internal validity in quantitative research (e.g., history, maturity, testing) and 12 threats to external validity (e.g., population validity, ecological validity, multiple treatment interference) at the data-collection stage. Onwuegbuzie identifies 21 threats (e.g., statistical regression, multicollinearity, violated assumptions) and five threats (e.g., matching bias, researcher bias) to internal validity and external validity at the data-analysis stage. Finally, Onwuegbuzie identifies seven and three threats to internal validity and external validities (respectively) at the data-interpretation stage (see Onwuegbuzie & Johnson, 2006). Further, Onwuegbuzie and Leech (2007) identify 14 threats to external credibility (e.g., catalytic validity, communicative validity, action validity) and 15 threats to internal credibility (e.g., observational bias, researcher bias, confirmation bias) in qualitative research.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "451\n",
              "\n",
              "Table 3. Guidelines for Developing Inferences and Meta-inferences Component Guidelines 1. In making inferences, keep the research purposes and research questions in the foreground when analyzing and interpreting data. 2. If one investigates more than one research question, state each question separately and examine or summarize all of the results that are relevant to that question. 3. Review the statistical results, text information, field notes, and summary notes from the literature reviews. 4. Make tentative interpretations about each part of the results to address each research question. 5. After going through several iterations of interpretations, examine the answers to the questions or the interpretation to see if they can be combined. Compare, contrast, combine, or try to explain differences.\n",
              "\n",
              "General guidelines\n",
              "\n",
              "1. In qualitative research, inferences should capture the meaning of phenomena under consideration for the participants. 2. Inferences should be made based on of qualitative data-analysis results. Qualitative inferences 3. Research questions and design decisions will influence the theoretical reasoning technique (i.e., deductive versus inductive) that researchers use to develop qualitative inferences. 4. Use the appropriate qualitative standards to assess the quality of qualitative inferences. 1. Inferences should establish relationships between variables while providing reasonable certainty that such relationships do not happen by chance. 2. Inferences should be made based on quantitative data analysis. Quantitative inferences 3. Inferences should be free of systematic bias in interpreting the results. 4. Use the appropriate quantitative standards to assess the quality of quantitative inferences. 1. In mixed-methods research, the quality of inferences depends on the strength of inferences that emerge from the study’s qualitative and quantitative strands. 2. To develop meta-inferences in mixed-methods research, one can use inductive, deductive, both inductive and deductive, or abductive theoretical reasoning. 3. Meta-inferences must directly address the initial and intended purposes for using mixed methods. 4. Researchers’ study designs also influence their inferences. For example, in sequential mixed designs, researchers have to determine the purpose at the beginning of the study, or it might emerge from the inferences of the first strand. 5. One should assess the quality of meta-inferences made based on qualitative and quantitative inferences using design quality and explanatory quality (see Appendix B). One should also address other relevant legitimation types, such as sample integration legitimation, inside-outside legitimation, and conversion legitimation. 6. Possible patterns of mixed-methods research findings include: divergence, convergence, and complementarity. If the results diverge, one needs to identify the cause and reexamine the results. If the results converge, then the integration may provide a good argument for inference quality. If the results complement one other, one needs to use two or more methods to investigate the phenomenon under study.\n",
              "\n",
              "Meta-inferences\n",
              "\n",
              "Note: we primarily adapted these guidelines from Teddlie and Tashakkori (2009) and Erzberger and Kelle (2003)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "452\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Figure 1: Model of Decision Choice for Conducting Mixed-methods Research (Step 1)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "453\n",
              "\n",
              "Figure 2: Model of Decision Choice for Conducting Mixed-methods Research (Step 2)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "454\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Figure 3: Model of Decision Choice for Conducting Mixed-methods Research (Step 3)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "455\n",
              "\n",
              "Figure 4: Model of Decision Choice for Conducting Mixed-methods Research (Steps 4, 5, & 6)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "456\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "4\n",
              "\n",
              "An Illustrative Study\n",
              "\n",
              "With the 14 classification dimensions of mixed-methods research we discuss in Section 3, studies can involve a mixed-methods approach in many possible ways. In this section, we illustrate one possible type of mixed-methods study in depth. We apply the guidelines we discuss previously to examining factors that influence technology adoption in households. For this illustration, we re-analyzed the qualitative data from Venkatesh and Brown (2001) and the quantitative data from Brown and Venkatesh (2005) using a mixedmethods research approach. Table 4 summarizes this illustrative study. We also include references to the model of decision choice (Figures 1 to 4) in Table 4. In Sections 4.1 to 4.8, we discuss the mixed-methods study in detail. We discuss each step of the study, which includes our selecting the design and our applying the mixed-methods guidelines we present in Section 3.\n",
              "\n",
              "4.1\n",
              "\n",
              "Step 1: Decide on the Appropriateness of a Mixed-methods Approach\n",
              "\n",
              "In reporting the appropriateness of a mixed-methods approach, researchers need to describe why a mixedmethods study is necessary. Researchers should start with and clearly state their research questions and then the purposes of mixed-methods research (Leech, 2012). Further, they need to state their study’s epistemological assumptions. The illustrative study addresses three research questions: one qualitative research question, one quantitative research question, and one mixed-methods research question. Although Venkatesh and Brown (2001) frame their study with objectives, we can translate them into the following question: “What are the factors that determine household PC adoption among adopters and non-adopters?” This research question was addressed in study 1. The qualitative component in this research question is broad (identifying the adoption factors) but specific enough to focus on the issue of technology adoption in households. Prior literature, at the time of the original research activities (late 1990s), did not provide adequate foundation for understanding IT adoption in households. For this reason, using qualitative data to answer this exploratory question was appropriate. Using data collected by Venkatesh and Brown, we employed a quantizing method to transform the qualitative data. We addressed the quantitative research question from Brown and Venkatesh’s (2005) paper 6 in the second study (study 2); that is: “Does the model of adoption of technology in households (MATH) explain household adoption and non-adoption of PCs?”. Brown and Venkatesh (2005) addressed this question using a survey methodology to operationalize the constructs identified in the qualitative phase of the study and empirically test MATH. We investigated the following mixed-methods question: “In what way do the results from the quantitative data collection (study 2) support or refute the results from the qualitative data collection (study 1)?”. We state our mixed-methods research question using a procedural focus—it explicitly directs the procedures for mixing the strands of a mixed-methods study and is tied to the specific design being used (Plano Clark & Badiee, 2010). This mixed-methods question focuses on the need to triangulate the findings from the study’s qualitative and quantitative phases. Because we can address the quantitative research question only after answering the qualitative research question, the questions depended on each other. Further, we can conduct triangulation only after addressing both the qualitative and quantitative research questions. The relationship of our research questions to the research process was predetermined—that is, we stated the questions at the beginning of the study based on our understanding of the literature.\n",
              "\n",
              "6\n",
              "\n",
              "Brown and Venkatesh (2005) also address the second research question: “Does the inclusion of the household lifecycle components improve MATH?”. However, this question was not relevant for the illustrative study because study 2 tested the model (i.e., MATH) developed in study 1, which did not include the household lifecycle components. Thus, we do not discuss it here.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "457\n",
              "\n",
              "Table 4. Summary of the Illustrative Study Decision consideration Other design decision(s) likely to affect current decision Design decision and reference to the decision tree Identify the research questions (Decision tree: Figure 1, #1A) • We wrote the qualitative and quantitative research questions separately first and a mixed-methods research question second. • The qualitative research question was: “What are the factors that determine household PC adoption among adopters and non-adopters?”. • The quantitative research question was: “Does MATH explain household adoption and non-adoption of PCs?”. • The mixed-methods research question was “In what way do the results from quantitative data collection (study 2) support or refute the results from qualitative data collection (study 1)?”. • We wrote the research questions in the question format. • The quantitative research question depended on the results of the qualitative research question. The mixed-methods question depended on the results of both qualitative and quantitative research questions. • The relationship between the questions and the research process is predetermined.\n",
              "\n",
              "Property\n",
              "\n",
              "Research questions\n",
              "\n",
              "Qualitative or quantitative method alone was not adequate for addressing the None research question. Thus, we used a mixed-methods research approach.\n",
              "\n",
              "Step 1: decide on the appropriateness of mixedmethods research\n",
              "\n",
              "• Mixed-methods research helps researchers seek convergence or corroboration of results from Purposes of different Research mixed-methods methods. questions research • We used mixedmethods research to obtain complementary views of the same phenomenon. Both qualitative and quantitative components of the Epistemological study used the perspective same paradigmatic assumptions.\n",
              "\n",
              "Corroboration/confirmation with an emergent element of complementarity. (Decision tree: Figure 1, #1B)\n",
              "\n",
              "Research questions, Single paradigm stance. (Decision tree: Figure 1, #1C) purposes of mixed methods\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "458\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table 4. Summary of the Illustrative Study Decision consideration The researchers believed in the importance of research questions and embraced various methodological approaches from different worldviews. The mixedmethods study aimed to develop and test a theory. Other design decision(s) likely to affect current decision Design decision and reference to the decision tree\n",
              "\n",
              "Property\n",
              "\n",
              "Paradigmatic assumptions\n",
              "\n",
              "Research questions, purposes of mixed methods\n",
              "\n",
              "Pragmatism (we used positivism in both qualitative and quantitative components of the study). (Decision tree: Figure 1, #1D)\n",
              "\n",
              "Design investigation strategy\n",
              "\n",
              "Research questions, paradigmatic assumptions\n",
              "\n",
              "• Study 1: exploratory investigation. • Study 2: confirmatory investigation. (Decision tree: Figure 2, #2A)\n",
              "\n",
              "The study Strands/phases involved multiple of research phases. The qualitative and quantitative Step 2: develop components of the strategies for Mixing strategy study were mixed mixed-methods at the dataresearch analysis and inferential stages. designs We started with the qualitative Time orientation phase, followed by the quantitative phase. The qualitative Priority of and quantitative methodological components were approach equally important. The samples for the quantitative and qualitative components of the study differed, but they came from the same underlying population. • Qualitative data collection in study 1. • Quantitative data collection in study 2.\n",
              "\n",
              "Purposes of Multistrand design. mixed-methods (Decision tree: Figure 2, #2B) research Purposes of mixed-methods Partially mixed methods. research, (Decision tree: Figure 2, #2C) strands/phases of research Research questions, Sequential (explanatory) design. strands/phases (Decision tree: Figure 2, #2D) of research Research questions, Equivalent status design. strands/phases (Decision tree: Figure 2, #2E) of research\n",
              "\n",
              "Step 3: develop strategies for collecting and analyzing mixed-methods data\n",
              "\n",
              "Sampling design strategies\n",
              "\n",
              "Design investigation strategy, time orientation\n",
              "\n",
              "Probability sampling with sequential design using parallel samples. (Decision tree: Figure 3, #3A)\n",
              "\n",
              "Data collection strategies\n",
              "\n",
              "• Study 1: closed- and open-ended Sampling questioning (i.e., Venkatesh and Brown design (2001) drew the methodology employed in strategies, time study 1 from the concepts of qualitative orientation, interviewing). strands/phases • Study 2: closed-ended questioning (i.e., of research traditional survey design). (Decision tree: Figure 3, #3B)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "459\n",
              "\n",
              "Table 4. Summary of the Illustrative Study Decision consideration • We analyzed the qualitative data quantitatively. • We analyzed the qualitative data first and the quantitative data second. Other design decision(s) likely to affect current decision Design decision and reference to the decision tree\n",
              "\n",
              "Property\n",
              "\n",
              "Data analysis strategy\n",
              "\n",
              "Time orientation, data collection strategy, Sequential qualitative-quantitative analysis. strands/phases (Decision tree: Figure 3, #3C) of research\n",
              "\n",
              "Step 4: draw meta-inferences Types of from mixedreasoning methods results\n",
              "\n",
              "In our analysis, we focused on Designdeveloping and investigation testing/confirming strategy hypotheses. • The qualitative inferences met the appropriate qualitative standards. • The quantitative inferences met the appropriate quantitative standards. • We assessed the quality of meta-inferences. We discussed all potential threats to inference quality and provided remedies. Mostly primary design strategies, samplingdesign strategies, data-collection strategies, data-analysis strategies, type of reasoning\n",
              "\n",
              "Inductive and deductive theoretical reasoning. (Decision tree: Figure 4, #4)\n",
              "\n",
              "Step 5: assess Inference the quality of quality meta-inferences\n",
              "\n",
              "Design and explanatory quality; sample integration; inside-outside; weakness minimization; conversion; multiple validities. (Decision tree: Figure 4, #5A & 5B)\n",
              "\n",
              "Step 6: discuss Inference potential threats quality and remedies\n",
              "\n",
              "Data-collection Threats to sample integration; inside-outside; strategies, data conversion; and multiple validities. data-analysis (Decision tree: Figure 4, #6) strategies\n",
              "\n",
              "Based on our research questions, the primary purpose of our illustrative study is triangulation or corroboration/confirmation, with an emergent element of complementarity. We used qualitative and quantitative techniques to validate the results through triangulation, and we used both qualitative and quantitative data to produce a more complete understanding of PC adoption and use through complementarity. The complementarity purpose seeks to enhance, illustrate, or clarify results from one method type using results from other methods (Caracelli & Greene, 1993). Because we used the results from study 2 to test and confirm the results from study 1, we can consider complementarity as a secondary purpose of our illustrative study. In the illustrative study, we adopted a single paradigm perspective. The overall mixed-methods study adopted the pragmatism paradigm (i.e., it combined positivist qualitative data collection and analysis with the positivist quantitative data collection and analysis). Although the nature of data in study 1 is qualitative, pragmatists believe that one can conduct a qualitative study using the positivist paradigm. Given the nature of the qualitative data analysis and subsequent statistical analysis in study 1, we consider study 1 to be a positivist qualitative study. Similar to study 1, study 2 adopted the positivist paradigm. Using two different methods supported our triangulation purpose, that is, to corroborate results across studies. The illustrative study focused on MATH and empirically derived and validated this model for adopters and non-adopters to identify the factors that influence technology adoption in households. Venkatesh and Brown (2001) proposed MATH using the theory of planned behavior (TPB) (Ajzen, 1985, 1991) as the framework. According to the TPB, behavioral intention, which was the theory’s key dependent variable, is determined by attitude toward behavior, subjective norm, and perceived behavioral control. Using this framework, Venkatesh and Brown sought to understand and explain household PC adoption.\n",
              "Volume 17 Issue 7\n",
              "\n",
              "\n",
              "460\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "4.2\n",
              "\n",
              "Step 2: Develop Strategies for Mixed-methods Research Designs\n",
              "\n",
              "In this stage, we determined the strands/phases of research, design investigation strategy, priority of methodological approach, mixing strategy, and time orientation of the study. When reporting the strategies, researchers need to delineate why they used a mixed-methods research design (Leech, 2012). Consistent with the research questions and paradigmatic assumptions discussed previously, we characterized study 1 as a predominantly exploratory study: although Venkatesh and Brown (2001) drew the initial constructs’ definitions from previous literature, they derived the final MATH constructs from the qualitative data. Study 2 was a confirmatory quantitative study: we analyzed quantitative data and operations with statistical analysis and inference. To achieve the purposes of mixed-methods research, a mixed-methods multistrand design was the appropriate design because study 2 needed to validate the results of study 1. Based on the strategy of mixing, this study adopted a partially mixed-methods design in which mixing occurs at the data analysis and inferential stages. Given that both the qualitative and quantitative components of the study contributed equally to address the research questions, our illustrative study followed an equivalent status design. Further, based on our research questions, the study’s overall mixed-methods research design followed a sequential design approach in which findings from the qualitative study informed the quantitative study (see Creswell et al., 2003; Creswell & Plano Clark, 2007). Therefore, we needed to conduct the qualitative study before the quantitative study.\n",
              "\n",
              "4.3\n",
              "\n",
              "Step 3: Develop Strategies for Collecting and Analyzing Mixed-methods Data\n",
              "\n",
              "When writing a mixed-methods research report, much like all research reports, one should include enough information so that readers can fully understand how the researchers conducted the research (Gliner, Morgan, & Leech, 2009; Leech, 2012). Given that our mixed-methods study used a sequential mixedmethods design to develop and test MATH, our mixed-methods sampling-design strategy was a probability sampling with sequential design using parallel samples. The qualitative study was longitudinal: that is, it comprised data that Venkatesh and Brown (2001) collected from an initial interview of factors influencing purchase or use decisions and follow-up interviews six months after the initial interview to measure the dependent variables (i.e., purchase or use behaviors). Similarly, study 2 was longitudinal: that is, it comprised data Brown and Venkatesh (2005) collected from an initial survey to identify factors influencing purchase or use decisions and a follow-up survey six months after the initial survey to measure purchase behavior for those who did not own a PC at the time of the initial survey and use behavior for owners at the time of the initial survey. Thus, each study comprised two sub-samples: adopters and non-adopters. We used actual use behavior and purchase behavior as the dependent variables of adopters and non-adopters, respectively (see Brown & Venkatesh, 2005). Appendix C overviews the studies, sample sizes, and measurement timing. Brown and Venkatesh (2005) developed, pre-tested, and tested the scales for the MATH constructs. In writing a mixed-research report, researchers also need to describe and justify the analysis and explain how they combined and integrated their data sets. Consistent with our time-orientation, sampling-design, and datacollection strategies, we used a sequential qualitative-quantitative analysis design strategy with an emergent element of data-transformation technique (i.e., quantized) as our data-analysis strategy. In Sections 4.4 and 4.5, we discuss the qualitative and quantitative data analysis in study 1 and study 2, respectively.\n",
              "\n",
              "4.4\n",
              "\n",
              "Study 1 Data Analysis\n",
              "\n",
              "In our illustrative study, we re-analyzed Venkatesh and Brown’s (2001) data set 7 to examine not only its descriptive statistics but also its quantized data. An important component of the method in Venkatesh and Brown’s (2001) study was that, once respondents who were primary decision makers in households identified a particular factor, the interviewers asked them to indicate the degree to which that factor was important in their decision to adopt or not to adopt a PC for household use. This technique provided not only the factors that the coders derived from coding the qualitative data but also the associated magnitude of importance (Babbie, 1990; Stone, 1978).\n",
              "\n",
              "7\n",
              "\n",
              "Please see Venkatesh and Brown (2001) for full methodological details, including the interview script used to gather data.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "461\n",
              "\n",
              "4.4.1\n",
              "\n",
              "Coding and Data Transformation\n",
              "\n",
              "Venkatesh and Brown (2001) employed two individuals to code the qualitative data. They provided the coders with construct definitions from existing models as Miles and Huberman (1994) suggest. Each coder preliminarily analyzed 30 randomly selected participants’ responses. Each participant could have provided multiple responses (reasons) for each decision. After this first round of coding, Venkatesh and Brown brought the coders together to discuss their coding. They resolved inter-coder discrepancies via discussion. The coders then coded the remaining data and held out any responses that did not fit easily into any of the constructs. Consistent with Weber (1990), they coded a given response against each of the constructs to determine the fit of the response with the conceptual definitions of the constructs. Although traditional content coding relies on an existing, tested coding scheme, no such coding scheme existed for household adoption of PCs when Venkatesh and Brown (2001) conducted the study. Thus, they derived a coding scheme from the TPB framework (Venkatesh & Brown, 2001). For example, if the respondents indicated that entertainment was a factor that drove their decision to buy a PC for household use, the coder coded this response as “applications for fun” and as a hedonic outcome under an attitudinal belief structure. Appendix D provides other coding examples. The coding process revealed 13 key factors of technology adoptions in households (i.e., MATH). These factors include attitudinal beliefs (i.e., applications for personal use, utility for children, utility for work-related use, applications for fun, and status gains), normative beliefs (i.e., friends and family influences, secondary sources’ influences, and workplace referents’ influences), and control beliefs (i.e., fear of technological advances, declining cost, cost, perceived ease of use, and requisite knowledge). Appendix E presents the construct definitions of MATH. Venkatesh and Brown (2001) identified the key factors of technology adoption in households through a coding process and, in this illustrative study, we converted the qualitative data into quantitative data (i.e., quantized them). Appendix F reports the descriptive statistics and correlations of the quantized data analysis. Given that we elicited open-ended responses and measured the magnitude of importance on a five-point scale, each coded response and the associated importance resulted in a single indicator for a specific construct. Therefore, in this case, the indicator variables and latent variables had a one-to-one correspondence. Although using single indicators does pose a potential problem, such use of coded qualitative data and the corresponding magnitudes (quantitative data) was consistent with approaches that Babbie (1990) and Miles and Huberman (1994) suggest. We assessed the quantitative validity of quantized data using several different techniques (see Appendix G).\n",
              "\n",
              "4.4.2\n",
              "\n",
              "Qualitative Validation of Study 1\n",
              "\n",
              "Before we analyzed the quantized data, we needed to establish validity in the qualitative data-collection procedures. We report three types of validity as Venkatesh et al. (2013) discuss: 1) design validity, 2) analytical validity, and 3) inferential validity. Design validity comprises descriptive validity, credibility, and transferability. We established descriptive validity (i.e., the accuracy of what researchers report) by providing information about the research setting (see earlier discussion about data-collection strategies) (Maxwell, 1992). To ensure study 1’s credibility and transparency (i.e., the extent to which qualitative research’s results are credible and believable), Venkatesh and Brown (2001) collected data from a large random sample of households via telephone interviews. To ensure transferability (i.e., the degree to which one can generalize qualitative research results to other contexts), Venkatesh and Brown used a longitudinal study with two waves of measurement: an initial interview (during a three-week window in March/April 1997) and a follow-up interview six months later. Venkatesh and Brown compared the characteristics of the sample to the population in general and found that the random sample of households included in this study highly represented the population of American households (see Venkatesh & Brown, 2001). Analytical validity comprises theoretical validity and plausibility, dependability, and consistency. During the data collection, Venkatesh and Brown (2001) established theoretical validity and plausibility (i.e., the extent to which a study’s theoretical explanations and the findings fit the data and are, therefore, credible and defensible) by using a well-designed protocol to collect the data (Orlikowski, 1993). They first pre-tested the interview protocol to solicit comments and suggestions about the instrument from respondents (Venkatesh & Brown, 2001). With the pre-test, they also identified wording issues that they needed to address. During the interview, the interviewers asked every question and in the prescribed order per the interview protocol, which helped maintain data reliability and credibility (Judd, Smith, & Kidder, 1991). To promote theoretical validity, they also used an existing theory (i.e., TPB) along with several established research bases in technology adoption, customer behavior, and psychology as guiding frameworks for the proposed\n",
              "Volume 17 Issue 7\n",
              "\n",
              "\n",
              "462\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "theoretical model (see Venkatesh & Brown, 2001). Dependability in qualitative research emphasizes the need for the researcher to account for every change that occurs in the setting (Venkatesh et al., 2013). Venkatesh and Brown assessed dependability through inter-rater (or coder) reliability (IRR), which measures consistency in qualitative data-analysis procedures. The IRR was .89, which indicates a high degree of consistency. Venkatesh and Brown used triangulation to identify themes that several data sources shared and derived the coding schemes from an existing theoretical framework (Jick, 1979). To maintain consistency, Venkatesh and Brown trained 12 interviewers who had an average of 3.2 years’ experience in interviewing (including at least six months’ experience in telephone interviewing) on this particular interview protocol/script. Interviewers followed the same interview protocol/script for all the interviewees. Inferential validity comprises interpretive validity and confirmability. Little (if any) IS research has reported inferential validity. We believe that one needs to report this type of validity because qualitative researchers focus on not only validly describing the objects, events, and behaviors in the setting they study but also what these objects, events, and behaviors mean to the people engaged with them (Maxwell, 1992). In the original study, Venkatesh and Brown (2001) achieved interpretive validity (i.e., the degree to which researchers accurately understand participants’ views, thoughts, feelings, intentions, and experiences) by obtaining participant’s feedback during the interview. They also coded and reported the data as close as possible to participants’ accounts and interview transcripts and notes. They documented all procedures for checking and cross-checking the data throughout the study to ensure the qualitative study’s confirmability (i.e., the degree to which one can confirm or corroborate results with others) (Lincoln & Guba, 1985).\n",
              "\n",
              "4.4.3\n",
              "\n",
              "Results of Study 1\n",
              "\n",
              "Among users that already possessed a PC at the time of the initial survey (n = 201), we tested the model with use as the dependent variable. Appendix H presents the results of study 1. MATH with five key predictors—all three utilitarian outcomes, applications for fun, and status gains—explained 58 percent of the variance in use behavior. Appendix H also presents the results associated with the model for households that did not possess a PC at the time of the initial survey with follow-up purchase behavior that we measured six months after the initial survey as the dependent variable (n = 435). MATH explained 57 percent of the variance in purchase behavior.\n",
              "\n",
              "4.5\n",
              "\n",
              "Study 2 Data Analysis\n",
              "\n",
              "The results of study 1 suggest that various factors in MATH influence adoption and use of technologies in households. In study 2, we operationalized the MATH constructs for survey research. We reported construct reliability and validity.\n",
              "\n",
              "4.5.1\n",
              "\n",
              "Quantitative Validation of Study 2\n",
              "\n",
              "We re-analyzed Brown and Venkatesh’s (2005) data using PLS. The measurement model results supported reliability and convergent and discriminant validity: all ICRs were greater than .70 and all AVEs were greater than inter-construct correlations. Acceptable loadings (>.65) and low cross-loadings (<.30) in model tests for adopters and non-adopters further supported discriminant validity. Appendix I presents the ICRs, AVEs, descriptive statistics, and correlations. Although internal validity is a weakness of survey-based research, the longitudinal data collection here helped us provide better support for causality. The demographics comparison between the respondents and non-respondents at both periods (i.e., the initial survey and the follow-up survey conducted six months after the initial survey) showed no significant differences, which indicates that threats to internal validity (e.g., selection, history, maturation) did not influence the results (Brown & Venkatesh, 2005). We measured statistical conclusion validity by using an appropriate dataanalysis procedure and tool and by ensuring no statistical assumptions were violated. These validity criteria (i.e., internal validity, construct validity, discriminant validity, and statistical conclusion validity) also confirmed that the quantitative inference criteria were met (Venkatesh et al., 2013).\n",
              "\n",
              "4.5.2\n",
              "\n",
              "Results of Study 2\n",
              "\n",
              "Among users that already possessed a PC at the time of the initial survey (n = 370), we tested the models with use as the dependent variable. Appendix H shows the belief structures of MATH explained 57 percent of variance in use behavior. Appendix H also presents the PLS analysis results associated with the model testing of the data from households that did not possess a PC at the time of the initial survey with follow-up purchase behavior conducted six months after the initial survey as the dependent variable. MATH explained 50 percent of the variance in purchase behavior.\n",
              "Volume 17 Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "463\n",
              "\n",
              "4.6\n",
              "\n",
              "Step 4: Draw Meta-inferences from Mixed-methods Results\n",
              "\n",
              "In making the qualitative inferences, we followed the guidelines in Table 3. At the beginning of study 1, we inductively built a theoretical framework based on previous models (e.g., TPB). We used the resulting theoretical framework as the basis of study 2. We used inductive and (primarily) deductive theoretical reasoning to develop the meta-inferences. In our mixed-methods study, we assessed the credibility of inferences obtained from analyzing qualitative and quantitative data (i.e., triangulation with the emergence of complementarity). To do so, we used a triangulation technique to develop the meta-inferences. Triangulation techniques: 1) allow researchers to be more confident in their results, 2) can stimulate the creation of inventive methods and new ways of understanding a problem from multiple perspectives, 3) may help uncover various dimensions of a phenomenon, and (4) can lead to a synthesis or integration of theories (Jick, 1979). We developed the qualitative inferences first and the quantitative inferences second (see Table 5). In our illustrative study, the results showed a great deal of convergence but also revealed some inconsistent findings. Overall, we found that the same set of factors represented significant predictors of home PC adoption and use in both the qualitative and quantitative studies. Although the questionnaire used in the quantitative study was derived from the results of the interviews, we found two significant differences in findings between the studies. In the qualitative study, requisite knowledge was significant for current nonadopters but not significant in the quantitative study. In the qualitative study, status gains was significant for adopters but not significant in the quantitative study. One of the limitations of our study was that we did not re-examine the divergent findings using a new dataset (Erzberger & Kelle, 2003). However, we offered a theoretical explanation to resolve the divergent findings. Because these divergent findings unlikely resulted from the authors’ mistake in collecting or analyzing the data (see Sections 4.4 and 4.5), we felt that re-examining theoretical assumptions was sufficient to address this issue. We explain these divergent findings next. First, analyzing the qualitative data for the current non-adopters group showed that the majority of respondents indicated requisite knowledge influenced their decision to adopt a PC. At the same time, they considered fear of technology change to be the main barrier. Based on the arguments they formulated, current non-adopters found requisite knowledge a dominant issue because they found learning a new technology to be difficult. Thus, requisite knowledge likely had no direct effect (or the effect was small) on purchase behavior. However, other variables could mediate this relationship. For example, in their study, Kim and Kankanhalli (2009) found that requisite knowledge (i.e., self-efficacy) had no direct effect on user resistance. Rather, switching cost mediated the effect of self-efficacy on user resistance. With respect to our results, we need further investigation to test whether requisite knowledge has an indirect effect on purchase behavior through mediating variables. For instance, individuals’ belief that they have the knowledge necessary to use a PC may influence their perception of the utility they would achieve when using the PC, which, in turn, would influence their purchase behavior. This potential mediating relationship could, therefore, explain the non-significant direct effect of requisite knowledge that we found in study 2. Second, we found status gains to be significant among current adopters in the qualitative study but not significant in the quantitative study. Contrary to the finding from the quantitative study, prior research has reported that status gains was an important determinant of adoption behaviors (e.g., Fisher & Price, 1992; Kim & Han, 2009). Moreover, the innovation literature has indicated that social outcomes, such as status gains, are important in the early stage of technology adoption (Venkatesh & Brown, 2001). Social rewards do not likely influence later adopters because the status value of adopting diminishes as more people adopt (Brown & Venkatesh, 2003). Overall, our meta-inferences are consistent with MATH’s theoretical concepts. Integrating the qualitative and quantitative research strands has successfully added value beyond the individual studies. Given that study 1 and 2 data are from different sets of participants and different data-collection procedures, the findings’ similarity indicates we used strong theoretical models as our research foundation. The results’ richness and robustness gives us confidence about the factors that predict household PC adoption and use. The mixed-methods design helped us identify and understand the factors that influence household PC adoption and use. The qualitative study helped us identify a set of factors and their importance, and the quantitative study helped us empirically examine the theoretical model (developed from the qualitative study) to identify what factors drive household PC adoption and use and how these factors help explain the behavior differences between adopters and non-adopters. Taken together, these studies explain the factors that drive household PC adoption and use. Table 5 summarizes our meta-inferences.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "464\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table 5. Development of Qualitative Inferences, Quantitative Inferences, and Meta-inferences Context Qualitative inference The effect of applications for personal use was significant for both current adopters and non-adopters. Quantitative inference Meta-inference Explanation\n",
              "\n",
              "Consistent with the qualitative findings. Utilitarian outcomes (i.e., personal use and workrelated) were positively associated with use behavior (for current adopters) and purchase (for non-adopters).\n",
              "\n",
              "Utility for children and utility for workrelated use were positively associated with the Consistent with the use behavior of qualitative findings. current owners. However, only utility for work-related use was significant for non-adopters. Attitudinal belief structures\n",
              "\n",
              "Applications for fun Hedonic outcome (i.e., was positively applications for fun) was associated with the Consistent with the positively associated with use behavior of qualitative findings. use behavior (for current current owners and adopters) and purchase (for purchase behavior non-adopters). of non-owners.\n",
              "\n",
              "-\n",
              "\n",
              "Status gains was significant for Status gains was current owners but not significant for not for current non- both groups. owners.\n",
              "\n",
              "There was no relationship between status gains and use behavior (for current owners) and purchase (for current non-owners).\n",
              "\n",
              "The innovation literature has indicated that social outcomes, such as status gains, are important in the early stage of technology adoption (Venkatesh & Brown, 2001). Social rewards do not likely influence later adopters because the status value of adopting diminishes as more people adopt (Brown & Venkatesh, 2003).\n",
              "\n",
              "Normative belief structures\n",
              "\n",
              "• There was no relationship Friends and family between social influences and secondary and use behavior. sources positively • There was no relationship influenced purchase between secondary behavior of current Consistent with the sources and use behavior non-owners. qualitative findings. of current owners. However, neither • Social influences and predictor was secondary sources were significant for positively associated with current owners. purchase behavior.\n",
              "\n",
              "-\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "465\n",
              "\n",
              "Table 5. Development of Qualitative Inferences, Quantitative Inferences, and Meta-inferences Context Qualitative inference Quantitative inference Meta-inference Explanation\n",
              "\n",
              "Control belief structures\n",
              "\n",
              "Based on the arguments they formulated, current non-adopters found requisite knowledge a dominant issue because they found learning a new technology to be difficult. Thus, requisite knowledge likely had no Consistent with the direct effect (or the effect • One’s control beliefs (i.e., qualitative findings, was small) on purchase fear of technology change, except the behavior. However, other declining cost, and Fear of technology qualitative study variables could mediate this perceived ease of use) change, declining found that requisite relationship. We need were not associated with cost, perceived knowledge was further investigation to test use behavior. ease of use, and significant for whether requisite knowledge requisite knowledge • One’s control beliefs (i.e., has an indirect effect on current nonfor PC use were not fear of technology change, purchase behavior through owners, whereas significant for declining cost) were mediating variables. For the quantitative current owners. negatively associated with instance, individuals’ belief study found that However, they were purchase. that they have the requisite significant only for knowledge was not • One’s control belief (i.e., knowledge necessary to use current non-owners. perceived ease of use) was a PC may influence their significant for positively associated with perception of the utility they current nonpurchase. owners. would achieve in using the PC, which, in turn, would influence their purchase behavior. This potential mediating relationship could, therefore, explain the nonsignificant direct effect of requisite knowledge we found in study 2.\n",
              "\n",
              "4.7\n",
              "\n",
              "Step 5: Assess the Quality of Meta-inferences\n",
              "\n",
              "After we discussed the validity of quantitative and qualitative components (see Sections 4.4 and 4.5), we assessed the quality of the meta-inferences. As we state in Section 4.6, the results from both studies were consistent, which evidences the mixed-methods data’s high quality (i.e., reliability). We ensured the design quality by selecting the most appropriate research designs based on our research questions and the purposes of our mixed-methods study. We checked the explanation quality following the procedures that Venkatesh et al. (2013) recommend. Table 6 presents each type of validity criterion. Further, we assess the quality of meta-inferences using Onwuegbuzie and Johnson’s (2006) typology (see Table 7).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "466\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table 6. Quality of Meta-inferences Criteria Indicators • The study started with the qualitative phase (study 1) to address the first research question (i.e., what are the factors that determine household PC adoption among adopters and nonadopters?) followed by the quantitative phase (study 2) to address the second research question (i.e., does MATH explain household adoption and non-adoption of PCs?). We addressed the first question from Venkatesh and Brown (2001) using a qualitative method because, at the time when they conducted the study, prior literature did not provide adequate foundation for understanding IT adoption in households. We addressed the second research question from Brown and Venkatesh (2005) using a survey methodology to operationalize the Design suitability constructs identified in the qualitative phase of the study (study 1) and empirically test the model of adoption of technology in households. • We answered the mixed-methods research question (i.e., in what way do the results from the quantitative data collection (study 2) support or refute the results from the qualitative data collection (study 1)?) by triangulating the findings from the qualitative and quantitative studies. • Based on the research questions and specified purposes of the project, we carefully selected the mixed-methods designs (see Table 4). • We integrated various design components (e.g., sampling, data collection and analysis procedures) and applied the selected criteria to address the research questions. • We used two major sources of data: 1) open- and closed-ended qualitative interviews (Venkatesh & Brown, 2001) and 2) standardized questionnaire surveys to measure the various constructs described in MATH (Brown & Venkatesh, 2005). Design adequacy • Both the qualitative and quantitative study were longitudinal. In collecting the qualitative data, the interviewers followed the same protocol to maintain consistency. A comparison between the sample characteristics and the population characteristics in general showed that the sample represented the population. In collecting the quantitative data, the measurement items were carefully developed, pre-tested, and tested based on the results of study 1. • We adopted a sequential mixed-methods data-analysis approach to analyze the data. • We converted qualitative data into quantitative data. We statistically analyzed quantized data to test the hypothesized relationships. • We analyzed quantitative data using PLS-SEM. We chose PLS because it is robust and has few identifiability issues (Hair, Ringle, & Sarstedt, 2011). • Percentage of explained variance in the structural model of both the qualitative and quantitative studies was consistent, which suggests the study designs were appropriate to create the expected effect. • Meta-inferences resulted from triangulating the qualitative and quantitative findings. For example, the qualitative inference is “The effect of applications for personal use was significant for both current adopters and non-adopters” and the quantitative inference is “Utility for children and utility for work-related use were positively associated with the use behavior of current owners. However, only utility for work-related use was significant for non-adopters.”. We integrate these inferences to develop a meta-inference (i.e., utilitarian outcomes (i.e., personal use and work-related) were positively associated with use behavior (for current adopters) and purchase (for non-adopters)) (see Table 5 for details). • We theoretically explain the inconsistent findings across studies. For example, the qualitative study revealed that status gains was significant for owners but not for current non-owners, whereas the quantitative study showed status gains was not significant for both groups. We explain this consistency by reviewing the innovation literature (see Table 5 for details). • Inferences were consistent with the initial hypotheses of MATH. • The model is generalizable to the household population in the US but not necessarily to other countries, unless individuals adopted the PC in a similar way in these countries. • The outcomes and inferences might be applicable to study the adoption of other related technologies.\n",
              "\n",
              "Analytical adequacy\n",
              "\n",
              "Integrative efficacy\n",
              "\n",
              "Inference transferability\n",
              "\n",
              "Meta-inferences clearly represented the study’s initial purposes. The study’s primary purpose was triangulation or corroboration/confirmation, with an emergent element of complementarity. Integrative The mixed-methods designs implemented in the illustrative study were sufficient to achieve the correspondence study’s goals. Using the qualitative study, we identified the factors that determine household PC adoption among adopters and non-adopters. We then examined the predictive power of these factors in the quantitative study.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "467\n",
              "\n",
              "Table 7. Legitimation of Meta-Inferences (Onwuegbuzie & Johnson, 2006) Legitimation Sample integration legitimation Inside-outside legitimation Weakness minimization legitimation Conversion legitimation Multiple validity legitimation Indicators The study adopted a sequential mixed-methods sampling strategy with parallel samples to collect the qualitative and quantitative data. • The researchers (i.e., Venkatesh & Brown (2001)) employed two individuals/coders to code the qualitative data. • Everyone on the research team reviewed the data analysis and integration. We identified the potential threats and remedies of each method were (see step 6). • We conducted conversion based on theoretical perspectives. • We established the validity of the quantified data. • When addressing the legitimation of the qualitative component, we addressed and established the relevant qualitative validities. • When addressing the legitimation of the quantitative component, we addressed and established the relevant quantitative validities. • We also addressed the relevant mixed-methods legitimation types. • We developed meta-inferences based on the qualitative and quantitative inferences. • The results supported the theory. • We addressed research questions using mixed-methods research.\n",
              "\n",
              "Political legitimation\n",
              "\n",
              "In our illustrative study, although we addressed most of the legitimation issues, we did not address sequential legitimation, paradigmatic mixing, and commensurability legitimation. One method to assess sequential legitimation is to change the sequence of the research study. Because we re-analyzed alreadycollected data for this illustration, we could not assess sequential legitimation in this work. However, because most of our qualitative and quantitative inferences were consistent, we believe that the threat to sequential legitimation is not a major issue in our study. We did not address paradigmatic mixing in our illustration because we employed a pragmatism paradigm (i.e., both studies used a positivist approach). However, we successfully integrated the qualitative and quantitative inferences to develop meta-inferences. We also discussed the inference quality of the qualitative and quantitative data analysis. Finally, one can address commensurability legitimation if one can negotiate cognitively the importance of Gestalt switches— switching back and forth from a qualitative lens to a quantitative lens (Onwuegbuzie & Johnson, 2006). Onwuegbuzie and Johnson (2006) suggest that one can do so through cognitive and empathy training, and, if researchers have a limited ability to do this gestalt switch, then the researchers can ignore commensurability legitimation. Conducting mixed-methods research involves inherent challenges that make it more difficult than conducting a monomethod study. We review some of the challenges we encountered in the illustrative study (see Appendix B). First, researchers must understand and explain the rationale for using a mixed-methods research approach in their study (Teddlie & Tashakkori, 2009). Although we used two, independent papers for our illustrative study, they were from the same research program in which the authors employed a mixed-methods approach for collecting and analyzing the data. The initial challenge we encountered in this illustrative study was finding the rationale for combining the qualitative and quantitative data in the face of seemingly incompatible paradigms. Selecting an appropriate paradigm is a necessary step to justify one’s using a mixedmethods approach. To deal with this issue, we employed a pragmatism paradigm approach by combining the positivist qualitative data collection and analysis with the positivist quantitative data collection and analysis. Understanding the philosophical assumptions underlying each paradigm can also be a challenge for researchers because it requires knowledge and methodological expertise in multiple areas. The second challenge is associated with selecting the most suitable design to address the research questions. The process of selecting the best mixed-methods research design involves several steps as the decision tree presents (see Figure 1). To select the most appropriate design, researchers need to understand the characteristics and goals of each design choice. For example, in our illustrative study, we discussed the rationale for selecting a multistrand design that led to our selecting a sequential (explanatory) design and an equivalent status design. The design options discussed in this paper could be overwhelming, especially for those who are new to the field. Conducting mixed-methods research also requires more time and resources (e.g., funding, staffing). Without enough time and resources in one’s research team, a mixedmethods research project can be challenging.\n",
              "Volume 17 Issue 7\n",
              "\n",
              "\n",
              "468\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "The issue of nomenclature and basic definitions used in mixed-methods research is another challenge in conducting a mixed-methods study (Teddlie & Tashakkori, 2003). For example, although quantitative research studies routinely use the term “validity”, many qualitative researchers object to using this term (Onwuegbuzie & Johnson, 2006). In contrast, some qualitative researchers (e.g., Maxwell, 1992) do not refute using the term “validity” in qualitative research. Similarly, in the context of mixed-methods research, some scholars have used different terms to refer to the same concepts. For example, Teddlie and Tashakkori (2003) propose the term inference quality to refer to validity in the context of mixed-methods research (Venkatesh et al., 2013), whereas Onwuegbuzie and Johnson (2006) recommend that validity in mixed-methods research be termed legitimation. We believe that mixed-methods researchers should adopt a common nomenclature for validation to differentiate mixed-methods validation from qualitative and quantitative validation (Teddlie & Tashakkori, 2003; Venkatesh et al., 2013). We should diminish the differences in terminology to maintain consistency across mixed-methods studies. In our illustrative study, some of the results from the qualitative study were inconsistent with the qualitative study. As a result, we had to examine our findings more closely and review the existing literature more carefully to create a more advanced theoretical explanation (Teddlie & Tashakkori, 2009). Identifying the major source of inconsistency can be challenging in mixed-methods research because it requires researchers to reexamine the data, reassess the inference quality, go back to the literature, and even collect a new dataset (Erzerber & Kelle, 2003). Despite the challenge of identifying the source of inconsistency, divergent inferences in a mixed-methods study might lead to a better understanding of the phenomenon under study (Teddlie & Tashakkori, 2009).\n",
              "\n",
              "4.8\n",
              "\n",
              "Step 6: Discuss Potential Threats and Remedies\n",
              "\n",
              "Although several possible threats to the inference quality of mixed-methods research exist, one can minimize these threats through several remedial actions. Table 8 lists the threats and remedial actions in our illustrative study.\n",
              "Table 8. Potential Threats to Inference Quality and Remedial Actions (Adapted from Creswell & Plano Clark, 2007) Areas Legitimation type Threats Selecting different individuals for the qualitative and quantitative data collection. Unequal sample sizes for the qualitative and quantitative dataset. Threat(s) to inside-outside legitimation Introducing potential bias in the data-collection techniques. Remedial actions 1. The researchers involved in collecting data for study 1 and 2 drew the sampling frame for the quantitative and qualitative data collection from the same population. 2. Both studies had a fairly large sample size. 3. A professional marketing firm collected data, and the interviewers involved in collecting data used a specific interview protocol/script for all interviewees. 1. We quantized the qualitative data by creating codes and then counting codes and evaluating their weights. 2. We assessed and discussed validity for both studies.\n",
              "\n",
              "Threat(s) to sample integration Data collection\n",
              "\n",
              "Data analysis\n",
              "\n",
              "Threat(s) to data conversion Threat(s) to multiple validities\n",
              "\n",
              "Inadequate data transformation approaches. Not addressing validity issues.\n",
              "\n",
              "In our illustrative study, we discuss only one of many alternative designs that mixed-methods researchers can use. Researchers can be flexible in selecting their designs based on the objectives of their study. For example, one can use a multiple paradigmatic stance (e.g., researchers might use interpretivism in their qualitative study and positivism in their quantitative study) to address the research questions proposed in our illustrative study. Researchers can also adopt either qualitative dominant or quantitative dominant designs depending on the purpose of their study. For example, if one primarily focuses on identifying factors that determine PC adoption in households, then one should select the qualitative dominant design with the interpretivism paradigm and sequential-exploratory design.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "469\n",
              "\n",
              "Researchers can flexibly integrate the 14 properties discussed to help them select the most suitable mixedmethods designs for their studies. We suggest that, when planning a mixed-methods study, researchers should consider these properties and select those most relevant to the objectives of their study. Among these 14 properties, research questions, purposes of mixed-methods research, and paradigmatic assumptions are absolutely fundamental during the study’s conceptualization stage. For example, in our illustrative study, we used the research questions and purposes of mixed-methods research as our basic foundation for selecting the mixed-methods design based on the assumptions underlying pragmatism. At the methodological stage, the components of time orientation, data collection strategies, and data analysis strategies are critical and should not be overlooked in mixed-methods research because they determine the quality of inferences. Other properties, such as priority of methodological approach, can be less salient depending on the research questions. For example, if it is unclear whether the qualitative or quantitative data will ultimately be the most important in the results and inferences, then priority of approach is not a critical element of design dimensions (Teddlie & Tashakkori, 2010). One should also assess the inference quality carefully because inferences are the most important aspects or outcomes of mixed-methods research (Teddlie & Tashakkori, 2010).\n",
              "\n",
              "5\n",
              "\n",
              "Discussion\n",
              "\n",
              "This paper extends Venkatesh et al.’s (2013) guidelines by identifying and integrating 14 variations of mixedmethods research properties. These guidelines offer a new perspective to accommodate the diversity of mixed-methods designs. Further, we illustrate one possible type of mixed-methods research in depth. We also discuss the development and validation of meta-inferences (i.e., validation of mixed-methods research) in our illustrative study. This paper contributes to the development of mixed-methods research by viewing mixed methods as an integrative model of design based on various properties of mixed-methods research (Maxwell & Loomis, 2003). Finally, this paper advances our understanding of mixed-methods research by presenting the variety of possible mixed-methods applications and demonstrating that a mixed-methods approach may generate stronger inferences because such an approach integrates qualitative and quantitative inferences.\n",
              "\n",
              "5.1\n",
              "\n",
              "Contributions\n",
              "\n",
              "This research makes several key contributions to the literature on mixed methods. First, we extend the guidelines of Venkatesh et al. (2013) for mixed-methods research by integrating 14 properties of mixed methods into the guidelines. Our guidelines complement the other existing mixed-methods research guidelines (e.g., Maxwell & Loomis, 2003; Nastasi et al., 2010; Tashakkori & Teddlie, 1998, 2003b). Critically, our guidelines integrate various dimensions of mixed-methods research to accommodate different types of mixed-methods designs. Although we position the guidelines in an IS context, due to the dearth of guidance on how to better execute mixed-methods research, they are also broadly applicable beyond IS. Second, we show how one can use mixed-methods research to extract significant findings that the limitations inherent in a single method alone can compromise. Through this study, we provide researchers with the information necessary to select the best mixed-methods designs for their research project based on 14 general properties of the studies that scholars have well established in mixed-methods research. We also offer a decision tree to map the flow and relationship among the design strategies. Third, we illustrate one possible type of mixed-methods research in depth by characterizing the study from multiple dimensions of mixed-methods research. Our illustration shows that researchers can select the designs of mixed-methods research that best fit with their research questions and purposes. This illustration provides an opportunity to open up the research process from which the research community may learn about the best practices and the challenges in conducting mixed-methods research. Fourth, we contribute to the development of mixed-methods research, particularly in the IS field. We argue that the use of mixed methods, if done well, may drive the development of IS research. Although using mixed-methods research in social science is in its adolescence (Teddlie & Tashakkori, 2003), its use in IS is relatively new. We suggest that IS researchers familiarize themselves with the theoretical paradigms and different properties of mixed-methods research. Although research methods and theoretical paradigms that underlie these methods should follow the research questions, IS researchers should be able to integrate those different paradigms and not rely solely on a single paradigm (Tashakkori & Teddlie, 1998; Venkatesh et al., 2013). We also advise IS researchers to be flexible in making their research design decisions depending on the purposes of their mixed-methods research study.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "470\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Finally, we illustrate only one possible type of mixed-methods study. Thus, we need future research to illustrate how to conduct different types of mixed-methods studies based on different properties of mixed-methods research discussed in this paper. For instance, one could conduct mixed-methods research to answer a research question with expansion and developmental purposes to increase the validity of constructs and inquiry results by selecting the most appropriate methods and maximizing the method strengths (Greene et al., 1989). Based on their research question(s) and well-defined purposes, researchers then can select the most appropriate paradigmatic assumptions and determine their mixed-methods design strategies.\n",
              "\n",
              "6\n",
              "\n",
              "Conclusions\n",
              "\n",
              "In this paper, we extend Venkatesh et al.’s (2013) guidelines for mixed-methods research by elaborating various properties of such studies. The integrative framework that we presented accommodates different types of mixed-methods research. We deliberately tried to be comprehensive in selecting and reviewing the mixed-methods properties to offer researchers the opportunity to properly use a mixed-methods approach in their study. We illustrate one possible type of mixed-methods research in depth—one of the first illustrations that applies various properties of mixed-methods research by incorporating qualitative and quantitative data collection and analysis in a sequential manner and that explains the decisions made at various stages of the research endeavor. In this illustration, we also present how we developed and validated meta-inferences in a broader research program. Note that the specific guidelines we propose and illustrate reflect a certain set of preferences and are dominated by our paradigmatic assumptions in-use. We do not seek to constrain all mixed-methods researchers to follow the same research designs as our example illustrates. Instead, we provide general guidelines that enable authors to critically think about their designs prior to the study and offer justification of their approaches after the study. Thus, authors and reviewers need to be flexible in adapting the guidelines based on the objectives of their study and the ontological and epistemological assumptions underlying the different components of their mixed-methods studies. We hope this work motivates researchers to adopt mixed methods in their research projects to gain richer insights into phenomena they investigate.\n",
              "\n",
              "Acknowledgements\n",
              "We express our appreciation to the SE, Dr. Suprateek Sarker, and the anonymous reviewers for their helpful suggestions on the paper. We also thank Adam LeBrocq for his editing and typesetting of the paper.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "471\n",
              "\n",
              "References\n",
              "Ågerfalk, P. J. (2013). Embracing diversity through mixed methods research. European Journal of Information Systems, 22(3), 251-256. Ajzen, I. (1985). From intention to actions: A theory of planned behavior. In J. Kuhl & J. Beckmann (Eds.), Action control: From cognition to behavior (pp. 11-393). New York, NY: Springer Verlag. Ajzen, I. (1991). The theory of planned behavior. Organizational Behavior and Human Decision Processes, 50(2), 179-211. Babbie, E. (1990). Survey research methods. Belmont, CA: Wadsworth Publishing Company. Bazeley, P. (2004). Issues in mixing qualitative and quantitative approaches to research. In R. Buber & J. Gadner (Eds.), Applying qualitative methods to marketing management research (pp. 141-156). Hampshire, United Kingdom: Palgrave Macmillan. Bhattacherjee, A., & Premkumar, M. (2004). Understanding changes in belief and attitude toward information technology usage: A theoretical model and longitudinal test. MIS Quarterly, 28(2), 229-254. Biesta, G. (2010). Pragmatism and the philosophical foundations of mixed methods research. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 95-118). Thousand Oaks, CA: Sage. Brown, S. A., & Venkatesh, V. (2003). Bringing non-adopters along: The challenge facing the PC industry. Communications of the ACM, 46(3), 76-80. Brown, S. A., & Venkatesh, V. (2005). Model of adoption of technology in the household: A baseline model test and extension incorporating household life cycle. MIS Quarterly, 29(3), 399-426. Bryman, A. (2006). Integrating quantitative and qualitative research: How is it done? Qualitative Research, 6(1), 97-113. Caracelli, V. J., & Greene, J. C. (1993). Data analysis strategies for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 15(2), 195-207. Caracelli, V. J., & Greene, J. C. (1997). Crafting mixed-method evaluation design. New Direction for Evaluation, 74, 19-32. Castro, F. G., Kellison, J. G., Boyd, S. J., & Kopak, A. 2010. A methodology for conducting integrative mixed methods research and data analyses. Journal of Mixed Methods Research, 4(4), 342-360. Cohen, J. (1988). Statistical power and analysis for the behavioral sciences (2nd ed.). Hilldale, NJ: Lawrence Erlbaum Associates. Collins, K. M. (2010). Advanced sampling designs in mixed research: Current practices and emerging trends in the social and behavioral sciences. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 353-378). Thousand Oaks, CA: Sage. Collins, K. M., Onwuegbuzie, A. J., & Jiao, Q. G. (2007). A mixed methods investigation of mixed methods sampling designs in social and health science research. Journal of Mixed Methods Research, 1(3), 267-294. Compeau, D. R., & Higgins, C. A. (1995). Computer self-efficacy: Development of a measure and initial test. MIS Quarterly, 19(2), 189-211. Creswell, J. W. (1995). Research design: Qualitative and quantitative approaches. Thousand Oaks, CA: Sage Publications. Creswell, J. W. (1998). Qualitative inquiry and research design: Choosing among the five traditions. Thousand Oaks, CA: Sage. Creswell, J. W. (2003). Research designs: Qualitative, quantitative, and mixed methods approaches (2nd ed.). Thousand Oaks, CA: Sage. Creswell, J. W. (2009). Research designs: Qualitative, quantitative, and mixed methods approaches (3rd ed.). Thousand Oaks, CA: Sage.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "472\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Creswell, J. W. (2013). Research designs: Qualitative, quantitative, and mixed methods approaches (4th ed.). Thousand Oaks, CA: Sage Publications. Creswell, J. W., Fetters, M. D., & Ivankova, N. V. (2004). Designing a mixed methods study in primary care. Annals of Family Medicine, 2(1), 7-12. Creswell, J. W., & Plano Clark, V. L. (2007). Designing and conducting mixed methods research. Thousand Oaks, CA: Sage. Creswell, J. W., Plano Clark, V. L., Gutmann, M. L., & Hanson, W. E. (2003). Advanced mixed methods research designs. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 209-240). Thousand Oaks, CA: Sage. Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319-339. da Costa, L., & Remedios, R. (2014). Different methods, different results examining the implications of methodological divergence and implicit processes for achievement goal research. Journal of Mixed Methods Research, 8(2), 162-179. Dellinger, A. B., & Leech, N. L. (2007). Toward a unified validation framework in mixed methods research. Journal of Mixed Methods Research, 1(4), 309-332. Denzin, N. K. (2012). Triangulation 2.0. Journal of Mixed Methods Research, 6(2), 80-88. Driscoll, D. L., Apiah-Yeboah, A., Salib, P., & Rupert, D. J. (2007). Merging qualitative and quantitative data in mixed methods research: How to and why not. Ecological and Environmental Anthropology (University of Georgia), 3(1), 19-28. Erzberger, C., & Kelle, U. (2003). Making inferences in mixed methods: The rules of integration. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 457-490). Thousand Oaks, CA: Sage. Feilzer, M. Y. (2010). Doing mixed methods research pragmatically: Implications for the rediscovery of pragmatism as a research paradigm. Journal of Mixed Methods Research, 4(1), 6-16. Fielding, N. G. (2012). Triangulation and mixed methods designs data integration with new research technologies. Journal of Mixed Methods Research, 6(2), 124-136. Fisher, R. J., & Price, L. L. (1992). An investigation into the social context of early adoption behavior. Journal of Consumer Research, 19(3), 477-486. Gliner, J. A., Morgan, G. A., & Leech, N. L. (2009). Research methods in applied settings: An integrated approach to design and analysis (2nd ed.). New York, NY: Taylor & Francis Group. Greene, J. C. (2005). The generative potential of mixed methods inquiry. International Journal of Research & Methods in Education, 28(2), 201-211. Greene, J. C. (2007). Mixed methods in social inquiry. San Francisco: Jossey-Bass. Greene, J. C., & Caracelli, V. J. (2003). Making paradigmatic sense of mixed methods practice. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 91-110). Thousand Oaks, CA: Sage Publications. Grenee, J. C., Caracelli, V. & Graham, W. (1989). Toward a conceptual framework for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 11, 255-274. Greene, J. C., & Hall, J. N. (2010). Dialectics and pragmatism: Being of consequence. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 119144). Thousand Oaks, CA: Sage. Guba, E. G., & Lincoln, Y. S. (1994). Competing paradigms in qualitative research. In N. K. Denzin & Y. S. Lincoln (Eds.), Handbook of qualitative research (pp. 105-117). Thousand Oaks, CA: Sage. Guest, G. (2012). Describing mixed methods research: An alternative to typologies. Journal of Mixed Methods Research, 7(2), 141-151.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "473\n",
              "\n",
              "Hair, J. F., Ringle, C. M., & Sarstedt, M. (2011). PLS-SEM: Indeed a silver bullet. Journal of Marketing Theory and Practice, 19(2), 139-152. Harrison, R. L., & Reilly, T. M. (2011). Mixed methods designs in marketing research. Qualitative Market Research: An International Journal, 14(1), 7-26. Harrits, G. S. (2011). More than method? A discussion of paradigm differences within mixed methods research. Journal of Mixed Methods Research, 5(2), 150-166. Higgins, J. P. T., & Green, S. (2011). Cochrane handbook for systematic reviews of interventions version 5.1.0. The Cochrane Collaboration. Retrieved from www.cochrane-handbook.org. Ivankova, N. V., Creswell, J. W., & Stick, S. L. (2006). Using mixed methods sequential explanatory design: From theory to practice. Field Methods, 18(1), 3-20. Jick, T. D. (1979). Mixing qualitative and quantitative methods: Triangulation in action. Administrative Science Quarterly, 24(4), 602-611. Johnson, B., & Turner, L. A. (2003). Data collection strategies in mixed methods research. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 297319). Thousand Oaks, CA: Sage. Johnson, R. B., & Onwuegbuzie, A. J. (2004). Mixed methods research: A research paradigm whose time has come. Educational Researcher, 33(7), 14-26. Johnson, R. B., Onwuegbuzie, A. J., & Turner, L. A. (2007). Toward a definition of mixed methods research. Journal of Mixed Methods Research, 1(2), 112-133. Joseph, D., Ng, K. Y., Koh, C., & Ang, S. (2007). Turnover of information technology professionals: A narrative review, meta-analytic structural equation modeling, and model development. MIS Quarterly, 31(3), 547-577. Judd, C. M., Smith, E. R., & Kidder, L. H. (1991). Research methods in social relations (6th ed.). Fort Worth, TX: Holt, Rinehart, and Winston, Inc. Keil, M., & Tiwana, A. (2006). Relative importance of evaluation criteria for enterprise systems: A conjoint study. Information Systems Journal, 16(3), 237-262. Kim, B., & Han, I. (2009). What drives the adoption of mobile data services? An approach from a value perspective. Journal of Information Technology, 24, 35-45. Kim, H.-W., & Kankanhalli, A. (2009). Investigating user resistance to information systems implementation: A status quo bias perspective. MIS Quarterly, 33(3), 567-582. Koh, C., Ang, S., & Straub, D. W. (2004). IT outsourcing success: A psychological contract perspective. Information Systems Research, 15(4), 356-373. Lee. A. S. (1991). Integrating positivist and interpretive approaches to organizational research. Organizational Science, 2(4), 342-365. Leech, N. L. (2012). Writing mixed research report. American Behavioral Scientist, 56(6), 866-881. Leech, N. L., & Onwuegbuzie, A. J. (2009). A typology of mixed methods research designs. Quality & Quantity, 43(2), 265-275. Lincoln, Y. S., & Guba, E. G. (1985). Naturalistic inquiry. Thousand Oaks, CA: Sage. Maxwell, J. A. (1992). Understanding and validity in qualitative research. Harvard Educational Review, 62(3), 279-300. Maxwell, J. A. (1996). Qualitative research design: An interactive approach. Thousand Oaks, CA: Sage. Maxwell, J. A., & Loomis, D. M. (2003). Mixed methods design: An alternative approach. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 241272). Thousand Oaks, CA: Sage. Maxwell, J. A., & Mittapalli, K. (2010). Realism as a stance for mixed methods research. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 145168). Thousand Oaks, CA: Sage.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "474\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Merriam, S. B. (1998). Qualitative research and case study applications in education. Revised and expanded from “case study research in education”. San Francisco, CA: Jossey-Bass Publishers. Mertens, D. M. (2003). Mixed methods and the politics of human research: The transformative-emancipatory perspective In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 135-164). Thousand Oaks, CA: Sage. Mertens, D. M. (2005). Research and evaluation in education and psychology: Integrating diversity with quantitative, qualitative, and mixed methods. Thousand Oaks, CA: Sage. Miles, M. B., & Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook. Thousand Oaks, CA: Sage. Miller, S. (2003). Impact of mixed methods and design on inference quality. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 423-457). Thousand Oaks, CA: Sage. Morgan, D. L. (2007). Paradigms lost and pragmatism regained methodological implications of combining qualitative and quantitative methods. Journal of Mixed Methods Research, 1(1), 48-76. Morse, J. M. (2003). Principles of mixed methods and multimethod research design. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 189-208). Thousand Oaks, CA: Sage. Morse, J. M. (2010). Procedures and practice of mixed method design: Maintaining control, rigor, and complexity. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 339-352). Thousand Oaks, CA: Sage. Nastasi, B. K., Hitchcock, J. H., & Brown, L. M. (2010). An inclusive framework for conceptualizing mixed methods design typologies: Moving toward fully integrated synergistic research models. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 305-338). Thousand Oaks, CA: Sage. Nastasi, B. K., Hitchcock, J. H, Sarkar, S., Burkholder, G., Varjas, K., & Jayasena, A. (2007). Mixed methods in intervention research: Theory to adaptation. Journal of Mixed Methods Research, 1(2), 164-182. Newman, I., Ridenour, C., Newman, C., Mario, G., & DeMarco, G. M. P., Jr. (2003). A Typology of research purposes and its relationship to mixed methods. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 167-188). Thousand Oaks, CA: Sage. Onwuegbuzie, A. J. (2003). Expanding the framework of internal and external validity in quantitative research. Research in the Schools, 10(1), 71-89. Onwuegbuzie, A. J., & Collins, K. M. (2007). A typology of mixed methods sampling designs in social science research. Qualitative Report, 12(2), 281-316. Onwuegbuzie, A. J., & Johnson, R. B. (2006). The validity issue in mixed research. Research in the Schools, 13(1), 48-63. Onwuegbuzie, A. J., & Leech, N. L. (2006). Linking research questions to mixed methods data analysis procedures. The Qualitative Report, 11(3), 474-498. Onwuegbuzie, A. J., & Leech, N. L. (2007). Validity and qualitative research: An oxymoron? Quality & Quantity, 41, 233-249. Orlikowski, W. J. (1993). CASE tools as organizational change: Investigating incremental and radical change in system development. MIS Quarterly, 17(3), 309-340. Orlikowski, W. J., & Baroudi, J. J. (1991). Studying information technology in organizations: Research approaches and assumptions. Information Systems Research, 2(1), 1-28. Patton, M. Q. (1990). Qualitative evaluation and research method (2nd ed.). Newbury Park, CA: Sage. Petter, S. C., & Gallivan, M. J. (2004). Toward a framework for classifying and guiding mixed method research in information system. Paper presented at the Proceedings of the 37th Hawaii International Conference on System Science, Hawaii.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "475\n",
              "\n",
              "Plano Clark, V. L., & Badiee, M. (2010). Research questions in mixed methods research. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 275304). Thousand Oaks, CA: Sage. Sandelowski, M. (2000). Combining qualitative and quantitative sampling, data collection, and analysis techniques in mixed-method studies. Research in Nursing & Health, 23, 246-255. Sandelowski, M. (2003). Tables or Tableaux? The challenges of writing and reading mixed methods studies. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 321-350). Thousand Oaks, CA: Sage. Stone, E. (1978). Empirical research strategies, research methods in organizational behavior. Research Methods in Organizational Behavior, 7, 111-139. Tashakkori, A., & Creswell, J. W. (2007). Exploring the nature of research questions in mixed methods research. Journal of Mixed Methods Research, 1(3), 207-211. Tashakkori, A., & Teddlie, C. (1998). Mixed methodology: Combining qualitative and quantitative approaches. Thousand Oaks, CA: Sage. Tashakkori, A., & Teddlie, C. (2003a). Issues and dilemmas in teaching research methods courses in social and behavioral sciences: US perspective. International Journal of Social Research Methodology, 6(1), 61-77. Tashakkori, A., & Teddlie, C. (2003b). The past and future of mixed methods research: From data triangulation to mixed model designs. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 671-701). Thousand Oaks, CA: Sage. Tashakkori, A., & Teddlie, C. (2008). Quality of inferences in mixed methods research. In M. M. Bergman (Ed.), Advances in mixed methods research: Theories and applications (pp. 53-65). London, UK: Sage. Tashakkori, A., & Teddlie, C. (2010). Indicators of quality in mixed methods and their relevance to evaluation quality. Panel presented at the Presidential Strand Panel, San Antonio, Texas. American Evaluation Association. Taylor, S., & Todd, P. A. (1995). Understanding information technology usage: A test of competing models. Information Systems Research, 6(2), 144-176. Teddlie, C., & Tashakkori, A. (2003). Major issues and controversies in the use of mixed methods in the social and behavioral sciences. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (1st ed.) (pp. 3-50). Thousand Oaks, CA: Sage. Teddlie, C., & Tashakkori, A. (2006). A general typology of research designs featuring mixed methods. Research in the Schools, 13(1), 12-28. Teddlie, C., & Tashakkori, A. (2009). The foundations of mixed methods research: Integrating quantitative and qualitative techniques in the social and behavioral sciences. Thousand Oaks, CA: Sage. Teddlie, C., & Tashakkori, A. (2010). Overview of contemporary issues in mixed methods research. In A. Tashakkori & C. Teddlie (Eds.), Handbook of mixed methods in social and behavioral research (2nd ed.) (pp. 1-44). Thousand Oaks, CA: Sage. Teddlie, C., & Yu, F. (2007). Mixed methods sampling a typology with examples. Journal of Mixed Methods Research, 1(1), 77-100. Trochim, W. M. K., & Donnelly, J. P. (2007). The research methods knowledge base (3rd ed.). Mason, OH: Thompson Learning. Van de Ven, A. H. (2007). Engaged scholarship: A guide for organizational and social research. Oxford, NY: Oxford University Press. Venkatesh, V., & Brown, S. A. (2001). A longitudinal investigation of personal computers in homes: Adoption determinants and emerging challenges. MIS Quarterly, 25(1), 71-102. Venkatesh, V., Brown, S. A., & Bala, H. (2013). Bridging the qualitative-quantitative divide: Guidelines for conducting mixed methods research in information systems. MIS Quarterly, 37(1), 21-54. Weber, R. P. (1990). Basic content analysis. Thousand Oaks, CA: Sage.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "476\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Webster, J., & Martocchio, J. J. (1992). Microcomputer playfulness: Development of a measure with workplace implications. MIS Quarterly, 16(2), 201-226. Zachariadis, M., Scott, S., & Barret, M. (2013). Methodological implications of critical realism for mixed methods research. MIS Quarterly, 37(3), 855-879.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "477\n",
              "\n",
              "Appendix A: Review of Selected Theoretical Literature in Mixedmethods Research\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description Researchers write separate quantitative questions or hypotheses and qualitative questions or hypotheses. Researchers write separate quantitative questions or hypotheses and qualitative questions or hypotheses and follow them with a mixed-methods question. Researchers write only mixed-methods questions that reflect the procedures or the content (or write the mixedmethods question in both a procedural and a content approach) and do not include separate quantitative and qualitative questions. Mixed-methods researchers could state their research questions in the form of questions, aims, and/or hypotheses.\n",
              "\n",
              "Type I Type II Creswell (2009) Type III\n",
              "\n",
              "Rhetorical style: format\n",
              "\n",
              "Research questions\n",
              "\n",
              "Plano Clark & Badiee (2010)\n",
              "\n",
              "• Separate questions only: “the researcher writes separate questions for the qualitative and quantitative strands of the study” (Plano Clark & Badiee, 2010, p. 290). • General, overarching mixed-methods question: “the researcher writes a broad question that is addressed with both quantitative and qualitative approaches” (Plano Clark & Badiee, 2010, p. 290). • Hybrid mixed-methods issue question: “the researcher writes one question with two distinct parts and uses a Rhetorical style: level quantitative approach to address one part and a of integration qualitative approach to address the other part” (Plano Clark & Badiee, 2010, p. 290). • Mixed-methods procedural question: “the researcher writes a narrow question that directs the integration of the qualitative and quantitative strands of the study” (Plano Clark & Badiee, 2010, p. 291). • Combination: “the researcher combines at least one mixed methods question with separate quantitative and qualitative questions” (Plano Clark & Badiee, 2010, p. 291). • Independent: “the researcher writes two or more research questions that are related, and one question does not depend on the results of the other questions” (Plano Clark & Badiee, 2010, p. 291). • Dependent: “the researcher writes a question that depends on the results of another research question” (Plano Clark & Badiee, 2010, p. 291). • Predetermined: “the researcher writes a question based on literature, practice, personal tendencies, and/or disciplinary considerations at the outset of the study” (Plano Clark & Badiee, 2010, p. 292). • Emergent: “the researcher formulates a new or modified question during the design, data collection, data analysis, and interpretation” (Plano Clark & Badiee, 2010, p. 292).\n",
              "\n",
              "The relationship of questions to other questions\n",
              "\n",
              "The relationship of questions to the research process\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "478\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Venkatesh et al. (2013), Tashakkori & Teddlie (1998, 2003), Morgan (2007), Johnson & Onwuegbuzie (2004) Mertens (2003, 2005) Paradigmatic perspectives Harrits (2011), Maxwell & Mittapalli (2010) Greene (2007), Greene & Hall (2010) Dimensions Description This paradigm considers practical consequences and real effects to be vital components of meaning and truth. It rejects a forced choice between existing paradigms with regard to logic, ontology, and epistemology.\n",
              "\n",
              "Pragmatism\n",
              "\n",
              "Transformativeemancipatory Critical realism\n",
              "\n",
              "This paradigm considers the ultimate goal for conducting research to be in creating a more just and democratic society. This paradigm does not recognize the existence of some absolute truth or reality to which one can compare an object or account. This paradigm recognizes the legitimacy of multiple paradigmatic traditions because they represent “multiple way of seeing and hearing, multiple ways of making sense of the social world, and multiple standpoints on what is important and to be valued and cherished” (Greene & Hall, 2010, p. 124).\n",
              "\n",
              "Dialectical\n",
              "\n",
              "Teddlie & Tashakkori (2003) Greene et al. (1989), Tashakkori & Teddlie (1998)\n",
              "\n",
              "Other major Researchers can use multiple paradigmatic stances to paradigmatic support mixed-methods research. perspectives (e.g., positivist, constructivism, postpositivist) Single paradigm stance Multiple paradigm stance Triangulation Both qualitative and quantitative studies are in the same paradigm. Qualitative and quantitative studies are in different paradigms. Researchers use mixed-methods research to seek convergence and corroboration of results from different methods and designs studying the same phenomenon. Researchers use mixed-methods research to seek elaboration, enhancement, illustration, and clarification of the results from one method with results from the other method. Researchers use mixed-methods research to discover paradoxes and contradictions that lead to a re-framing of the research question. Researchers use the findings from one method to help inform another method. Researchers use mixed-methods research to expand the breadth and range of research by using different methods for different inquiry component. Researchers use mixed-methods research to elaborate, enhance, illustrate, and clarify the results from one method with results from another method. Researchers use mixed-methods research to make sure they obtain a complete picture of a phenomenon. Researchers use the findings from one method are used to help inform another method.\n",
              "\n",
              "Epistemological perspective\n",
              "\n",
              "Complementarity Greene et al. (1989), Johnson & Onwuegbuzie (2004) Purposes of mixed-methods research\n",
              "\n",
              "Initiation Development Expansion\n",
              "\n",
              "Creswell (2003), Tashakkori & Teddlie (2003), Venkatesh et al. (2013)\n",
              "\n",
              "Complementarity Completeness Developmental\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "479\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description Researchers use mixed-methods research to explain or expand on the understanding obtained in a previous strand of a study. Researchers use mixed-methods designs to assess the credibility of inferences obtained from one approach. Mixed-methods designs enable different methods to overcome the weaknesses of each other. Researchers use mixed-methods research to obtain divergent views of the same phenomenon. Mixed-methods research that builds general laws. Mixed-methods research that confirms findings, replicates others’ work, reinterprets previously collected data, clarifies structural and ideological connections between important social processes, and strengthens the knowledge base. Mixed-methods research that deconstructs/reconstructs power structures, reconciles discrepancies, refutes claims, sets priorities, resists authority, influences change, and sets policy. Mixed-methods research that measures consequences of practice, tests treatment effects, and measures outcomes. Mixed-methods research that understands phenomena, culture, change, and people. Mixed-methods research that tests innovations, hypotheses, new ideas, and new solutions.\n",
              "\n",
              "Expansion Corroboration/ confirmation Compensation Diversity Predict Add to the knowledge base Have a personal, social, institutional, and/or organizational impact Measure change Newman, Ridenour, Understand complex Newman, Mario, phenomena & DeMarco (2003) Test new ideas\n",
              "\n",
              "Mixed-methods research that explores phenomena, Generate new ideas generates hypotheses, generates theory, uncovers relationships, uncovers culture, and reveals culture. Mixed-methods research that informs the public, Inform constituencies heightens awareness, describes the present, and complies with authority. Examine the past Mixed-methods research that interprets/reinterprets the past, acknowledges past misunderstandings, reexamines tacit understanding, and examines the social and historical origins of current social problems. Mixed-method research in which “researchers are likely to believe that qualitative and quantitative data and approaches will add insights as one considers most, if not all, research questions” (Johnson et al., 2007, p. 123).\n",
              "\n",
              "Equal status\n",
              "\n",
              "Priority/ dominance\n",
              "\n",
              "Johnson et al. (2007)\n",
              "\n",
              "“The type of mixed research in which one relies on a qualitative, constructivist-poststructuralist-critical view of the research process, while concurrently recognizing that Qualitative dominant the addition of quantitative data and approaches are likely to benefit most research projects” (Johnson et al., 2007, p. 124). “Quantitative dominant mixed methods research is the type of mixed research in which one relies on a quantitative, post-positivist view of the research process, while concurrently recognizing that the addition of qualitative data and approaches are likely to benefit most research projects” (Johnson et al., 2007, p. 124).\n",
              "\n",
              "Quantitative dominant\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "480\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description “Researchers conduct the study using both the quantitative and the qualitative approaches about equally to understand the phenomenon under study” (Johnson et al., 2007, p. 18). “Researchers conduct the study within a single dominant paradigm with a small component of the overall study drawn from an alternative design” (Johnson et al., 2007, p. 18). “Researchers use different types of methods at different levels of data aggregation” (Johnson et al., 2007, p. 18). A type of research design in which one mixes the quantitative and qualitative portions of the study at specific stages (e.g., sampling, data collection, data analysis, or data inference).\n",
              "\n",
              "Equivalent status design Tashakkori & Teddlie (1998)\n",
              "\n",
              "Dominant-less dominant study Design with multilevel use of approaches 8 Partially mixed methods\n",
              "\n",
              "Teddlie & Tashakkori (2009)\n",
              "\n",
              "A type of research design in which researchers mix the quantitative and qualitative portions of the study at all Fully mixed methods stages (the objective, data analysis and inference stages of the research process). Parallel mixed designs Sequential mixed designs Conversion mixed designs Multilevel mixed designs Fully integrated mixed designs Sequential In these designs, one mixes different methods (qualitative and quantitative methods) in a parallel manner either simultaneously or with some time lapse. In these designs, one mixes different methods (qualitative and quantitative methods) across chronological phases of the study. In these parallel designs, one mixes different methods when one transforms and analyzes one type of data both qualitatively and quantitatively. In these parallel or sequential designs, one mixes different methods across multiple levels of analysis. In these designs, one mixes different methods in an interactive manner at all stages of the study. Mixed-methods research in which “researchers conduct a qualitative phase of a study and then a separate quantitative phase, or vice versa” (Tashakkori & Teddlie, 1998, p. 46). “The researchers conducts the qualitative and quantitative phase at the same time” (Tashakkori & Teddlie, 1998, p. 18). Mixed-methods research “in which the researcher seeks to elaborate on or expand the findings of one method with another method” (Creswell, 2003, p. 16). Mixed-methods research “in which the researcher converges quantitative and qualitative data in order to provide a comprehensive analysis a comprehensive analysis of the research problem” (Creswell, 2003, p. 16).\n",
              "\n",
              "Mixing strategies Teddlie & Tashakkori (2009)\n",
              "\n",
              "Creswell (1995), Tashakkori & Teddlie (1998) Time orientation\n",
              "\n",
              "Concurrent\n",
              "\n",
              "Sequential Creswell (2003) Concurrent\n",
              "\n",
              "8\n",
              "\n",
              "Tashakkori and Teddlie (2003) propose “designs with multilevel use of approaches” as another potential approach.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "481\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description Mixed-methods research “in which the researcher uses a theoretical lens as an overarching perspective within a design that contains both quantitative and qualitative data” (Creswell, 2003, p. 16). There are three types of sequential mixed-methods designs: (a) Sequential explanatory (i.e., characterized by one’s collecting and analyzing quantitative data and, subsequently, collecting and analyzing qualitative data; a theoretical perspective may or may not be present); (b) Sequential exploratory (i.e., characterized by one’s initially collecting and analyzing qualitative data and, subsequently, collecting and analyzing quantitative data; a theoretical perspective may or may not be present); (c) Sequential transformative (i.e., one can user either method first; one must prioritize either the quantitative or the qualitative phase; a theoretical perspective is present to guide the study). There are three types of concurrent mixed-methods designs: (a) Concurrent triangulation (i.e., one uses two different methods are to confirm, cross-validate, or corroborate findings in a single study; generally no predominant method to guide the project; a theoretical perspective may or may not be present); (b) Concurrent nested (i.e., one collects data in one phase during which one collects quantitative and qualitative data simultaneously; it has a predominant method to guide the project; a theoretical perspective may or may not be present); (c) Concurrent transformative (i.e., one uses a specific theoretical perspective that may take on the design features of either a triangulation or nested design). Researchers conduct qualitative and quantitative studies as part of a single study. Qualitative and quantitative studies are parts of research programs.\n",
              "\n",
              "Transformative\n",
              "\n",
              "Sequential\n",
              "\n",
              "Creswell et al. (2003), Creswell (2013)\n",
              "\n",
              "Concurrent\n",
              "\n",
              "Strands/ phases of research\n",
              "\n",
              "Tashakkori & Teddlie (2003), Teddlie & Tashakkori (2009) Teddlie & Tashakkori (2006)\n",
              "\n",
              "Single phase (or single study) Multiple phases (or research program)\n",
              "\n",
              "Quasi-mixed This type of design involves only one single phase of the monostrand design conceptualization-experiential-inferential process yet (monostrand includes both qualitative and quantitative components. conversion design)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "482\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description\n",
              "\n",
              "(a) Concurrent mixed designs are “designs in which there are at least two relatively independent strands: one with qualitative questions and data collection and analysis techniques and the other with quantitative questions and data collection and analysis techniques” (Teddlie & Tashakkori, 2006, p. 20). (b) Sequential mixed designs are “designs in which there are at least two strands that occur chronologically” (Teddlie & Tashakkori, 2006, p. 21). (c) Conversion mixed designs are “multistrand Mixed-methods concurrent designs in which mixing of qualitative and multistrand designs quantitative approaches occurs in all components/stages, with data transformed (qualitized or quantized) and analyzed both qualitatively and quantitatively” (Teddlie & Tashakkori, 2006, p. 23). (d) Fully integrated designs are “multistrand concurrent designs in which mixing of qualitative and quantitative approaches occurs in an interactive (i.e., dynamic, reciprocal, interdependent, iterative) manner at all stages of the study” (Teddlie & Tashakkori, 2006, p. 23). Quasi-mixed One mixes these designs (including the concurrent quasimultistrand designs mixed design) at the experiential stage only. Sample integration “The extent to which the relationship between the quantitative and qualitative sampling designs yields quality meta-inferences” (Onwuegbuzie & Johnson, 2006, p. 57). “The extent to which the researcher accurately presents and appropriately utilizes the insider’s view and the observer’s view for purposes such as description and explanation” (Onwuegbuzie & Johnson, 2006, p. 57). “The extent to which the weakness from one approach is compensated by the strengths from the other approach” (Onwuegbuzie & Johnson, 2006, p. 57). “The extent to which the quantizing or qualitizing yields quality meta-inferences” (Onwuegbuzie & Johnson, 2006, p. 57).\n",
              "\n",
              "Inside-outside\n",
              "\n",
              "Weakness minimization Conversion\n",
              "\n",
              "Inference quality\n",
              "\n",
              "Onwuegbuzie & Johnson (2006) (see also Dellinger & Leech, 2007)\n",
              "\n",
              "“The extent to which the researcher’s epistemological, ontological, axiological, methodological, and rhetorical beliefs that underlie the quantitative and qualitative Paradigmatic mixing approaches are successfully (a) combined or (b) blended into a usable package” (Onwuegbuzie & Johnson, 2006, p. 57). Commensurability “The extent to which the meta-inferences made reflect a mixed worldview based on the cognitive process of Gestalt switching and integration” (Onwuegbuzie & Johnson, 2006, p. 57). “The extent to which addressing legitimation of the quantitative and qualitative components of the study result from the use of quantitative, qualitative, and mixed validity types, yielding high quality meta-inferences” (Onwuegbuzie & Johnson, 2006, p. 57).\n",
              "\n",
              "Multiple validities\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "483\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description “The extent to which the consumers of mixed methods research value the meta-inferences stemming from both the quantitative and qualitative components of a study” (Onwuegbuzie & Johnson, 2006, p. 57). “The degree to which the investigator has selected and implemented the most appropriate procedures for answering the research questions” (Teddlie & Taskakkori, 2009, p. 302). “The degree to which credible interpretations have been made on the basis of obtained results” (Teddlie & Taskakkori, 2009, p. 338). Researchers typically state the purpose of the study in terms of research questions. Researchers contain at least one research hypothesis in which they make a prediction of results a priori. Researchers combine procedures (i.e., asking individuals for information and/or experiences; seeing what people do, recording what they do, or making inferences; asking individuals about their relationship with others; and using data collected and or documented by others).\n",
              "\n",
              "Political\n",
              "\n",
              "Teddlie & Tashakkori (2009), Venkatesh et al. (2013)\n",
              "\n",
              "Design quality\n",
              "\n",
              "Explanatory quality Exploratory investigation Confirmatory investigation Multiple modes of data collection (both quantitative and qualitative datacollection techniques)\n",
              "\n",
              "Design strategies\n",
              "\n",
              "Tashakkori & Teddlie (1998)\n",
              "\n",
              "Tashakkori & Teddlie (1998)\n",
              "\n",
              "Data-collection strategies Creswell (2003)\n",
              "\n",
              "Data collection may involve a quantitative checklist or Both predetermined instrument and the visiting of a research site or the and emerging observing of the behavior of individuals without methods predetermined questions. Both open- and closed-ended questions Multiple forms of data drawing on all possibilities Statistical and text analysis Concurrent mixed analysis Sequential QUALQUAN analysis Data collection might involve a standardized questionnaire and open-ended questions. Data collection involves a general combination of qualitative and quantitative data collection. The type of data may be numeric information gathered on scales of instruments or more textual information, audio recording of participant’s voice, or written notes. Researchers simultaneously analyze qualitative and quantitative data (e.g., parallel mixed analysis, concurrent analysis of the same data) (quantitizing/qualitizing). Researchers analyze qualitative data (subjective/imaginative interpretation) and, subsequently, analyze quantitative data (data/operations and statistical analysis). Researchers analyze quantitative data analysis (data/operations and statistical analysis) and, subsequently, analyze qualitative data (subjective/imaginative interpretation). Researchers merge qualitative and quantitative data to understand a research problem. In the embedded design, researchers embed one form of data in another—maybe either a monostrand or multistrand design with concurrent or sequential approach. Researchers use qualitative data to help explain or elaborate initial quantitative results. Issue 7\n",
              "\n",
              "Tashakkori & Teddlie (1998) Data-analysis strategies\n",
              "\n",
              "Sequential QUANQUAL analysis Triangulation Creswell & Plano Clark (2007) Embedded Explanatory\n",
              "\n",
              "Volume 17\n",
              "\n",
              "\n",
              "484\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description In this mixed-methods design, researchers collect quantitative data after collecting qualitative data to test and explain relationships found based on analyzing qualitative data. This design involves a concurrent design using “exactly the same sample members participate in both the qualitative and quantitative phases of the study” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a concurrent design in which “the samples for the qualitative and quantitative components of the research are different but are drawn from the same population of interest” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a concurrent design in which “the sample members selected for one phase of the study represent a subset of those participants chosen for the other facet of the investigation” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a concurrent design using “two or more sets of samples that are extracted from different levels of the study” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a sequential design using “exactly the same sample members participate in both the qualitative and quantitative phases of the study” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a sequential design in which “the samples for the qualitative and quantitative components of the research are different but are drawn from the same population of interest” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a sequential design in which “the sample members selected for one phase of the study represent a subset of those participants chosen for the other facet of the investigation” (Onwuegbuzie & Collins, 2007, p. 292). This design involves a sequential design using “two or more sets of samples that are extracted from different levels of the study” (Onwuegbuzie & Collins, 2007, p. 292). The basic mixed-methods sampling strategies include purposive sampling and probability sampling. Purposive sampling refers to “selecting units (e.g., individuals, groups of individuals, institutions) based on specific purposes associated with answering a research study’s questions”. Probability sampling involves “selecting a relatively large number of units from a population, or from specific subgroups (strata) of a population, in a random manner where the probability of inclusion for every member of the population is determinable” (Teddlie & Yu, 2007, p. 77).\n",
              "\n",
              "Exploratory\n",
              "\n",
              "Concurrent design using identical samples Concurrent design using parallel samples\n",
              "\n",
              "Concurrent design using nested samples Concurrent design using multilevel samples Sequential design using identical samples Sequential design using parallel samples\n",
              "\n",
              "Collins et al. (2007), Onwuegbuzie & Collins (2007)\n",
              "\n",
              "Sampling designs\n",
              "\n",
              "Sequential design using nested samples Sequential design using multilevel samples\n",
              "\n",
              "Teddlie & Yu (2007)\n",
              "\n",
              "Basic mixedmethods sampling strategies\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "485\n",
              "\n",
              "Table A1. Review of Selected Theoretical Literature in Mixed-methods Research Property of mixedmethods approach Reference(s) Dimensions Description Sequential mixed-methods sampling involves selecting “units of analysis for an MM study through the sequential use of probability and purposive sampling strategies (QUAN-QUAL), or vice versa (QUAL-QUAN)” (Teddlie & Yu, 2007, p. 89). Concurrent mixed-methods sampling involves selecting “units of analysis for a mixed methods study through the simultaneous use of both probability and purposive sampling” (Teddlie & Yu, 2007, p. 89). Multilevel mixed-methods sampling refers to “a general sampling strategy in which probability and purposive sampling techniques are used at different levels of the study” (Teddlie & Yu, 2007, p. 89). These sampling techniques generally involve using multiple sampling strategies (e.g., using both sequential mixed-methods and concurrent mixed-methods sampling).\n",
              "\n",
              "Sequential mixedmethods sampling\n",
              "\n",
              "Concurrent mixedmethods sampling\n",
              "\n",
              "Multilevel mixedmethods sampling Sampling using multiple mixedmethods sampling strategies\n",
              "\n",
              "Type of reasoning\n",
              "\n",
              "Morse (2003)\n",
              "\n",
              "Inductive theoretical reasoning is the process in which one uses a small observation to infer a larger theory. In Inductive theoretical inductive reasoning, researchers try to develop a new reasoning theory, work in the discovery mode, and try to find answers to relatively new problems. Deductive theoretical reasoning works from a more Deductive theoretical general theory to a more specific observation or reasoning hypothesis. This reasoning tests a theory or hypothesis.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "486\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Appendix B: Mixed-methods Inference Quality\n",
              "Table B1. Mixed-methods Inference Quality (Adapted from Venkatesh et al., 2013) Quality aspects Quality criteria Description Challenges/dilemmas Selecting the most appropriate paradigm(s) for mixed-methods research and integrating different paradigmatic approaches. The degree to which methods selected Design suitability/ and research design employed are appropriateness appropriate for answering the research question.\n",
              "\n",
              "Design quality\n",
              "\n",
              "Quantitative: the degree to which one implements the design components for the quantitative study (e.g., sampling, measures, data collection procedures) Selecting the most suitable designs to address the research questions. Design adequacy with acceptable quality and rigor. Time and resources required to Qualitative: the degree to which one collect different types of data. implements the qualitative design components with acceptable quality and rigor. Quantitative: the degree to which the quantitative data analysis procedures/strategies are appropriate and adequate to provide plausible answers to the research questions. Qualitative: the degree to which qualitative data-analysis procedures/strategies are appropriate and adequate to provide plausible answers to the research questions. The degree to which interpretations from the quantitative analysis closely follow the relevant findings, are consistent with theory and the state of knowledge in the field, and are generalizable. The degree to which interpretations from the qualitative analysis closely follow the relevant findings, are consistent with theory and the state of knowledge in the field, and are transferable. Integrative efficacy: the degree to which one effectively integrates inferences made in each strand of a mixed-methods research inquiry into a theoretically consistent meta-inference. Integrative inference/metainference Inference transferability: the degree to which meta-inferences from mixedmethods research are generalizable or transferable to other contexts or settings. Integrative correspondence: the degree to which meta-inferences from mixedmethods research satisfy the initial purpose for using a mixed-methods approach. Identifying the major source of inconsistency when the two sets of inferences do not agree with each other.\n",
              "\n",
              "Analytic adequacy\n",
              "\n",
              "The issue of nomenclature and basic definitions used in mixedmethods research.\n",
              "\n",
              "Quantitative inferences\n",
              "\n",
              "Qualitative inferences\n",
              "\n",
              "Explanation quality\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "487\n",
              "\n",
              "Appendix C: Overview of Research Studies\n",
              "Table C1. Overview of Research Studies Time 1 constructs measured Time 2 constructs measured (6 months after time 1)\n",
              "\n",
              "Adopters/users (N = 201) Study 1\n",
              "\n",
              "Independent variables: application for personal use, utility for children, utility for work-related use, applications for fun, status gains, friends and Dependent variable: usage family, secondary sources, workplace referents, behavior Fear of technological change, decline cost, cost, perceived ease of use, requisite knowledge for PC use. Independent variables: application for personal use, utility for children, utility for work-related use, applications for fun, status gains, friends and Dependent variable: family, secondary sources, workplace referents, purchase behavior fear of technological change, decline cost, cost, perceived ease of use, requisite knowledge for PC use. Independent variables: application for personal use, utility for children, utility for work-related use, applications for fun, status gains, friends and Dependent variable: usage family, secondary sources, workplace referents, behavior fear of technological change, decline cost, cost, perceived ease of use, requisite knowledge for PC use. Independent variables: application for personal use, utility for children, utility for work-related use, applications for fun, status gains, friends and Dependent variable: family, secondary sources, workplace referents, purchase behavior fear of technological change, decline cost, cost, perceived ease of use, requisite knowledge for PC use.\n",
              "\n",
              "Non-adopters (N = 435)\n",
              "\n",
              "Adopters/users (N = 370) Study 2 Non-adopters (N = 610)\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "488\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Appendix D: Coding for the Study 1\n",
              "Table D1. Coding for the Study 1 (Adapted from Venkatesh & Brown, 2001) Belief structure Applications for personal use Utility for children Example quotes “...my wife uses that cooking program a lot.” “I saw some neat programs like Quicken and Turbo tax so I got a computer.” “We have a ninth-grader who uses it for his school work…” “The kids have to know computers these days. We make sure they are learning how to use it.” “I just work from home a lot more now. There’s no way I could have kept my job if I didn’t get a computer.” “I can drive in to work after rush hour because I get work done at home.” “We play all sorts of games on it. It’s fun.” “I just have fun...surfing the net, talking with people on golf newsgroups, and what not…” “My friends are counting on me to tell them what machine to get. I gotta keep up with this stuff because that’s why they think I’m cool.” “I don’t know I just always got these toys because people who are smart get them.” “My sons advised me to buy it.” “...two of my church friends who said they are doing amazing stuff with it.” “There was a 60 Minutes or Dateline special that said something about a stalker or kidnapper ever since that we don’t want our kids to have anything to do with it.” “It’s too scary…there are so many stories in the papers about all these porn on the Internet. I wouldn’t want my son to get near all that…This is just another problem like drugs were when I was growing up.” “My boss said he has a computer at home, so I thought, gee, maybe I should get one so I can be like him.” “The guy in the next cube told me about how nice it is to work at home and if I got a computer I could probably do that.” “It’s just changing way too fast. If I buy a computer today, it’s like too old tomorrow.” “I am just plain scared that if I buy something, it’s going to be like obsolete in like a year. Then who knows, I have to buy another one.” “I wanna wait till they’re like VCR prices, so may be in like five years.” “Prices are dropping so fast, and I didn’t buy for so long so I am just going to wait till it’s 50 bucks or maybe 100.” “We don’t have the money.” “Computers are for rich people.” “I did that Windows stuff for a while at work, they’re like too hard to use. I heard that Apples...that’s the same as Macintosh, right? Anyway, they are easy to use but not that Windows stuff, but people are saying like there’s no use learning the Apple stuff.” “It’s way too hard for me. I’m a tailor, even using a cash register is like too hard for me.”\n",
              "\n",
              "Utility for work-related use\n",
              "\n",
              "Applications for fun\n",
              "\n",
              "Status gains Friends and family influence\n",
              "\n",
              "Secondary sources influence\n",
              "\n",
              "Workplace referents influence\n",
              "\n",
              "Fear of technological change\n",
              "\n",
              "Declining cost Cost\n",
              "\n",
              "Perceived ease of use\n",
              "\n",
              "“I don’t even know how to type. It’ll take me forever to learn it.” Requisite knowledge for PC use “I don’t know a darn thing about computers other than everyone wants to learn something about them.”\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "489\n",
              "\n",
              "Appendix E: Belief Structures of MATH\n",
              "Table E1. Belief Structures of MATH\n",
              "\n",
              "Belief structure\n",
              "Applications for personal use Utility for children Utility for work-related use Applications for fun Status gains Friends and family influence Secondary sources influence Workplace referents influence Fear of technological change Declining cost Cost Perceived ease of use\n",
              "\n",
              "Definition\n",
              "“The extent to which using a PC enhances the effectiveness of household activities” (Venkatesh & Brown, 2001, p. 82). “The extent to which using a PC enhances the effectiveness of household activities” (Venkatesh & Brown, 2001, p. 82). The extent to which using a PC enhances the effectiveness of performing workrelated activities (Venkatesh & Brown, 2001). “The pleasure derived from PC use” (Venkatesh & Brown, 2001, p. 82). These are specific to PC use, rather than general traits (see Webster & Martocchio, 1992). The increase in prestige that coincides with a purchase of the PC for home use (Venkatesh & Brown, 2001). “The extent to which members of a social network influence one another’s behavior” (Venkatesh & Brown, 2001, p. 82). In this case, the members are friends and family. The extent to which information from TV, newspaper and other secondary sources influences behavior (Venkatesh & Brown, 2001). The extent to which co-workers influence behavior (see Taylor & Todd, 1995) The extent to which rapidly changing technology is associated with fear of obsolescence or apprehension regarding a PC purchase (Venkatesh & Brown, 2001). The extent to which cost of a PC is decreasing in such a way that it inhibits adoption (Venkatesh & Brown, 2001). The extent to which the current cost of a PC is too high (Venkatesh & Brown, 2001). The degree to which using the PC is free from effort (Davis, 1989; see also Venkatesh & Brown, 2001).\n",
              "\n",
              "The individual’s belief that he/she has the knowledge necessary to use a PC. This Requisite knowledge for PC use is very closely tied to the concept of computer self-efficacy (Compeau & Higgins, 1995; see also Venkatesh & Brown, 2001).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "490\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Appendix F: Study 1 Descriptive Statistics and Correlations\n",
              "Table F1. Study 1 Descriptive Statistics and Correlationsa\n",
              "MATH Applns. for 1 personal use Utility for 2 children Utility for 3 work-rel. use Applns. for 4 fun Status 5 gains Infl. of 6 friends and family Infl. of 7 secondary sources Peer 8 influence Fear of 9 tech. change Declining 10 cost 11 Cost M 4.55 4.57 4.77 4.20 4.01 4.03 4.10 3.22 3.03 3.88 2.90 SD 1.04 1.20 1.21 1.05 0.85 0.77 0.77 0.64 0.60 0.93 0.70 0.78 0.77 N/A M 3.44 3.88 4.02 4.17 4.03 3.80 4.14 3.66 5.03 4.98 5.06 2.99 4.13 N/A SD 1.13 1.03 .29*** 0.90 .37*** .15* 1.06 0.99 1.06 0.77 0.68 0.90 0.84 0.76 0.78 0.74 .21** .31*** .22** .02 .02 .04 .14* .22** .02 .11 .08 .23** 1 2 3 4 5 .17* 6 7 8 .16* 9 .02 10 .06 .10 .14* .10 .03 .01 .06 .01 11 .02 .03 12 13 14 .30*** .44*** .21** .21** .22** .20** .24*** .26*** .02 .01 .27***\n",
              "\n",
              ".24** .33*** .20** .32*** .30*** .16* -.21** .26*** .23** .21** .02 .24** .03 .04 .02 .04 .20** -.16* .08 .02 .03 -.21** .02 .10 -.20** .01 .02 .16* .02 .35***\n",
              "\n",
              "-.17* .21** .20** .22** .10 .02 .16* .02 .10 .27*** .17* .25*** .04 .03 .02 .04 .02 .05 .10 .19* .16* .18* .21** .21**\n",
              "\n",
              ".19* .28*** .19* .22** .26*** .22** .03 .05 .04 .01 .22** .17* .18* .03 .02 .00 .08 .03 .17* .16* .02 .15* .18* .15*\n",
              "\n",
              ".31*** .67*** .31***\n",
              "\n",
              ".22** .27*** .68*** .35*** -.04 -.20** -.16* -.02 -.17* .20** .15* .02 .08 .18* .02 .04 -.03 .01 .15* .15* .02 .08 .08 .02 .01 .21**\n",
              "\n",
              ".42*** .27*** -.20** -.25* -.35*** .40*** .12 .00 .12 .11 .17* .31*** .22** .20** .18* .02 .18* .24**\n",
              "\n",
              ".21** .37*** -.22** .04 .02 .10\n",
              "\n",
              "Percd. 12 ease of 4.61 use Requisite 13 4.80 knowledge Usage/ 14 N/A Purchase\n",
              "\n",
              ".34*** .22*** .27***\n",
              "\n",
              ".15* -.21** .16* .12\n",
              "\n",
              "N/A .28*** .30*** .22** .20** .19**\n",
              "\n",
              "Below-diagonal elements are correlations for current users with a dependent variable of usage behavior. Above-diagonal elements are correlations for current non-users with a dependent variable of adoption behavior. * p < .05; ** p < .01; *** p < .001. a Note that we used a five-point scale for the perceptual measures in this study.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "491\n",
              "\n",
              "Appendix G: Quantitative Validity of Quantized Data\n",
              "After we collected the quantitative data, we assessed the validity of the quantized data from study 1 (analyzed in the first phase of the study) using different techniques. First, we performed a normality test to ensure that the model specification was appropriate for our data. The test of normality revealed that the residuals were normally distributed and the skewness and kurtosis coefficients were fairly similar across two datasets. Second, we performed mean differences tests to examine whether the sample means differed across two data collections (see Table G1). Given that we collected the two datasets at different periods using two different methods and scales, we expected nominal significant differences across two datasets. As expected, the results showed some statistically significant differences (e.g., the mean of status gains among adopters was higher in study 2 than in study 1). Because we analyzed the data independently of one another, these mean differences were unlikely a major problem in our study.\n",
              "Table G1. Mean Difference Test Results Adopters (Study 1 vs. Study 2) MATH 1 2 3 4 5 6 7 8 9 10 11 12 13 Applns. for personal use Utility for children Utility for work-rel. use Applns. for fun Status gains Infl. of friends and family Infl. of secondary sources Peer influence Fear of tech. change Declining cost Cost Percd. ease of use Requisite knowledge t-value -1.21 -0.09 -1.26 -1.44 -2.90** -0.72 1.37 1.67 3.27*** -2.63** -0.84 -2.06* -1.92 95% CI (-0.28) – (0.07) (-0.21) – (0.18) (-0.33) – (0.07) (-0.31) – (0.04) (-0.35) – (-0.07) (-0.18) – (0.08) (-0.03) – (0.22) (-0.01) – (0.19) (0.06) – (0.27) (-0.36) – (-0.05) (-0.16) – (0.06) (-0.27) – (-0.01) (-0.26) – (0.01) Non-adopters (Study 1 vs. Study 2) t-value -1.93 -1.55 -1.43 2.44* -3.37*** 3.59*** 3.15** 5.19*** -1.62 2.13* 3.31*** 2.40* 3.39*** 95% CI (-0.26) – (0.00) (-0.22) – (0.02) (-0.18) – (0.03) (0.03) – (0.06) (-0.31) – (-0.08) (0.10) – (0.35) (0.05) – (0.24) (0.13) – (0.28) (-0.19) – (0.02) (0.01) – (0.21) (0.06) – (0.24) (0.02) – (0.21) (0.06) – (0.24)\n",
              "\n",
              "* p < .05; ** p < .01; *** p < .001; CI: Confidence Interval\n",
              "\n",
              "Further, we performed a standardized mean difference (SMD) or Cohen’s d test to estimate the method effect employed in each phase of the study (see Table G2). The SMD assumes that the differences in standard deviations among studies reflect differences in measurement scales and not real differences in variability among study populations (Higgins & Green, 2011). An SMD of zero indicates that the two samples have equivalent effects and the SMD increases as the difference between two samples increases. Cohen (1988) offers the following guidelines for interpreting the magnitude of the SMD in the social sciences: small, SMD = 0.2; medium, SMD = 0.5; and large, SMD = 0.8. The results revealed that most of the SMD scores in the non-adopters condition were significant, which indicates the two samples have nonequivalent effects. The significance of SMD coefficients confirmed the need to adopt a mixed-methods research approach in this study to minimize the weaknesses of either quantitative or qualitative methods and uncover the contradictory findings (if any) through comparing the qualitative data- and quantitative data-analysis results.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "492\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Table G2. Standardized Mean Difference (d) Test Results Adopters MATH 1 2 3 4 5 6 7 8 9 10 11 12 13 Applns. for personal use Utility for children Utility for work-rel. use Applns. for fun Status gains Infl. of friends and family Infl. of secondary sources Peer influence Fear of tech. change Declining cost Cost Percd. ease of use Requisite knowledge d -0.12 -0.00 -0.11 -0.13 -0.26 -0.06 0.12 0.15 0.35 -0.23 -0.07 -0.18 -0.16 95% CI (-0.27) – (0.06) (-1.18) – (0.16) (-0.28) – (0.06) (-0.30) – (0.04) (-0.43) – (-0.09) (-2.23) – (0.11) (-0.04) – (0.29) (-0.01) – (0.32) (0.18) – (0.53) (-0.40) – (-0.06) (-0.24) – (0.09) (-0.35) – (0.00) (-0.34) – (0.01) Sig. Ns Ns Ns Ns S Ns Ns Ns S S Ns Ns Ns d -0.12 -0.09 -0.08 0.15 -0.21 0.22 0.19 0.33 -0.17 0.13 0.21 0.14 0.22 Non-adopters 95% CI (-0.24) – (0.00) (-0.22) – (0.02) (-0.31) – (0.03) (0.03) – (0.28) (-0.34) – (-0.09) (0.10) – (0.35) (0.07) – (0.32) (0.21) – (0.45) (-0.29) – (-0.05) (0.01) – (0.25) (0.08) – (0.33) (0.02) – (0.27) (0.09) – (0.34) Sig. Ns S Ns S S S S S Ns S S S S\n",
              "\n",
              "S: Supported; Ns: Not supported\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "493\n",
              "\n",
              "Appendix H: Study 1 and 2 Results\n",
              "Table H1. Study 1 and 2 Results Study 1 Current owners DV: use (R2 = .58) I.V. Attitudinal beliefs Applications for personal use Utility for children Utility for work-related use Applications for fun Status gains Normative beliefs Friends and family Secondary sources Workplace referents Control beliefs Fear of technological change Declining cost Cost Perceived ease of use Requisite knowledge for PC use Ns Ns Ns Ns Ns -.25*** .14* -.19** .14* .15* Ns Ns Ns Ns Ns -.22*** .15* -.16* .16* Ns Ns Ns Ns .21* .15* Ns Ns Ns Ns .17* .17* Ns .30*** .15* .19** .30*** .16* .28*** Ns .20** .14* Ns .33*** .17* .15* .28*** Ns .28*** Ns .21** .17* Ns ß Current non-owners DV: purchase (R2 = .57) ß Study 2 Current owners DV: use (R2 = .57) ß Current non-owners DV: purchase (R2 = .50) ß\n",
              "\n",
              "* p < .05; ** p < .01; *** p < .001; Ns: not supported\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "494\n",
              "\n",
              "Guidelines for Conducting Mixed-methods Research: An Extension and Illustration\n",
              "\n",
              "Appendix I: Study 2 ICRs, AVEs, Descriptive Statistics, and Correlations\n",
              "Table I1. Study 2 ICRs, AVEs, and Descriptive Statistics MATH 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Applns. for personal use Utility for children Utility for work-rel. use Applns. for fun Status gains Infl. of friends and family Infl. of secondary sources Peer influence Fear of tech. change Declining cost Cost Percd. ease of use Requisite knowledge Usage/purchase M 4.66 4.58 4.90 4.33 4.22 4.08 4.01 3.13 2.86 4.09 2.95 4.75 4.93 N/A SD 1.03 1.07 1.10 0.99 0.78 0.82 0.71 0.56 0.58 0.87 0.62 0.76 0.78 N/A ICR .92 .90 .88 .87 .85 .90 .88 .92 .90 .89 .80 .92 .80 N/A AVE .77 .80 .83 .82 .81 .76 .77 .80 .81 .84 .80 .79 .80 N/A M 3.57 3.98 4.10 4.01 4.23 3.57 3.99 3.45 5.18 4.87 4.91 2.87 3.98 N/A SD 0.98 1.02 0.88 1.02 0.88 0.96 0.74 0.59 0.86 0.80 0.66 0.82 0.65 N/A ICR .81 .80 .79 .85 .81 .80 .75 .75 .80 .82 .80 .90 .82 N/A AVE .89 .85 .90 .80 .80 .80 .87 .77 .80 .75 .83 .90 .82 N/A\n",
              "\n",
              "Table I2. Study 2 Correlationsa\n",
              "MATH 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Applns. for 1 .31*** .41*** .22** .13 .22** .20** .16* .07 .06 .06 .18* .20*** .27*** personal use Utility for 2 .28*** .22** .36*** .19* .30*** .31*** .15 -.20* .10 .06 .09 .09 .25*** children Utility for 3 .39*** .19* .22** .22* .20* .10 .19* -.17* .03 -.16* .20*** .21** .23** work-rel. use Applns. for 4 .20** .29*** .21** .08 .22* .03 .13 .10 .06 .05 .22*** .18* .20** fun 5 Status gains .07 .08 .06 .10 .11 .09 .08 -.20** .09 .09 .09 .08 .19* Infl. of friends 6 .16* .28*** .18* .20* .13 .29** .66*** .06 .02 .06 .09 .07 .19* and family Infl. of 7 secondary .19** .26*** .19** .08 .10 .25** .29*** .13 .06 .09 .06 .13 .20** sources Peer 8 .08 .16* .18* .19* .26*** .69*** .36*** -.23** .09 .12 .10 .17* influence Fear of tech. 9 .08 .08 .13 -.09 -.22** -.13 .08 .08 .40*** .23*** -.18** -.23* .19* change Declining 10 .13 .09 .08 -.03 .08 .09 .10 .09 .37*** .39*** .13 .17* -.36*** cost 11 Cost .02 .10 .16* -.12 .03 -.02 .05 .16* .20* .34*** .01 .08 .19* Percd. ease 12 .19** .02 .16* .19* .10 .02 .09 .08 -.19 .07 .14 .31*** .22** of use Requisite 13 .18* .06 .10 .12 .07 .16* .07 .16* -.20* .08 .12 .29*** .24*** knowledge Usage/ 14 .26*** .28*** .19** .19* .18* .16* .19* .17* .13 .11 0.18** .23** .19* .23** purchase Below-diagonal elements are correlations for current users with a dependent variable of usage behavior. Above-diagonal elements are correlations for current non-users with a dependent variable of adoption behavior (adapted from Brown & Venkatesh, 2005). * p < .05; ** p < .01; *** p < .001. a Note that we used a seven-point scale for the perceptual measures in this study.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "495\n",
              "\n",
              "About the Authors\n",
              "Viswanath Venkatesh is a Distinguished Professor and George and Boyce Billingsley Chair in Information Systems at the Walton College of Business, University of Arkansas. His research focuses on understanding the diffusion of technologies in organizations and society. For over two decades, he has worked with several companies and government agencies, and has rigorously studied real-world phenomena. The sponsorship of his research has been about US$10M. His work has appeared in leading journals in human-computer interaction, information systems, organizational behavior, psychology, marketing, medical informatics, and operations management journals. He is one of only two scholars to have published 20 or more papers in MIS Quarterly (MISQ). From 2006-2015, he was the most productive in terms of publications in the premier journals in information systems (i.e., Information Systems Research (ISR) and MISQ). He is widely regarded as one of the most influential scholars in business and economics, with about 55,000 citations and about 14,000 citations per Google Scholar and Web of Science, respectively. In 2014, he was recognized by Thomson Reuters as one of only 95 high-impact scholars in business and economics (highlycited.com) based on publications from 2000 to 2012. In 2008, his MISQ (2003) paper was identified as a current classic by Science Watch (a Thompson Reuters’ service), and, since 2009, it has been the most influential paper in one of the four Research Front Maps in business and economics. Since 2012, he serves as a Senior Editor (SE) at MISQ. From 2008 to 2011, he served as an SE at ISR. MISQ named him “Reviewer of the Year” in 1999. Susan A. Brown is the McClelland Professor of Management Information Systems in the Eller College of the University of Arizona. Prior to joining the University of Arizona, Dr. Brown was an assistant professor at Indiana University. She completed her PhD at the University of Minnesota and an MBA at Syracuse University. Her research interests include technology implementation, individual adoption, computermediated communication, technology-mediated learning, and related topics. She has received funding for her research from the National Science Foundation, and other public and private organizations. Her work has appeared in journals including MIS Quarterly, Information Systems Research, Journal of the Association for Information Systems, Organizational Behavior and Human Decision Processes, Journal of Management Information Systems, and others. She has served as an AE at MIS Quarterly, Information Systems Research, Journal of the Association for Information Systems, and Decision Sciences and is currently an SE at MIS Quarterly and co-EIC of AIS Transactions on Replication Research. Yulia W. Sullivan is an assistant professor of MIS in the School of Management at Binghamton University, State University of New York. She earned her PhD degree in information systems from the University of North Texas and her MBA from Chosun University, South Korea. Her research interests include humancomputer interaction, cognitive information systems, and mixed methods research.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 7\n",
              "\n",
              "\n",
              "Copyright of Journal of the Association for Information Systems is the property of Association for Information Systems and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "V40wiwBFG5g7",
        "colab_type": "code",
        "outputId": "81df7760-d39d-4e4f-cbb9-1e543775126c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc8.sents if 'p <' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[* p < .05; ** p < .01;, ** p < .001., (0.06) – (0.24)\n",
              " \n",
              " * p < .05;, ** p < .01;, ** p < .001; CI: Confidence Interval\n",
              " \n",
              " Further, we performed a standardized mean difference (SMD) or Cohen’s d test to estimate the method effect employed in each phase of the study (see Table G2)., ß Current non-owners DV: purchase (R2 = .50) ß\n",
              " \n",
              " * p < .05;, ** p < .01;, ** p < .001; Ns: not supported\n",
              " \n",
              " Volume 17\n",
              " \n",
              " Issue 7\n",
              " \n",
              " \n",
              " 494\n",
              " \n",
              " Guidelines for Conducting Mixed-methods Research:, * p < .05; ** p < .01;, ** p < .001.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "metadata": {
        "id": "Az9bkViY36fF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JiY3WG8GVeJ",
        "colab_type": "code",
        "outputId": "361bf7e8-6960-43bc-d6d7-b01230334f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11930
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc9 = nlp(open(u\"KimK#GopalA#HobergG_2016_Does Product Market Competition Drive CVC Investment - Evidence from the U.S. IT Industry_Information Systems Research_2.txt\").read())\n",
        "doc9\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This article was downloaded by: [130.89.97.167] On: 17 February 2017, At: 08:09 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA\n",
              "\n",
              "Information Systems Research\n",
              "Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org\n",
              "\n",
              "Does Product Market Competition Drive CVC Investment? Evidence from the U.S. IT Industry\n",
              "Keongtae Kim, Anandasivam Gopal, Gerard Hoberg\n",
              "\n",
              "To cite this article: Keongtae Kim, Anandasivam Gopal, Gerard Hoberg (2016) Does Product Market Competition Drive CVC Investment? Evidence from the U.S. IT Industry. Information Systems Research 27(2):259-281. http://dx.doi.org/10.1287/isre.2016.0620 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages\n",
              "\n",
              "INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org\n",
              "\n",
              "\n",
              "Information Systems Research\n",
              "Vol. 27, No. 2, June 2016, pp. 259–281 ISSN 1047-7047 (print) ISSN 1526-5536 (online) http://dx.doi.org/10.1287/isre.2016.0620 © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Does Product Market Competition Drive CVC Investment? Evidence from the U.S. IT Industry\n",
              "Keongtae Kim\n",
              "College of Business, City University of Hong Kong, Kowloon, Hong Kong, keongkim@cityu.edu.hk\n",
              "\n",
              "Anandasivam Gopal\n",
              "Robert H. Smith School of Business, University of Maryland, College Park, Maryland 20742, agopal@rhsmith.umd.edu\n",
              "\n",
              "Gerard Hoberg\n",
              "Marshall School of Business, University of Southern California, Los Angeles, California 90089, hoberg@marshall.usc.edu\n",
              "\n",
              "W\n",
              "\n",
              "e study the effect of product market competition on the propensity to use corporate venture capital (CVC) as a part of an information technology (IT) ﬁrm’s innovation strategy. Using novel measures of product market competition based on product descriptions from ﬁrm 10-K statements and accounting for potential endogeneity, we investigate how product market competition between 1997 and 2007 relates to the magnitude of CVC spending. We ﬁrst ﬁnd that ﬁrms in competitive markets make higher research and development (R&D) and CVC investments. In addition, we ﬁnd that increasing product market competition leads to a shift away from internal R&D spending and into CVC. These movements are signiﬁcantly stronger for technology leaders, i.e., ﬁrms with deep patent stocks, in the IT industry. We also ﬁnd that CVC appears to be an effective way of exploiting external knowledge for technology leaders in the IT-producing industry, but not for technology slow starters. CVC investments lead to signiﬁcantly more patent applications for technology leaders but no appreciable difference for slow starters. Our results provide new insights for theories of innovation in competitive, dynamic markets, potentially as part of a portfolio that includes internal R&D as well as open innovation models. Keywords : information technology; product market competition; corporate venture capital; technology leadership; innovation; econometrics; text analysis History : Chris Forman, Senior Editor; Kai-Lung Hui, Associate Editor. This paper was received on May 2, 2013, and was with the authors 16 months for 4 revisions. Published online in Articles in Advance April 11, 2016.\n",
              "\n",
              "1.\n",
              "\n",
              "Introduction\n",
              "\n",
              "Innovation is the lifeblood of technology companies. Developing new product and process innovations is a key contributor to a ﬁrm’s competitive position, especially for information technology (IT) producing ﬁrms (Kleis et al. 2012). IT-producing ﬁrms need to manage a more robust and ﬂexible pipeline of innovative activities, given the pace and complexity of technological change inherent to this industry (McAfee and Brynjolfsson 2008, Wadhwa and Kotha 2006). Managing the supply side of innovation entails using multiple channels to access knowledge and learning since relying only on traditional research and development (R&D) may be associated with inertia, weaker incentives, and lower productivity (Dushnitsky and Lenox 2005b, Fulghieri and Sevilir 2009). Therefore, managers need to look outside the ﬁrm for innovative ideas and insight. This is a variation of the classic make-or-buy decision but speciﬁcally associated with innovation. Firms have several avenues available to access such knowledge from outside thereby complementing traditional R&D. Alliances, licensing,\n",
              "259\n",
              "\n",
              "and acquisitions are three traditional strategies that have been studied in the literature (Cassiman and Veugelers 2006). Newer channels, such as crowdsourcing (Leimeister et al. 2009) and innovation tournaments (Boudreau et al. 2011), typically classiﬁed as open innovation (Chesbrough 2006), have also emerged since the mid-2000s. One such open innovation strategy is the focus of the research reported here, i.e., corporate venture capital (CVC). CVC is deﬁned as minority equity investments by established ﬁrms in start-ups, typically alongside traditional venture capitalists (VC) (Dushnitsky and Lenox 2005a). The beneﬁts of CVC are primarily nonﬁnancial; a study commissioned by the National Institute of Standards and Technology (NIST) in 2008 identiﬁed knowledge and learning objectives as the primary drivers of CVC investments (MacMillan et al. 2008). However, not all ﬁrms choose to invest in CVC: The importance of knowledge and learning is likely to be most relevant in technology-intensive industries where persistent innovation is a key determinant of success (Tambe et al. 2012). Not surprisingly,\n",
              "\n",
              "\n",
              "260\n",
              "Figure 1\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "(Color online) Industry Sectors Receiving Venture Capital and CVC Investment All venture investment 2006 (%) 19 5 17 6 10 4 10 2 78 69 64 42 40 27 26 20 17 16 15 08 01 100 CVC investment 2006 (%) 13 4 22 0 86 12 0 10 5 53 10 1 38 35 48 21 08 12 15 03 02 00 100 Top sectors for VC investment Software Biotechnology Medical devices Telecommunications Top sectors for CVC investment Biotechnology Software Telecommunications Semiconductors Media/entertainment\n",
              "\n",
              "Industry sector Software Biotechnology Medical devices and equipment Telecommunications Semiconductors Industrial/energy Media and entertainment Networking and equipment IT services Electronics/instrumentation Business products and services Consumer products and services Financial services Computers and peripherals Healthcare services Retailing/distribution Other Total Source. MacMillan et al. (2008).\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "a large proportion of CVC is found in the IT sector (Dushnitsky and Lenox 2006), as indicated in Figure 1, which illustrates that the IT industry accounts for the majority of CVC investments from 1997 to 2007, the period of our study. Beyond a need for innovation and knowledge, a critical factor that might also have a strong incremental inﬂuence on the observed intensity of CVC spending in the IT industry is product market competition. It is well established that IT-producing ﬁrms work in hyper-competitive markets (Lee et al. 2010) where there are strong winner-take-all dynamics (McAfee and Brynjolfsson 2008). In the IT industry, fast moving technological progress and the resulting enhanced product market competition increases the ﬁrm’s incentives to develop innovative products and services. Therefore, as a ﬁrst response, these incentives lead to an increased focus on R&D investments. However, if traditional R&D is structurally limited in terms of how responsive it can be to a marketplace characterized by increasing product market competition, IT ﬁrms will need to consider alternative sources of innovation that provide ﬂexibility, knowledge, and competitive advantages in a shortened timeframe (Dushnitsky and Lenox 2005b). Although traditional R&D is still key to success in the IT industry, increased competition will lead the ﬁrm to consider alternative forms of innovation spending. While this increased spending may show up in the form of alliances, acquisitions, and licensing agreements, we contend that CVC, as a channel, will be particularly attractive as a response to increased competition (Fulghieri and Sevilir 2009). Compared to other\n",
              "\n",
              "external innovation strategies, CVC provides a combination of ﬂexibility, technological knowledge, institutional familiarity, and strategic beneﬁts. Thus, in this paper, the ﬁrst research question we address is: Does product market competition faced by IT-producing ﬁrms lead to higher investments in CVC spending, all else equal? CVC, as a form of innovation spending, might provide the ﬁrm with access to knowledge and learning. Yet, are all ﬁrms equally likely to invest in CVC? Because CVC requires the ability to evaluate the entrepreneurial venture’s technology portfolio, assess its long-term ﬁnancial and strategic outlook, and make appropriate investment decisions, we argue that IT ﬁrms with deep technological stocks of knowledge are likely to be better positioned to use the CVC channel (Melville et al. 2007). In addition, these ﬁrms are more likely to respond strategically to increasing competition by realizing the limits of traditional R&D and the potential value of external innovation funding. Therefore, we posit that ﬁrms with technological leadership in patenting (i.e., deep patent stocks) are more likely to respond to increased competition through the CVC channel. Entrepreneurs also prefer to receive CVC funding from investor ﬁrms that have the technological capacities to understand and effectively use their technologies (Katila et al. 2008). Therefore, we hypothesize and show that technology leadership moderates the relationship between product market competition and CVC investments. Finally, we document the effectiveness of CVC strategies by considering the impact of CVC investments on ex-post innovation output. If, as indicated by prior work on the CVC model (Dushnitsky and Lenox 2005b), CVC investors pursue knowledge and\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "261\n",
              "\n",
              "learning about new technologies and products, we should observe systematically higher innovation output ex-post resulting from such investments. Here again, we argue that the technological leadership in an investing ﬁrm is likely to moderate the beneﬁts from CVC on innovation output. Firms with technological leadership in innovation are more likely to ﬁrst, make better judgments in terms of their choice of which entrepreneurs to support, and second, to more effectively use the resulting knowledge. Therefore, our second research question asks: Do CVC investments lead to higher innovation output? If so, is the relationship moderated by the technology leadership of the investing ﬁrm? We address these research questions using data collected on all CVC investments made by U.S. ITproducing ﬁrms between 1997 and 2007. During this period, the level of CVC activity in the United States grew from roughly $800 million to over $2 billion.1 We collect CVC funding information from the VentureExpert data set, augmented with data from CompuStat and the National Bureau of Economic Research (NBER) patent database. A speciﬁc challenge is to devise appropriate measures for product market competition. Standard measures in the literature based on Standard Industrial Classiﬁcation (SIC) or North American Industry Classiﬁcation Systems (NAICS) codes have inherent limitations as they are static and rigid (Tang 2006). The static nature of these measures is particularly limiting given the dynamic nature of market boundaries and competition in the fast moving IT industry, with rapid entry of new ﬁrms and exit of incumbents (Lee et al. 2010, McAfee and Brynjolfsson 2008). We use a novel measure of competition based on 10-K product descriptions. The textual network industry classiﬁcation (TNIC) was pioneered by Hoberg and Phillips (2010, 2016). TNIC classiﬁcations are based on the product market vocabulary in each ﬁrm’s 10-K and are updated every year. Firms using similar product market vocabularies are classiﬁed as being in the same industry, allowing for a more accurate measure of competition that captures the dynamic nature of the IT industry. Yearly updating of the entire TNIC product space ensures that ongoing product innovations are reﬂected in market boundaries and the identiﬁcation of each ﬁrm’s competitors. To our knowledge, this level of variability in competition cannot be matched using SIC-based measures. Our results verify that competition is linked to innovation spending in the IT-producing industry through a positive relationship between competition and traditional “internal ﬁrm” R&D. We subsequently\n",
              "1\n",
              "\n",
              "http://www.nvca.org/pressreleases/2016-nvca-yearbook-captures -busy-year-for-venture-capital-activity/.\n",
              "\n",
              "ﬁnd that IT-producing ﬁrms respond to product market competition by increasing their propensity to invest in CVC and the amount they invest. A standard deviation increase in TNIC competition (in total similarity) is associated with a 76% increase in CVC intensity, corresponding to an increase from 1.7% to 3.1% in ﬁrm annual sales. Furthermore, we ﬁnd that the shift toward CVC is accompanied by a shift away from internal R&D spending. These effects are stronger for technology leaders than for technology slow starters. Finally, we conﬁrm that CVC investments also improve ex-post innovation productivity, but only for technology leaders. We make three contributions to the information systems (IS) literature. First, much of the literature focuses on how technology investments in the ﬁrm generate greater innovation (Kleis et al. 2012, Nerkar and Paruchuri 2005, Tanriverdi 2005). However, less is known about the implications of alternative channels to access innovation, especially in the face of increasing competition. IT ﬁrms face a strategic imperative to continually innovate as their product markets become more competitive; in such settings, the traditional supply model of internal R&D appears increasingly limited in its ability to adequately respond (Schildt et al. 2005). Therefore, ﬁrms need to make choices in terms of their allocation of innovation spending between internal and external sources. In this paper, we examine product market competition, a systemic feature of the IT industry (McAfee and Brynjolfsson 2008), as one such driver of this shift from internal to external innovation spending. Second, in evaluating the impact of product market competition on IT ﬁrms, we direct attention away from the traditional focus on the demand side (Joshi et al. 2010, Lee et al. 2010) to the supply side, i.e., the innovation-producing part of the ﬁrm. We argue that increased competition faced by the ﬁrm necessitates changes in strategy not only as to how it competes in the marketplace but also in the decisions it makes to overcome the shortcomings of traditional internal R&D spending. We thus broaden the focus of IS research on competition to include its effects on the use of external innovation. Finally, we show that R&D plays a more important contingent role in IT ﬁrms than previously acknowledged. Deep stocks of technological knowledge accumulated as a result of a robust R&D pipeline (Kleis et al. 2012), allow the ﬁrm to respond nimbly to increasing competition. The presence of an effective R&D division allows ﬁrms to productively tap valuable exploratory knowledge from their CVC investments, as CVC investment in startups contributes valuable knowledge well beyond ﬁnancial gains. Our work highlights the complementary role of R&D and open innovation models for accessing knowledge\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "262\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "in IT-producing ﬁrms, especially when competition increases.\n",
              "\n",
              "2.\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Literature Review and Hypotheses\n",
              "\n",
              "We start by brieﬂy discussing the close relationship between CVC and the IT industry, thereby establishing the incidence and legitimacy of CVC in the IT industry. We then argue that in the fast moving IT industry, higher levels of competition are associated with increased innovation spending in the form of R&D. We further propose that since internal R&D can be inertial and slow, ﬁrms facing competition will look outside the ﬁrm for technological insight and knowledge, thereby providing rationale for why CVC may be particularly appealing in the range of external options available to the ﬁrm. Finally, we hypothesize that CVC investments are likely to provide value for the investor ﬁrm in terms of innovation output (patents), but only for technological leaders. 2.1. Corporate Venture Capital and the IT Industry CVC is deﬁned as the establishment of a structurally distinct entity in an established ﬁrm dedicated to equity investing in young and promising enterprises (Gompers and Lerner 1998). CVC units retain several characteristics of traditional VCs. They perform due diligence when evaluating potential portfolio companies, they make deals that protect their assets and those of the entrepreneur, and they participate in syndication with VCs (Gompers and Lerner 2001). CVC units often complement private investors by providing industry-speciﬁc knowledge and information about complementary assets; they also frequently serve as a “beta” site for testing and certifying the new entrepreneur (Chesbrough 2006, De Clercq et al. 2006). However, CVC units also differ from traditional VC ﬁrms in several critical aspects. First, unlike VCs, CVC units use internal funds and do not raise capital from institutional investors (Chesbrough 2006). Second, unlike VC ﬁrms, ﬁnancial returns are not the primary motive behind CVC investments (Dushnitsky and Lenox 2006). CVC ﬁrms pursue strategic objectives, i.e., early exposure to innovative technologies that might disrupt business models, access to new markets and businesses (Schildt et al. 2005), identiﬁcation of prospective acquisition targets (Benson and Ziedonis 2009), and potential means to dissuade competitors from acquiring a portfolio company (Fulghieri and Sevilir 2009). Third, CVC ﬁrms rarely take board positions or lead syndication efforts, preferring to invest alongside VCs (De Clercq et al. 2006). The literature also notes that CVC units use different compensation schemes to avoid conﬂicts of interest (Block and Ornati 1988), suggesting again that ﬁnancial incentives, in the form of entrepreneurial exits, may not dominate CVC investments; instead,\n",
              "\n",
              "access to knowledge, intelligence, and innovative insight are of greater importance to investing ﬁrms (Dushnitsky and Lenox 2005b). The emergence of the CVC model in the early 1990s was sparked by, and intimately associated with, the rise of the IT industry (Gaba and Meyer 2008). Disappointing returns associated with traditional R&D in the late 1980s and rapid technological shifts in computing acted as catalysts for the interest in CVC (Mowery 1999). Several factors linked to the IT industry inﬂuenced the growth of CVC as an investment strategy. First, traditional VC activity was starting to achieve celebrity status due to successes in identifying early stage IT ﬁrms, and rapidly accelerating the growth of these ﬁrms into established brand names (Gompers and Lerner 2001). Having grown through the VC model, ﬁrms such as eBay, Apple, and Intel have thus pioneered their own CVC activities (Gaba and Meyer 2008). Second, over 70% of VC disbursements ﬂowed to IT industry entrepreneurs, fueled in part by the Internet, telecom, and online commerce (Gompers and Lerner 2001). In addition, the relatively short clock speed observed in IT-producing ﬁrms allowed VCs to quickly recoup their investments, relative to slower sectors such as biotechnology (Mendelson and Pillai 1999). These successes further propelled the adoption of CVC programs, especially among IT-producing ﬁrms (Gaba and Meyer 2008). Third, and signiﬁcantly, the VC model showed how relatively small investments in speciﬁc new ventures could provide very high returns; Kortum and Lerner (2000) estimated that a dollar invested in CVC generated four times as much innovation as equivalent investments in traditional R&D. Even modest CVC spending relative to R&D spending could have relatively high payoffs. Not surprisingly, CVC disbursements as a share of total VC spending grew from 5.5% in 1995 to over 14% in 2000, with two-thirds of overall funding emerging from ITproducing ﬁrms.2 Leading IT ﬁrms, such as Intel, have helped legitimize CVC as an acceptable channel for innovation spending: “Intel and other electronic ﬁrms typically view venture capital investing as one of the three pillars of innovation, along with internal R&D and acquisitions” (Roberts 2006, p. 54). Despite these accolades, the CVC model is not without risk. A central concern is that the VC model is not easily deployed in the corporate context (Gaba and Meyer 2008, Gompers and Lerner 2001). VC ﬁrms rely heavily on tacit knowledge and personal networks for guidance and valuation; these are challenging to emulate by corporate CVC units. In addition, CVC units must offer compensation schemes that attract top talent while minimizing friction between\n",
              "2\n",
              "\n",
              "http://www.nvca.org/pressreleases/2016-nvca-yearbook-captures -busy-year-for-venture-capital-activity/.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "263\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "departmental heads (R&D managers in particular) and in-house CVC heads (Block and Ornati 1988, Edelson 2001). Unlike traditional VC heads, CVC heads must also balance parent-ﬁrm preferences against their own valuations of entrepreneurs; this might limit their overall effectiveness (Dushnitsky and Lenox 2005b, Gaba and Meyer 2008). As a result, corporate venturing programs have been referred to as cyclical, unstable, naïve, and prone to failure (Edelson 2001, Gaba and Meyer 2008). The potential drawbacks of CVC coupled with the potential beneﬁt of access to innovative knowledge that the strategy offers (Chesbrough 2006, Dushnitsky and Lenox 2005b, Wadhwa and Kotha 2006), beg the question: When would IT-producing ﬁrms choose to pursue CVC as a strategy to access external knowledge? We argue that one salient aspect of the IT industry that may systematically “tip” ﬁrms into using the CVC model is the presence of competition arising from volatility in product market boundaries and the fast pace of innovation. Before hypothesizing about the relationship between competition and CVC, we ﬁrst discuss the broader question of the effects of competition on ﬁrm-level innovation spending. Product Market Competition and Innovation Spending At its core, CVC represents innovation spending on the part of the ﬁrm. Therefore, we must ﬁrst describe a baseline relationship between competition and innovation spending in the IT industry. The literature shows that the relationship between competition and innovation spending is rather ambiguous. On one hand, theories of industrial organization argue that innovative output should decline with competition (Dixit and Stiglitz 1977), since competition reduces any post-innovative rents earned by the innovator (Grossman and Helpman 1991). This Schumpeterian viewpoint argues that monopoly power is a precondition for innovation (Schumpeter 1942). Monopoly ﬁrms face less market uncertainty, can secure postinnovation rents, can preempt entry, face fewer ﬁnancial constraints (Dasgupta and Stiglitz 1980, Gilbert and Newbery 1982), and therefore are more likely to innovate. An alternative view is that competition tends to increase innovation (Arrow 1962, Lee and Wilde 1980); this is referred to as the “escape competition” hypothesis (Aghion et al. 2005). Innovation offers the possibility of creating new products to replace incumbents, thereby allowing the innovator to “escape” competition (Arrow 1962). Alternatively, ﬁrm managers might seek private beneﬁts from innovation to keep their jobs (Hart 1983). For example, managers might innovate to keep costs down as revenues are reduced through competition; this leads to a positive relationship 2.2.\n",
              "\n",
              "between competition and innovation. Finally, Aghion et al. (2005) argue that competition may increase innovation spending because it reduces the ﬁrm’s preinnovation rents more than its post-innovation rents. The empirical literature on the effect of competition on innovation spending is ambiguous. Nickell (1996) and Blundell et al. (1999) show a positive association between competition and innovation spending, while Schumpeter (1942) and Tang (2006) report the opposite. Aghion et al. (2005) ﬁnd an inverted U-shaped relationship. Theoretical models have suggested that the relationship between competition and innovation is likely nuanced and is contingent on factors such as the form of innovation (product versus process, for instance) (Bonanno and Haworth 1998), the presence of variable R&D cost (Lee and Wilde 1980), “neck-toneck” industry structure (Nelson and Winter 2009), and intellectual property (IP) appropriability in the industry (Levin et al. 1985). In effect, the relationship between competition and innovation depends on salient features of the industry at hand. In the IT industry, relevant to our study, we identify critical factors that help establish the likely relationship between competition and innovation spending. First, the industry is characterized by volatility in market boundaries as market positions frequently change (Lee et al. 2010). This volatility is driven by high clock speed (McAfee and Brynjolfsson 2008, Mendelson and Pillai 1999). Thus, incumbent market leaders cannot “coast” on older or existing technologies for long. Second, competition in the IT industry is aggressive and comes in the form of new products and technologies that might supplant incumbents (Kleis et al. 2012, Sambamurthy et al. 2003). In such markets, innovation and new product development are likely to be more important than price competition or branding (Wadhwa and Kotha 2006). Therefore, managing the supply side of innovation is critical for the longterm success of the ﬁrm. Because market leadership changes frequently due to rapid developments in new products and technologies, and innovation is needed to develop new products and services, the IT industry is likely to be characterized by the “escape competition” effect (Aghion et al. 2005, Arrow 1962, Lee and Wilde 1980). Therefore, as a baseline, we propose the following hypothesis: Hypothesis 1 (H1). Higher levels of competition in a product market will be associated with an increase in R&D investments, within the IT industry. Because the data-generating process underlying product market competition in the IT industry is dynamic, and competitive boundaries change frequently, this hypothesis can only be fully tested using a dynamic industry classiﬁcation. Static deﬁnitions may not fully capture the nature or effect of competition.\n",
              "\n",
              "\n",
              "264\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "Given appropriate dynamic measures of competition, we test this hypothesis using absolute R&D spending (in dollars) (Kleis et al. 2012) scaled by ﬁrm sales. Product Market Competition and CVC Spending Although H1 posits that competition is positively associated with R&D spending, internal R&D is limited in its ability to keep up with the required speed and range of innovation needed in competitive product markets. Internal R&D tends to be inertial; managers ﬁnd it hard to adapt to new market conditions, and are often trapped by legacy success (competency traps) (Chesbrough 2006). In addition, existing R&D is focused on existing customers, and extending the services of existing technological platforms and products (i.e., “own and protect” approach) rather than truly path-breaking innovations (Diener and Piller 2010). Thus, R&D is viewed as less efﬁcient than external innovation, especially in fast moving industries (Aghion et al. 2005). When small capital investments can produce potentially disruptive technologies (disruptive here referring to the introduction of new technologies that create new structure and competitive dynamics in the industry; Mendelson 2000), the productivity differences between internal R&D and external innovation spending become even larger (Aghion et al. 2005). Because the IT-producing industry is characterized by relatively low capital requirements (the stereotypical “IT entrepreneur in a garage”) and a high incidence of disruptors (Walsh et al. 2002), particularly in the turbulent software and service sectors (McAfee and Brynjolfsson 2008), the incentive to augment traditional R&D is likely to be keenly felt. This incentive should be increasing in product market competition for two reasons. First, increasing competition is typically associated with lower barriers to entry. As a result, the capital requirements needed to threaten or challenge incumbents should be low, allowing greater chances of market loss, and even exit. Second, the incidence of game-changing technological players emerging in the marketplace should increase with higher competition; the odds of successful disruption of the existing market structure for the enterprising entrepreneurial ﬁrm or aggressive incumbent competitor are higher. Successful technological aggressor ﬁrms seek the possibility of escaping competition via innovation and new technologies, especially in the dynamic IT context (McAfee and Brynjolfsson 2008, Mendelson and Pillai 1999). The combination of low capital and quicker disruption, coming from increasing competition, will incentivize ﬁrms to move some innovation spending outside the ﬁrm (Aghion et al. 2005). Even though the above arguments for external innovation are couched in the language of incentives 2.3.\n",
              "\n",
              "and efﬁciency, the true value of external innovation comes from providing knowledge to the investor ﬁrm, a critical source of competitive advantage in technology-based industries (Grant 1996). It has been acknowledged that all forms of external innovation spending, such as licensing, alliances, and acquisitions, as well as CVC, lead to knowledge gains for the ﬁrm (Ahuja and Katila 2001, Arora 1996, Dushnitsky and Lenox 2005b, Iansiti 2000, Rothaermel 2001). However, if access to knowledge is the key to driving innovation spending outside the ﬁrm, why might CVC be preferred by ﬁrms, even when controlling for alternative external innovation pathways? We propose that CVC may be of particular importance as a knowledge source in the presence of competition for two reasons. First, CVC is especially useful in providing access to explorative knowledge (Schildt et al. 2005) associated with “search, variation, risk taking, experimentation, play, ﬂexibility, discovery and innovation” (March 1991, p. 71). By contrast, licensing, alliances, and acquisitions are strongly associated with exploitative learning, characterized by “reﬁnement, choice, production, efﬁciency, selection, and implementation” (March 1991, p. 71). Increasing product market competition requires that ﬁrms access knowledge that helps them escape competition; exploratory learning is more useful in terms of providing break-through technological options to the ﬁrm to escape competition (Schildt et al. 2005), thereby making CVC more attractive. Second, unlike acquisitions and alliances, which are rooted in present R&D capabilities of the ﬁrm (Iansiti 2000), CVC allows the investor ﬁrm to retain ﬂexibility in the relationship at a relatively low cost and with no commitment beyond the modest ﬁnancial one in the short term. In competitive markets, the ability to retain ﬂexibility and reversibility in the relationship with the external innovation ﬁrm is particularly attractive (Sahaym et al. 2010, Van de Vrande et al. 2006). Schildt et al. (2005) reinforce this relationship, noting that among options available to investor ﬁrms, CVC is the most ﬂexible, and hence more attractive as competition increases. Furthermore, given the close relationship between CVC and the IT industry (Gaba and Meyer 2008), using CVC to gain external knowledge and innovation is familiar to IT ﬁrms’ managers, through narratives from peers. Mechanisms such as board observation rights, shared personnel, technology review processes, and ofﬁcial liaison roles, allow CVC investors to gain knowledge from their portfolio ﬁrms. For example, Agilent Technologies’ CVC unit worked closely with their portfolio company to share information about technologies, qualify investment opportunities, and connect the portfolio company to its own initiatives (Dushnitsky and Lenox\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "265\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "2005b). Intel’s investments in Berkeley Networks provided similar value: “As Intel performed its due diligence on its investment, it began to see the outlines of a possible strategy shift culminating in the Intel Internet Exchange Architecture, launched in 1999. The investment in Berkeley Networks allowed Intel to identify a promising opportunity more quickly than it might have otherwise” (Dushnitsky and Lenox 2005b, p. 618). Chesbrough (2002) writes about how Microsoft earmarked $1 billion to invest in startups that could help advance its new .Net architecture. Through these investments, Microsoft hoped not only to get the ﬁrms to exploit its new architecture but also to identify opportunities that could be incorporated into the architecture for new products and services, thereby gaining over rivals Sun Microsystems and IBM. Even during the early years of CVC, Lerner (2013) discusses how Analog Devices created a CVC program to invest in new technology ﬁrms with the goal of gathering strategic information at relatively low cost. All of these knowledge-oriented beneﬁts from CVC are likely to be even more attractive to the investor ﬁrm during times of increased competition. Clearly, if knowledge is the key strategic beneﬁt behind CVC, such investments likely entail knowledge transfer in both directions (to and from the entrepreneur ﬁrm to the CVC investor). As a result, CVC investment can be seen as a trade-off between the value of knowledge transfer gained (incentive for more CVC) and the cost of leaking proprietary knowledge to other ﬁrms (disincentive for more CVC). Firms in less competitive product markets, where barriers to entry are high and need to be maintained, likely see more of the disincentives of leakage, as this can directly weaken their current barriers to entry. By contrast, ﬁrms in competitive industries, who lack barriers to entry, likely see more of the incentives to gaining knowledge from CVC. This trade-off reinforces the notion that as competition increases, the intensity of CVC should also increase. We thus propose: Hypothesis 2 (H2). Higher levels of competition in a product market will be associated with an increase in CVC investments, within the IT industry. Analogous to our testing of H1, we examine whether dynamically measured competition can explain CVC dollars spent scaled by ﬁrm sales. While we have argued that product market competition will lead to greater CVC investments, total innovation spending is typically limited or constrained in IT ﬁrms (Kleis et al. 2012). Firms are required to strategically allocate their innovation spending between traditional R&D and external channels such as CVC. Therefore, the arguments for CVC, predicated on the value of explorative knowledge and ﬂexibility, can be extended to further predict that IT\n",
              "\n",
              "ﬁrms will explicitly shift funds from internal R&D to CVC as a response to competition. This stronger prediction, based on theoretical work by Fulghieri and Sevilir (2009), comes from two effects. First, the incentive effect predicts that an external entrepreneur has, and retains, stronger incentives to innovate compared to internal R&D, consistent with Aghion and Tirole (1994) and Chesbrough (2006). This is in contrast to acquisitions (where external innovation is internalized) and alliances (where internal R&D sets the tone for the alliance; Lane and Lubatkin 1998). The armslength nature of CVC helps maintain strong incentives for the portfolio ﬁrm (Schildt et al. 2005). The second effect that is distinctive to CVC and that enhances its value in competitive environments is the strategic effect (Fulghieri and Sevilir 2009), which posits that CVC investments are likely to generate information about the entrepreneur as a potential acquisition target (Benson and Ziedonis 2009). In addition, the CVC investment can thus prevent other ﬁrms from funding or acquiring the entrepreneur, making it unlikely that any knowledge gains from the relationship can freely enter the market and be used by competitors. Internal R&D does not provide any such beneﬁts because of its weak incentives; the same is true for licensing arrangements or alliances, which are alternative channels for external innovation, since they may not preclude competitors from accessing the same know-how (Sahaym et al. 2010). Furthermore, acquisitions may provide the focal ﬁrm with exploitative knowledge assets but signiﬁcantly reduce any ﬂexibility (Schildt et al. 2005), while also reducing the incentives of the external innovator (Aghion and Tirole 1994).3 Thus, as competition increases, ﬁrms are likely to re-allocate their innovation spending from R&D into CVC, leading to the following hypothesis: Hypothesis 3 (H3). Higher levels of competition in a product market will be associated with an increase in CVC investments in the IT industry, relative to internal R&D. To test this hypothesis, we examine the effect of competition on the ratio of CVC spending to total\n",
              "3\n",
              "\n",
              "An indirect validation of these arguments is provided by Benson and Ziedonis (2010) who ask: Do CVC investors acquire their portfolio ﬁrms and if so, do they gain greater value than when they acquire other ﬁrms? The results show that most CVC investors do not buy portfolio ﬁrms and that such acquisitions are valuedestroying, relative to other acquisitions. If a CVC investment, which is ﬁnancially much smaller than an acquisition or equivalent investments in R&D, can provide knowledge and insight to the investor, as has been shown by Dushnitsky and Lenox (2005b), then there is no need to acquire the portfolio ﬁrm. Furthermore, if the investor does purchase the ﬁrm, Benson and Ziedonis (2010) argue that this is indicative of “managerial overconﬁdence or agency problems at the CVC program level” (p. 497), leading to value destruction. However, the knowledge-based beneﬁts from CVC are still evident in the literature (Schildt et al. 2005).\n",
              "\n",
              "\n",
              "266\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "CVC plus R&D spending (the shift toward CVC relative to R&D). The Moderating Role of Technological Leadership Hypotheses H2 and H3 argue for the direct effect of competition on CVC, but do not account for the effects of heterogeneity on the R&D capabilities of the investor ﬁrm. We are primarily interested in the ﬁrm’s R&D technological leadership, i.e., the extent to which the ﬁrm is a leader in terms of its abilities to develop, maintain, and enhance technological and product innovation in the ﬁrm’s R&D function (Fichman 2004, Aghion et al. 2005). Prior literature shows that technology leaders, with deep technological expertise, tend to have better processes and personnel to absorb new technologies (Fichman 2004). By virtue of these capabilities, they are more likely to be “mindful” of the beneﬁts of the various innovation options available to them, rather than simply exhibiting herding behavior (Swanson and Ramiller 2004). In the alliances and licensing literature as well, knowledge gains for ﬁrms from such activities are driven by the extent to which participating ﬁrms have existing deep knowledge stocks (Lane and Lubatkin 1998, Malhotra et al. 2005, Vasudeva and Anand 2011). It follows, then, that in the CVC channel, technological leader ﬁrms (with signiﬁcant patent portfolios) are likely to have better “intuition” about the value of speciﬁc investments and their potential role in allowing the ﬁrm to escape competition. This stems from raw entrepreneurial quality as well as synergies between the entrepreneurial ﬁrm and the investor (Chesbrough 2002, Dushnitsky and Lenox 2005b). The investor’s technological leadership is even more relevant in the case of explorative learning (Schildt et al. 2005), which is more uncertain; Cohen and Levinthal (1990) argue that having related knowledge facilitates the identiﬁcation, interpretation, and assimilation of related knowledge. If H2 and H3 hold, and if explorative knowledge is relevant to generating the positive link between CVC spending and the level of competition, these effects should hold especially for technology leaders facing competition. Technology leaders will be better at identifying and preemptively acting to combat competitive threats relative to ﬁrms that do not have deep capabilities. Therefore: Hypothesis 4 (H4). The relationship between product market competition and CVC investments is greater for ﬁrms that have deep technological expertise, namely technology leaders in their product markets. We measure technology leadership using the ﬁrm’s deeply lagged patent stocks, thereby avoiding endogeneity concerns while ensuring that leaders have a mature ability to turn innovation into patents. 2.4.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "The previous hypotheses were based on the logic that the focal ﬁrm’s status as a technology leader, combined with potential access to knowledge, are central to the relationship between competition and CVC investment. A straightforward implication is that the proposed knowledge transfer from CVC should directly generate higher ex-post innovation output (i.e., in patents), and that technology leaders should beneﬁt most from these transfers (Cockburn and Henderson 1998, Katila et al. 2008). By virtue of their technological capabilities and ability to convert research activity into patents and thereafter products or services, technological leaders are more likely to convert knowledge gained from CVC into innovation output. While all ﬁrms may perceive competitive threats, the corresponding ability to appropriate returns from “outside” likely resides only with leaders (Chesbrough 2006). Therefore: Hypothesis 5 (H5). CVC investments have a positive impact on innovation output in the IT industry, but only for technology leaders.\n",
              "\n",
              "3.\n",
              "\n",
              "Data and Methodology\n",
              "\n",
              "3.1. Sample Construction We construct a data set of U.S. public ﬁrms that invested in CVC between 1997 and 2007 (we limit attention to these years due to the availability of TNIC data and NBER patent data) from the VentureXpert database. VentureXpert provides detailed information on entrepreneurial ventures funded by independent VCs as well as corporate investors, along with other information such as the round of funding and information about the entrepreneur. We ﬁrst collect data on all potential corporate investors who were observed to be CVC investors from 1997 through 2007, providing a list of 800 potential CVC ﬁrms.4 Using various sources of information (Google, Lexus/Nexus, etc.), we then manually linked these CVC ﬁrms to their corporate parents. Firms that represented ﬁnancial companies, partnerships, funds or those with a parent that we could not identify were dropped, leaving us with 326 distinct CVC parent ﬁrms, of which 154 are publicly-traded IT ﬁrms.5 We excluded CVC\n",
              "4\n",
              "\n",
              "We include the following VentureXpert categories for VC funds: Non-Financial Corp. Afﬁliate or Subsidiary Partnership, Venture/PE Subsidiary of Non-Financial Corp., Venture/PE Subsidiary of Other Companies NEC, Venture/PE Subsidiary of Service Providers, Direct Investor/Non-Financial Corp., Direct Investor/Service Provider, SBIC Afﬁliate with Non-Financial Corp., and Non-Financial Corp. Afﬁliate or Subsidiary.\n",
              "5\n",
              "\n",
              "Of 326 distinct CVC ﬁrms, 268 are publicly traded. This suggests that about 57% of public corporate investors are IT ﬁrms. We deﬁne IT ﬁrms based on the following SIC4 codes: 3570, 3571, 3572, 3576, 3577, 3578, 3579, 3661, 3663, 3674, 3812, 3822, 3825, 3826, 3827, 3842, 3845, 3861, 4812, 4813, 4822, 4832, 4833, 4841, 4899, 7370, 7371, 7372, 7373, or 7374.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "267\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "ﬁrms that have a foreign parent, leaving us with a sample of 145 ﬁrms that invested in CVC at least once between 1997 and 2007. We included other years without any CVC investment for each ﬁrm after the ﬁrst year in which it was observed to invest in CVC. Periods before this ﬁrst observed CVC investment were omitted as it is unclear whether the ﬁrm had any interest in CVC in these periods.6 Our ﬁnal sample thus comprises 1,185 ﬁrm-year observations for 145 ﬁrms between 1997 and 2007 and forms our primary database. We add relevant information from other sources to this database. For each ﬁrm-year observation in our baseline database, we require information on the product market competition faced by the given ﬁrm in that year. Therefore, we use TNIC data from Hoberg and Phillips (2010, 2016), described in §3.2. Additional ﬁnancial and organizational data on investor ﬁrms was taken from Compustat. We also used the Center for Research in Security Prices (CRSP) database to obtain monthly stock returns for investor ﬁrms. Finally, we collect patent data for each ﬁrm in each year from the NBER patent citation database (Hall et al. 2001) to capture the level of technological expertise of the ﬁrm in a given year. Consistent with the literature, we used application year rather than grant year for patent activity because the application date is closer to the date of the actual innovative activity. This combined database provides us with an unbalanced panel of ﬁrm-year CVC investments, and describes the extent to which ﬁrms allocate innovation dollars to CVC in a given year. We discuss the individual variables in detail in §3.2. 3.2. Variable Deﬁnitions Product market competition, the primary independent variable in our analysis, is measured using the TNIC data set provided by Hoberg and Phillips (henceforth “HP”) (Hoberg and Phillips 2010, 2016). While a detailed description of the data set is provided in the online appendix (available as supplemental material at http://dx.doi.org/10.1287/isre.2016.0620), we brieﬂy outline the TNIC variables here. The TNICbased competition variables are based on the text of product descriptions of ﬁrm 10-K ﬁling statements. Competitors are identiﬁed as pairs of ﬁrms with textual similarity of their product descriptions that\n",
              "6\n",
              "\n",
              "exceeds a threshold, which in all yields an industry classiﬁcation indicating the set of ﬁrms with which each ﬁrm likely competes. The intuition is that ﬁrms that use highly similar product market vocabularies operate in the same product markets and are rivals. Once a set of rival ﬁrms is identiﬁed for each focal ﬁrm using computational linguistics, dynamic measures of product market competition can then be computed because the set of rivals is recalculated in each year, as 10-Ks are re-ﬁled annually. The database is comprehensive on publicly-traded ﬁrms, as HP Web crawl and parse all business descriptions from the full set of 10-K ﬁlings on the SEC Edgar database. Using the textual cosine similarity method on the vector of product market nouns found in ﬁrms’ business descriptions, HP then computes a matrix of ﬁrm-pair similarities, which lie in the range 0 1 for every possible pairing of ﬁrms. This procedure is repeated every year, thereby allowing these similarity metrics to vary over time for the same focal ﬁrm (the metrics are deﬁned in more detail in the online appendix). The annual similarity matrix is analogous to a network identifying product market relatedness. A given ﬁrm A’s industry is composed of ﬁrms whose product similarity to ﬁrm A exceeds a threshold.7 The threshold is chosen such that the TNIC industry granularity matches that of three-digit SIC codes.8 The central advantage that the TNIC provides in our setting is that TNIC updates each focal ﬁrm’s competitive industry every year as every ﬁrm’s 10-K product description changes. TNIC industries also relax the transitivity property of SIC codes, allowing us to improve power and to measure competition strictly relative to each given ﬁrm, rather than in coarse SIC code groups. TNIC data underscores how much ﬁrm competitive environments change: Roughly 35% of TNIC rivals in a given year t are no longer rivals one year later. This is due to ongoing innovation and product changes by IT ﬁrms seeking product differentiation. TNIC thus allows for a more updated and ﬁne-grained deﬁnition of competition that is useful given the high level of competition\n",
              "7\n",
              "\n",
              "As a robustness check, we also examined versions of TNIC that are calibrated to be as granular as SIC-2 and SIC-4 (more coarse and more ﬁne) and found that our main ﬁndings are robust to these alternatives.\n",
              "8\n",
              "\n",
              "When we include observations on ﬁrms before their ﬁrst observed CVC investment, the incremental number of observations is just 18 (increasing sample size from 1,185 to 1,203). Most CVC ﬁrms in our data set had already made such investments before the period of our study. Additionally, given the unbalanced nature of the panel, many ﬁrms enter and exit the panel during the period of analysis. Finally, some observations were dropped due to missing data. Including these 18 observations in the analysis provides nearly identical results to those presented here.\n",
              "\n",
              "HP also show that the TNIC focuses on horizontal and not vertical links, as links derived from 10-K business descriptions are uncorrelated with vertical links derived using the Bureau of Economic Analysis (BEA) input-output tables. In line with this, we examine whether our main results are robust to the treatment of ﬁrms that report producing in more than one industry (i.e., diversiﬁed ﬁrms). We use the COMPUSTAT segment tapes to identify how many segments each ﬁrm has. We now drop ﬁrms that have more than one segment. Our results do not qualitatively change even when we drop diversiﬁed ﬁrms in our sample. This suggests that the effect of diversiﬁed ﬁrms is small in our sample.\n",
              "\n",
              "\n",
              "268\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "observed in the IT-producing industries (Tanriverdi and Lee 2008). Based on TNIC industry classiﬁcations, we consider three product market competition measures, i.e., the CVC ﬁrm’s total similarity across its TNIC industry, the total number of its TNIC rival ﬁrms, and the Lerner Index (LI) in a TNIC industry.9 The total similarity of ﬁrm A is deﬁned as the sum of product market pairwise cosine similarities relative to ﬁrm A across ﬁrms in A’s TNIC industry. Higher total similarity implies more competition in the ﬁrm’s TNIC industry. In addition, we compute the total number of ﬁrms operating in the focal ﬁrm’s TNIC per year. A larger number of ﬁrms indicates higher product market competition.10 We believe that the TNIC total similarity and ﬁrm count competition measures are more informative in explaining CVC investments because typical measures of industry structure, such as the LI and Herﬁndahl–Hirschman (HHI) indices, are based on past or current performance, whereas the two TNIC variables measure present and forwardlooking competition (Hoberg and Phillips 2016). In particular, the new measures focus on the existence and proximity of ﬁrms in a given ﬁrm’s market rather than the distribution of ﬁrm sales or income levels (which often do not reﬂect the fact that the IT ﬁrms’ potential is forward looking). Nevertheless, we include the industry LI for comparison with prior work (Aghion et al. 2005). We calculate the industry LI based on the price cost margin in the ﬁrm’s TNIC as follows: 1 N Proﬁti LI = 1 − N i=1 Salesi Higher industry LI indicates more competition. Because each focal ﬁrm has a unique industry (TNIC membership does not require transitivity), these industry-level variables are not the same for all ﬁrms in a focal ﬁrm’s industry. This ability to avoid restrictive transitivity requirements does not apply to traditional industry classiﬁcations such as SIC and NAICS as all ﬁrms in the same industry (e.g., SIC 730) have the same competition measures. To construct our key dependent variables for R&D and CVC spending, we compute them as actual dollars spent scaled by the ﬁrm’s sales, to reﬂect scale.\n",
              "9\n",
              "\n",
              "Larger ﬁrms have more resources and incentives to invest in innovation (Dushnitsky and Lenox 2005a) as competition increases. Note also that our results are robust to instead using absolute values of R&D or CVC spending. Following prior work (Blundell et al. 1999, Dushnitsky and Lenox 2005a), we use patent stock to capture ﬁrm technological capability. This variable represents the technological depth of the R&D function in each ﬁrm, as a precursor to identifying technology leaders and slow starters. The patent stock is calculated as the sum of last year’s patent stock and the current year’s number of patent applications Patent stockit = Number of patentsit + 1 − Patent stockit−1\n",
              "\n",
              "In extant literature, measures of an increase in competition include a decline in industry concentration (Tirole 1988), an increase in the substitutability across differentiated products (Tang 2006), a decline in ﬁrm proﬁtability (Aghion et al. 2005), an increase in the number of ﬁrms (Gilbert and Newbery 1982), and a switch from Cournot to Bertrand competition (Bonanno and Haworth 1998).\n",
              "10\n",
              "\n",
              "where is the rate of stock depreciation. Each patent is depreciated at a rate of 15%. Thus, older patents have less impact on the ﬁrm’s patent stock than recent patents. We also include a control for ﬁrm-level CVC experience, measured as the number of years between the ﬁrst observed CVC investment by that ﬁrm in VentureXpert and the current year. We use CompuStat to obtain the following ﬁrmspeciﬁc ﬁnancial variables, i.e., ﬁrm size, free cash ﬂow, and sales growth, consistent with prior work (Benson and Ziedonis 2009, Dushnitsky and Lenox 2005a). We control for the size of corporate CVC investors because larger ﬁrms are likely to have more resources to devote to innovation activities. Financial slack, captured by free cash ﬂow, is valuable for all forms of investments, but may be particularly important for innovation investments (Blundell et al. 1999). External ﬁnancing is believed to be more costly than internal ﬁnancing in an imperfect capital market for information reasons (Myers and Majluf 1984). External ﬁnancing may also be riskier because competitors may acquire valuable information for innovation projects when a ﬁrm wants to ﬁnance externally (Blundell et al. 1999). We include sales growth to control for changes in demand (Sahaym et al. 2010). We use the CRSP database to obtain the standard deviation of monthly stock returns as a proxy for market uncertainty. We also use the Securities Data Company (SDC) Platinum database to identify all Mergers and Acquisitions (M&A) transactions for each ﬁrm and use the total M&A spending as a control variable, when necessary. The variable descriptions for all variables used in the analysis are shown in Table 1. 3.3. Empirical Implementation The base equation for testing the impact of competition on innovation spending is Yit =\n",
              "0+ 1 · PMCit −1 + 2 · Xit −1 + i+ t + it\n",
              "\n",
              "While the two measures, TNIC total similarity and number of ﬁrms, are highly correlated, they capture theoretically different inﬂuences. Total similarity puts more weight on ﬁrms having high product market similarity with a focal ﬁrm, whereas the number of ﬁrms put equal weight on all of the ﬁrms in the focal ﬁrm’s industry.\n",
              "\n",
              "(1)\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "269\n",
              "\n",
              "Table 1 Variables\n",
              "\n",
              "Variable Deﬁnitions Deﬁnition The annual amount of CVC investment Total sum of product similarity of a ﬁrm with other ﬁrms within its industry The number of ﬁrms within a ﬁrm’s industry Lerner index based on TNIC classiﬁcation The standard deviation of monthly stock returns Firm’s total assets The depreciated sum of number of patents applied Free cash ﬂow R&D expenses The ratio of current year’s sales to last year’s sales Number of years since a ﬁrm’s ﬁrst investment The annual amount of merger and acquisitions The number of alliance and joint venture deals Source VentureXpert Hoberg-Phillips TNIC data Hoberg-Phillips TNIC data Hoberg-Phillips TNIC data CRSP Compustat NBER patent citation data Compustat Compustat Compustat VentureXpert SDC platinum SDC platinum\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "CVC amount TNIC total similarity TNIC number of ﬁrms TNIC Lerner index SD of monthly stock returns Firm size Patent stock Freecash R&D Sales growth rate CVC experience M&A amount Number of alliances and JV\n",
              "\n",
              "where the subscript represents ﬁrm i in year t . As discussed earlier, Y denotes CVC intensity or R&D intensity. Portfolio Management Competition (PMC) represents the TNIC-based competition level for each ﬁrm, and X includes vectors of ﬁrm-speciﬁc characteristics that could affect the ﬁrm’s innovation investments. Note that we include the same set of control variables in both CVC and R&D equations, with the exception of CVC experience included only in the equation for CVC intensity. Firm- and year-ﬁxed effects are represented by i and t , respectively. We lag all independent variables by one year in the analysis to better reﬂect a ﬁrm’s search process for innovation, and consistent with the literature (Blundell et al. 1999). We ﬁrst estimate Equation (1) using an ordinary least squares (OLS) regression with robust standard errors clustered by ﬁrm and a ﬁrm ﬁxed-effects (FE) model to account for time-invariant ﬁrm characteristics. However, omitted variable bias and reverse causality concerns are not addressed in these models. An example of such an omitted variable could be the emergence of a signiﬁcant technological opportunity, such as the ratiﬁcation of an important protocol or a breakthrough in a related technological sector (Cohen 2010). In addition, although we argue that competition may lead to larger CVC (or R&D), the reverse relationship is also possible: Firms investing in CVC (or R&D) may be successful in differentiating their products from their rivals, thereby reducing competition (Sutton 1991). Without addressing these concerns, OLS and FE estimations are potentially biased. We account for endogeneity using the generalized method of moments (GMM)-based dynamic panel data estimator (Arellano and Bond 1991, Blundell and Bond 1998). Although the best-case solution to the endogeneity of competition would be to identify an appropriate instrument, ﬁnding such instruments that generate exogenous variation in competition (since competition is by deﬁnition an industry-level construct) is challenging. Hence, we use the construction of internal instruments in the data as provided\n",
              "\n",
              "by dynamic panel data models. Also, because some independent variables such as ﬁrm size and patent stock are not strictly exogenous, such models also allow us to construct instruments for these variables as well. Therefore, the main analysis we present in the paper is based on the Blundell and Bond (1998) “System” GMM estimation, which uses lagged levels and differences of independent variables as internal instruments. Because average competition is relatively persistent in our sample, it is advisable to use system GMM with differences in variables as additional moments rather than lagged levels of the competition variables in the differences GMM speciﬁcation11 (Bobba and Coviello 2007). We therefore use this approach in estimating Equation (1). Because too many instruments can lead to a ﬁnite sample bias and high pass rates of speciﬁcation tests such as the Hansen J-test, we follow Roodman (2009) and use only certain lags for the system GMM speciﬁcations. We checked the validity of the moment conditions required by system GMM using the Hansen test, which does not reject the assumption that our instruments are exogenous (Blundell and Bond 1998, Roodman 2009). H5 pertains to the effect of CVC investments on ex-post innovation output for the ﬁrm. We adopt the Poisson speciﬁcation with ﬁrm effects for this test. We consider the following: E Iit xit\n",
              "i\n",
              "\n",
              "= exp xit +\n",
              "\n",
              "i\n",
              "\n",
              "(2)\n",
              "\n",
              "where Iit represents the number of patents in year t by ﬁrm i and xit is the vector of observable explanatory variables including CVC investment, R&D investment, ﬁrm size, patent stocks, and M&A. The term i represents a ﬁrm-speciﬁc effect reﬂecting any permanent difference in innovation productivity across\n",
              "11\n",
              "\n",
              "The correlation coefﬁcient between current competition and oneyear lagged competition is 0.83, 0.84, and 0.43 for total similarity, the number of companies, and the industry LI, respectively.\n",
              "\n",
              "\n",
              "270\n",
              "Table 2\n",
              "Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CVC amount in million $ CVC/Sales R&D in million $ R&D/sales TNIC total similarity TNIC number of ﬁrms TNIC Lerner index SD of monthly stock returns Firm size asset in million $ Patent stock Free cash Sales growth rate CVC experience in years M&A amount in million $ Number of alliances and JVs\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "Summary Statistics and Correlations\n",
              "N Mean Std. dev. 1 2 3 4 5 6 7 8 9 10 11 12 13 14\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "1,185 12 84 1,185 0 02 1,185 439 83 1,185 0 17 1,185 469 19 1,185 120 65 1,185 2 91 1,185 16 89 1,185 9 464 63 1,185 648 51 1,185 925 06 1,185 1 24 1,185 7 37 1,185 990 66 1,185 4 101\n",
              "\n",
              "19 1 2\n",
              "\n",
              "4\n",
              "\n",
              "59 23 1 00 0 31 0 01 983 94 0 43 0 47 −0 02 393 82 0 05 101 76 0 09 8 54 0 00 11 24 −0 03 894 77 0 30 885 07 0 27 390 22 0 44 0 79 0 00 6 74 0 07 462 56 0 15 10 401 0 38\n",
              "\n",
              "1 00 −0 02 −0 00 −0 01 −0 01 0 01 0 09 −0 02 −0 02 −0 02 −0 02 −0 04 −0 01 −0 01\n",
              "\n",
              "1 00 −0 03 −0 03 −0 01 −0 02 −0 21 0 59 0 77 0 73 −0 08 0 35 0 34 0 62\n",
              "\n",
              "1 00 0 40 0 22 0 46 0 07 −0 11 −0 04 −0 09 −0 03 −0 09 −0 04 −0 05\n",
              "\n",
              "1 00 0 92 0 26 0 19 −0 13 −0 13 −0 06 0 09 −0 23 0 01 0 11\n",
              "\n",
              "1 00 0 11 0 27 −0 16 −0 14 −0 07 0 13 −0 27 0 01 0 13\n",
              "\n",
              "1 00 0 03 −0 03 −0 01 −0 03 0 05 −0 01 −0 02 −0 04\n",
              "\n",
              "1 00 −0 28 1 00 −0 17 0 58 1 00 −0 28 0 81 0 62 1 00 0 17 −0 06 −0 08 −0 07 1 00 −0 40 0 33 0 36 0 35 −0 18 1.00 −0 11 0 48 0 21 0 45 0 00 0.13 1.00 −0 10 0 52 0 59 0 59 0 00 0.17 0.35\n",
              "\n",
              "ﬁrms. To account for the ﬁrm-speciﬁc effect, we ﬁrst use the ﬁxed-effects Poisson estimator. One concern about this estimator is that it relies on an assumption of strictly exogenous independent variables, which may be violated since some variables such as patent stock (technology leadership) are potentially predetermined. Therefore, we also report analysis using the pre-sample mean estimator approach of Blundell et al. (1999) as a robustness check (Galasso and Simcoe 2011). This estimator uses the pre-sample information on innovation available to sample ﬁrms to identify the initial advantages enjoyed by different ﬁrms in terms of innovation productivity and substitutes the pre-sample information for the unobservable FE. We use patent stock in the pre-sample period (1995) to capture this unobservable ﬁrm-speciﬁc effect. We thus use a longer history of innovations to generate a cleaner estimation of ﬁrm-speciﬁc effects.\n",
              "\n",
              "4.\n",
              "\n",
              "Results and Discussion\n",
              "\n",
              "Table 1 provides deﬁnitions for the variables used in our study. Table 2 provides the descriptive statistics and the correlation table for the key variables. We log-transform all independent variables except for the standard deviation of monthly stock returns and CVC experience. The descriptive statistics, however, are calculated based on raw data. From Table 2, the mean annual CVC investment for the CVC sample across all years is $12.8 million. Considering only years with non-zero CVC spending increases the mean annual CVC spending to $30.5 million. For reference, the mean R&D expenditure in the sample is $439.8 million. CVC investment amounts in the sample exhibit a wide variation in magnitude from 0 to $947.1 million over time. The median total product similarity is 358.53 (expressed as a percentage) while the median number of ﬁrms and median TNIC-based LI are 94 and 1.35, respectively. We construct CVC experience by\n",
              "\n",
              "searching for CVC investments by the CVC parent ﬁrm since 1970, with the earliest investment appearing in 1974. Columns (1)–(9) of Table 3 present the results of regressions using OLS, ﬁrm FE, and system GMM where the dependent variable is R&D intensity, i.e., annual R&D investment divided by sales. In each set of results, the three TNIC-based competition variables are introduced in the analysis in turn, i.e., the total similarity variable for the ﬁrm, the number of ﬁrms, and the LI. Columns (1)–(3) of the OLS results indicate that ﬁrms in competitive markets are likely to increase the amount of their R&D investments. All three coefﬁcients pertaining to competition are significant and positive. The Blundell and Bond “System” GMM for Equation (1) are shown in columns (7)–(9) of Table 3 (Blundell and Bond 1998). Across the full set of results, we observe statistically weak support for H1, which posits a positive association between competition and R&D investments. When we estimate ﬁrm FE models shown in columns (4)–(6), we see qualitatively similar results. We next turn to the effect of competition on CVC investments. Table 4, Panel A, presents regressions with the same set of speciﬁcations as in Table 3. The dependent variable is CVC intensity, i.e., annual CVC investments divided by sales. The results of the regressions explaining CVC investments are statistically and economically stronger than those for R&D investments in Table 3. The results from the OLS and ﬁrm FE regressions, shown in columns (1)–(6), show signiﬁcant support for the association between product market competition and CVC investments. For the GMM estimations, the three TNIC-based competition measures are signiﬁcantly positively associated with CVC investments as shown in columns (7)–(9) of Table 4, Panel A, and provide strong support for H2. To determine the economic signiﬁcance of these results, we\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "271\n",
              "\n",
              "Table 3\n",
              "\n",
              "Product Market Competition and R&D Investments OLS (1) (2) (3) (4) 0 216∗∗ 0 106 1 366∗∗ 0 632 0 262∗∗ 0 127 0 004 0 004 −0 013 0 069 −0 077 0 048 −0 051∗ 0 026 −0 217∗∗ 0 087 Yes 0 756∗∗ 0 373 0 002 0 040 0 006 0 005 0 025 0 042 −0 031 0 061 −0 007 0 012 −0 181∗∗ 0 079 Yes Firm FE (5) (6) (7) 0 535∗∗ 0 242 1 217 0 992 0 072 0 117 0 004∗∗∗ 0 001 −0 019 0 074 −0 151∗∗∗ 0 041 −0 050∗∗∗ 0 018 −0 263∗∗∗ 0 077 Yes 0 16 0 88 0 02 0 48 1,185 System GMM (8) (9)\n",
              "\n",
              "DV: Ln(R&D/sales)\n",
              "\n",
              "TNIC total similarity/1,000\n",
              "\n",
              "0 425∗∗ 0 177\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "TNIC number of ﬁrms/1,000 ln TNIC Industry LI SD of monthly stock returns ln Firm Size ln Patent stock ln Free cash ln Sales growth rate Year dummies Hansen Diff Hansen AR(1) AR(2) Adjusted R2 Observations 0 005 0 004 −0 007 0 070 −0 072 0 051 −0 057∗∗ 0 027 −0 177∗∗ 0 080 Yes\n",
              "\n",
              "0 005 0 004 −0 005 0 071 −0 069 0 054 −0 061∗∗ 0 029 −0 186∗∗ 0 081 Yes\n",
              "\n",
              "0 006 0 005 0 021 0 041 −0 013 0 059 −0 007 0 013 −0 185∗∗ 0 078 Yes\n",
              "\n",
              "0 006 0 005 0 023 0 041 −0 017 0 060 −0 007 0 013 −0 189∗∗ 0 079 Yes\n",
              "\n",
              "0 005 0 009 −0 020 0 141 −0 139∗∗ 0 055 −0 051 0 031 −0 265∗∗∗ 0 071 Yes 0 11 0 53 0 01 0 38 1,185\n",
              "\n",
              "0 004 0 008 −0 007 0 133 −0 147∗∗∗ 0 040 −0 048∗ 0 026 −0 260∗∗∗ 0 078 Yes 0 19 0 40 0 02 0 47 1,185\n",
              "\n",
              "0 3783 1,185\n",
              "\n",
              "0 3703 1,185\n",
              "\n",
              "0 3766 1,185\n",
              "\n",
              "0 0663 1,185\n",
              "\n",
              "0 0607 1,185\n",
              "\n",
              "0 0523 1,185\n",
              "\n",
              "Notes. The table reports OLS, ﬁrm FE, and system GMM regressions using lagged independent variables. Standard errors are clustered by ﬁrms for OLS and ﬁrm FE estimates. For OLS and FE regressions, we also include SIC3 industry dummies. The system GMM estimates are Windmeijer corrected for robust standard errors. The values reported for the Hansen test are p-values for the null hypothesis of instrument validity. The Diff Hansen reports the p-values for the validity of the additional moment restrictions required by the system GMM. The values reported for AR(1) and AR(2) are the p-values for ﬁrst- and second-order auto correlated disturbances in the ﬁrst differenced equations. To construct instruments for system GMM, we use lag 2 of the levels for all of the exploratory variables for the transform equation and lag 1 of the same variables in differences for the levels equation. Year dummies are assumed to be exogenous in this speciﬁcation. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "calculated the impact of competition on the CVC investments of the average ﬁrm using the System GMM coefﬁcients. The coefﬁcient of 1.432 in column (7) indicates that a one standard deviation increase in total similarity12 increases the annual amount of CVC divided by sales by 76%; hence the share of CVC to sales increases by more than 1.3 percentage points, from 1.7% to 3.1%. The effect is economically significant and larger than the effect of total similarity on R&D investments (i.e., a 17% increase). Similarly, the coefﬁcients in columns (8) and (9) show that a one standard deviation increase in the number of ﬁrms\n",
              "12 It is our view that the level of total similarity is not universally interpretable in terms of its raw value across various frameworks. That is, if another researcher measures total similarity using another metric such as geographic distance, for example, the measures of total similarity they would obtain are not comparable to those using TNICs. As a result, we caution readers against inferring too much from the raw magnitudes in isolation. However, we do believe that the magnitudes are interpretable once the standard deviation is reported. For example, knowing that ﬁrm A faces one standard deviation higher competition as measured using total similarity has economic meaning.\n",
              "\n",
              "and industry LI generates an increase in normalized CVC of 61% and 114%, respectively. Hypothesis H3 argues that increased competition is likely to make CVC investments more attractive than internal R&D, given the beneﬁts of ﬂexibility (Schildt et al. 2005). Table 4, Panel B, provides estimates where the dependent variable is the log ratio of CVC spending to the total spending on innovation (CVC + R&D). The results show a positive association between product market similarity and the ratio of CVC to CVC + R&D spending, as hypothesized. The results from the OLS and the FE models are also qualitatively similar. Overall, the results lend support to H3, i.e., ﬁrms do appear to shift innovative spending to CVC as a result of competition. We note that ﬁrm-speciﬁc R&D spending is extremely high compared to the average CVC spending; therefore, it would be erroneous to interpret this result as suggesting that CVC is a substitute for internal R&D. Rather, we believe that as IT-producing ﬁrms experience increased competition in their product markets, they are likely to ﬁnd value in small but strategic investments in open innovation models such\n",
              "\n",
              "\n",
              "272\n",
              "Table 4\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "Product Market Competition on CVC Investment and CVC/(R&D + CVC) OLS (1) (2) (3) (4) Firm FE (5) (6) (7) 1 432∗∗ 0 575 5 392∗∗ 2 277 0 615∗∗∗ 0 191 Yes Yes 0 123 0 251 Yes Yes 4 672∗ 2 641 0 555∗ 0 315 Yes Yes 0 49 0 58 0 00 0 71 System GMM (8) (9)\n",
              "\n",
              "TNIC total similarity/1,000\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "1 235∗∗∗ 0 402 4 720∗∗∗ 1 673\n",
              "\n",
              "Panel A: (DV: ln(CVC/Sales)) 1 475∗∗∗ 0 432\n",
              "\n",
              "TNIC number of ﬁrms/1,000 ln(TNIC Industry LI) Controls Year dummies Hansen Diff Hansen AR(1) AR(2) Adjusted R2 TNIC total similarity/1,000 TNIC number of ﬁrms/1,000 ln(TNIC Industry LI) Controls Year dummies Hansen Diff Hansen AR(1) AR(2) Adjusted R2 Observations Yes Yes Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes 0 42 0 91 0 00 0 55\n",
              "\n",
              "Yes Yes 0 57 0 82 0 00 0 61\n",
              "\n",
              "0 2626 0 973∗∗ 0 378\n",
              "\n",
              "0 2611\n",
              "\n",
              "0 2580\n",
              "\n",
              "0 2995\n",
              "\n",
              "0 2967\n",
              "\n",
              "0 2915 1 572∗∗ 0 714 4 852 3 494 −0 050 0 285 Yes Yes 1 246∗∗∗ 0 452 Yes Yes 0 57 1 00 0 00 0 73 1,185\n",
              "\n",
              "Panel B: (DV: ln(CVC/(RD + CVC))) 1 264∗∗∗ 0 418 3 589∗∗ 1 625 0 439∗∗ 0 207 Yes Yes 4 497∗∗ 2 227\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes 0 70 1 00 0 00 0 61 1,185\n",
              "\n",
              "Yes Yes 0 65 1 00 0 00 0 67 1,185\n",
              "\n",
              "0 7254 1,185\n",
              "\n",
              "0 7299 1,185\n",
              "\n",
              "0 7152 1,185\n",
              "\n",
              "0 3431 1,185\n",
              "\n",
              "0 3430 1,185\n",
              "\n",
              "0 3498 1,185\n",
              "\n",
              "Notes. The table reports OLS, ﬁrm FE, and system GMM regressions using lagged independent variables. Standard errors are clustered by ﬁrms for OLS and ﬁrm FE. For OLS and FE regressions, we also include SIC3 industry dummies. The system GMM estimates are Windmeijer corrected for robust standard errors. The values reported for the Hansen test are p-values for the null hypothesis of instrument validity. The Diff Hansen reports the p-values for the validity of the additional moment restrictions required by the system GMM. The values reported for AR(1) and AR(2) are the p-values for ﬁrst- and second-order auto correlated disturbances in the ﬁrst differenced equations. To construct instruments in the ln(CVC/Sales) and ln(CVC/(RD + CVC)) models, we use lag 2 (lags 3–4) of all of the control variables for the transform equation and lag 1 (2) of the same variables in differences for the levels equation, respectively. Year dummies are assumed to be exogenous in this speciﬁcation. For ease of presentation, we omit results for control variables. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "as CVC as vehicles for new knowledge or technologies relative to increased investments in R&D. As to H4 postulating a moderating effect of technology leadership, Table 5 displays results for extended tests of technological capability. Consistent with prior work, we use patent stock to deﬁne technological leadership in the sample (Dushnitsky and Lenox 2005a, Joshi et al. 2010, Kleis et al. 2012). In addition, to identify exogenous variation in leadership as much as possible, we consider deep lags in the patent stock variable, under the assumption that deep lags are less likely to be correlated with contemporaneous error terms in the regression. Speciﬁcally, the ﬁrm’s technological capability is measured as its patent stock lagged by ﬁve years, which is then ranked within its contemporaneous TNIC. We assign dummy variables to ﬁrms that appear in the top quartile (25%) of\n",
              "\n",
              "the ﬁrms in its TNIC, representing technology leaders. We also create dummy variables for ﬁrms in the next quartile and for ﬁrms in the bottom 50th percentile in the TNIC. This speciﬁcation allows us to identify exogenous variation in technology leadership to the greatest extent possible. In robustness tests, we use patent stock for the ﬁrm in the year before its ﬁrst observed CVC investment as well as patent stock from the previous year (lagged one year) to characterize technology leadership, and show consistent results. We ﬁnd that competition is especially inﬂuential for technology leaders in terms of CVC spending. Columns (1)–(3) of Table 5 report that the coefﬁcients for competition are signiﬁcant and positive mainly for technology leaders, i.e., for ﬁrms in the top 25% in terms of patent stock. This result is consistent with\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "273\n",
              "\n",
              "Table 5\n",
              "\n",
              "Heterogeneity by Technological Leadership (In Terms of Patent Stock) DV ln CVC/Sales DV ln CVC/ RD + CVC ln(TNIC Industry LI) (3)\n",
              "∗\n",
              "\n",
              "Competition\n",
              "\n",
              "TNIC total similarity/1,000 (1)\n",
              "\n",
              "TNIC number of ﬁrms/1,000 (2)\n",
              "\n",
              "TNIC total similarity/1,000 (4)\n",
              "\n",
              "TNIC number of ﬁrms/1,000 (5)\n",
              "\n",
              "ln(TNIC Industry LI) (6) 1 207∗∗ 0 613 −1 559 1 574 −0 002 0 454 −1 325 1 310 0 770 1 140 Yes Yes 0 46 1 00 0 00 0 92 1,185\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Competition × Top25% in Technological capability Competition × 25%–50% in Technological capability Competition × Bottom50% in Technological capability Top25% in Technological capability Bottom50% in Technological capability Controls Year dummies Hansen Diff Hansen AR(1) AR(2) Observations\n",
              "\n",
              "2 014 0 773 1 070 2 643 0 733 0 606 −0 668 1 343 1 013 1 281\n",
              "\n",
              "∗∗∗\n",
              "\n",
              "7 918 4 179 3 057 12 544\n",
              "\n",
              "0 828 0 282 −1 236 1 243\n",
              "\n",
              "∗∗∗\n",
              "\n",
              "1 958 1 019 0 284 2 816\n",
              "\n",
              "∗\n",
              "\n",
              "6 515 4 479 9 836 14 059 2 521 5 124 0 406 2 332 2 193 1 949 Yes Yes 0 56 1 00 0 00 0 88 1,185\n",
              "\n",
              "3 466 4 703 −0 332 1 972 1 571 1 795 Yes Yes 0 64 1 00 0 00 0 69 1,185\n",
              "\n",
              "0 505 0 380 −0 915 1 250 −0 283 1 318 Yes Yes 0 71 1 00 0 00 0 77 1,185\n",
              "\n",
              "0 233 0 812 −1 240 1 829 0 693 1 746 Yes Yes 0 56 1 00 0 00 0 81 1,185\n",
              "\n",
              "Yes Yes 0 48 1 00 0 00 0 67 1,185\n",
              "\n",
              "Notes. The table reports system GMM regressions using lagged independent variables. The system GMM estimates are Windmeijer corrected for robust standard errors. The values reported for the Hansen test are p-values for the null hypothesis of instrument validity. The Diff Hansen reports the p-values for the validity of the additional moment restrictions required by the system GMM. The values reported for AR(1) and AR(2) are the p-values for ﬁrst- and second-order auto correlated disturbances in the ﬁrst differenced equations. To construct instruments in the ln(CVC/Sales) and ln(CVC/(RD + CVC)) models, we use lag 2 (3) of the levels for all of the control variables for the transform equation and lag 1 (2) of the same variables in differences for the levels equation. Year dummies are assumed to be exogenous in this speciﬁcation. Technological capability of a ﬁrm is measured as the patent stock in ﬁve years before its CVC investment and then ranked in its TNIC at the given year. For ease of presentation, we omit results for control variables. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "“leader” ﬁrms wanting to sustain their technological leadership, which could be lost in highly competitive markets (Joshi et al. 2010, Mendelson 2000). Also, they are likely to know how to select better portfolio companies given their strong technology base, and are more likely to extract positive synergies between the portfolio companies and their own capabilities. Technology followers, however, show no signiﬁcant reaction to competition, even when they have used CVC as an option in the past. Because we control for ﬁrm size and free cash ﬂow, these ﬁndings are likely not driven by differences in ﬁnancial conditions between technology leaders and slow starters. To the extent that patent applications may vary across the IT-producing industry, the patent stock variable may reﬂect institutional features of the industry (Aghion et al. 2005, Hall et al. 2001). Therefore, we also use total R&D spending to measure technological capability (Alcacer and Chung 2007) and follow the same deep-lag approach to identifying technology leaders. The results are fully robust and are available on request. Replacing the sales-normalized CVC investment with the CVC/(CVC + R&D) variable allows us to gauge the relative importance of CVC over R&D across ﬁrm technology capabilities (see columns (4)–(6) of\n",
              "\n",
              "Table 5). We ﬁnd that technology leaders tend to put relatively more emphasis on CVC relative to R&D in competitive markets. Technology slow starters do not increase the relative amount of CVC relative to internal R&D in competitive markets, providing further support for H4 arguing for why the presence of product market competition may make CVC attractive visà-vis R&D spending. Finally, we examine the link between CVC investments and ex-post innovative output (H5) in Table 6, contingent on technology leadership deﬁned above. We ﬁnd a signiﬁcant and different impact of CVC investment on innovative output between technology leaders and slow starters for both the ﬁxed-effects Poisson and pre-sample mean (labeled BGV) estimators. We ﬁnd a robust positive association between CVC investment and innovation (measured by number of patents) only for technology leaders (see columns (3) and (4)). In columns (1) and (3) we control for ﬁrm-speciﬁc effects using the pre-sample mean approach and observe consistent results. We include total M&A spending per year as a control variable since it is possible that an increase in patenting can be attributed to M&A activity rather than CVC (Tong and Li 2011). The effect of CVC investments on patent applications is robust to the inclusion of the ﬁrm’s\n",
              "\n",
              "\n",
              "274\n",
              "Table 6\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "CVC Investment and Innovation Output (Through the Year 2004) (1) Number of patents (2) Number of patents 0 011∗∗∗ 0 001 0 029∗∗∗ 0 009 0 014 0 024 −0 006 0 036 0 126∗∗∗ 0 046 0 999∗∗∗ 0 057 0 072 0 076 0 004 0 010 0 029∗∗ 0 013 0 463∗∗∗ 0 017 −0 042∗∗ 0 020 0 014∗∗∗ 0 002 0 126∗∗∗ 0 043 0 990∗∗∗ 0 058 0 091 0 076 0 004 0 011 0 302 0 259 0 301 0 494 BGA 902 0 011∗∗∗ 0 001 0 016∗∗∗ 0 006 0 006 0 005 0 031∗∗ 0 013 0 533∗∗∗ 0 018 −0 052∗∗ 0 020 0 014∗∗∗ 0 002 −0 040 0 051 0 238∗∗∗ 0 074 Firm FE 783 (3) Number of patents (4) Number of patents\n",
              "\n",
              "ln CVC/Sales ln CVC/Sales × Top25% in Technological capability ln CVC/Sales × 25%–50% in Technological capability ln CVC/Sales × Bottom50% in Technological capability ln Firm Size ln Patent Stock ln R&D/sales ln M&A/sales Top25% in Technological capability Bottom50% in Technological capability Firm effects Observations\n",
              "\n",
              "0 026∗∗∗ 0 010\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "BGA 902\n",
              "\n",
              "Firm FE 783\n",
              "\n",
              "Notes. The table reports results of Poisson regressions using lagged independent variables for the sample through the year 2004. Standard errors are clustered by ﬁrms. Firm FE is based on the ﬁxed-effects Poisson estimator, while BGV ﬁxed effects are based on including pre-sample means of the dependent variable (Blundell et al. 1999). BGV models also include SIC3 dummies. Technological capability of a ﬁrm is measured as the patent stock in ﬁve years before its CVC investment and then ranked in its TNIC at the given year. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "M&A activities. In summary, these results support H5. We next consider several empirical extensions to these base results.\n",
              "\n",
              "5.\n",
              "5.1.\n",
              "\n",
              "Empirical Extensions\n",
              "\n",
              "Variation in the Effect of Competition on CVC in the IT Industry We further explore the relationship between competition and CVC by dividing our sample into more reﬁned product markets and explore whether competition in the IT subcategories shows differential effects on CVC investment. We include dummy variables to indicate the ﬁrm’s majority business of software, hardware or telecommunications, based on their primary SIC code. We interact these dummy variables with competition measures to examine the marginal effect of competition on CVC in these particular segments. The results in Table 7 show that as competition increases, ﬁrms in the software segment are most responsive in increasing their CVC investments and the relative share of CVC investments in total innovation spending. The interaction term of product market competition and the software dummy is signiﬁcant (at p = 0 10) across all speciﬁcations. By contrast, the\n",
              "\n",
              "hardware segment shows a positive coefﬁcient but is not signiﬁcant, while the telecom segment is less responsive. We thus ﬁnd that the fast moving and volatile software sector is the most responsive to competition through investments in CVC, while telecom appears to be less so. One potential explanation could be the difference in competition that ﬁrms in each segment face; software and hardware ﬁrms face more competition.13 While a full analysis of these differences is outside the scope of this paper, we note some potential explanations below. Nowak and Grantham (2000) argue that there were three forces at play in the software industry in the late 1990s. These forces were the move from a softwareexpert programming paradigm to the componentbased software development (CBSD) paradigm, the rise of globalization, and extremely low barriers to entry in the industry (low capital costs, high labor mobility). The move to CBSD ensures that compatibility with the platform is retained, even for the small entrepreneur entering the marketplace, thus\n",
              "13\n",
              "\n",
              "For TNIC total similarity, TNIC number of ﬁrms, and TNIC LI, software has values of 546, 146, 3.37, hardware 452, 119, 2.73, and telecommunications 320, 59, 2.28, respectively.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "275\n",
              "\n",
              "Table 7\n",
              "\n",
              "Industry Heterogeneity (Software, Hardware, and Telecommunications) DV ln CVC/Sales DV ln CVC/ RD + CVC ln(TNIC Industry LI ) 1 074∗∗ 0 495 0 255 0 650 −0 273 0 613 −0 501 0 698 −0 928 0 798 Yes Yes 0 47 1 00 0 00 0 64 1,185 TNIC total similarity/1,000 1 396∗ 0 793 1 017 1 349 −2 477 5 959 −0 193 0 979 1 015 2 308 Yes Yes 0 20 1 00 0 00 0 78 1,185 TNIC number of ﬁrms/1,000 9 591∗ 5 126 2 577 5 453 1 821 24 598 −1 791 1 182 −0 797 2 057 Yes Yes 0 00 1 00 0 00 0 74 1,185 ln(TNIC Industry LI ) 0 948∗∗ 0 440 0 145 0 478 −0 679 0 847 −0 334 0 559 0 228 0 890 Yes Yes 0 42 1 00 0 00 0 95 1,185\n",
              "\n",
              "Competition measure Competition × Software\n",
              "\n",
              "TNIC total similarity/1,000 1 751∗∗ 0 787 1 238 1 617 −3 867 4 718 −0 310 1 079 0 579 2 090 Yes Yes 0 30 1 00 0 00 0 45 1,185\n",
              "\n",
              "TNIC number of ﬁrms/1,000 11 475∗∗∗ 4 245 6 846 5 065 −4 038 22 302 −1 507 1 108 −0 763 1 845 Yes Yes 1 00 1 00 0 00 0 39 1,185\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Competition × Hardware Competition × Telecom Software Telecom Controls Year effects Hansen Diff Hansen AR(1) AR(2) Observations\n",
              "\n",
              "Notes. The table reports system GMM regressions using lagged independent variables. The system GMM estimates are Windmeijer corrected for robust standard errors. The estimates also pass the Hansen test for instrument validity. To construct instruments, we use lag 2 of all of the other variables for the transform equation and lag 1 of the same variables in differences for the levels equation. Year dummies are assumed to be exogenous in this speciﬁcation. For ease of presentation, we omit results for control variables. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "lowering switching costs for the customer. Globalization and low capital costs contribute to the growing fragmentation of the software industry. These factors contribute to hyper-competition in software product markets, and to opportunities to use open innovation to access new knowledge. A similar dynamic occurred to a lesser degree in the hardware industry, where sustained technical innovation and lowered entry barriers led to the “platformization” of hardware (Bresnahan and Greenstein 1999), especially in the micro-computing segment. As the client-server model emerged and the PC industry grew, competition grew around distinct hardware platforms even as entry barriers dropped. Bresnahan and Greenstein (1999, p. 33) state, “Technical leadership is divided. Hardware and component markets are more competitive. Sellers complain, rather than brag, about the pace of change.” In such contexts, open innovation and the ability to tap into external innovation (i.e., CVC) became feasible and attractive. By contrast, the telecom industry was characterized by fewer but more dominant entities (“telcos” such as BellSouth), strong network effects, high entry barriers, regulation, and inﬂuential standard-setting agencies (Godoe 2000). In such contexts, technological progress was generated in ﬁrms rather than from an extended ecosystem of players, i.e., Large Technological Projects (LTP) rather than Large Technological Networks (LTN) (Godoe 2000). The operating model\n",
              "\n",
              "for technical innovation in this sector was thus internal R&D, leading to high R&D intensity but less innovation generated from new entrants and hence limited use of open innovation. Note that these are preliminary explanations; a thorough examination of these differences remains for future research. 5.2. Using SIC-Based Competition Measures We examine whether our TNIC measures are more informative than SIC-based competition measures in explaining CVC investment.14 Table 8 displays the results of analyses in which SIC3-based competition measures are used in lieu of the TNIC variables. Product similarity measures are not available for the SIC3 classiﬁcation; hence this measure is not included. The results in Table 8, Panel A, show that the relationship between competition based on SIC3 and CVC investment is generally positive and signiﬁcant in the case of the OLS and FE regressions. However, the coefﬁcients become insigniﬁcant after accounting for endogeneity, suggesting that SICbased competition measures are unable to pick up\n",
              "14\n",
              "\n",
              "Hoberg and Phillips (2016) perform several tests to show that TNIC measures of competition are informative, especially relative to SIC codes. For example, measures of low competition based on the TNIC better correlate with higher proﬁtability at the ﬁrm and industry level. Also, and perhaps more directly, ﬁrms with higher levels of TNIC competition also disclose more in the way of direct mentions of phrases such as “high competition” in the MD&A section of their 10-Ks relative to SIC-code measures of competition.\n",
              "\n",
              "\n",
              "276\n",
              "Table 8\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "SIC3-Based Competition on CVC Investment and CVC/(RD + CVC) OLS (1) (2) (3) Firm FE (4) (5) 0 219 0 556 0 038 0 169 Yes Yes 0 237 0 177 Yes Yes 0 48 0 90 0 00 0 74 System GMM (6)\n",
              "\n",
              "SIC number of ﬁrms/1,000\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "5 269∗∗∗ 1 913\n",
              "\n",
              "ln TNIC Industry LI Controls Year dummies Hansen Diff Hansen AR(1) AR(2) Adjusted R2 SIC number of ﬁrms/1,000 ln TNIC Industry LI Controls Year dummies Hansen Diff Hansen AR(1) AR(2) Adjusted R2 Observations Yes Yes Yes Yes\n",
              "\n",
              "Panel A: (DV: ln(CVC/Sales)) 5 755∗∗∗ 1 793 0 096 0 176 Yes Yes Yes Yes\n",
              "\n",
              "Yes Yes 0 44 0 92 0 00 0 66\n",
              "\n",
              "0 2595 5 203∗∗ 2 044\n",
              "\n",
              "0 2511\n",
              "\n",
              "0 3036\n",
              "\n",
              "0 2913 0 030 0 603 0 101 0 190 Yes Yes 0 272 0 247 Yes Yes 0 40 0 89 0 00 0 87 1,185\n",
              "\n",
              "Panel B: (DV: ln(CVC/(RD + CVC))) 5 656∗∗∗ 1 943 0 166 0 199 Yes Yes Yes Yes\n",
              "\n",
              "Yes Yes 0 43 0 93 0 00 0 83 1,185\n",
              "\n",
              "0 2559 1,185\n",
              "\n",
              "0 2487 1,185\n",
              "\n",
              "0 2847 1,185\n",
              "\n",
              "0 2740 1,185\n",
              "\n",
              "Notes. The table reports OLS, ﬁrm FE, and system GMM regressions using lagged independent variables. Standard errors are clustered by ﬁrms for OLS and ﬁrm FE estimates. For OLS and FE regressions, we also include SIC3 industry dummies. The system GMM estimates are Windmeijer corrected for robust standard errors. The values reported for the Hansen test are p-values for the null hypothesis of instrument validity. The Diff Hansen reports the p-values for the validity of the additional moment restrictions required by the system GMM. The values reported for AR(1) and AR(2) are the p-values for ﬁrst- and second-order auto correlated disturbances in the ﬁrst differenced equations. To construct instruments in the ln(CVC/Sales) and ln(CVC/(RD+CVC)) models, we use lag 2 of all of the control variables for the transform equation and lag 1 of the same variables in differences for the levels equation. Year dummies are assumed to be exogenous in this speciﬁcation. For ease of presentation, we omit results for control variables. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "enough variation across ﬁrms and over time compared to TNIC measures, which are designed to capture dynamic variation as 10-Ks are ﬁled each year. In unreported robustness tests, we also use NAICS 6digit codes to create competition measures. Here too, although the FE and OLS regressions provide significant positive coefﬁcients, subsequent models yield positive but insigniﬁcant coefﬁcients. Because competition in IT industries is intense and time varying, the TNIC-based measures are likely to be more suitable as they accurately capture competitive pressures faced by focal ﬁrms, and therefore better explain CVC activities relative to SIC-based or NAICS-based industries. Controlling for Other Forms of External Innovation Although we treat CVC as a form of external innovation spending by the focal ﬁrm, we acknowledge that there are other forms of external innovation spending that the ﬁrm can also use including acquisitions, alliances, and joint ventures. Even though these are distinct from CVC (Schildt et al. 2005), we establish that controlling for them does not change our 5.3.\n",
              "\n",
              "results as to the effect of product market competition on CVC investments. We re-estimate the System GMM with two added control variables, i.e., ln(amount of M&As) and ln(number of alliances and joint ventures deals), obtained from the SDC Platinum data set. We acknowledge that these variables are likely to be endogenous; however, as a robustness check, we believe it is useful to rule out that their effects can explain the competition-CVC relationship. The results, shown in Table 9, are robust to the addition of these control variables, suggesting that our CVC measure is not simply a proxy for such other mechanisms.15 5.4. Robustness Checks Beyond the empirical extensions presented above, we also conducted extensive robustness checks to ensure\n",
              "15\n",
              "\n",
              "In unreported regressions, we ensure that the other tests presented in the paper are also robust to the addition of these two variables. However, in view of their clear endogeneity, we do not include them in the main results presented above. We thank the senior editor for raising this point.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "277\n",
              "\n",
              "Table 9\n",
              "\n",
              "Controlling for Other Forms of External Innovation System GMM (1) (2) (3) (4) 1 560∗∗∗ 0 537 5 595∗ 3 098 0 626∗∗ 0 305 0 217 0 253 0 036 0 047 Yes Yes 1 444∗∗ 0 643 3 297 3 362 0 554∗ 0 295 0 104 0 264 0 018 0 049 Yes Yes 1,185 4 032 3 528 0 596∗ 0 354 0 024 0 046 Yes Yes 4 398 3 588 0 623∗ 0 331 System GMM (5) (6)\n",
              "\n",
              "Panel A: (DV: ln(CVC/Sales)) TNIC total similarity/1,000\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "1 672∗∗∗ 0 569\n",
              "\n",
              "TNIC number of ﬁrms/1,000 ln TNIC Industry LI ln No. of Alliances and JVs ln(amount of M&As) Controls Year dummies TNIC total similarity/1,000 TNIC number of ﬁrms/1,000 ln TNIC Industry LI ln No. of Alliances and JVs ln(amount of M&As) Controls Year dummies Observations Yes Yes 1,185 −0 117 0 238 Yes Yes 1 414∗∗ 0 641 0 040 0 253\n",
              "\n",
              "0 090 0 264\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "Yes Yes\n",
              "\n",
              "0 035 0 045 Yes Yes\n",
              "\n",
              "Panel B: (DV: ln(CVC/(RD+CVC)))\n",
              "\n",
              "−0 090 0 245\n",
              "\n",
              "Yes Yes 1,185\n",
              "\n",
              "0 018 0 055 Yes Yes 1,185\n",
              "\n",
              "0 038 0 048 Yes Yes 1,185\n",
              "\n",
              "Yes Yes 1,185\n",
              "\n",
              "Notes. The table reports system GMM regressions using lagged independent variables. The system GMM estimates are Windmeijer corrected for robust standard errors. The values reported for the Hansen test are p-values for the null hypothesis of instrument validity. The Diff Hansen reports the p-values for the validity of the additional moment restrictions required by the system GMM. The values reported for AR(1) and AR(2) are the p-values for ﬁrst- and second-order auto correlated disturbances in the ﬁrst differenced equations. To construct instruments in the ln(CVC/Sales) and ln(CVC/(RD + CVC)) models, we use lag 2 (lags 3–4) of all of the control variables for the transform equation and lag 1 (2) of the same variables in differences for the levels equation, respectively. Year dummies are assumed to be exogenous in this speciﬁcation. For ease of presentation, we omit results for control variables. ∗ Signiﬁcant at 10%; ∗∗ signiﬁcant at 5%; ∗∗∗ signiﬁcant at 1%.\n",
              "\n",
              "the validity of our results. These include estimating the interaction of product market competition with product market ﬂuidity, the use of additional instrumentation based on the TNIC, estimation of a panel-data Heckman analysis, additional analyses to account for technological and ﬁnancial shocks, and the use of different dependent variables. The details of these tests are provided in the online appendix.\n",
              "\n",
              "6.\n",
              "\n",
              "Conclusion\n",
              "\n",
              "We start by examining the baseline relationship between product market competition and ﬁrm-level innovation spending. Considerable prior work in studying this relationship has provided conﬂicting results, indicating that an industry-speciﬁc focus can better inform this question. We focus on the ITproducing industry, and ask two inter-related questions: First, do ﬁrms in the fast moving IT-producing industry respond to increased product market competition by investing more in internal R&D? Second, do ﬁrms prefer investments in CVC relative\n",
              "\n",
              "to internal R&D? We show that competition does lead to enhanced R&D spending. We also show that, although the managerial literature on open innovation (Chesbrough 2006) as well as formal models (Aghion et al. 2005, Fulghieri and Sevilir 2009) argue that diverting spending to sources such as CVC may be appropriate in more competitive environments, to our knowledge there is no clear empirical evidence in the literature of these shifts. Through a series of analyses, we show that IT ﬁrms do spend more on CVC vis-à-vis internal R&D as a response to competition. Not all IT ﬁrms appear to beneﬁt from CVC investments. Our analysis shows that technology leaders are more likely to respond to competition through the use of CVC and, correspondingly, also beneﬁt from such activities through an increase in innovative output. This presents a conditional argument for the use of CVC, and by extension, the use of open innovation models for IT ﬁrms: CVC may be a valuable strategy to acquire intellectual property (or knowledge) from the outside only if the ﬁrm has internal\n",
              "\n",
              "\n",
              "278\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "technological expertise. Chesbrough (2006, p. 14) indicates as much: “You cannot be an informed consumer of external ideas and technology if you don’t have some very sharp people working in your own organization.” Therefore, while we show that ﬁrms tend to divert some innovation spending from internal R&D to CVC as competition increases, our analysis also indicates that there are limits to how much CVC may be used if it involves starving internal R&D. Our results provide a complementary ﬁnding to the existing IS literature that indicates the value of knowledge capabilities enabled by IT investments in ﬁrms, especially in turbulent environments (Joshi et al. 2010, Pavlou and El Sawy 2010). While this research suggests that IT-enabled knowledge capabilities make the ﬁrm better at managing market conditions under volatility, our work indicates that, on the supply side of innovation, internal innovation capabilities are necessary to effectively use opportunities provided by innovation models such as CVC. Our work is also subject to certain limitations, which we summarize here. First, we use novel measures of competition based on 10-K textual network product similarities. This approach is motivated by seminal studies linking product differentiation to competition including Chamberlin (1933) and Hotelling (1929). However, although we also consider the LI, a limitation is that there are other aspects of competition beyond product differentiation. Examples of other innovative ways to measure competition include news articles (Kim et al. 2015) and Internet trafﬁc at the SEC’s EDGAR website (Lee et al. 2015). Despite the existence of such alternatives, we focus on TNIC total similarities because they offer many advantages relevant for our sample of ﬁrms in the U.S. IT industry where hyper-competition is present. Most notably they are updated dynamically year-toyear. Also, Hoberg and Phillips (2016) illustrate why they are substantially more powerful than the traditional SIC code based approach. A second limitation is that we are unable to locate a fully exogenous instrument for product market competition in our setting. To mitigate this concern, we report a variety of empirical speciﬁcations to establish the robustness of our ﬁndings. Corroborating our results using exogenous policy shocks would be useful in further establishing the causal relationship between product market competition and CVC spending, and represents a fruitful avenue for future research. We now brieﬂy discuss the implications of our work for practice and theory. Our work has implications for management of innovative ﬁrms in IT-producing industries with an ongoing interest in CVC as a source of innovative ideas. Increasing competition in IT-producing markets is a potent force and requires ﬁrms to effectively use elements of open innovation in their portfolios. To the extent that entrepreneurs\n",
              "\n",
              "and independent research units have strong incentives for innovating and are more nimble than traditional R&D units, IT ﬁrms should ﬁnd value in CVC activities. Still, our ﬁndings suggest that CVC strategies of IT ﬁrms should vary depending on their knowledge leadership. For technology leaders, we show that a strategic move into CVC has signiﬁcant beneﬁts in the form of patenting. For technology slow starters, it may be better to focus ﬁrst on improving internal R&D before incurring the risks of CVC. However, in the long run, technology leaders should retain a balance between internal R&D and CVC spending; internal technological expertise is necessary to continue extracting knowledge from external innovation. Our work focuses on the IT-producing sector but an open question pertains to whether similar relationships extend to other CVC contexts, such as biotechnology where a signiﬁcant proportion of CVC activity is noted. There is limited work in the literature where contrasts between the IT and biotechnology industries have been made. However, we can provide some initial conjectures about how competition may inﬂuence the biotechnology industry. If CVC is viewed as a means of acquiring knowledge and learning about new technologies, and if this is of particular relevance in biotechnology (Katila et al. 2008), then CVC is likely to be attractive (Van de Vrande et al. 2006). However, scholars have argued that while part of the motivation in IT may be to acquire valuable IP through acquisitions and through a weak IP regime (Dushnitsky and Lenox 2005b), the beneﬁts of CVC in biotechnology arise from the ability to form inter-ﬁrm alliances or licensing arrangements (Gompers and Lerner 2001). In such cases, the entrepreneur’s IP is still protected but the entrepreneur gains through access to the investing ﬁrm’s complementary assets (Basu et al. 2011). Therefore, the reason for investing in CVC is not identical to the reasons observed for IT-producing ﬁrms. In addition, biotechnology and pharma ﬁrms experience different dynamics in terms of competition: Product development cycles are long, expensive, and highly regulated, with considerable revenues coming from few products. In such contexts, the response to competition is built not on shorter-term product development, as in IT, but in long-term alliance building and complementary use of open innovation with other mechanisms such as licensing, co-production, and joint ventures (Van de Vrande et al. 2006). Thus, the relationship between competition and CVC may not be as signiﬁcant as that noted in IT-producing industries. Future research is needed to rigorously compare these two industries and to formally test these conjectures. Our study extends current research on the evolving concept of open innovation models in the IT context.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "279\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "While our focus is speciﬁcally CVC, the literature discusses many other forms of open innovation that may be useful to ﬁrms, such as user innovation, crowdsourcing, and technological alliances (Han et al. 2012, Huang et al. 2014, Leimeister et al. 2009). Our work clearly indicates that while the ability to use open innovation models, given the low levels of investments necessary vis-à-vis traditional R&D may exist across all ﬁrms, technology leaders are more likely to achieve successful assimilation of new knowledge realized through open innovation. Therefore, when considering the returns on investment from open innovation models, attributes of the investing ﬁrm and its capabilities should be considered. Research that focuses on data or models speciﬁc to one ﬁrm will miss these elements and hence systematically show high or low returns. We argue that a broad-based view of the value of open innovation models is necessary, much the same way a systematic attempt at understanding IT business value was undertaken in the IS community in the 1990s (McAfee and Brynjolfsson 2008). Finally, we add to the small but growing literature in the IS community that addresses entrepreneurship and VC-related topics, especially given the large contribution that the IT industry has made to the broader entrepreneurship and VC industries (Aggarwal et al. 2012, Gompers and Lerner 2001). During our study, the majority of VC and CVC investments were made in the IT-producing segment of the economy. We believe there is a natural home for examinations of research questions pertaining to entrepreneurship in the IS community, particularly when viewed through a technological lens. Our analysis of the varying behavior of software, hardware, and telecommunications ﬁrms in terms of CVC investments is a ﬁrst step in this direction and represents many opportunities for future work at the nexus of IT, entrepreneurship, and innovation. Supplemental Material\n",
              "Supplemental material to this paper is available at http://dx .doi.org/10.1287/isre.2016.0620.\n",
              "\n",
              "References\n",
              "Aggarwal R, Gopal R, Gupta A, Singh H (2012) Putting money where the mouths are: The relation between venture ﬁnancing and electronic word-of-mouth. Inform. Systems Res. 23(3): 976–992. Aghion P, Tirole J (1994) The management of innovation. Quart. J. Econom. 109(4):1185–1209. Aghion P, Bloom N, Blundell R, Grifﬁth R, Howitt P (2005) Competition and innovation: An inverted-U relationship. Quart. J. Econom. 120(2):701–728. Ahuja G, Katila R (2001) Technological acquisitions and the innovation performance of acquiring ﬁrms: A longitudinal study. Strategic Management J. 22(3):197–220. Alcacer J, Chung W (2007) Location strategies and knowledge spillovers. Management Sci. 53(5):760–776. Arellano M, Bond S (1991) Some tests of speciﬁcation for panel data: Monte Carlo evidence and an application to employment equations. Rev. Econom. Stud. 58(194):277–297. Arora A (1996) Contracting for tacit knowledge: The provision of technical services in technology licensing contracts. J. Development Econom. 50(2):233–256. Arrow K (1962) Economic welfare and the allocation of resources for invention. Nelson R, ed. The Rate and Direction of Inventive Activity: Economic and Social Factors (National Bureau of Economic Research, Princeton University Press, Princeton, NJ), 609–626. Basu S, Phelps C, Kotha S (2011) Towards understanding who makes corporate venture capital investments and why. J. Bus. Venturing 26(2):153–171. Benson D, Ziedonis RH (2009) Corporate venture capital as a window on new technologies: Implications for the performance of corporate investors when acquiring startups. Organ. Sci. 20(2):329–351. Benson D, Ziedonis RH (2010) Corporate venture capital and the returns to acquiring portfolio companies. J. Financial Econom. 98(3):478–499. Block Z, Ornati OA (1988) Compensating corporate venture managers. J. Bus. Venturing 2(1):41–51. Blundell R, Bond S (1998) Initial conditions and moment restrictions in dynamic panel data models. J. Econometrics 87(1):115–143. Blundell R, Grifﬁth R, Van Reenen J (1999) Market share, market value and innovation in a panel of British manufacturing ﬁrms. Rev. Econom. Stud. 66(228):529–554. Bobba M, Coviello D (2007) Weak instruments and weak identiﬁcation, in estimating the effects of education, on democracy. Econom. Lett. 96(3):301–306. Bonanno G, Haworth B (1998) Intensity of competition and the choice between product and process innovation. Internat. J. Indust. Organ. 16(4):495–510. Boudreau KJ, Lacetera N, Lakhani KR (2011) Incentives and problem uncertainty in innovation contests: An empirical analysis. Management Sci. 57(5):843–863. Bresnahan TF, Greenstein S (1999) Technological competition and the structure of the computer industry. J. Indust. Econom. 47(1): 1–40. Cassiman B, Veugelers R (2006) In search of complementarity in innovation strategy: Internal R&D and external knowledge acquisition. Management Sci. 52(1):68–82. Chamberlin E (1933) The Theory of Monopolistic Competition (Harvard University Press, Cambridge, MA). Chesbrough HW (2002) Making sense of corporate venture capital. Harvard Bus. Rev. 80(3):90–99. Chesbrough HW (2006) Open Innovation: The New Imperative for Creating and Proﬁting from Technology (Harvard Business Press, Boston). Cockburn IM, Henderson RM (1998) Absorptive capacity, coauthoring behavior, and the organization of research in drug discovery. J. Indust. Econom. 46(2):157–182. Cohen WM (2010) Fifty years of empirical studies of innovative activity and performance. Handbook Econom. Innovation 1: 129–213.\n",
              "\n",
              "Acknowledgments\n",
              "The authors thank Ritu Agarwal, Gordon Phillips, the senior editor, the associate editor, and three anonymous reviewers for their many useful and constructive suggestions that have helped to signiﬁcantly improve the paper. The authors also thank the seminar participants at the 2012 INFORMS Conference on Information Systems and Technology, the 2012 Academy of Management annual meeting, and the 2012 Statistical Challenges in eCommerce Research. The authors also gratefully acknowledge ﬁnancial support from the Smith School’s doctoral program and summer faculty research grants program for this work. All errors remain the authors’.\n",
              "\n",
              "\n",
              "280\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "Cohen WM, Levinthal DA (1990) Absorptive capacity: A new perspective on learning and innovation. Admin. Sci. Quart. 35(1): 128–152. Dasgupta P, Stiglitz J (1980) Uncertainty, industrial structure, and the speed of R&D. Bell J. Econom. 11(1):1–28. De Clercq D, Fried VH, Lehtonen O, Sapienza HJ (2006) An entrepreneur’s guide to the venture capital galaxy. Acad. Management Perspectives 20(3):90–112. Diener K, Piller FT (2010) The Market for Open Innovation: Increasing the Efﬁciency and Effectiveness of the Innovation Process (RWTH Aachen University, Aachen, Germany). Dixit AK, Stiglitz JE (1977) Monopolistic competition and optimum product diversity. Amer. Econom. Rev. 67(3):297–308. Dushnitsky G, Lenox MJ (2005a) When do ﬁrms undertake R&D by investing in new ventures? Strategic Management J. 26(10): 947–965. Dushnitsky G, Lenox MJ (2005b) When do incumbents learn from entrepreneurial ventures? Corporate venture capital and investing ﬁrm innovation rates. Res. Policy 34(5):615–639. Dushnitsky G, Lenox MJ (2006) When does corporate venture capital investment create ﬁrm value? J. Bus. Venturing 21(6): 753–772. Edelson H (2001) The downside of a corporate VC fund. Directors Boards 25(3):41–44. Fichman RG (2004) Going beyond the dominant paradigm for information technology innovation research: Emerging concepts and methods. J. Assoc. Inform. Systems 5(8):314–355. Fulghieri P, Sevilir M (2009) Organization and ﬁnancing of innovation, and the choice between corporate and independent venture capital. J. Financial Quant. Anal. 44(6):1291–1321. Gaba V, Meyer AD (2008) Crossing the organizational species barrier: How venture capital practices inﬁltrated the information technology sector. Acad. Management J. 51(5):976–998. Galasso A, Simcoe TS (2011) CEO overconﬁdence and innovation. Management Sci. 57(8):1469–1484. Gilbert RJ, Newbery DMG (1982) Preemptive patenting and the persistence of monopoly. Amer. Econom. Rev. 72(3):514–526. Godoe H (2000) Innovation regimes, R&D and radical innovations in telecommunications. Res. Policy 29(9):1033–1046. Gompers P, Lerner J (2001) The venture capital revolution. J. Econom. Perspectives 15(2):145–168. Gompers PA, Lerner J (1998) What drives venture capital fundraising? Brookings Papers on Economic Activity: Microeconomics, Washington, DC, 149–192. Grant RM (1996) Toward a knowledge-based theory of the ﬁrm. Strategic Management J. 17(S2):109–122. Grossman GM, Helpman E (1991) Quality ladders in the theory of growth. Rev. Econom. Stud. 58(193):43–61. Hall BH, Jaffe AB, Trajtenberg M (2001) The NBER patent citations data ﬁle: Lessons, insights and methodological tools. Working Paper, National Bureau of Economic Research, Cambridge, MA. Han K, Oh W, Im KS, Chang RM, Oh H, Pinsonneault A (2012) Value cocreation and wealth spillover in open innovation alliances. MIS Quart. 36(1):291–325. Hart OD (1983) The market mechanism as an incentive scheme. Bell J. Econom. 14(2):366–382. Hoberg G, Phillips G (2010) Product market synergies and competition in mergers and acquisitions: A text-based analysis. Rev. Financial Stud. 23(10):3773–3811. Hoberg G, Phillips G (2016) Text-based network industries and endogenous product differentiation. J. Political Econom. Forthcoming. Hotelling H (1929) Stability in competition. Econom. J. 39(153): 41–57. Huang Y, Singh PV, Srinivasan K (2014) Crowdsourcing new product ideas under consumer learning. Management Sci. 60(9): 2138–2159. Iansiti M (2000) How the incumbent can win: Managing technological transitions in the semiconductor industry. Management Sci. 46(2):169–185. Joshi K, Chi L, Datta A, Han S (2010) Changing the competitive landscape: Continuous innovation through IT-enabled knowledge capabilities. Inform. Systems Res. 21(3):472–495.\n",
              "\n",
              "Katila R, Rosenberger JD, Eisenhardt KM (2008) Swimming with sharks: Technology ventures, defense mechanisms and corporate relationships. Admin. Sci. Quart. 53(2):295–332. Kim N, Lee H, Kim W, Lee H, Suh JH (2015) Dynamic patterns of industry convergence: Evidence from a large amount of unstructured data. Res. Policy 44(9):1734–1748. Kleis L, Chwelos P, Ramirez RV, Cockburn I (2012) Information technology and intangible output: The impact of IT investment on innovation productivity. Inform. Systems Res. 23(1):42–59. Kortum S, Lerner J (2000) Assessing the contribution of venture capital to innovation. RAND J. Econom. 31(4):674–692. Lane PJ, Lubatkin M (1998) Relative absorptive capacity and interorganizational learning. Strategic Management J. 19(5):461–477. Lee C-H, Venkatraman N, Tanriverdi H, Iyer B (2010) Complementarity-based hypercompetition in the software industry: Theory and empirical test, 1990–2002. Strategic Management J. 31(13): 1431–1456. Lee CM, Ma P, Wang CC (2015) Search-based peer ﬁrms: Aggregating investor perceptions through Internet co-searches. J. Financial Econom. 116(2):410–431. Lee T, Wilde LL (1980) Market structure and innovation: A reformulation. Quart. J. Econom. 94(2):429–436. Leimeister JM, Huber M, Bretschneider U, Krcmar H (2009) Leveraging crowdsourcing: Activation-supporting components for IT-based ideas competition. J. Management Inform. Systems 26(1):197–224. Lerner J (2013) Corporate venturing. Harvard Bus. Rev. 91(10):86–94. Levin RC, Cohen WM, Mowery DC (1985) R&D appropriability, opportunity, and market structure: New evidence on some Schumpeterian hypotheses. Amer. Econom. Rev. 75(2):20–24. MacMillan I, Roberts E, Livada V, Wang A (2008) Corporate venture capital CVC Seeking innovation and strategic growth. National Institute of Standards and Technology, U.S. Department of Commerce, Washington, DC. Malhotra A, Gosain S, El Sawy OA (2005) Absorptive capacity conﬁgurations in supply chains: Gearing for partner-enabled market knowledge creation. MIS Quart. 29(1):145–187. March JG (1991) Exploration and exploitation in organizational learning. Organ. Sci. 2(1):71–87. McAfee A, Brynjolfsson E (2008) Investing in the IT that makes a competitive difference. Harvard Bus. Rev. 86(7/8):98–107. Melville N, Gurbaxani V, Kraemer K (2007) The productivity impact of information technology across competitive regimes: The role of industry concentration and dynamism. Decision Support Systems 43(1):229–242. Mendelson H (2000) Organizational architecture and success in the information technology industry. Management Sci. 46(4): 513–529. Mendelson H, Pillai RR (1999) Industry clockspeed: Measurement and operational implications. Manufacturing Service Oper. Management 1(1):1–20. Mowery DC (1999) U.S. Industry in 2000: Studies in Competitive Performance (National Academies Press, Washington, DC). Myers SC, Majluf NS (1984) Corporate ﬁnancing and investment decisions when ﬁrms have information that investors do not have. J. Financial Econom. 13(2):187–221. Nelson RR, Winter SG (2009) An Evolutionary Theory of Economic Change (Harvard University Press, Cambridge, MA). Nerkar A, Paruchuri S (2005) Evolution of R&D capabilities: The role of knowledge networks within a ﬁrm. Management Sci. 51(5):771–785. Nickell SJ (1996) Competition and corporate performance. J. Political Econom. 104(4):724–746. Nowak MJ, Grantham CE (2000) The virtual incubator: Managing human capital in the software industry. Res. Policy 29(2): 125–134. Pavlou PA, El Sawy OA (2010) The “third hand”: IT-enabled competitive advantage in turbulence through improvisational capabilities. Inform. Systems Res. 21(3):443–471.\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "\n",
              "Kim, Gopal, and Hoberg: Does Product Market Competition Drive CVC Investment?\n",
              "Information Systems Research 27(2), pp. 259–281, © 2016 INFORMS\n",
              "\n",
              "281\n",
              "\n",
              "Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.\n",
              "\n",
              "Roberts B (2006) Show me the technology. Electronic Bus. 36(8): 52–56. Roodman D (2009) How to do xtabond2: An introduction to difference and system GMM in Stata. Stata J. 9(1):86–136. Rothaermel FT (2001) Incumbent’s advantage through exploiting complementary assets via interﬁrm cooperation. Strategic Management J. 22(6–7):687–699. Sahaym A, Steensma HK, Barden JQ (2010) The inﬂuence of R&D investment on the use of corporate venture capital: An industry-level analysis. J. Bus. Venturing 25(4):376–388. Sambamurthy V, Bharadwaj A, Grover V (2003) Shaping agility through digital options: Reconceptualizing the role of information technology in contemporary ﬁrms. MIS Quart. 27(2): 237–263. Schildt HA, Maula MV, Keil T (2005) Explorative and exploitative learning from external corporate ventures. Entrepreneurship Theory Practice 29(4):493–515. Schumpeter JA (1942) Capitalism, Socialism and Democracy (Harper & Row, New York). Sutton J (1991) Sunk Costs and Market Structure (MIT Press, Cambridge, MA). Swanson EB, Ramiller NC (2004) Innovating mindfully with information technology. MIS Quart. 28(4):553–583. Tambe P, Hitt LM, Brynjolfsson E (2012) The extroverted ﬁrm: How external information practices affect innovation and productivity. Management Sci. 58(5):843–859.\n",
              "\n",
              "Tang J (2006) Competition and innovation behaviour. Res. Policy 35(1):68–82. Tanriverdi H (2005) Information technology relatedness, knowledge management capability, and performance of multibusiness ﬁrms. MIS Quart. 29(2):311–334. Tanriverdi H, Lee C-H (2008) Within-industry diversiﬁcation and ﬁrm performance in the presence of network externalities: Evidence from the software industry. Acad. Management J. 51(2):381–397. Tirole J (1988) The Theory of Industrial Organization (MIT Press, Cambridge, MA). Tong TW, Li Y (2011) Real options and investment mode: Evidence from corporate venture capital and acquisition. Organ. Sci. 22(3):659–674. Van de Vrande V, Lemmens C, Vanhaverbeke W (2006) Choosing governance modes for external technology sourcing. R&D Management 36(3):347–363. Vasudeva G, Anand J (2011) Unpacking absorptive capacity: A study of knowledge utilization from alliance portfolios. Acad. Management J. 54(3):611–623. Wadhwa ANU, Kotha S (2006) Knowledge creation through external venturing: Evidence from the telecommunications equipment manufacturing industry. Acad. Management J. 49(4):819–835. Walsh ST, Kirchhoff BA, Newbert S (2002) Differentiating market strategies for disruptive technologies. IEEE Trans. Engrg. Management 49(4):341–351.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "CKiHsD4BG6oY",
        "colab_type": "code",
        "outputId": "50539da8-b606-47e2-d428-3984262ff3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc9.sents if 'p =' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The interaction term of product market competition and the software dummy is signiﬁcant (at p = 0 10) across all speciﬁcations.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "metadata": {
        "id": "V5-w3B71463i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mnn5qGeGUaj",
        "colab_type": "code",
        "outputId": "dcf15527-6417-4b58-d165-ba590a3fb4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12161
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc10 = nlp(open(u\"NuijtenA#KeilM#CommandeurH_2016_Collaborative partner or opponent - How the messenger influences the deaf effect in IT projects_European Journal of Information Systems_6.txt\").read())\n",
        "doc10\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25, European Journal of Information Systems (2016) (2016) 1– 19 534–552\n",
              "© 2016 Operational Research Society Ltd. All rights reserved 0960-085X/16 16\n",
              "www.palgrave.com/journals www.palgrave-journals.com/ejis/\n",
              "\n",
              "EMPIRICAL RESEARCH\n",
              "\n",
              "Collaborative partner or opponent: How the messenger influences the deaf effect in IT projects\n",
              "Arno Nuijten1, Mark Keil2 and Harry Commandeur1\n",
              "Erasmus School of Economics, Erasmus University Rotterdam, Rotterdam, The Netherlands; 2Department of Computer Information Systems, Georgia State University, Atlanta, U.S.A. Correspondence: Arno Nuijten, Erasmus School of Economics, Erasmus University Rotterdam, Room H 13-06, P.O. Box 1738 NL3000 DR Rotterdam, The Netherlands. Tel: +31 10 4081508; E-mail: nuijten@ese.eur.nl\n",
              "1\n",
              "\n",
              "Abstract Prior research suggests that information technology (IT) project escalation can result from the deaf effect, a phenomenon in which decision makers fail to heed risk warnings communicated by others. Drawing inspiration from stewardship theory, we posited that when messengers carrying risk warnings about a project are seen as collaborative partners, decision makers are more likely to heed the message. Conversely, we theorized that when messengers are seen as opponents, decision makers are more likely to exhibit the deaf effect. We further posited that certain psychological factors (i.e., framing and perceived control) would moderate the effect of the messenger-recipient relationship on the deaf effect. To test these ideas, we conducted two experiments. When messengers were seen as collaborative partners, recipients assigned more relevance to the risk warning and perceived a higher risk, making them less willing to continue the project. Framing the outcomes associated with redirecting or continuing the project in terms of losses (rather than gains) weakened this effect. However, when recipients perceived a high degree of control over the project the effect was strengthened. Implications for both research and practice are discussed. European Journal of Information Systems (2016) 25(6), 534–552. doi:10.1057/ejis.2016.6; advance online publication, 22 March 2016\n",
              "Keywords: IT project escalation; deaf effect; stewardship theory; agency theory; framing; risk\n",
              "\n",
              "Introduction\n",
              "In March 2013, an outside consulting ﬁrm issued warnings regarding the $600 million federal health insurance website project known as Healthcare.gov. Senior Obama administration ofﬁcials were briefed regarding the content of the consulting ﬁrm’s report. But the warnings were ignored and the Healthcare.gov website was rolled out as scheduled on October 1, 2014. This resulted in a ﬁasco in which millions of Americans tried to enroll in health care plans under the new Affordable Care Act but were unable to do so (Eilperin & Somashekhar, 2013). The very troubled launch of Healthcare.gov and the chaos that it created illustrates what can happen when people become overly committed to failing courses of action and ignore risk warnings. Unfortunately, this underlying failure pattern – escalation of commitment to a failing course of action – is an all-too-frequent and serious problem in information technology (IT) projects like the launch of Healthcare.gov (Keil et al, 2000a).\n",
              "\n",
              "Associate Editor: Prof. Kieran Conboy Received: 11 July 2014 Revised: 19 November 2014 2nd Revision: 25 November 2015 3rd Revision: 26 January 2016 Accepted: 3 February 2016\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 535\n",
              "\n",
              "Prior research on escalation has tended to focus on the psychological factors that encourage individuals to continue pursuing a previously chosen course of action despite negative feedback, such as personal responsibility (Staw, 1976), sunk cost (Arkes & Blumer, 1985; Garland, 1990), and project completion level (Conlon & Garland, 1993; Keil et al, 1995b; Moon, 2001). Some studies, however, have suggested that IT project escalation can also be caused by the so-called ‘deaf effect’ (Keil & Robey, 2001; Cuellar, 2009), which refers to a communication failure in which decision makers exhibit resistance to or fail to heed risk warnings communicated by subordinates or other individuals who play the role of bad news messengers. Keil et al (2014) suggest that executives often turn a deaf ear to IT project status reports. This phenomenon may be particularly salient in IT projects which often involve power and politics that invariably shape how one chooses to respond to problems and the repercussions that will result (Markus, 1983; Sillince & Mouakket, 1997). Prior work on the deaf effect is somewhat limited, and has been grounded in concepts from social psychology (Chaiken, 1980), communication (Rakow, 1986), and IT project escalation (Keil & Robey, 1999; Cuellar et al, 2006). In deaf effect situations, messengers deliver risk warnings to decision makers who must decide whether to ascribe relevance to these messages and ultimately whether or not to heed the warning or continue the project as planned. The inﬂuence of the messenger-recipient relationship (MRR) on the deaf effect represents a key theoretical gap in our understanding of this phenomenon. In this paper, we draw inspiration from stewardship theory, positing that when messengers carrying risk warnings about a project are seen as collaborative partners (i.e., working on the same side toward the same goal), decision makers will be more likely to listen and respond appropriately to the message. Conversely, when messengers carrying risk warnings about a project are seen as opponents (i.e., working in opposition and not being viewed as being on the same side), decision makers will more likely turn a deaf ear to the message. Speciﬁcally, we address two important theoretical gaps concerning the deaf effect and escalation of commitment: (1) the impact of MRR, as informed by stewardship theory, on whether the decision maker turns a deaf ear to risk warnings (i.e., deaf effect), and (2) the moderating effect of two key psychological factors that are associated with escalation behavior (i.e., gain/loss framing and perceived control) on the relationship between MRR and the deaf effect. Our aim is to answer the following three research questions: (1) Are recipients less likely to exhibit the deaf effect when a risk warning comes from a messenger who is viewed as a collaborative partner rather than an opponent? (2) What are the mediating mechanisms through which the MRR inﬂuences the deaf effect? (3) How is the effect of the MRR on the deaf effect moderated by key psychological factors that have been associated with escalation (e.g., framing and perceived control)?\n",
              "\n",
              "To address these questions, we conducted two scenariobased experiments in which we manipulated the MRR (collaborative partner vs opponent) to determine if decision makers are less likely to exhibit the deaf effect when a risk warning comes from a messenger who is viewed as a collaborative partner rather than an opponent. In the ﬁrst experiment, we further manipulated the framing of the outcomes associated with redirecting or continuing the project in gain/loss terms to investigate its role as a moderator of the relationship between MRR and the deaf effect. In the second experiment, we manipulated perceived control to investigate its role as a moderator of the relationship between MRR and the deaf effect. The remainder of the paper is organized as follows: ﬁrst, we discuss the theoretical background for our study, justify our hypotheses, and present our research model. Next, we describe the experiment and present the results that were obtained. Finally, we discuss the implications of our study both for research and practice.\n",
              "\n",
              "Theoretical background and hypotheses\n",
              "While negative feedback is inherently part of an escalation situation, prior research has paid relatively little attention as to where this negative feedback generally comes from or the manner in which it is delivered.\n",
              "\n",
              "The deaf effect as a cause of escalation In the context of IT project escalation, the negative feedback often comes in the form of a risk warning delivered by another member of the organization to the decision maker. Thus, the deaf effect perspective on escalation highlights the notion that there are often risk warnings associated with escalation situations (as in the case of the Healthcare.gov example given above) and that escalation can occur when decision makers choose to ignore these risk warnings. Here, we posit that whether such warnings are heeded or ignored hinges on the MRR. To date, there has been very little research on the deaf effect in the context of IT project escalation. On the basis of interviews with internal auditors, Keil & Robey (2001) ﬁrst described the deaf effect as a failure to respond to messages of an impending IT project failure. Some of the auditors recalled instances in which they had issued risk warnings about projects only to ﬁnd that their concerns were ignored by senior management. Subsequent research in this area has been very limited and has focused on the inﬂuence of messenger characteristics (e.g., credibility and role prescription) on the deaf effect. For example, Cuellar et al (2006) conducted an experiment in which they found that the credibility of the messenger inﬂuenced both the perceived message relevance and the degree to which the decision maker chose to heed the risk warning, as evidenced by their willingness to continue a previously chosen course of action. In an extension of Cuellar et al (2006), Lee et al (2014) found that the role prescription of the messenger also inﬂuenced perceived message relevance, thereby shaping\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 536\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "the extent to which the decision maker heeds the messenger’s risk warning. Speciﬁcally, decision makers were more likely to heed risk warnings that came from individuals who were role prescribed to issue such warnings (e.g., internal auditors) as opposed to individuals who were not and this relationship was mediated by perceived message relevance. Lee et al (2014) also found that risk perception mediates the relationship between messenger credibility and willingness to continue a troubled IT project. While the above studies point to the importance of certain messenger characteristics (e.g., credibility and role prescription), they do not explore the nature of the MRR and how this inﬂuences the deaf effect. This represents an important gap in our understanding. Many organizations employ internal auditors who are role prescribed to provide credible risk warnings on large IT projects should such warnings be required. While prior work focused on credibility and role prescription of the messenger, in this study, we focus on situations involving internal auditors who are assumed to be both role prescribed and credible. Therefore, our focus is not on messenger characteristics, but rather on the MRR.\n",
              "\n",
              "than an opponent. Consistent with Kasima et al (2011), we posit that internal auditors can follow either an agency theory perspective or a stewardship theory perspective depending upon the role they wish to play. Thus, we distinguish a MRR in which the messenger is seen as a collaborative partner (stewardship theory) from one in which the messenger is regarded as an opponent (agency theory). In order to address our ﬁrst two research questions, namely whether recipients are less likely to exhibit the deaf effect when a risk warning comes from a messenger who is viewed as a collaborative partner rather than an opponent (RQ1), and what potential mediating mechanisms govern this relationship (RQ2), we offer the following theorizing and hypotheses (H1 and H2) concerning the effect of MRR on the deaf effect and mediating role of both message relevance and perceived risk in this relationship.\n",
              "\n",
              "Stewardship theory and the MRR Drawing inspiration from the corporate governance literature, we suggest that internal auditors can follow different approaches in carrying out their work. In the past, auditors have often positioned themselves or been seen as ‘the enemy’ because of their emphasis on a monitoring and control-oriented approach to their work. Such an approach, which is consistent with an agency theory ( Jensen & Meckling, 1976; Fama, 1980; Fama & Jensen, 1983) view of the world (i.e., that agents will not act in the interest of the principal and must therefore be closely monitored and disciplined) is not trust-based and ultimately tends to create an adversarial MRR. Under the agency theory view of governance, those who work on IT projects cannot be trusted to reveal the true status of the project and to manage it in a way that takes the best interests of the organization into account. Therefore, auditors must closely monitor such projects and the personnel who work on them. Thus, under the agency theory approach to project governance, project personnel are apt to regard the auditor (or messenger) as an ‘opponent’ who is not to be trusted. In contrast, stewardship theory represents an alternative view of governance (see, for example, Sundaramurthy & Lewis (2003) for a comparison of stewardship theory and agency theory) that emphasizes a more collaborative approach. Under stewardship theory, the auditor’s role would be to service and advise (as opposed to discipline and monitor per agency theory). We believe that stewardship theory (Davis et al, 1997) therefore offers an alternative approach to IT project governance; one that is more trust-based. Thus, under the stewardship theory approach to project governance, project personnel are apt to regard the auditor (or messenger) as a collaborative partner rather\n",
              "\n",
              "Effect of MRR on the deaf effect through message relevance Prior work by Keil & Robey (2001), involving interviews with auditors, suggests that the relationship between auditor and decision maker may inﬂuence the deaf effect in escalation situations. For example, one interviewee suggested that an auditor is likely to have more inﬂuence if s/he is perceived as a ‘team player’ instead of a ‘policeman’ as illustrated in the following remark:\n",
              "I think the way we handled it made a difference . . . I think just the way we came about it, as a team player instead of a policeman . . . Even though we are an independent appraisal organization, we are still part of the same corporate team, and our goals are their goals basically. We all want the company to do well. (Keil & Robey, 2001). Thus, we have some anecdotal evidence that a collaborative relationship may minimize the deaf effect. Conversely, in a study of internal auditing, Chambers et al (1988) found that ‘the auditee’s reaction to the inspection style of auditing was hostile. The auditee was inclined not to listen to the auditor or to beneﬁt from the ﬁndings’ (Chambers et al, 1988, p. 73). On the basis of stewardship theory, we posit that when an auditor is seen as a collaborative partner, decision makers will be less likely to turn a deaf ear to risk warnings issued by the auditor. The underlying logic for this assertion is that decision makers are more likely to be responsive to risk warnings when the messenger has the clear goal to contribute to management performance instead of exposing management failures. Moreover, when an auditor is seen as a collaborative partner, we theorize that MRR’s inﬂuence on the deaf effect is mediated by message relevance. Message relevance as a proposed mediator is consistent with the Evans’ heuristic-analytic theory which posits that human information processing consists of two stages: heuristic processing and analytic processing (Evans, 1996, 2006). According to this model, in the heuristic processing\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 537\n",
              "\n",
              "stage, people ﬁlter the information to which they are exposed and assign relevance to particular aspects. Lee et al (2014) found that message relevance served to mediate the relationship between certain messenger characteristics (i.e., the messenger’s role prescription and credibility) and the deaf effect. In our research, we posit that message relevance will mediate the relationship between MRR and the deaf effect. The logic for this assertion is that people will assign more relevance to a message when it comes from a collaborative partner who is trusted and shares the decision maker’s goals, rather than from an opponent. When more relevance is assigned to a risk warning, the decision maker will be less likely to exhibit the deaf effect. Therefore, we propose the following mediation hypothesis: H1: When an auditor is seen as a collaborative partner rather than an opponent, decision makers are less likely to turn a deaf ear to the auditor’s risk warning and this effect will be mediated by message relevance.\n",
              "\n",
              "this relationship holds across cultures in escalation situations (Keil et al, 2000b, c). Moreover, there is one prior study on the deaf effect in which risk perception has actually been shown to mediate the relationship between the credibility of a messenger and escalation (Lee et al, 2014). Therefore, we propose the following mediation hypothesis: H2: When an auditor is seen as a collaborative partner rather than an opponent, decision makers are less likely to turn a deaf ear to the auditor’s risk warning and this effect will be mediated by perceived risk.\n",
              "\n",
              "Effect of MRR on deaf effect through perceived risk As theorized earlier, the MRR will inﬂuence the deaf effect. Speciﬁcally, when an auditor is seen as a collaborative partner, decision makers will be less likely to turn a deaf ear to risk warnings issued by the auditor. Furthermore, we theorize that MRR’s inﬂuence on the deaf effect is mediated by perceived risk. Perceived risk refers to a ‘decision maker’s assessment of the risk inherent in a situation’ (Sitkin & Pablo, 1992, p. 12). In this research, we propose that when an auditor is perceived as a collaborative partner and issues a risk warning, the decision maker’s perceived risk associated with the project will be greater than when the risk warning is issued by an auditor who is seen as an opponent. The underlying logic for this assertion is that decision makers will tend to discount the reported level of risk in a project when a messenger who is regarded as an opponent issues a risk warning that characterizes the project as having a certain level of risk. This tendency to discount is based on the fact that a messenger who is seen as an opponent will be regarded as having a motivation to expose management failures, and thus the level of risk ascribed to a project by such a messenger will be perceived as being exaggerated by the recipient. Conversely, we theorize that the reported level of risk in a project will not be discounted when the message comes from a collaborative partner who is perceived to have the same goals and therefore is regarded as being ‘on the same side’. We further theorize that perceived risk inﬂuences the deaf effect. Prior research has shown that perceived risk inﬂuences risk decisions (Slovic et al, 1982; Sitkin & Pablo, 1992; Sitkin & Weingart, 1995). Speciﬁcally, greater perceived risk has been shown to reduce risky decisionmaking behavior (Sitkin & Weingart, 1995). Prior studies have also shown a signiﬁcant negative relationship between risk perception and the decision to continue a software development project (Keil et al, 2000c) and that\n",
              "\n",
              "Davis et al (1997) call for further research on the inﬂuence of psychological variables on stewardship mechanisms, suggesting to us that it may be fruitful to examine the moderating role of certain psychological factors on the MRR. Two key psychological factors that are associated with escalation of commitment and may interact with the collaborative partnering approach prescribed by stewardship theory are gain/loss framing and perceived control. In order to address our third research question, namely how the effect of the MRR on the deaf effect is moderated by key psychological factors that have been associated with escalation (RQ3), we offer the following theorizing and hypotheses (H3a and H3b) concerning the moderating role of gain/loss framing and perceived control.\n",
              "\n",
              "Gain/Loss framing as a moderator of the relationship between MRR and the deaf effect Prospect theory suggests that decisions can be inﬂuenced by the way in which they are framed. On the basis of prospect theory, individuals are risk-seeking when a decision is framed as a choice between losses, but are risk averse when a decision is framed as a choice between gains (Kahneman & Tversky, 1979). Gain/loss framing refers to the presentation of outcomes as positive or negative deviations (gains or losses) from a neutral reference point, which is assigned a value of 0 (Tversky & Kahneman, 1981). One of the classic examples of framing involves the possibility of a disease outbreak that will kill 600 people (Tversky & Kahneman, 1981). When people are confronted with two alternative programs to combat the disease they tend to choose the program that will save 200 people over the program that confers a 1/3 probability of saving 600 people and a 2/3 probability that no people will be saved. However, when the two programs are framed differently, people tend to choose the program that will confers a 1/3 probability that nobody will die and a 2/3 probability that 600 people will die over the program that will result in 400 people dying. Thus, even though the two choice problems are identical in terms of expected outcomes, when the choices involve gain framing, people demonstrate risk aversion (i.e., preferring the sure thing over the gamble); however, when the choices involve loss framing, people demonstrate risk seeking (i.e., preferring the gamble over the sure thing).\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 538\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "Gain/loss framing is relevant to the ﬁeld of project escalation since it inﬂuences the risk preference of decision makers with regard to a project. Speciﬁcally, a loss framing has been associated with risk-seeking behavior which can engender escalation of commitment to a failing project (Rutledge & Harrell, 1993; Sharp & Salter, 1997) and the so-called sunk cost effect (Arkes & Blumer, 1985; Whyte, 1986; Garland, 1990). In escalation situations, decision makers may have a tendency to add further resources to a troubled project because the costs already incurred are perceived as losses, thereby invoking a loss framing. Numerous studies have invoked prospect theory and framing and to explain escalation, but the effect of gain/ loss framing on the strength of the relationship between MRR and escalation remains unknown. In escalation situations involving the deaf effect, we posit that gain/loss framing of the outcomes associated with redirecting or continuing the project will have an inﬂuence on the decision maker’s willingness to listen to the risk warning. Speciﬁcally, when an individual is placed in a loss frame, s/he may feel less compelled to listen to a risk warning. Our interest in this research, however, centers not on the main effect of gain/loss framing, but on how it interacts with the MRR. Speciﬁcally, we theorize that gain/loss framing of the project outcomes moderates the inﬂuence of MRR on the deaf effect. The underlying logic for this assertion is that when a decision maker is placed in a loss frame s/he will be less sensitive to the collaborative nature of the MRR. One explanation for this is that the loss frame makes the decision maker more riskseeking, and thus more resistant to risk warnings issued from either a collaborative partner or an opponent, thus weakening the impact of MRR differences on the deaf effect. Thus we state the following hypothesis: H3a: The gain/loss framing of the outcomes associated with redirecting or continuing the project moderates the inﬂuence of MRR on the deaf effect. Speciﬁcally, the inﬂuence of MRR on the deaf effect is weakened when the outcomes are framed as losses.\n",
              "\n",
              "context of the MRR. Therefore, the effect of perceived control on the strength of the relationship between MRR and escalation remains unknown. Perceived control provides decision makers with a sense that they can inﬂuence events in ways that mitigate risks. Consistent with Du et al (2007), we deﬁne perceived control as an individual’s own judgment of the extent to which s/he can control a project outcome, which should be distinguished from the actual level of control that an individual has (Thompson et al, 1998, p. 154). Prior research has shown that perceived control inﬂuences escalation decisions (Du et al, 2007; Jani, 2008). Speciﬁcally, a high level of perceived control is associated with risk-seeking behavior (i.e., escalation of commitment). In escalation situations involving the deaf effect, we posit that the decision maker’s perceived control over the project will have an inﬂuence on his/her willingness to listen to the risk warning. To the extent that an individual has high perceived control over a project, s/he may feel less compelled to listen to a risk warning. Our interest in this research, however, centers not on the main effect of perceived control, but on how it interacts with the MRR. Speciﬁcally, we theorize that perceived control moderates the inﬂuence of MRR on the deaf effect. The underlying logic for this assertion is that a messenger who exposes management failures and is therefore regarded as an opponent will likely trigger competitive arousal among decision makers who have a high level of perceived control over the project. Competitive arousal (Ku et al, 2005) coupled with high perceived control will encourage decision makers to exhibit the deaf effect. Conversely, decision makers with a low level of perceived control may feel less challenged to compete with a messenger who is viewed as an opponent. To summarize, decision makers with a high perceived control over the project will be more sensitive to differences in the MRR and therefore we expect the effect of MRR on the deaf effect will be stronger under the condition of high perceived control. Thus we state the following hypothesis: H3b: The decision maker’s perceived control over the project moderates the inﬂuence of MRR on the deaf effect. Speciﬁcally, the inﬂuence of MRR on the deaf effect is strengthened when perceived control over the project is high. In order to test our hypotheses, we conducted two experiments as explained in the methods section below. We opted for two separate experiments rather than one to keep the complexity at a manageable level and to maximize our degree of control. The research models for the two experiments are shown in Figures 1 and 2. As indicated in the ﬁgures, the mediation paths hypothesized in H1 and H2 are included in both models, the hypothesized moderating effect of gain/loss framing (H3a) is shown in Figure 1, and the hypothesized moderating effect of perceived control (H3b) is shown in Figure 2.\n",
              "\n",
              "Perceived control as a moderator of the relationship between MRR and the deaf effect Another factor that may inﬂuence how a decision maker responds to a messenger’s risk warnings is the decision maker’s perceived level of control over the project outcome. In our review of relevant literature, we identiﬁed only one previous study that has investigated the relationship between perceived control and escalation (Du et al, 2007). In a role-playing experiment, Du et al (2007) found that a higher degree of perceived control was associated with a lower level of risk perception and more risk seeking behavior (i.e., a greater tendency to escalate). However, it is important to note that the primary focus of the Du et al (2007) study was on the impact of attention-shaping tools on risk assessment. Thus, while they also examined escalation decision-making, they did not do so within the\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 539\n",
              "\n",
              "Interaction Term\n",
              "\n",
              "Message Relevance Message Framed as Gains (Frame)\n",
              "\n",
              "Frame x MRR\n",
              "\n",
              "Continue\n",
              "\n",
              "Messenger is Collaborative Partner (MRR)\n",
              "\n",
              "Independent Variables\n",
              "\n",
              "Perceived Risk\n",
              "\n",
              "Risk Propensity\n",
              "\n",
              "Gender\n",
              "\n",
              "Work Experience\n",
              "\n",
              "Mediating Variables Legend\n",
              "H1: MRR Message Relevance Perceived Risk Continue\n",
              "\n",
              "Control Variables\n",
              "\n",
              "H2:\n",
              "H3a:\n",
              "\n",
              "MRR\n",
              "MRR\n",
              "\n",
              "Continue\n",
              "\n",
              "Continue is moderated by Message Framing\n",
              "\n",
              "Note:\n",
              "\n",
              "Dashed lines refer to paths that are included in the model and tested but not formally hypothesized\n",
              "\n",
              "Figure 1\n",
              "\n",
              "Research model Experiment 1.\n",
              "\n",
              "Interaction Term\n",
              "PercContr x MRR\n",
              "\n",
              "Message Relevance\n",
              "\n",
              "Perceived Control\n",
              "\n",
              "Continue\n",
              "\n",
              "Messenger is Collaborative Partner (MRR)\n",
              "\n",
              "Independent Variables\n",
              "\n",
              "Perceived Risk\n",
              "\n",
              "Risk Propensity\n",
              "\n",
              "Gender\n",
              "\n",
              "Work Experience\n",
              "\n",
              "Mediating Variables Legend\n",
              "H1: H2: H3b: Note:\n",
              "\n",
              "Control Variables\n",
              "\n",
              "MRR Message Relevance Continue Continue Perceived Risk MRR MRR Continue is moderated by Perceived Control Dashed lines refer to paths that are included in the model and tested but not formally hypothesized\n",
              "\n",
              "Figure 2\n",
              "\n",
              "Research model Experiment 2.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 540\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "In addition to the hypothesized mediation paths and interactions shown in Figures 1 and 2, we have included additional paths in our models based on prior research as well as the need to include lower-order terms in order to test interactions. For example, the relationship between perceived control and perceived risk is well understood (Du et al, 2007), as is the relationship between gain/loss framing and perceived risk (Sitkin & Weingart, 1995). Therefore, we included these paths in our models but do not hypothesize them. Further, we include the paths from gain/loss frame (Figure 1) and perceived control (Figure 2) to escalation so that we may test the hypothesized moderation effects, but since these relationships are well established in literature we do not state formal hypotheses for them (Rutledge & Harrell, 1993; Sharp & Salter, 1997; Du et al, 2007). Finally, for the sake of completeness we include the paths from gain/loss frame to message relevance (Figure 1) and from perceived control to message relevance (Figure 2). While these relationships have not been previously tested, they lie outside our area of theoretical focus and therefore we do not state formal hypotheses for them. In our models, we control for the effects of gender, work experience, and risk propensity. Prior research has shown that both gender and work experience can inﬂuence the deaf effect (Cuellar et al, 2006). Risk propensity refers to a personality trait to seek or avoid risks (Sitkin & Pablo, 1992; Sitkin & Weingart, 1995). Lee et al (2014) found that risk propensity can inﬂuence the deaf effect. Thus, we include these three variables in order to control for their effects.\n",
              "\n",
              "in a part-time professional degree program who were enrolled in management accounting and control courses at a Dutch university. Participants had an average age of 28.8 years (SD 5.7 years) and an average work experience of 5.0 years (SD 5.1 years). Ninety-eight percent of the subjects were of Dutch nationality. Sixty-two percent were male and 38% were female. These part-time executive students were preferred over undergraduate students, because of their extensive work experience, which was relevant for the perceived control treatments we used. If work experience-related factors, such as perceived control are studied, executives may exhibit different patterns in continuation decisions than undergraduate students do (Chang & Ho, 2004). In each experiment, participation was voluntary and took place during the ﬁrst 20 min of class. Participants were randomly assigned to one of the four treatment conditions.\n",
              "\n",
              "Method\n",
              "To test our hypotheses, two scenario-based laboratory experiments were conducted. In Experiment 1, we manipulated MRR (collaborative partner vs opponent) and gain/loss framing in a 2×2 factorial design. In Experiment 2, we manipulated MRR and perceived control (high vs low) in a 2×2 factorial design.\n",
              "\n",
              "Subjects For Experiment 1, we recruited 199 masters’ students who were enrolled in accounting and information systems (IS) courses at four universities located within a 70-mile radius in the Netherlands and Belgium. Participants had an average age of 22.7 years (SD 2.3 years) and an average work experience of 1.1 years (SD 1.8 years). Ninety-three percent of the participants had a European nationality, dominated by Dutch (26%) and Belgian (38%) citizenship. Fifty-eight percent were male and 42% were female. Student subjects were deemed to be appropriate for this experiment because framing is a cognitive bias that should not be a function of work experience. In a business decision making context, Mowen & Mowen (1986) found that both students and managers exhibited the same pattern of decision bias due to the way the information was framed. For Experiment 2, we recruited 140 executives\n",
              "\n",
              "Scenario and treatments The scenarios for the two experiments were based on the scenario used by Wong et al (2008), which was adapted from the radar blank plane case used by Arkes & Blumer (1985), variations of which have been used by many researchers to study escalation of commitment (Garland, 1990; Conlon & Garland, 1993; Keil et al, 1995a). To create the messenger-recipient context required to study the deaf effect, we adapted the approach used by Cuellar et al (2006). Our treatment for gain/loss framing in Experiment 1 was patterned after Wong et al (2008), and our treatment for perceived control in Experiment 2 was patterned after Du et al (2007). We asked the participants to consider themselves to be the project owner of a strategic IS-project within an insurance company. In accordance with the company’s standard procedures, the internal audit department assessed the quality of the testing activities just before implementation of the new IS. On the basis of this assessment, the internal auditor reported serious weaknesses in the testing activities designed to assess the reliability of critical data exchange with other IS. He estimates there is a 2/3 probability that exchange of data would show reliability problems in the ﬁrst month of operations. As a consequence, he reports that the project should be redirected and should not be continued as planned. The MRR was manipulated by portraying the internal auditor as either a collaborative partner or an opponent. For the collaborative partner manipulation, we drew on stewardship theory (Davis et al, 1997; Sundaramurthy & Lewis, 2003), describing the MRR as one that involved a high level of trust (Davis et al, 1997) and where the auditor has a history of contributing to management performance (Davis et al, 1997; Kasima et al, 2011). For the opponent manipulation, we described the MRR as one that involved a low level of trust (Davis et al, 1997) and where the auditor is seen as a ‘policeman’ (Chambers et al, 1988;\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 541\n",
              "\n",
              "Keil & Robey, 2001) and has a history of exposing management failings (Davis et al, 1997; Kasima et al, 2011). In Experiment 1, our manipulation of gain/loss frame was adapted from Wong et al (2008). The scenario and manipulations we used in Experiment 1 are presented in the Appendix. In Experiment 2, we replaced the gain/loss frame manipulation with a manipulation of perceived control consistent with Du et al (2007). The scenario and manipulations we used in Experiment 2 are also presented in the Appendix.\n",
              "\n",
              "Table 1a\n",
              "\n",
              "Mean values of Partnermc by treatment condition in Experiment 1\n",
              "Frame Loss Gain 3.42 (1.16) N = 50 5.64 (0.79) N = 49 4.52 (1.49) N = 99 3.51 (1.21) N = 99 5.54 (0.88) N = 99\n",
              "\n",
              "MRR\n",
              "\n",
              "Opponent Partner\n",
              "\n",
              "3.60 (1.26) N = 49 5.45 (0.93) N = 50 4.53 (1.45) N = 99\n",
              "\n",
              "Constructs and measures All of our independent variables were manipulated and treated as dichotomous variables. In Experiments 1 and 2 we manipulated MRR and created a variable Partner (1 = collaborative partner; 0 = opponent). In Experiment 1, we also manipulated gain/loss framing and created a variable Frame (1 = Framed as gains; 0 = Framed as losses). In Experiment 2, we manipulated perceived control and created a variable PercContr (1 = High perceived control; 0 = Low perceived control). The Appendix shows all of the construct measures that were employed. Our two mediating variables (MsgRelev and PercRisk) used in both experiments were measured using established scales. Speciﬁcally, three measurement items adapted from Cuellar et al (2006) were used to assess the relevance the decision maker ascribes to the auditor’s message (MsgRelev). Perceived risk (PercRisk) was measured using four items adopted from Sitkin & Weingart (1995). The dependent variable used in both experiments is the decision to continue a troubled IS project (Continue) despite the risk warning by the internal auditor. Two measurement items (Continue1 and Continue2) were used to assess this construct. The ﬁrst (Continue1) was adopted from Cuellar et al (Cuellar et al, 2006; Cuellar et al, 2007) and the second (Continue2) was created for this study. We added the second measurement item because a single item measure cannot be assessed for reliability. Consistent with prior studies (Keil et al, 2000b; Cuellar et al, 2006), we included risk propensity as a control variable (RiskProp) which was measured using four items adapted from Sitkin & Weingart (1995).\n",
              "\n",
              "Table 1b\n",
              "\n",
              "Manipulation test ANOVA for Experiment 1\n",
              "Type III Sum of Squares 0.002 204.716 1.710 F 0.002 181.830 1.518 Sig. 0.963 0.000 0.219\n",
              "\n",
              "Independent variable Main Effect Frame Main Effect MRR Interaction Effect Frame×MRR\n",
              "\n",
              "Dependent Variable: Partnermc R2 is 0.486\n",
              "\n",
              "Results\n",
              "Manipulation checks and descriptive statistics For each experiment, we conducted manipulation checks to ensure that our treatments were effective. We developed a manipulation check (Partnermc) for MRR using a 3-item scale (see Partnermc under the Measures section of the Appendix) and computing a composite score. Table 1a presents the mean values of Partnermc for each of the four treatment conditions in Experiment 1. As indicated, the MRR treatment resulted in higher Partnermc values for the collaborative partner treatment groups and lower Partnermc values for the opponent treatment groups. Moreover, the Partnermc values did not appear to be affected by gain/loss framing.\n",
              "\n",
              "To conﬁrm these results, a two-way ANOVA with interaction was conducted in which the manipulations were entered as independent variables and Partnermc was the dependent variable. The results of this ANOVA, shown in Table 1b, conﬁrm that the MRR manipulation was effective and that there was no signiﬁcant interaction effect. We developed an additional manipulation check for Experiment 2 for our perceived control manipulation (PercContrmc) using a 3-item scale (see Perceived Controlmc under the Measures section of the Appendix) and computing a composite score. Table 2a presents the mean values of Partnermc (our manipulation check for MRR) and PercContrmc for each of the four treatment conditions in Experiment 2. As indicated, the MRR treatment resulted in higher Partnermc values for the collaborative partner treatment groups and lower Partnermc values for the opponent treatment groups. Moreover, Partnermc values did not appear to be inﬂuenced by the perceived control manipulation. Further, the perceived control treatment resulted in higher PercContrmc values for the high perceived control treatment groups and lower PercContrmc values for the low perceived control treatment groups. Moreover, PercContrmc values did not appear to be affected by the MRR manipulation. To conﬁrm these results, a two-way MANOVA with interaction was conducted in which the treatment conditions were entered as independent variables and the manipulation checks (Partnermc and PercContrmc) were the dependent variables. The results of this MANOVA, shown in Table 2b, conﬁrm that both manipulations were effective and that there was no signiﬁcant interaction effect.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 542\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "Table 2a\n",
              "\n",
              "Mean values of Partnermc and PercContrmc by treatment condition in Experiment 2\n",
              "PercContr Low High 3.85 (1.25) 2.94 (1.35) N = 33 5.38 (0.73) 2.94 (1.12) N = 34 Partnermc PercContrmc N = 67 4.63 (1.27) 2.94 (1.23) Partnermc PercContrmc N = 67 Partnermc PercContrmc N = 33 Partnermc PercContrmc N = 34 4.73 (1.24) 5.04 (1.04) 5.52 (0.79) 4.78 (1.20) Partnermc PercContrmc N = 68 3.91 (1.08) 5.31 (0.78) Partnermc PercContrmc N = 66 5.45 (0.76) 3.86 (1.48) 3.88 (1.16) 4.13 (1.62)\n",
              "\n",
              "MRR\n",
              "\n",
              "Opponent\n",
              "\n",
              "Partnermc PercContrmc Partnermc PercContrmc\n",
              "\n",
              "Partner\n",
              "\n",
              "Table 2b\n",
              "Independent variable\n",
              "\n",
              "Manipulation test MANOVA for Experiment 2\n",
              "Dependent variable PercContrmca Type III Sum of Squares F-Value (Sig) Dependent variable Partnermcb Type III Sum of Squares 0.361 82.240 0.063 F-Value (Sig)\n",
              "\n",
              "Table 3\n",
              "Construct Continue Message Relevance Perceived Risk\n",
              "\n",
              "Item to construct loadings for experiments 1 and 2\n",
              "Item Continue1 Continue2 MsgRelev1 MsgRelev2 MsgRelev3 PercRisk1 PercRisk2 PercRisk3 PercRisk4 RiskProp1 RiskProp2 RiskProp3 RiskProp4 Item-to-Construct Item-to-Construct loading experiment 1 loading experiment 2 0.977 0.975 0.896 0.909 0.858 0.839 0.850 0.829 0.875 0.695 0.722 0.770 0.783 0.961 0.962 0.900 0.891 0.896 0.819 0.828 0.828 0.869 0.815 0.766 0.867 0.821\n",
              "\n",
              "Main effect 148.179 114.751 (0.000) PercContr Main effect MRR 2.416 1.871 (0.174) Interaction effect 2.268 1.757 (0.187) PercControl×MRR\n",
              "a 2 b 2\n",
              "\n",
              "0.371 (0.544) 84.506 (0.000) 0.064 (0.800)\n",
              "\n",
              "R is 0.464. R is 0.381.\n",
              "\n",
              "Risk Propensity\n",
              "\n",
              "Measurement model assessment We chose Partial Least Squares (PLS) analysis to test our research models. PLS allows us to assess both the measurement model and structural model together (Gefen et al, 2000; Gefen et al, 2011). PLS is appropriate for testing theories in the early stages of development and allows us to test both mediation and moderation in the context of the model as a whole. In this study we used SmartPLS (Ringle et al, 2005) version 2.0. Before testing our structural model, we followed the recommended steps to establish the validity of our measurement model through tests of convergent and discriminant validity as described by Chin (1998) and Fornell & Larcker (1981).\n",
              "\n",
              "Convergent validity Two different assessments were made for convergent validity: (1) individual item reliability, and (2) construct reliability. Individual item reliability was assessed by examining the item-to-construct loadings for each construct that was measured with multiple indicators. In order for the shared variance between each item and its associated construct to exceed the error variance, the standardized loadings should be greater than 0.70.\n",
              "\n",
              "As seen in Table 3, all of our loadings exceeded this threshold across Experiments 1 and 2. We also considered the construct reliability for each block of measures, as shown in Table 4. Composite reliability scores and Cronbach’s α scores both measure the internal consistency among a given construct’s items. Unlike the more traditional Cronbach’s α, the composite reliability score does not assume that all indicators are equally weighted. Therefore, α tends to be a lower bound estimate of reliability (Chin, 1998). Bearden et al (1993) claim that a score of 0.7 indicates extensive evidence of reliability. Table 4 shows that the reliability for each of our constructs exceeds this threshold for both experiments. Fornell & Larcker (1981) view Average Variance Extracted (AVE) as a measure of construct reliability. The guideline threshold for AVE is 0.5, which means that 50% or more of variance of the indicators is accounted for (Chin, 1998). As Table 4 indicates, all of the constructs in our measurement model exceeded the established criterion for AVE.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 543\n",
              "\n",
              "Table 4\n",
              "AVE Experiment 1 Continue MsgRelev PercRisk RiskProp 0.953 0.789 0.720 0.553 Experiment 2 0.924 0.802 0.700 0.670\n",
              "\n",
              "Construct reliability\n",
              "Composite reliability Experiment 1 0.976 0.918 0.911 0.831 Experiment 2 0.960 0.924 0.903 0.890 Cronbach’s α Experiment 1 0.951 0.866 0.870 0.730 Experiment 2 0.918 0.878 0.857 0.843\n",
              "\n",
              "Discriminant validity We conducted two tests for discriminant validity. First, we calculated each indicator’s loading on its own construct as well as its cross-loading on all other constructs. Moving across the columns of Table 5, we see that each indicator loads higher on its own construct (in bold) than it does on any other constructs. Moreover, going down the rows of Table 5, we see that the indicators for a given construct have a higher loading with their own construct than do the indicators associated with any other construct. This provides good evidence of discriminant validity (Chin, 1998). As a second test of discriminant validity, we considered whether the AVEs of the latent constructs were greater than the square of the correlations among the latent constructs. When this is true, more variance is shared between the latent construct and its block of indicators than with another construct (Chin, 1998). As can be seen by reading across the rows of Table 6, our measures passed this test, thus providing additional evidence of discriminant validity. Structural model assessment With adequate measurement models in place for the two experiments, we tested our hypotheses by examining the structural models. The explanatory power of a structural model can be evaluated by looking at the R2 value in the ﬁnal dependent construct. As presented in Figures 3a and 3b, the explanatory power of our structural models is satisfactory with R2 for the ﬁnal dependent construct Continue of 0.521 in Experiment 1 and 0.463 in Experiment 2. After computing path estimates for the structural models, we used bootstrapping to obtain the corresponding t-values. With signiﬁcance levels of 0.05, 0.01, and 0.001, the acceptable t-values for a one-tailed test would be 1.645, 2.326, and 3.091, respectively (appropriate given the directional nature of the hypotheses). Path coefﬁcients and t-values for the two models are presented in Figures 3a and 3b. Testing of mediation hypotheses As shown in Figures 3a and 3b, the MRR-to-message relevance and message relevance-to-continue paths are signiﬁcant in both experiments, as are the MRR-to-perceived risk and the perceived risk-to-continue paths. In Table 7 we present the results of our mediation analysis associated with these paths as hypothesized in H1 and H2 using PLS. In order to test\n",
              "\n",
              "these mediation hypotheses, we used the parameter estimates from the bootstrapping procedure in PLS, and calculated the standard error of each mediation effect. We then calculated a t-statistic for each mediation effect by dividing the magnitude of the mediation effect (i.e., the observed mediation effect from the original sample) by the standard error of the mediation effects. Probing indirect effects in this manner (i.e., using bootstrapping) is advantageous relative to the more traditional Sobel test approach because it does not impose any distributional assumptions regarding the indirect effect. Moreover, conducting the mediation analysis based on the PLS bootstrapping results has an advantage over the Shrout & Bolger (2002) regression based bootstrapping approach because it allows for testing of mediation effects within the context of the entire structural model (see, for example, Rai & Hornyak, 2013) as opposed to isolating one portion of the model at a time and running a series of mediation tests. Following this approach, we obtained signiﬁcant t-values for all of the hypothesized mediation effects as shown in Table 7. As a robustness check, we also conducted bootstrapping-based regressions following the Shrout & Bolger (2002) approach, using the Preacher & Hayes (2008) SPSS macro for multiple mediator models developed by Hayes. Results were consistent with those obtained through PLS. Taken as a whole, our results provide strong support for H1 and H2, as the hypothesized mediation paths were signiﬁcant across both experiments. As suggested by Preacher & Kelley (2011), we considered not only the statistical signiﬁcance of indirect effects but also their effect sizes. Following the presentation guidelines provided by Iacobucci (2008), we show in Table 7 the speciﬁc indirect effects via MsgRelev and PercRisk, in the context of the PLS-estimated direct effect and the total effect of MRR on Continue, for both our experiments. These speciﬁc indirect effects were calculated as the product of the two unstandardized paths from MRR to mediator and from mediator to Continue consistent with Preacher & Hayes (2008). Furthermore, as suggested by Iacobucci (2008) we present the ratio of the speciﬁc indirect effects to the total effect and the ratio of the direct effect to the total effect, across both our experiments.\n",
              "\n",
              "Testing of moderation hypotheses In order to test our moderation hypotheses, we constructed interaction terms using the product indicator approach as proposed by Chin et al (1996). As shown in Figure 3a the\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 544\n",
              "\n",
              "Table 5\n",
              "Gender Experiment 1 0.11 0.07 1.00 −0.00 −0.03 −0.09 −0.05 −0.06 −0.03 −0.03 0.08 0.11 0.10 0.19 0.22 0.08 0.15 1.00 −0.11 −0.11 −0.17 0.06 0.06 0.07 −0.01 0.13 0.10 0.01 −0.01 0.18 −0.54 −0.52 −0.04 0.89 0.90 0.85 0.22 0.24 0.21 0.26 −0.13 −0.15 −0.15 −0.13 0.00 0.41 0.35 0.16 −0.16 −0.14 −0.20 −0.28 −0.22 −0.10 −0.12 0.69 0.72 0.77 0.78 0.22 0.34 0.25 0.05 −0.09 −0.10 −0.19 −0.21 −0.13 −0.23 −0.24 0.81 0.76 0.86 0.82 −0.04 0.10 0.11 0.22 0.02 0.00 −0.02 −0.01 −0.06 −0.04 −0.01 0.04 0.21 0.26 0.13 1.00 −0.36 −0.31 −0.15 0.90 0.89 0.89 0.26 0.21 0.04 0.16 −0.01 −0.02 −0.19 −0.18 0.03 −0.39 −0.40 −0.05 0.21 0.23 0.29 0.83 0.85 0.82 0.87 −0.15 −0.15 −0.19 −0.14 −0.04 −0.45 −0.47 0.05 0.17 0.16 0.20 0.81 0.82 0.82 0.86 −0.20 −0.20 −0.25 −0.14 0.10 −0.11 −0.19 0.18 0.06 −0.01 0.03 0.11 0.11 −0.01 0.15 0.01 −0.05 −0.02 −0.08 1.00 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 MsgRelev PercRisk RiskProp WorkExp\n",
              "\n",
              "Item to own construct correlation vs correlations with other constructs\n",
              "\n",
              "Construct\n",
              "\n",
              "Item\n",
              "\n",
              "Continue\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "Experiment 1 0.96 0.96 0.12 −0.29 −0.26 −0.37 −0.44 −0.39 −0.40 −0.36 0.24 0.11 0.33 0.23 −0.16\n",
              "\n",
              "Experiment 2\n",
              "\n",
              "Continue Continue1 Continue2 Gender Gender MsgRelev MsgRelev1 MsgRelev2 MsgRelev3 PercRisk PercRisk1 PercRisk2 PercRisk3 PercRisk4 RiskProp RiskProp1 RiskProp2 RiskProp3 RiskProp4 WorkExp WorkExp\n",
              "\n",
              "0.97 0.97 0.10 −0.46 −0.49 −0.50 −0.34 −0.34 −0.36 −0.34 0.26 0.27 0.30 0.32 0.11\n",
              "\n",
              "Collaborative partner or opponent\n",
              "Arno Nuijten et al\n",
              "\n",
              "Table 6\n",
              "Continue MsgRelev\n",
              "\n",
              "AVEs vs square of correlations among latent constructs\n",
              "PercRisk RiskProp Gender\n",
              "\n",
              "Construct Average Variance Extracted (AVE)\n",
              "\n",
              "Experiment 1 — 0.30 0.16 0.15 0.01 0.01 — 0.07 0.03 0.00 0.00 — 0.12 0.22 0.09 0.01 0.02\n",
              "\n",
              "Experiment 2\n",
              "\n",
              "Experiment 1 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 Experiment 1 Experiment 2 — 0.04 0.02 0.02 0.00\n",
              "\n",
              "Continue MsgRelev PercRisk RiskProp Gender WorkExp\n",
              "\n",
              "0.95 0.78 0.72 0.55 1.00 1.00\n",
              "\n",
              "0.92 0.80 0.70 0.67 1.00 1.00\n",
              "\n",
              "— 0.04 0.00 0.00\n",
              "\n",
              "— 0.06 0.00 0.01\n",
              "\n",
              "— 0.02 0.05\n",
              "\n",
              "— 0.00 0.00\n",
              "\n",
              "— 0.04\n",
              "\n",
              "— 0.03\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 545\n",
              "\n",
              "R2 = 0.205 Frame x MRR - 0.089 (t=1.884)*\n",
              "\n",
              "Message Relevance Message Framedas Gains (Frame)\n",
              "\n",
              "Continue\n",
              "\n",
              "R2 = 0.521\n",
              "\n",
              "Messenger is Collaborative Partner (MRR) Perceived Risk Risk Propensity\n",
              "\n",
              "0.019 (t=0.350)\n",
              "\n",
              "Gender\n",
              "\n",
              "Work Experience\n",
              "\n",
              "R2 = 0.066 Legend H1: H2: H3a: Note: MRR MRR Message Relevance Perceived Risk Continue Continue\n",
              "\n",
              "Continue is moderated by Message Framing MRR Dotted lines refer to paths that were non-significant\n",
              "\n",
              "* significant at p < 0.05 level (one-tailed test) ** significant at p < 0.01 (one-tailed test) *** significant at p < 0.001 (one-tailed test)\n",
              "\n",
              "Figure 3a\n",
              "\n",
              "Experiment 1 structural model results.\n",
              "\n",
              "R2 = 0.233\n",
              "PercContr x MRR\n",
              "\n",
              "Message Relevance\n",
              "\n",
              "Perceived Control\n",
              "\n",
              "- 0.181 (t=2.671)**\n",
              "\n",
              "Continue\n",
              "\n",
              "R2 = 0.463\n",
              "\n",
              "Messenger is Collaborative Partner (MRR) Perceived Risk Risk Propensity\n",
              "\n",
              "0.129 (t=1.988)*\n",
              "\n",
              "Gender\n",
              "\n",
              "Work Experience\n",
              "\n",
              "R2 = 0.138\n",
              "Legend\n",
              "H1: H2: H3b: Note: MRR MRR Message Relevance Perceived Risk Continue Continue\n",
              "\n",
              "MRR Continue is moderated by Perceived Control Dotted lines refer to paths that were non-significant\n",
              "\n",
              "* significant at p < 0.05 level (one-tailed test) ** significant at p < 0.01 (one-tailed test) *** significant at p < 0.001 (one-tailed test)\n",
              "\n",
              "Figure 3b\n",
              "\n",
              "Experiment 2 structural model results.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 546\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "Table 7\n",
              "\n",
              "Mediation analysis of Experiments 1 and 2\n",
              "t-value Effect Confidence Intervala Lower Upper 5.6 71.4 23.0 100.0 Ratio to Total Effect (%) Conclusion\n",
              "\n",
              "Experiment 1 MRR to Continue\n",
              "\n",
              "Direct effect Indirect effect via MsgRelev Indirect effect via PercRisk Total effect\n",
              "\n",
              "−5.204*** −2.349**\n",
              "\n",
              "−0.013 −0.169 −0.054 −0.236\n",
              "\n",
              "−0.223 −0.098\n",
              "\n",
              "−0.121 −0.022\n",
              "\n",
              "H1 confirmed H2 confirmed\n",
              "\n",
              "Experiment 2 MRR to Continue\n",
              "\n",
              "Direct effect Indirect effect via MsgRelev Indirect effect via PercRisk Total effect\n",
              "\n",
              "−2.110* −2.416**\n",
              "\n",
              "−0.184 −0.097 −0.075 −0.355\n",
              "\n",
              "−0.162 −0.136\n",
              "\n",
              "−0.009 −0.032\n",
              "\n",
              "51.8 27.1 21.1 100\n",
              "\n",
              "H1 confirmed H2 confirmed\n",
              "\n",
              "a Confidence intervals refer to the 5 and 95% percentiles of the bootstrapped indirect effects. *significant at P<0.05 level; **significant at P<0.01; ***significant at P<0.001 (one-tailed test).\n",
              "\n",
              "interaction term (Frame×MRR) was signiﬁcant at the P<0.05 level in a 1-tailed test (t = 1.884), indicating that the effect of MRR on the deaf effect is weakened when we move from a gain frame to a loss frame. This result provides support for H3a. As shown in Figure 3b the interaction term (PercContr×MRR) was signiﬁcant at the P<0.01 level in a 1-tailed test (t = 2.671), indicating that the effect of MRR on the deaf effect is strengthened as we move from low perceived control to high perceived control, thus supporting H3b.\n",
              "\n",
              "8 7 6\n",
              "Continue\n",
              "\n",
              "5 4 3\n",
              "\n",
              "Continue\n",
              "\n",
              "Interaction plots Figures 4a and 4b provide a visual display of the two interaction effects. To construct these ﬁgures, we used the mean values associated with each treatment condition and a composite score for the dependent variable. Figure 4a shows that decision makers in a loss frame are more likely to continue the project and thus exhibit the deaf effect to a risk warning. As expected, the line in the loss domain is ﬂatter than the line in the gain domain. Thus, when project outcomes are framed in terms of losses, people are less inﬂuenced by the MRR than when project outcomes are framed in terms of gains. Figure 4b shows that decision makers who have a high level of perceived control over the project are more likely to continue the project and thus exhibit the deaf effect to a risk warning. As expected, the line in the high control domain is steeper than the line in the low control domain. Thus, when perceived control is high, people are more inﬂuenced by the MRR than when perceived control is low.\n",
              "\n",
              "Message Framed as Gains\n",
              "\n",
              "2\n",
              "Message Framed as Losses\n",
              "\n",
              "1 Messenger is Opponent Messenger is Collaborative Partner\n",
              "\n",
              "Figure 4a\n",
              "\n",
              "Interaction plot with Gain/Loss framing as moderator.\n",
              "\n",
              "8\n",
              "Decisionmaker has High Perceived Control\n",
              "\n",
              "7\n",
              "Decisionmaker has Low Perceived Control\n",
              "\n",
              "6 5 4 3 2 1 Messenger is Opponent Messenger is Collaborative Partner\n",
              "\n",
              "Discussion and implications\n",
              "The deaf effect perspective on escalation highlights the notion that there are often risk warnings associated with escalation situations (as in the case of the Healthcare.gov example which we used to introduce our paper) and that escalation can occur when decision makers choose to ignore these risk warnings. While negative feedback is inherently part of an escalation situation, prior research\n",
              "\n",
              "Figure 4b\n",
              "\n",
              "Interaction plot with perceived control as moderator.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 547\n",
              "\n",
              "has paid relatively little attention as to where this negative feedback generally comes from or the manner in which it is delivered. In many instances, the negative feedback concerning a project and its status comes in the form of a risk warning delivered by another member of the organization to the decision maker. In our study we posited that whether such warnings are heeded or ignored hinges on the MRR, as informed by stewardship theory. Before discussing the implications of our study for research and practice, it is appropriate to consider limitations.\n",
              "\n",
              "Limitations While the experimental approach allowed us to achieve high internal validity, it obviously does not allow us to model all of the complexities of actual work settings. As is the case with all experiments, one must therefore be cautious in generalizing the results. To achieve the high level of internal validity afforded by laboratory experiments requires that researchers focus on a very small number of variables while tightly controlling possible confounding factors. By necessity, this means that it is neither practical nor desirable to attempt the level of realism normally associated with a workplace setting. This trade-off of higher internal validity for lower external validity is normal in laboratory experiments and should not necessarily be construed as a weakness. Experimental designs must therefore be evaluated not on the extent to which they reﬂect actual organizational settings, but rather on whether they contribute to our understanding of human decision-making (Dobbins et al, 1988). In our experiment we engaged university students in a decision task concerning an IT project. The use of student subjects is sometimes criticized because such subjects may lack the domain-speciﬁc knowledge needed to properly engage with the stimulus material. In our case, however, we relied not on undergraduate students with little or no work experience, but instead recruited masters’ students and executives with signiﬁcant work experience who were enrolled in a part-time professional program. Thus, we have every reason to believe that our participants were able to project themselves into the scenarios that formed the basis for our experiments. Moreover, since our goal was to generalize to theory and not to a particular population, the use of student subjects for our experiment was justiﬁed. Finally, as is customary in experiments of this type, we used self-reported measures. It is possible that self-reported measures for such constructs as message relevance might not show entirely consistent results with more objective measures (e.g., eye movement tracking). In spite of the aforementioned limitations, we believe that our results have implications for both research and practice. Implications for research Our study breaks new ground by addressing two theoretical gaps in the literature concerning the deaf effect and escalation of commitment: (1) the impact of MRR (collaborative partner vs opponent) on whether the decision\n",
              "\n",
              "maker turns a deaf ear to risk warnings, and (2) the moderating effect of two key psychological factors that are associated with escalation behavior (i.e., gain/loss framing and perceived control) on the relationship between MRR and the deaf effect. On the basis of two experiments that were inspired by the distinction between stewardship theory and agency theory views of project governance, involving 199 masters’ students and 134 executive students, respectively, we found empirical support for the impact of MRR (collaborative partner vs opponent) on the deaf effect, thus addressing the ﬁrst theoretical gap mentioned above. More speciﬁcally, we found that decision makers are less likely to turn a deaf ear to risk warnings, when the messenger is seen as a collaborative partner and that this effect is mediated by both message relevance (H1) and perceived risk (H2). In other words, our results provide empirical support for two mediating mechanisms between MRR and the deaf effect. First, people are less likely to turn a deaf ear to risk warnings when the messenger is seen as a collaborative partner, because they assign more relevance to a message when it comes from a collaborative partner (H1). These results are consistent with heuristic-analytic theory (Evans, 1996, 2006) and the notion that people ﬁlter the information to which they are exposed and assign relevance to particular aspects. Second, people are less likely to turn a deaf ear to risk warnings when the messenger is seen as a collaborative partner, because such messages will heighten their risk perception (H2). Conversely, when the messenger is seen as an opponent, people will tend to discount the reported level of risk in a project, thus leading to a lowering of risk perception. Such discounting is not likely to occur when the risk warning is issued by a messenger who is perceived as a collaborative partner with the same goals and who is therefore regarded as being ‘on the same side’. Our results for H1 and H2 contribute to the existing body of knowledge on the deaf effect as it relates to IT project escalation. While previous work (Cuellar et al, 2006; Lee et al, 2014) had pointed to the importance of certain messenger characteristics, our research adds to what was previously known by highlighting both the importance of the MRR and the mechanisms through which it inﬂuences the deaf effect. With regard to the second theoretical gap, our results provide empirical support for the moderating role of gain/ loss framing of project outcomes on the relationship between MRR and the deaf effect. More speciﬁcally, the inﬂuence of MRR on the deaf effect is weakened when the outcomes are framed as losses (H3a). Our results support the notion that when a decision maker is placed in a loss frame s/he will be less sensitive to the collaborative nature of the MRR. One explanation for this is that the loss frame makes the decision maker more risk-seeking, and thus more resistant to risk warnings issued from either a collaborative partner or an opponent, thus weakening the impact of MRR differences on the deaf effect. Further, our results provide empirical support for the moderating role of the decision maker’s perceived control\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 548\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "on the relationship between MRR and the deaf effect. Speciﬁcally, the inﬂuence of MRR on the deaf effect is strengthened when perceived control over the project is high (H3b). Our results support the notion that a decision maker who perceives a high level of control over a project will be more sensitive to the collaborative nature of the MRR and therefore less likely to listen to a risk warning when the messenger is seen as an opponent. One explanation for this may be that a messenger who is regarded as an opponent will likely trigger competitive arousal among decision makers who have a high level of perceived control over the project. Taken together, our results for H3a and H3b contribute to the existing body of knowledge on the deaf effect as it relates to IT project escalation. While previous work has pointed to the role that certain psychological factors such as gain/loss framing and perceived control can have on escalation of commitment (see, for example, Rutledge & Harrell, 1993; Du et al, 2007), our research adds to what was previously known by highlighting the important moderating role that these factors can have on the relationship between MRR and the deaf effect.\n",
              "\n",
              "Directions for future research Several directions for future research emerge from our study. First, it would be useful to investigate the MRR in an actual organizational setting to conﬁrm the ﬁndings that were obtained in our experiments. Second, it would be insightful to conduct a longitudinal study that allowed us better understand the dynamics of the MRR and the role that history and particular events play in this evolution. Third, it might be beneﬁcial to consider not only relationships between individuals at an abstract level, but also within the context of the teams, departments, and organizations to which they belong. While this would require a multi-level approach, and some additional complexity, the rewards could be substantial. Thus, while our study sheds light on the deaf effect, we believe that it also opens up promising avenues for future research and we encourage others to advance our understanding of this important phenomenon. Implications for practice In addition to the research implications mentioned above, the results of our study have important implications for practice. First, on the basis of not one, but two experiments, we provide clear evidence that when an auditor who provides risk warnings is seen as a collaborative partner (rather than an opponent), this can greatly reduce the deaf effect. Our ﬁrst experiment involved Masters’ students, and our second experiment involved executives who were enrolled in a part-time professional degree program. The fact that we obtained consistent results with both Masters’ students and executives adds to the robustness of our ﬁndings. One key implication for practice that emerges from our study is that auditors’ messages will have more impact when the auditors are perceived as\n",
              "\n",
              "collaborative partners rather than as opponents. This is due to the fact that recipients ascribe greater relevance and perceive greater risk when the auditor is viewed as a collaborative partner (as opposed to an opponent). Therefore, managers should do everything in their power to create an environment in which auditors are viewed as collaborators, as this will help to ensure that their risk warnings are not ignored. Second, our research shows that when an auditor frames possible project outcomes as a choice between losses, this weakens the effect of being seen as a collaborative partner. In other words, auditors who position themselves as collaborative partners in an effort to reduce the deaf effect will be well advised to frame their risk warnings as a choice between gains rather than a choice between losses. Third, our research shows that control represents a twoedged sword. On the one hand, when perceived control over a project is high, decision makers are less apt to heed risk warnings that come from an auditor who is perceived as an opponent. Thus the combination of an auditor who is viewed as an opponent and a decision maker who perceives that s/he has high control over the outcome of the project appears to be a particularly dangerous situation because it induces the decision maker to be deaf to risk warnings. On the other hand, in terms of reducing the deaf effect, the inﬂuence of the MRR is stronger when perceived control over the project is high. Based on our ﬁndings, managers would do well to create an environment in which auditors are seen as collaborators and this is particularly true if decision makers are apt to perceive a high degree of control over the projects that they manage. All of the above implications follow directly from our data. In a somewhat more speculative vein, we suggest that both messengers (e.g., auditors) and recipients bear some responsibility for creating and maintaining a relationship that is based on collaboration as opposed to competition. If, for example, an auditor establishes a pattern of behaving in an adversarial manner, s/he should not be too surprised if recipients turn a deaf ear to the auditor’s risk warnings. Likewise, recipients who are in the habit of viewing auditors as non-value adding obstructionists are not likely to listen seriously to auditors’ risk warnings. To a certain extent, individual auditors and recipients can decide for themselves whether they choose to behave as collaborative partners or opponents. However, it is important to remember that both auditors and recipients are part of a larger organizational culture and develop of history of interactions that cannot be ignored. If auditors and recipients have a history as adversaries, a sudden shift by auditors to a more collaborative approach will not necessarily be trusted or viewed as genuine. Thus, we speculate that if auditors want to be heard it is important to build up a history in which they are viewed as collaborative partners rather than opponents. We further speculate that auditors who behave in an instrumental way, shifting opportunistically from opponent to collaborative partner and back again from project to project will not be trusted as true collaborative partners.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 549\n",
              "\n",
              "About the authors\n",
              "Arno L.P. Nuijten is currently academic director of the IT auditing & advisory program at the Erasmus School of Accounting and Assurance, the Erasmus University Rotterdam, The Netherlands. His research interests are in the area of IT auditing, internal auditing and managerial decision making on IT risks in general, and IT projects in particular. For over 25 years he has consulted with large companies throughout Europe on a variety of business problems. He has a Ph.D. in Information Science from Erasmus University and holds his MSc in Information Systems from Tilburg University. Mark Keil is currently the John B. Zellars Professor of Computer Information Systems at the J. Mack Robinson College of Business, Georgia State University, Atlanta, United States. His research focuses on IT project management and includes work on preventing IT project escalation, identifying and managing IT project risks, and improving IT project status reporting. His interests also include IT implementation and use. He has published more than 100 refereed papers and served on the editorial boards of many academic journals. He holds B.S.E., S.M., and D.B.A. degrees from Princeton University, M.I.T. Sloan School, and Harvard Business School, respectively. Harry R. Commandeur is currently full professor of industrial economics and business economics and holds the F.D.J Goldschmeding Chair of Economics and Humanities at the Erasmus School of Economics, Erasmus University Rotterdam, The Netherlands. His research focuses on the relationship between market structure, corporate strategy and ﬁrm performance. He holds MSc and Ph.D. degrees from Erasmus University Rotterdam.\n",
              "\n",
              "References\n",
              "ARKES HR and BLUMER C (1985) The psychology of sunk costs. Organizational Behavior and Human Decision Processes 35(1), 124–140. BEARDEN WO, NETEMEYER RG and MOBLEY MF (1993) Handbook of Marketing Scales. Sage Publications, Newbury Park, CA. CHAIKEN S (1980) Heuristic versus systematic information processing and the use of source versus message cues in persuasion. Journal of Personality & Social Psychology 39(5), 752–766. CHAMBERS A, SELIM G and VINTEN G (1988) Internal Auditing, 2nd edn, Pitman, London. CHANG CJ and HO JLY (2004) Judgement and decision making in project continuation: a study of students as surrogates for experienced managers. ABACUS 40(1), 94–116. CHIN WW (1998) The partial least squares approach to structured equation modeling. In Modern Methods for Business Research (MARCOULIDES GA, Ed.), pp 295–336, Lawrence Erlbaum Associates, NJ. CHIN WW, MARCOLIN BL and NEWSTED PR (1996) A partial least squares latent variable modeling approach for measuring interaction effects: results of a Monte Carlo simulation study and voice mail emotion/ adoption study. Paper presented at the 17th International Conference on Information Systems, Cleveland, Ohio. CONLON DE and GARLAND H (1993) The role of project completion information in resource allocation decisions. The Academy of Management Journal 36(2), 402–413. CUELLAR MJ (2009) An Examination of the Deaf Effect Response to Bad News Reporting in Information Systems Projects. PhD, Georgia State University, Atlanta, Georgia, US. CUELLAR MJ, KEIL M and JOHNSON RD (2006) The deaf effect response to bad news reporting in information systems projects. E-Service Journal 5(1), 75–95. CUELLAR MJ, KEIL M, JOHNSON RD, BECK R, LIU S and PRETORIUS H (2007) The impact of collectivism on the deaf effect in IT projects. Paper presented at the 2nd International Research Workshop on Information Technology Project Management, Montreal, Canada. DAVIS JH, SCHOORMAN FD and DONALDSON L (1997) Toward a stewardship theory of management. Academy of Management Review 22(1), 20–47. DOBBINS GH, LANE IM and STEINER DD (1988) A note on the role of laboratory methodologies in applied behavioural research: don’t throw out the baby with the bath water. Journal of Organizational Behavior 9(3), 281–286. DU S, KEIL M, MATHIASSEN L, SHEN Y and TIWANA A (2007) Attention-shaping tools, expertise, and perceived control in IT project risk assessment. Decision Support Systems 43(1), 269–283. EILPERIN J and SOMASHEKHAR S (2013) Private Consultants Warned of Risks Before HealthCare.Gov’s Oct. 1 Launch. Washington Post, 18 November. EVANS JSBT (1996) Deciding before you think: Relevance and reasoning in the selection task. British Journal of Psychology 87(2), 223–240. EVANS JSBT (2006) The heuristic-analytic theory of reasoning: extension and evaluation. Psychonomic Bulletin & Review 13(3), 378–395. FAMA EF (1980) Agency problems and the theory of the firm. Journal of Political Economy 88(2), 288–307. FAMA EF and JENSEN MC (1983) Separation of ownership and control. The Journal of Law and Economics 26(2), 301–325. FORNELL C and LARCKER DF (1981) Structural equation models with unobservable variables and measurement errors. Journal of Marketing Research 18(2), 39–50. GARLAND H (1990) Throwing good money after bad: the effect of sunk costs on the decision to escalate commitment to an ongoing project. Journal of Applied Psychology 75(6), 728–731. GEFEN D, RIGDON EE and STRAUB D (2011) An update and extension to SEM guidelines for administrative and social science research. MIS Quarterly 35(2), III–XIV. GEFEN D, STRAUB D and BOUDREAU M (2000) Structural equation modeling techniques and regression: guidelines for research practice. Communications of the Association for Information Systems 4(7), 1–70. IACOBUCCI D (2008) Mediation Analysis, Vol. 156, Sage Publications, Los Angeles. JANI A (2008) An experimental investigation of factors influencing perceived control over a failing IT project. International Journal of Project Management 26(7), 726–732. JENSEN MC and MECKLING WH (1976) Theory of the firm: managerial behaviour, agency costs, and ownership structure. Journal of Financial Economics 3(4), 305–360. KAHNEMAN D and TVERSKY A (1979) Prospect theory: an analysis of decision under risk. Econometrica 47(2), 263–291. KASIMA M, HANAFIB S and RASHIDC A (2011) The veracity of the ERM implementation – An internal auditing perspective. Paper presented at the 2nd International Conference on Business and Economic Research, Kedah, Malaysia. KEIL M, MANN J and RAI A (2000a) Why software projects escalate: an empirical analysis and test of four theoretical models. MIS Quarterly 24(4), 631–664. KEIL M, MIXON R, SAARINEN T and TUUNAINEN V (1995a) Understanding runaway information technology projects: results from an international research program based on escalation theory. Journal of Management Information Systems 11(3), 65–85. KEIL M and ROBEY D (1999) Turning around troubled software projects: an exploratory study of the deescalation of commitment to failing courses of action. Journal of Management Information Systems 15(4), 63–87.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 550\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "KEIL M and ROBEY D (2001) Blowing the whistle on troubled software projects. Association for computing machinery. Communications of the ACM 44(4), 87–94. KEIL M, SMITH HJ, IACOVOU CL and THOMPSON RL (2014) The pitfalls of project status reporting. MIT Sloan Management Review 55(3), 57–64. KEIL M, TAN BCY, WEI K-K, SAARINEN T, TUUNAINEN V and WASSENAAR A (2000b) A cross-cultural study on escalation of commitment behavior in software projects. MIS Quarterly 24(2), 299–326. KEIL M, TRUEX D and MIXON R (1995b) The effects of sunk cost and project completion on information technology project escalation. IEEE Transactions on Engineering Management 42(4), 372–381. KEIL M, WALLACE L, TURK D, DIXON-RANDALL G and NULDEN U (2000c) An investigation of risk perception and risk propensity on the decision to continue a software development project. Journal of Systems and Software 53(2), 145–157. KU G, MALHOTRA D and MURNIGHAN JK (2005) Towards a competitive arousal model of decision-making: a study of auction fever in live and internet auctions. Organizational Behavior and Human Decision Processes 96(2), 89–103. LEE JS, CUELLAR MJ, KEIL M and JOHNSON RD (2014) The role of a bad news reporter in information technology project escalation: a deaf effect perspective. The Data Base for Advances in Information Systems 45(3), 8–29. MARKUS ML (1983) Power, politics, and MIS implementation. Communications of the ACM 26(6), 430–444. MOON H (2001) Looking forward and looking back: integrating completion and sunk-cost effects within an escalation-of-commitment progress decision. Journal of Applied Psychology 86(1), 104–113. MOWEN M and MOWEN JC (1986) An empirical examination of the biasing effects of framing on business decisions. Decision Sciences 17(4), 596–602. PREACHER KJ and HAYES AF (2008) Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models. Behavior Research Methods 40(3), 879–891. PREACHER KJ and KELLEY K (2011) Effect size measures for mediation models: quantitative strategies for communicating indirect effects. Psychological Methods 16(2), 93–115. RAI A and HORNYAK R (2013) The impact of sourcing enterprise system use and work process interdependence on sourcing professionals’ job outcomes. Journal of Operations Management 31(6), 474–488. RAKOW LF (1986) Rethinking gender research in communication. Journal of Communication 36(4), 11–26. RINGLE CM, WENDE S and WILL A (2005) SmartPLS – Software to Perform PLS Analyses. University of Hamburg, Germany. RUTLEDGE RW and HARRELL A (1993) Escalating commitment to an ongoing project: the effects of responsibility and framing of accounting information. International Journal of Management 10(3), 300–314. SHARP DJ and SALTER SB (1997) Project escalation and sunk costs: a test of the international generalizability of agency and prospect theories. Journal of International Business Studies 28(1), 101–121. SHROUT PE and BOLGER N (2002) Mediation in experimental and nonexperimental studies: new procedures and recommendations. Psychological Methods 7(4), 422–445. SILLINCE JAA and MOUAKKET S (1997) Varieties of political process during systems development. Information Systems Research 8(4), 368–397. SITKIN SB and PABLO AL (1992) Reconceptualizing the determinants of risk behavior. Academy of Management Review 17(1), 9–38. SITKIN SB and WEINGART LR (1995) Determinants of risky decision-making behavior: a test of the mediating role of risk perceptions and propensity. Academy of Management Journal 38(6), 1573–1592. SLOVIC P, FISCHHOFF B and LICHTENSTEIN S (1982) Facts versus fears: understanding perceived risks. In Judgment under Uncertainty: Heuristics and Biases (KAHNEMAN D, SLOVIC P and TVERSKY A, Eds), Cambridge University Press, Cambridge. STAW BM (1976) Knee-deep in the big muddy: a study of escalating commitment to a chosen course of action. Organizational Behavior and Human Performance 16(1), 27–44. SUNDARAMURTHY C and LEWIS M (2003) Control and collaboration: paradoxes of governance. Academy of Management Review 28(3), 397–415. THOMPSON SC, ARMSTRONG W and CRAIG T (1998) Illusions of control. Underestimations, and Accuracy: A Control Heuristic Explanation Psychological Bulletin 123(2), 143–161. TVERSKY A and KAHNEMAN D (1981) The framing of decisions and the psychology of choice. Science 211(4481), 453–458.\n",
              "\n",
              "WHYTE G (1986) Escalating commitment to a course of action: a reinterpretation. Academy of Management Review 11(2), 311–321. WONG KFE, KWONG JYY and NG CK (2008) When thinking rationally increases biases: the role of rational thinking style in escalation of commitment. Applied Psychology: An International Review 57(2), 246–271.\n",
              "\n",
              "Appendix\n",
              "Scenario and measures Scenario (Experiments 1 and 2) Imagine that you are the Senior Vice President of the Pensions Operations department within a large insurance company. You inherited a prestigious IS-project called PENSION-VIEW. As Project Owner, you became responsible for the successful implementation of PENSION-VIEW and for realizing the beneﬁts for your organization with this in-house developed system. With this IS-project you could be the ﬁrst insurance company in the market that grants all citizens (customers and potential customers) access to the complete set of their personal pension information. If your insurance company is the ﬁrst in the market to provide this service at a reliable level, the expected revenue to your company would be €60 million, as documented in a detailed business case for the project. Your main competitors have all decided to wait for the supplier of a standard software-package to provide a module to the insurance-market that integrates and presents their pension data. If your implementation is too late or does not prove reliable during the ﬁrst month of operations, you will miss your competitive advantage and your organization will gain nothing. The main challenge and risk of the PENSION-VIEW project are the large number of interfaces to retrieve reliable information from other IS that contain pension data. Your PENSION-VIEW project is close to implementation and under time-pressure to continue implementation as planned. According to standard procedures, Mr. Smith of the Internal Audit department has recently reviewed the testing procedures of your project. Treatments (Experiment 1) (Opponent manipulation)\n",
              "\n",
              "Mr. Smith has a long history of working AGAINST IS project teams with the goal of exposing project failings, thus embarrassing project owners. He is seen as policeman who does not add any value to the development process. Thus, Mr. Smith is treated as an OPPONENT WHO IS NOT TO BE TRUSTED.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "3 551\n",
              "\n",
              "(Collaborative partner manipulation)\n",
              "\n",
              "Mr. Smith has a long history of working COLLABORATIVELY with IS project teams with the goal of helping to identify and manage project risks, thus enabling project owners to be successful. He is seen as adding value to the process. Thus, Mr. Smith is treated as a TRUSTED PARTNER. Mr. Smith reports that he has found serious weaknesses in the design and execution of the testing activities on the data exchange with other IS. He estimates there is a 2/3 probability that exchange of data would show reliability problems in the ﬁrst month of operations. As a consequence, he reports that the project should be redirected and should not be continued as planned.\n",
              "\n",
              "thus embarrassing project owners. He is seen as policeman who does not add any value to the development process. Thus, Mr. Smith is treated as an OPPONENT WHO IS NOT TO BE TRUSTED.\n",
              "\n",
              "(Collaborative partner manipulation)\n",
              "\n",
              "Mr. Smith has a long history of working COLLABORATIVELY with IS project teams with the goal of helping to identify and manage project risks, thus enabling project owners to be successful. He is seen as adding value to the process. Thus, Mr. Smith is treated as a TRUSTED PARTNER. Mr. Smith reports that he has found serious weaknesses in the design and execution of the testing activities on the data exchange with other IS. He estimates there is a 2/3 probability that exchange of data would show reliability problems in the ﬁrst month of operations. As a consequence, he reports that the project should be redirected and should not be continued as planned. Taking into consideration that the interfaces with other IS are key for the success of the PENSION-VIEW project, you realize that:\n",
              "\n",
              "(Loss frame manipulation) Taking into consideration the business case of the PENSION-VIEW project, this would mean that: If you decide to CONTINUE this project as planned, there would be:\n",
              "● ●\n",
              "\n",
              "1/3 chance that the project will result in no LOSS compared with the business case 2/3 chance that the project will result in €60 million LOSS compared with the business case\n",
              "\n",
              "On the other hand, if you decide to REDIRECT this project, it will require an unplanned investment for additional testing and ﬁxing and will cause delays that carry ﬁnancial consequences. If you decide to REDIRECT, the project will result in a sure LOSS of €40 million compared with the business case.\n",
              "\n",
              "(High perceived control manipulation)\n",
              "\n",
              "(Gain frame manipulation) Taking into consideration the business case of the PENSION-VIEW project, this would mean that: If you decide to CONTINUE this project as planned, there would be:\n",
              "● ●\n",
              "\n",
              "1/3 chance that the project will result in a GAIN of €60 million 2/3 chance that the project will result in a GAIN of nothing\n",
              "\n",
              "Fortunately, all these IS are maintained and supported by your own organization. The owners of these ISs reside at your location and they directly report to you. The specialists on these information systems also reside at your location and are highly accessible to you. There are powerful controls in place for reporting and decision making. For all these reasons, you consider yourself to have a VERY HIGH LEVEL OF CONTROL over the outcome of this IS-project.\n",
              "\n",
              "On the other hand, if you decide to REDIRECT this project, it will require an unplanned investment for additional testing and ﬁxing and will cause delays that carry ﬁnancial consequences. If you decide to REDIRECT, the project will result in a sure GAIN of €20 million.\n",
              "\n",
              "(Low perceived control manipulation)\n",
              "\n",
              "Treatments (Experiment 2) (Opponent manipulation)\n",
              "\n",
              "Mr. Smith has a long history of working AGAINST IS project teams with the goal of exposing project failings,\n",
              "\n",
              "Unfortunately, all these information systems’ maintenance and support has been outsourced to an offshore location in China. The owners of these Information Systems are located at other departments at various locations. They do not report to you. The specialists on these information systems are located at the offshore location in China and are not at all accessible to you. There are no controls in place for reporting and decision making. For all these reasons, you consider yourself to have a VERY LOW LEVEL OF CONTROL over the outcome of this IS-project.\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n",
              "\n",
              "2 552\n",
              "\n",
              "Collaborative partner or opponent\n",
              "\n",
              "Arno Nuijten et al\n",
              "\n",
              "Measures Continue (used as the dependent variable for both Experiments 1 and 2)\n",
              "\n",
              "Risk Propensity (used as a control variable for both Experiment 1 and 2)\n",
              "\n",
              "Variable Variable Continue1 Item Wording (1 = Deﬁnitely Redirect; 8 = Deﬁnitely Continue) Indicate whether you would decide to continue the project as planned or redirect, and how strong your leaning would be (1 = Strongly Disagree; 7 = Strongly Agree) I will certainly continue the PENSION-VIEW project as planned (i.e., without redirection) RiskProp1\n",
              "\n",
              "Item Wording (Anchors: 1 = Extremely LESS likely than others; 7 = Extremely MORE likely than others) Your tendency to choose risky alternatives based on the assessment of other people on whom you must rely Your tendency to choose risky alternatives relying on an assessment that is high in technical complexity Your tendency to choose risky alternatives which could have major impact on the strategic direction of your organization Your tendency to choose risky alternatives despite considerable failures in risky choices you made in the past\n",
              "\n",
              "RiskProp2\n",
              "\n",
              "Continue2\n",
              "\n",
              "RiskProp3\n",
              "\n",
              "Message Relevance (used as a mediating variable for both Experiment 1 and 2)\n",
              "\n",
              "RiskProp4\n",
              "\n",
              "Variable MsgRelev1\n",
              "\n",
              "Item wording (Anchors: 1 = Strongly Disagree; 7 = Strongly Agree) The assessment of Mr. Smith was highly relevant in forming my decision to continue or redirect the PENSION-VIEW project The assessment of Mr. Smith was very important in forming my decision to continue or redirect the PENSION-VIEW project My decision was most inﬂuenced by the assessment of Mr. Smith\n",
              "\n",
              "Perceived Controlmc (used as a manipulation check for Experiment 2)\n",
              "\n",
              "MsgRelev2\n",
              "\n",
              "Variable PercContrmc1\n",
              "\n",
              "(Anchors) Item wording (1 = Very low; 7 = Very high) I consider the level of CONTROL I have over my PENSION-VIEW project to be (1 = Strongly disagree; 7 = Strongly agree) I believe I have a high level of CONTROL over my PENSION-VIEW project (0% = No control at all; 100% = Complete control) I rate the level of Control I have over my project as …\n",
              "\n",
              "MsgRelev3\n",
              "\n",
              "PercContrmc2 Risk Perception (used as a mediating variable for both Experiment 1 and 2)\n",
              "\n",
              "PercContrmc3\n",
              "\n",
              "Variable\n",
              "\n",
              "Anchors Item wording\n",
              "\n",
              "PercRisk1 (1 = Signiﬁcant Threat; 7 = Signiﬁcant (reversed) Opportunity) I characterize the current status of the project as PercRisk2 (1 = Potential for Loss; 7 = Potential for Gain) (reversed) I characterize the current status of the project as PercRisk3 (1 = Very Unlikely to succeed; 7 = Very Likely to (reversed) succeed) I characterize the current status of the project as PercRisk4 (1 = Unpromising; 7 = Promising) (reversed) I characterize the current status of the project as\n",
              "\n",
              "Partnermc (MRR; Used as a manipulation check for both Experiments 1 and 2)\n",
              "\n",
              "Variable Partnermc1\n",
              "\n",
              "(Anchors) Item Wording (1 = Strongly disagree; 7 = Strongly agree) I consider Mr. Smith to be a trusted partner to my PENSION-VIEW project (1 = Strongly disagree; 7 = Strongly agree) I consider Mr. Smith to be a collaborative partner to my PENSION-VIEW project (1 = Non-Trusted opponent; 7 = Trusted partner) I consider Mr. Smith to be a __________ to my PENSION-VIEW project\n",
              "\n",
              "Partnermc2\n",
              "\n",
              "Partnermc3\n",
              "\n",
              "European Journal of Information Systems\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "hJjNkHwqG8qz",
        "colab_type": "code",
        "outputId": "3b709fc3-9aef-4796-b950-2c58df56c114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc10.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message Relevance Perceived Risk Continue Continue\n",
              " \n",
              " Continue is moderated by Message Framing MRR Dotted lines refer to paths that were non-significant\n",
              " \n",
              " * significant at p < 0.05 level (one-tailed test),\n",
              " * significant at p < 0.01 (one-tailed test),\n",
              " ** significant at p < 0.001 (one-tailed test)\n",
              " \n",
              " Figure 3a\n",
              " \n",
              " Experiment 1 structural model results.\n",
              " ,\n",
              " MRR Continue is moderated by Perceived Control Dotted lines refer to paths that were non-significant\n",
              " \n",
              " * significant at p < 0.05 level (one-tailed test),\n",
              " * significant at p < 0.01 (one-tailed test),\n",
              " ** significant at p < 0.001 (one-tailed test)\n",
              " \n",
              " Figure 3b\n",
              " \n",
              " Experiment 2 structural model results.\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "metadata": {
        "id": "Kt5gQeat53BK",
        "colab_type": "code",
        "outputId": "7a29669b-8f68-4048-de30-362a0f8a0323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "cell_type": "code",
      "source": [
        "significant_Sents = [sent for sent in doc10.sents if 'significant' in sent.string]\n",
        "significant_Sents "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message Relevance Perceived Risk Continue Continue\n",
              " \n",
              " Continue is moderated by Message Framing MRR Dotted lines refer to paths that were non-significant\n",
              " \n",
              " * significant at p < 0.05 level (one-tailed test),\n",
              " * significant at p < 0.01 (one-tailed test),\n",
              " ** significant at p < 0.001 (one-tailed test)\n",
              " \n",
              " Figure 3a\n",
              " \n",
              " Experiment 1 structural model results.\n",
              " ,\n",
              " MRR Continue is moderated by Perceived Control Dotted lines refer to paths that were non-significant\n",
              " \n",
              " * significant at p < 0.05 level (one-tailed test),\n",
              " * significant at p < 0.01 (one-tailed test),\n",
              " ** significant at p < 0.001 (one-tailed test)\n",
              " \n",
              " Figure 3b\n",
              " \n",
              " Experiment 2 structural model results.\n",
              " ,\n",
              " *significant at P<0.05 level; **significant at P<0.01; ***significant at P<0.001 (one-tailed test).\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "metadata": {
        "id": "VAIvtDqF6yL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRfhvkLxGomA",
        "colab_type": "code",
        "outputId": "f691aedb-7161-435f-ed45-32aa8d7ddaf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10010
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc11 = nlp(open(u\"ParkS#KeilM#BockG#KimJ_2016_Winner's regret in online C2C Auctions - an automatic thinking perspective_Information Systems Journal_6.txt\").read())\n",
        "doc11\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doi: 10.1111/isj.12075 Info Systems J (2016) 26, 613–640 613\n",
              "\n",
              "Winner ’s regret in online C2C Auctions: an automatic thinking perspective\n",
              "Sang Cheol Park,* Mark Keil,† Gee-Woo Bock‡ & Jong Uk Kim‡\n",
              "*Department of Business Administration, Daegu University, Jillyang, Gyeongsan, Gyeongbuk, 712-714, Korea, email: sangch77@gmail.com, †John B. Zellars Professor of Computer Information Systems, J. Mack Robinson College of Business, Georgia State University, P.O. Box 4015, Atlanta, Georgia, 30302-4015, USA, email: mkeil@gsu.edu and ‡Sungkyunkwan University, Jongno-gu, Seoul 110-745, Korea, email: gwbock@skku.edu, jukim@skku.ac.kr\n",
              "\n",
              "Abstract. While human beings embody a unique ability for planned behaviour, they also often act automatically. In this study, we draw on the automatic thinking perspective as a meta-theoretic lens to explain why online auction bidders succumb to both trait impulsiveness and sunk cost, ultimately leading them to experience winner’s regret. Based on a survey of 301 online auction participants, we demonstrate that both trait impulsiveness as an emotional trigger and sunk cost as a cognitive trigger promote winner’s regret. By grounding our research model in the automatic thinking view, we provide an alternative meta-theoretical lens from which to view online bidder behaviour, thus bolstering our current understanding of winner’s regret. We also investigate the moderating effects of competition intensity on the relationships between the triggers of automatic thinking and winner’s regret. Our results show that both trait impulsiveness and sunk cost have signiﬁcant impacts on winner’s regret. We also found that the relationship between these two triggers and winner’s regret is moderated by competition intensity. Keywords: winner’s regret, automatic thinking, trait impulsiveness, sunk cost\n",
              "\n",
              "INTRODUCTION\n",
              "\n",
              "Online consumer-to-consumer (C2C) auction sites have become increasingly popular since the founding of eBay in 1995. Millions of consumers visit online auction sites, and these sites have become an important channel for acquiring goods. While consumers can sometimes save money by using online auction sites, participating in online auctions often comes at a price. For example, consumers may ﬁnd themselves spending more time than they would like to admit obsessively tracking the status of an auction. Worse yet, consumers may experience ‘auction fever’ and get so caught up with winning the auction that they end up experiencing ‘winner’s regret’ (Ku et al., 2005; Peters & Bodkin, 2007), deﬁned here as winning the auction but with the subjective emotional assessment of having overpaid for the item. Prior research on auctions has often referred to winner’s regret as the winner’s curse (Foreman & Murhighan, 1996; Amyx & Luehlﬁng, 2006;\n",
              "© 2015 Blackwell Publishing Ltd\n",
              "\n",
              "\n",
              "614\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Malhotra, 2010; Adam et al., 2011). In this paper, we use the term winner’s regret rather than winner’s curse because winner’s curse usually implies that the winning bidder pays more than an auction item is worth. The term winner’s regret does not carry this connotation and is deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). In order to gain a better understanding of why winner’s regret occurs in online auctions, we introduce the perspective of automatic thinking as a meta-theoretical frame. For a long time, economists have maintained that human behaviour is best described by the rational economic model, which basically holds that human beings are self-interested and capable of perfectly weighing the costs and beneﬁts in every decision, thus enabling optimal choices (Ariely, 2008). Although human beings do, in fact, frequently make rational decisions, this does not necessarily mean that they do this all, or even most, of the time. In line with this argument, psychologists distinguish between two modes of thinking, one that is intuitive and automatic, and another that is reﬂective and rational (Thaler & Sunstein, 2009; Kahneman, 2011). Thaler & Sunstein (2009) refer to the ﬁrst mode as automatic thinking. Automatic thinking provides a useful perspective for understanding the problem of winner’s regret, because automatic thinking is rapid and instinctual (Bazerman & Moore, 2009). For example, when people get nervous during a ﬂight that experiences turbulence or smile when they see a cute baby, they are using automatic thinking. In the online auction context, automatic thinking may help to explain why individuals experience auction fever and get so caught up in the auction process (Heyman et al., 2004; Ku et al., 2005). Despite often-voiced concerns regarding the pitfalls of auction fever, the problem of winner’s regret and why it occurs has not been examined from the perspective of automatic thinking. There are two types of triggers that result in automatic thinking: emotional triggers (Strack & Deutsch, 2004; Slovic et al., 2007; Hofmann et al., 2009) and cognitive triggers (Sloman, 1996; Kahneman, 2003; Klaczynski & Cottrill, 2004; Hofmann et al., 2009). Therefore, we believe that it is important to consider both emotional and cognitive triggers of automatic thinking in order to obtain a more complete understanding why winner’s regret occurs. In this study, we thus explore two factors related to the automatic thinking that may inﬂuence winner’s regret; trait impulsiveness, which is believed to be more emotional in nature, and sunk cost, which is believed to be more cognitive in nature. We chose these two factors because they are known to inﬂuence behaviour in related contexts. For example, escalation of commitment scholars have long suspected that sunk cost can create the kind of loss framing that is believed to promote escalation behaviour, and marketing researchers have pointed to impulsiveness as a trait that can inﬂuence retail buying behaviour. In addition to these two factors, competition intensity has been shown to affect bidding behaviour in online auctions, and we include it in our study to determine if it moderates the relationship between these automatic thinking triggers and winner’s regret. In summary, our aim is to better understand winner’s regret by considering both emotional and cognitive triggers of automatic thinking and the role of competition intensity in this context. In doing so, we seek to answer two research questions: 1 To what extent do emotional and cognitive triggers of automatic thinking help us to predict winner’s regret?\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "615\n",
              "\n",
              "2 To what extent is the relationship between these automatic thinking triggers and winner’s regret moderated by competition intensity? While prior research has examined the impact of certain escalation drivers such as sunk cost on willingness to continue bidding (Park et al., 2012) and how this can result in overbidding behaviour, we know of no research that has examined the effect of both cognitive and emotional triggers on winner’s regret.1 Thus, by addressing the previous research questions, we contribute to the current body of knowledge regarding individuals’ behaviour in online auctions. From the standpoint of theoretical contribution, ours is the ﬁrst study to draw on the lens of automatic thinking to examine two kinds of triggers (one cognitive and one emotional) that may contribute to winner’s regret. Speciﬁcally, by investigating the impact of sunk cost and impulsiveness as well as the moderating role of competition intensity, we shed new light on the phenomenon of winner’s regret in online auctions.\n",
              "\n",
              "B AC K G R O U N D\n",
              "\n",
              "Online auctions and winner’s regret Online auctions are conducted over the internet and differ from traditional auctions in some important respects. First, online auctions remove the geographical constraints of traditional auctions, thus enabling worldwide participation (Ariely & Simonson, 2003). Second, online auctions can last for several days and can allow for asynchronous bidding, which makes them more ﬂexible than traditional auctions and easier for people to participate in. While online auctions are attractive in several respects, prior research has documented a number of problems associated with participating in them. These include psychological distress (i.e. anxiety, aggression, anger and depression), habitual usage, negative impacts on ﬁnances or social relations and dependency and withdrawal symptoms (Peters & Bodkin, 2007). One problem, which is the focus of our research, is winner’s regret, deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). The emotion of regret stems from the comparison of an actual outcome with a better outcome that might have resulted. In an online auction context, individuals may experience regret when they compare the actual price that they paid with their reservation price (i.e. the highest price a buyer is willing to pay). While consumers may participate in online auctions to obtain a bargain, the reality is that in many instances they end up either overpaying for what they purchase or experience regret associated with the subjective emotional assessment of having overpaid for an item. Based on an analysis of 500 online auctions for compact discs and digital video discs, Ariely & Simonson\n",
              "1\n",
              "\n",
              "Overbidding generally refers to situations in which individuals bid beyond their reservation price. Thus, overbidding need not necessarily result in winning an auction or in winner’s regret, because many auction participants may engage in overbidding but only one person will win the auction. The distinction is important because winner’s regret could inﬂuence an individual’s willingness to use online auctions in the future.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "616\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "(2003) reported that 98% of all winning bidders overpaid. Overpayment can be reduced by providing bidders with an easy means of checking the retail prices of goods. Based on a sample of 416 online auctions, Amyx & Luehlﬁng (2006) found that only 8.7% of the winning bidders overpaid when they were using auction sites that provided links to websites that allowed bidders to check the reference price of identical retail merchandise available for sale at the same website. Naturally, when a bidder believes that he or she has paid too much for an item of uncertain value (Ku et al., 2005; Robert et al., 2011), regret can occur. A rational explanation for such regret can be formulated based on three assumptions: (1) while the average bidder may accurately estimate the value of the item up for sale in an auction, some bidders will underestimate this value and others will overestimate it; (2) the bidder who most greatly overestimates the value of the item will typically win the auction; and (3) the amount of overestimation will often be greater than the difference between the winning bidder’s estimate of the value of the item and what s/he ultimately had to bid in order to acquire it (Amyx & Luehlﬁng, 2006). Under this view, regret occurs as a result of uncertainty regarding an object’s value, and thus, bidders can be economically rational and still suffer regret when their information is poor. Empirical evidence suggests that regret can arise even when bidders have perfect information (Ariely & Simonson, 2003; Amyx & Luehlﬁng, 2006), and thus, the behaviour that leads up to this can be viewed as irrational. Oh (2002) examined regret in consumer-to-consumer auctions and concluded that bidders do not necessarily behave in an economically rational way. Speciﬁcally, they tend to bid on items for the sheer enjoyment that is intrinsic in an online competition, rather than on the basis of utility in pure monetary terms. Furthermore, online auctions can produce an emotionally charged climate in which individuals try to outbid one another in the hopes of acquiring a product (Turel et al., 2011). Given that competition intensity can be high in some auctions and that things typically become more intense as the auction nears completion, the longer an individual remains engaged in the bidding process the more likely it is that s/he will experience strong emotions (Adam et al., 2011). Those who experience auction fever may ﬁnd that they have little control over their bidding and buying behaviour, ultimately spending more than they anticipated and experiencing negative feelings such as regret as a result. Thus, whether bidding behaviour is seen as rational or irrational, and whether overpayment is real (in an objective sense relative to a reference price that represents an item’s actual worth) or not the subjective emotional assessment of having overpaid for an item is a problem that is relevant for both research and practice, as winner’s regret can cause customer dissatisfaction (Amyx & Luehlﬁng, 2006). Customers who are dissatisﬁed with their online auction experience are less likely to return to the auction site, and this can be damaging to online auction service providers. Unfortunately, little is known about the factors that lead to winner’s regret, deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). Park et al. (2012) investigated how key escalation drivers (e.g. completion effect, selfjustiﬁcation and sunk cost) affect an individual’s willingness to continue bidding, which in turn leads to overbidding behaviour (i.e. bidding in excess of one’s reservation price regardless of whether or not one wins the auction). Their study dealt with overbidding behaviour and did not consider examine winner’s regret per se. Moreover, they did not examine any emotional\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "617\n",
              "\n",
              "triggers that might be associated with this phenomenon. In this paper, we explore the problem of winner’s regret using the automatic thinking perspective as a meta-theoretical lens. Automatic thinking Automatic thinking is one of two information processing approaches that guide human perception, memory, decision and attention. Automatic thinking involves rapid and parallel processing and requires little effort. Schneider & Shiffrin (1977) distinguish automatic thinking from reﬂective thinking, which is much slower. Automatic thinking does not require higher-order mental operations, which include executive functions such as making deliberate judgments and evaluations (Strack & Deutsch, 2004). In other words, automatic thinking tends to be involuntary and requires no attention, whereas reﬂective thinking is voluntary and requires attention. Moors & De Houwer (2006) also reviewed the characteristics that distinguish automatic thinking from reﬂective thinking, concluding that one of the most important distinctions between the two is the degree to which actions are subject to conscious control. When peoples’ activities are automatic, they tend to be more likely to occur autonomously, i.e. they appear to occur on their own in the absence of central control. A third characteristic of automatic thinking is its inherent attentional efﬁciency. Generally speaking, activities associated with automatic thinking occur with a minimum of attentional capacity, which leaves more capacity for the performance of other tasks. Finally, automatic thinking can be quite difﬁcult to stop or modify, because it involves relatively little in the way of conscious monitoring. In the context of online auctions, automatic thinking may explain why individuals are prone to overpayment (either real or perceived) and to experience regret as a result. Ariely & Simonson (2003) demonstrated previously that overpayment in online auctions can be conceptualised as a form of automatic thinking, in which bidders lose their self-control and get caught up in the bidding process. The key features of automatic thinking and how they apply to online auction behaviour are shown in Table 1. Both cognitive and emotional factors can trigger automatic thinking. While cognitive factors often connote reﬂective thinking, this need not necessarily be the case. In other words, there may be aspects of cognition that remain somewhat opaque to reﬂective processes or that occur with such frequency that they become automatic over time. An example of the latter would be when driving a car and we come to a red light, we automatically know to stop and we engage the brake on the automobile. Clearly, there is cognition taking place in this action, but it is not something that we consciously think about unless we are a new driver. This example illustrates how something that at one time required reﬂection can become automatic with sufﬁcient practice. There\n",
              "Table 1. Applications of automatic thinking processes to online bidding behaviour Automatic decision process Unreﬂective Effortless Fast Application of the automatic process to online bidding behaviour Bidders may not control their bidding behaviour during the bidding stage. Bidders tend to automatically make bids without making the effort to compare auction price against retail price. Bidders tend to make decisions more quickly.\n",
              "\n",
              "Source: Adapted based on Thaler & Sunstein (2009) applied to our study context.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "618\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "are also instances in which cognitive factors inﬂuence our decision-making without our being fully aware of their impact. One example of this is sunk cost. Arkes & Blumer (1985) showed that when individuals invest in season tickets, they attend more shows. Presumably, this is because they have incurred sunk cost in buying the season tickets. However, if you were to ask these individuals why they chose to attend more shows, they might not even be aware that sunk cost was a factor inﬂuencing their attendance decisions. In this sense, sunk cost can be viewed as a cognitive factor that may operate at a subconscious level, resulting in automatic thinking. While sunk cost should not inﬂuence decisions from a rational economic perspective, numerous studies (e.g. Arkes & Blumer, 1985) have found that individuals ﬁnd it difﬁcult to ignore sunk cost when they make decisions. Moreover, the concept of sunk cost is not limited to ﬁnancial investments but also extends to investments of time and effort. Consistent with prospect theory, the effect of sunk cost on decision-making is believed to arise from the manner in which decisions are framed (Kahneman & Tversky, 1979; Tversky & Kahneman, 1981). Speciﬁcally, prior research has shown that individuals are more risk-seeking when decisions are framed as a choice between losses, and sunk cost can induce such a framing. Essentially, sunk costs evoke a loss framing, and this triggers risk-seeking behaviour in accordance with prospect theory. In the online auction context, ‘previous bids and/or time invested in the auction represent sunk costs’ (Ku et al., 2005, p. 92). Consistent with Park et al. (2012), we posit that individuals perceive sunk costs as losses that can only be recouped if the individual wins the auction. It is also possible that sunk cost (in the form of time and effort) may lead to the endowment effect, which refers to the fact that people tend to ascribe more value to things merely because they own them. Research on the endowment effect has shown that owners often value an item at more than twice the level that an average buyer is willing to pay (Thaler, 1980; Kahneman et al., 1990). Carmon & Ariely (2000) suggest that this disparity can be interpreted as form of loss aversion on the part of owners. Speciﬁcally, when one owns an item, giving it up is viewed as a loss, and it is well known that individuals exhibit loss aversion (Carmon & Ariely, 2000; Looney & Hardin, 2009; Hardin & Looney, 2012). In an online auction context, sunk costs associated with the bidding process may cause bidders to become so attached to what they are bidding on they begin to develop a sense of ownership over the item. If this occurs, the endowment effect may result in bidders’ overestimating the value of the item just as owners tend to overvalue their possessions. Indeed, prior research on online auctions has offered some empirical support for product attachment as a cause of overpayment (e.g. Carmon & Ariely, 2000; Ariely & Simonson, 2003). At some point, the idea of ‘losing’ the item by not winning the auction creates behaviour that is consistent with loss aversion. Loss aversion can be explained by the value function of prospect theory and the fact that individuals weigh losses roughly two and a half times greater than equivalent gains (Looney & Hardin, 2009). In sum, sunk costs (in the form of time and effort) may produce a loss framing or create an endowment effect that leads to loss aversion, either of which could theoretically result in bidding behaviour that can result in winner’s regret. Regardless of the exact mechanism through which sunk cost inﬂuences bidding behaviour, we suggest that it serves as cognitive trigger for automatic thinking in the online auction context and that it may lead to winner’s regret. In addition to cognitive factors, emotional factors can also trigger automatic thinking. Prior research suggests that people often make snap decisions based on their emotional state (Ariely &\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "619\n",
              "\n",
              "Simonson, 2003; Amyx & Luehlﬁng, 2006). Thus, emotions can inﬂuence an individual’s decision-making without the individual even being aware of the impact. For example, the angrier one feels, ‘the more one perceives others as responsible for a negative event’ (Lerner & Tiedens, 2006, p. 118). Indeed, numerous studies have demonstrated that strong emotions can have a signiﬁcant inﬂuence on decision-making behaviour (see, for example, Andrade & Ariely, 2009). In this research, we posit that emotional as well as cognitive factors can inﬂuence online bidding behaviour. One emotional factor that can trigger automatic thinking in the online auction context is trait impulsiveness. Trait impulsiveness involves a tendency to act on a whim, displaying behaviour characterised by little or no forethought, reﬂection or consideration of consequences (VandenBos, 2007). Trait impulsiveness has been linked to automatic thinking (Hofmann et al., 2009), and in the marketing literature, it has been shown to inﬂuence buying behaviour (Rook, 1987; Rook & Fisher, 1995). Individuals who are impulsive are more likely to acquire products when presented with the opportunity (Rook & Fisher, 1995). Presumably, this is because trait impulsiveness promotes automatic thinking, causing individuals to make decisions without consciously weighing costs and beneﬁts. An example of such impulsiveness is when an individual enters a supermarket with a list of groceries to buy and encounters a display case of candy bars in a prominently placed location near the checkout. Without necessarily weighing the costs and beneﬁts associated with the purchase, individuals may be inclined to make an impulse purchase and buy themselves a treat that they had not intended to buy. When this occurs, it is often because the individual has an emotional reaction to seeing the product. In this sense, impulsiveness can be viewed as an emotional factor that operates at a subconscious level, resulting in automatic thinking. In this research, we investigate trait impulsiveness as an emotional trigger for automatic thinking in the online auction context, which may lead to winner’s regret. While the impact of trait impulsiveness has not been explored in the online auction context, prior research does suggest that emotional factors (e.g. competitive arousal) can play a role in bidding behaviour (Ku et al., 2005). Although prior research has suggested that both cognitive and emotional factors may affect online bidding behaviour, the combined effect of these two types of factors has not been examined within the conﬁnes of a single study. Table 2 provides a representative list of 11 studies that have sought to identify cognitive or emotional factors that affect online auction outcomes. The list is not intended to be exhaustive, but it contains the major studies that have been published in this area, and we believe it is representative of online auction studies that have focused particular attention on bidding behaviour. As shown in Table 2, the studies that have been conducted to date invariably focus on either the cognitive aspects of why bidders decide to make bids or the emotional aspects that drive bidding behaviour. In order to better understand bidding behaviour in online auctions, however, we believe that it is important to consider both cognitive and emotional factors in researching this phenomenon, and this is the approach taken here.\n",
              "\n",
              "R E S E A R C H M O D E L A N D H Y P OT H E S E S\n",
              "\n",
              "Drawing on the meta-theoretical perspective of automatic thinking, we explore sunk cost as a cognitive trigger and trait impulsiveness as an emotional trigger that can lead to winner’s regret. Figure 1 illustrates our proposed research model. As shown in the model, both sunk cost and\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Table 2. Previous studies on bidder’s behaviours (chronological order) Variables\n",
              "\n",
              "620\n",
              "\n",
              "Focus\n",
              "\n",
              "Authors Cognitive Past auction experience Likelihood of a bidder bidding in the ﬁnal moments of an auction Likelihood of herding bias\n",
              "\n",
              "Bidding behaviour from emotional or cognitive perspective Independent variable(s) Dependent variable(s) Key contribution\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Wilcox (2000)\n",
              "\n",
              "Dholakia et al. (2002)\n",
              "\n",
              "Emotional\n",
              "\n",
              "More experienced bidders are more likely to bid according to theoretical predictions Examined herding bias in the psychological processes of bidders in online auctions\n",
              "\n",
              "Oh (2002)\n",
              "\n",
              "Emotional\n",
              "\n",
              "Auction attributes (volume of available listings, and posted reservation price) and agent attributes (buyer and seller experience) Product value, retail price dispersion and auction type Winner’s curse\n",
              "\n",
              "Examined the differences between C2C and B2C auctions by comparing the likelihood and magnitude of the winner’s curse\n",
              "\n",
              "Stafford & Stern (2002)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "Afﬁnity with the computer, intention to use, ease of use, perceived usefulness and involvement\n",
              "\n",
              "Bid/did not bid\n",
              "\n",
              "Gilkeson & Reynolds (2003)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "Opening price, number of unexpected bids, bidder experience, seller reputation and auction length Time of entry, time of exit and number of bids Reserve price value, research price disclosure and bidding history\n",
              "\n",
              "Auction success and ﬁnal closing price\n",
              "\n",
              "Examined consumer bidding behaviour on online auction sites by illuminating three different theories such as technology acceptance model, the afﬁnity theory and the involvement theory Investigated how the characteristics of online auctions impact outcomes such as auction success and ﬁnal closing price Bidding strategy properties\n",
              "\n",
              "Bapna et al. (2004)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640 Cognitive Auction interests\n",
              "\n",
              "Wally & Fortin (2005)\n",
              "\n",
              "Identiﬁed heterogeneous bidder strategies and the implications for auction design Found that reserve price, reserve disclosure and the initial bidding process had\n",
              "\n",
              "(Continues)\n",
              "\n",
              "\n",
              "Table 2. (Continued)\n",
              "Variables\n",
              "\n",
              "Focus\n",
              "\n",
              "Authors\n",
              "\n",
              "Bidding behaviour from emotional or cognitive perspective Independent variable(s) Dependent variable(s)\n",
              "\n",
              "Key contribution\n",
              "\n",
              "Peters & Bodkin (2007) Problematic online auction behaviours Price\n",
              "\n",
              "Emotional\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640 Emotional Compulsive consumption, compulsive gambling and internet addiction Trait competitiveness, impulse buying, hedonic need and strategic exit (mediator) Cognitive Completion effect, selfjustiﬁcation, sunk cost and willingness to continue bidding (as a mediator) Overbidding behaviour signiﬁcant effects on a bidder’s decision-making process. A combination of these three factors inﬂuences a bidder’s auction interest Explored bidders’ behaviour that could lead to an online auction addiction Investigated the effects of impulsive-buying tendencies, trait competitiveness and hedonic need fulﬁlment of strategic exit, and moderating role that hedonic need fulﬁlment on the relationship between impulse-buying tendencies and strategic exit Examined overbidding behaviour, deﬁned as making a bid that exceeded one’s reservation price, by drawing on escalation theory\n",
              "\n",
              "Angst et al. (2008)\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "Park et al. (2012)\n",
              "\n",
              "621\n",
              "\n",
              "\n",
              "622\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "impulsiveness are posited to have direct effects on winner’s regret. Both of these relationships, however, are posited to be moderated by competition intensity, which serves as a situational moderator. Trait impulsiveness is the tendency to act on a whim, with little or no planning or reﬂection. This construct has been studied extensively by both marketing researchers and clinical psychologists. For example, in the marketing area, Rook & Fisher (1995, p. 306) conceptualised impulsiveness as a consumer trait and deﬁned it as the tendency to buy ‘spontaneously, unreﬂectively, immediately, and kinetically’. In the context of retail shopping, individuals who rated higher on trait impulsiveness have been found to be more likely to experience powerful and persistent urges to buy something immediately and to act on these urges (Beatty & Ferrell, 1997). Marketing researchers agree that impulsive buying involves an instant-gratiﬁcation component (Rook & Fisher, 1995; Hausman, 2000) and that such behaviour occurs in the spur of the moment (Angst et al., 2008). Just as trait impulsiveness can inﬂuence purchase behaviour in retail settings, we believe that it can inﬂuence behaviour in an online auction context. Specifically, we posit that impulsive individuals will be more likely to repeatedly engage in ‘spur of the moment’ bidding decisions, making them more likely to bid past their reservation price without really thinking about the consequences of their behaviour. Further, evidence from clinical psychology research suggests that trait impulsiveness is associated with an inability to suppress emotional urges. For example, Doran et al. (2004) examined that the inﬂuence of trait impulsiveness on the smokers’ ability to maintain abstinence following a 1 day smoking cessation workshop. They found that higher levels of trait impulsiveness were predictive of a more rapid return to smoking following 48 h of nicotine abstinence. In line with this, Mitchell (1999) also found that smokers with high trait impulsiveness have greater difﬁculty inhibiting smoking than other smokers. Based on the preceding text, it appears that individuals with high trait impulsiveness have difﬁculty inhibiting their emotional urges and are thus more likely to experience ‘auction fever’. In other words, individuals who are impulsive are more likely to get emotionally caught up in the dynamics of the bidding process. Thus, we suspect that individuals who have high trait impulsiveness are more likely to continue bidding without careful consideration of whether their bid\n",
              "\n",
              "Figure 1. Research model. © 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "623\n",
              "\n",
              "exceeds their reservation price, ultimately leading to a greater likelihood of experiencing winner’s regret. Based on the preceding text, we hypothesise the following:\n",
              "\n",
              "H1: Trait impulsiveness will be positively related to winner’s regret.\n",
              "The sunk cost effect, which has been explained from a prospect theory perspective (Whyte, 1986), occurs when an individual’s decision-making is inﬂuenced by prior investments of time, effort or money that are not recoverable. In the online auction context, ‘previous bids and/or time invested in the auction represent sunk costs’ (Ku et al., 2005, p. 92). As the auction progresses, individuals will tend to perceive sunk cost to be greater as their investment of time and effort becomes higher. Consistent with Park et al. (2012), we posit that sunk costs are perceived as losses that can only be recouped if the individual continues to participate in the auction. Thus, in an illusory attempt to recover their sunk cost, individuals may actually bid beyond their reservation price, ultimately leading to winner’s regret should they win the auction. As noted earlier, it is also possible that sunk cost (in the form of time and effort) may lead to the endowment effect and that this results in overestimation of value, leading to overbidding and subsequently winner’s regret. Both theoretical mechanisms suggest the following hypothesis:\n",
              "\n",
              "H2: Sunk cost will be positively related to winner’s regret.\n",
              "In the online auction context, individuals are faced with the need to make rapid decisions and are often unable to reﬂect and conduct research that would guide their bidding behaviour. As a result, people often imitate others (Bonabeau, 2004). Such imitative behaviour can lead to the formation of informational cascades (Bikhchandani et al., 1992). Informational cascades occur when individuals follow the previous behaviour of others and disregard their own information. Previous studies on informational cascades have highlighted the importance of social inﬂuence in decision-making. On the basis of prior research (Gilkeson & Reynolds, 2003; Johns & Zaichkowsky, 2003; Ku et al., 2005), it is reasonable to assume that decision dynamics can be impacted by competition intensity. Park et al. (2012) found that the strength of the relationship between bidders’ willingness to continue bidding and overbidding behaviour was greater when competition intensity was higher. Ariely & Simonson (2003) suggest that most auction participants perceive other bidders as ‘competitors’ and associate auction outcomes with ‘winning’ and ‘losing’. Ku et al. (2005) suggest that competition intensity produces ‘competitive arousal’, an emotional state that causes individuals to shift from a motivation to acquire a product for a reasonable price to a motivation to win the auction at any cost (Ku et al., 2005). Their competitive arousal model places special emphasis on two antecedents of competitive arousal: heightened perceptions of rivalry and increasing time pressure (which characteristically occurs as an auction nears completion). Based on the preceding text, we expect there to be an interaction between competition intensity and trait impulsiveness such that a more intense competitive environment can stimulate those with high trait impulsiveness to be even more likely to bid past their reservation price, thus increasing the chances that they will experience winner’s regret. Thus, we offer the following hypothesis:\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "624\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "H3: The strength of the relationship between impulsiveness and winner’s regret will be greater when competition intensity is high.\n",
              "In the online auction context, bidders invest their time and effort. These investments represent sunk costs and can only be recovered if the auction is eventually won. Ku et al. (2005) reported that high sunk costs lead to increased self-reported levels of arousal and higher bidding activity. They argued that if bidding itself is arousing, this can ‘feed a vicious cycle of bidding’. In line with this, Park et al. (2012) found that auction participants’ willingness to continue to bidding is inﬂuenced by sunk cost. Interestingly, their results also suggest that competition intensity moderates the relationship between sunk cost and willingness to continue bidding, such that the effect of sunk cost is weakened under conditions of high competition intensity. They also reported that sunk cost had a direct effect on overbidding when competition intensity was low, but not when competition intensity was high. Together, these results indicate that competition intensity can play an important moderating role in this context, weakening the effect of sunk cost. Moon (2001) suggests that the effect of sunk cost differs depending on whether an individual is focused on the past or the future, and this provides a possible theoretical explanation as to why the sunk cost effect may be weakened when competition intensity is high. Speciﬁcally, the effect of sunk cost is theorised to be less potent when an individual is thinking about the future as opposed to the past. As Moon (2001) suggests, sunk cost causes decision-makers to think about the past, which leads them to try and recover monies already spent by continuing a previously chosen course of action. However, as competition intensity increases, we theorise that individuals become more absorbed in the auction dynamics and begin to envision a future state in which the auction is over and they also begin to realise that they may or may not win the auction. Moreover, as competition intensity increases, individuals are more likely to experience the effects of competitive arousal, which is an adrenaline-laden emotional state that can arise during highly competitive bidding (Ku et al., 2005). According to the competitive arousal model of Ku et al. (2005), there are two key antecedents of competitive arousal: heightened rivalry and increasing time pressure. These conditions can create an environment in which the desire to win the auction becomes so strong that it can lead to dysfunctional behaviour (Malhotra, 2010). We propose that when competition intensity is high, the combined effects of heightened rivalry and time pressure are likely to overshadow any effects of previous investments in time and effort (i.e. sunk cost). If competition intensity is high enough, competitive arousal could even create an environment in which bidders are willing to pay more than item is worth just to deprive other bidders from obtaining the item. If this occurs, the gratiﬁcation that comes from winning the auction at any cost may diminish the negative feelings that would normally be associated with winner’s regret. Thus, the relationship between sunk cost and winner’s regret may be weakened under conditions of high competition intensity. Thus, we expect that high competition intensity causes individuals not to think about the past but rather to focus on the future, and can even lead to situations where there is gratiﬁcation associated with depriving others from winning the auction, thereby weakening the effect of sunk cost on winner’s regret. Therefore, we expect that under conditions of high competition intensity, the effect of sunk cost on bidding behaviour is weakened and the predictive value of sunk cost on winner’s regret is reduced. Based on this logic, we advance the following hypothesis:\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "625\n",
              "\n",
              "H4: The strength of the relationship between sunk cost and winner’s regret will be greater when competition intensity is low.\n",
              "\n",
              "METHODOLOGY\n",
              "\n",
              "Research approach and construct operationalisation We employed a survey approach in order to test our research model (see Appendix A for our measures, which were all based on self-reports). Impulsiveness was operationalised using a four-item scale (IMP1–IMP4) adapted and modiﬁed from Rook & Fisher (1995), which was designed to assess the extent to which an individual acts spontaneously without thinking of the consequences. Sunk cost was operationalised using a three-item scale (SC1–SC3) adapted from Park et al. (2012), which captured the extent to which the bidder perceived that it would be difﬁcult to stop bidding due to prior investment of time or effort in the auction process. Competition intensity was operationalised using a three-item scale from Park et al. (2012). These measures were designed to tap into the number of people competing in the auction and how ﬁerce the competition was perceived to be (CI1–CI3). Our dependent variable, winner’s regret, was assessed using a single-item measure designed to capture regret associated with the subjective emotional assessment of having overpaid for an item. Single-item scales are frequently seen as being less reliable than multiitem scales, but ‘as far as internal consistency reliability is concerned, there is substantial evidence indicating acceptable reliability values for single-item scales’ (Fuchs & Diamantopoulos, 2009, p. 201). Single-item scales also tend to raise concerns regarding the assessment of convergent and discriminant validity, but again, the available evidence suggests that ‘single-item measures can be both reliable and valid’ (Wanous et al., 1997; Robins et al., 2001; Wanous & Hudy, 2001; Fuchs & Diamantopoulos, 2009, p. 203). The issue of whether or not to use a single-item measure depends on a variety of factors, but the nature of the construct is certainly an important consideration (Fuchs & Diamantopoulos, 2009; Petrescu, 2013). Petrescu (2013) notes that single-item measures can be used to assess concepts that are simple and easy to understand. This includes not only concrete concepts, such as sales or expenditures, but also behavioural constructs, such as repeat purchase intention. Fuchs & Diamantopoulos (2009) suggest that it is reasonable to employ single-item measures for unidimensional constructs where there is broad agreement as to what the construct means (e.g. favorability, price perception and buying intention). In our case, the winner’s regret construct was unidimensional and easy to understand, which made it possible to assess the construct with a single measurement item (‘After purchasing the item, I regretted that I had overpaid’). What makes the construct unidimensional is that it can be measured using a single ‘ruler’ that goes from low to high. While one could argue that the general concept of regret may have multiple dimensions (i.e. perhaps one may regret something for all kinds of different reasons), we are not attempting to capture the general concept of regret. Instead, we are focusing in on and measuring a very speciﬁc kind of regret (i.e. regret associated with the subjective\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "626\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "emotional assessment of having overpaid for an item). By restricting the concept in this way, we are able to treat it as a unidimensional construct. In addition to the previous constructs, we controlled for four variables that might inﬂuence bidding and perceptions in our research setting: (1) opening price on the focal item, (2) the total number of bids placed on the focal item, (3) maximum bidding price submitted by the bidder and (4) gender.\n",
              "\n",
              "Instrument validation and data collection An initial version of the questionnaire was developed with the idea that each subject would be asked to respond based on his or her most recent online auction experience, answering questions about regret after winning an auction. Four bilingual individuals with domain expertise in online auctions and experience with survey design provided feedback that was used to reﬁne the questionnaire. The survey was developed in English and translated into Korean by two individuals who were ﬂuent in both English and Korean. Two other individuals who were also ﬂuent in both English and Korean performed a backward translation to ensure consistency between the Korean version of the measurement items and the original English version. Minor adjustments were then made to eliminate any translation-related differences and to ensure that the meaning was equivalent. The questionnaire was then pilot tested with 196 undergraduate students, allowing us to check the psychometric properties of the scales (Straub et al., 2004). Convergent validity of each scale was assessed using a principal components factor analysis. A separate principal components factor analysis was run for each of the constructs. A single eigenvalue above 1 for each construct veriﬁed that the construct was unidimensional, hence, providing evidence of convergent validity for each scale. An exploratory factor analysis with all constructs revealed a clean factor structure and exhibited item-to-construct loadings that exceeded the desired threshold of 0.5. Cronbach’s alpha was used to assess the reliability of our measures in the pilot test, and all scales were judged to exceed the normal threshold of 0.7 for reliability (Hair et al., 1998). Subsequent to the pilot test, we administered a Web-based survey that targeted individuals who had participated in online auctions using one of Korea’s leading online auction sites. We contracted with a market research ﬁrm, which agreed to administer the survey to Koreans with prior experience using online auctions to acquire goods. Our aim was to obtain a representative sample of auction users that included participants from different age ranges. We instructed the market research ﬁrm to obtain 500 responses and to restrict the sample to those who indicated they had some actual experience of buying products in online auctions. Participants were asked to recall an online auction that they had participated in within the last month in which they had actually won the auction and to complete the survey based on that auction experience. Five dollars of cyber-money was provided to each survey recipient as an incentive to complete the survey. A total of 500 responses were obtained, but some had to be dropped because they were not fully completed, leaving us with 479 completed surveys. Because we were interested in studying winner’s regret in online auctions, we restricted our analysis to survey respondents who indicated that they felt regret after purchasing their item. A total of 301 survey respondents met this threshold and these cases were retained for further analysis. Of the 301 usable responses, 70 respondents purchased clothes (23.3%), 88 respondents purchased consumer electronics\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "627\n",
              "\n",
              "(e.g. digital camera, MP3 players and used computers) (29.2%) and 143 respondents (47.5%) purchased miscellaneous goods such as watch, shoes and wallets. Our survey respondents reported using the following online auction sites the most – Auction (http://www.auction.co.kr) (82%), G-market (http://www.gmarket.co.kr) (16%) and Onket (http://www.onket.com) (2%) – as these are the most popular online auction sites in Korea. All survey items for the constructs in our study were measured on a seven-point Likert scale, which ranged from strongly disagree (1) to strongly agree (7). Our data were analysed using partial least squares and regression using SmartPLS 2.0 (Ringle et al., 2005) and the SPSS 18.0, respectively. We used PLS for our analysis as it (1) enabled us to estimate the measurement model and the structural model simultaneously, (2) is suitable for exploratory models and (3) has fewer distributional assumptions (Gefen & Straub, 2005). We chose PLS over covariance-based Structural Equation Modeling (SEM) also because our emphasis is on prediction rather than model ﬁt.\n",
              "\n",
              "DATA A N A LY S I S A N D R E S U LT S\n",
              "\n",
              "Descriptive analysis Table 3 shows the demographic proﬁle of our respondents. 67.4% of our respondents were male and 32.6% were female. Most respondents (75.1%) were in the 21–40 years age group. Most respondents (46.2%) placed bids between one and three times and 27.2% of total respondents placed bids between four and six times. We employed two methods to test for common method bias (CMB) as it represents a potential threat to validity given our study design. First, we used Harman’s one-factor test (Podsakoff & Organ, 1986). According to Podsakoff & Organ (1986), if a single factor emerges from the factor analysis, this may be indicative of a serious CMB threat. In order to conduct this test, we entered all 11 measurement items into a principal component analysis and examined the results of the unrotated factor solution. Four factors were extracted, accounting for 26.39%, 24.38%, 24.21% and 8.90% of the variance, respectively. This result suggests that CMB was not a signiﬁcant threat in our study. Second, we conducted marker variable analysis per Lindell & Whitney (2001) in which unrelated constructs (termed marker variables) are used to adjust the correlations among the principle constructs. We identiﬁed two unrelated constructs (opening price and perceived ease of use), which were assessed as part of the survey. High correlations among any of the items of the study’s principal constructs and unrelated constructs would indicate common method bias as the constructs of opening price and perceived ease of use should be weakly related to the study’s principle constructs. The results of our marker variable analysis (described in Appendix F) suggest that common method bias was likely not a signiﬁcant threat in this study. Measurement model For the measurement model, each construct was modelled reﬂectively. The measurement model was tested by examining convergent and discriminant validity (Fornell & Larcker,\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "628\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Table 3. Sample demographics Items Gender Age (years) Category Men Women 10–19 20–29 30–39 40–49 Over 50 1–3 4–6 7–10 11–15 Over 16 1–3 4–6 7–10 11–15 Over 16 Frequency 203 98 8 92 134 57 10 139 82 31 24 25 136 116 32 15 5 Percentage (%) 67.40 32.60 2.70 30.60 44.50 18.90 3.30 46.20 27.20 10.30 8.00 8.30 45.2 38.5 10.6 4.0 1.7\n",
              "\n",
              "Number of Bids for a Product\n",
              "\n",
              "Number of visits to online auction site (monthly)\n",
              "\n",
              "1981). Two different assessments were made for convergent validity: (1) individual item reliability and (2) construct reliability. Individual item reliability was assessed by examining the item-toconstruct loadings for each construct that was measured with multiple indicators. In order for the shared variance between each item and its associated construct to exceed the error variance, the standardised loadings should be greater than 0.70. As can be seen in Appendix B, all of our item to-construct loadings exceeded the desired threshold. The next step in establishing measurement reliability was to examine the internal consistency for each block of measures (i.e. construct reliability). This was performed by examining the composite reliability, Cronbach’s alpha and the average variance extracted (AVE) for each block of measures, as shown in Table 4. Composite reliability and Cronbach’s alpha both measure the internal consistency within a given construct’s items. The threshold values for composite reliability and Cronbach’s alpha are not absolute ones, but our measures appear to be more than acceptable by established criteria. Bearden et al. (1993) claim that a score of 0.7 indicates ‘extensive’ evidence of reliability and a score of 0.8 or higher provide ‘exemplary’ evidence. As shown in Table 4, all of the constructs in our measurement model exhibited composite reliabilities of 0.86 or higher, and they all exhibited Cronbach’s alpha of 0.80 or higher. The guideline threshold for AVE is 0.5, meaning that 50% or more variance of the indicators is accounted for Chin (1998). As Appendix C indicates, all of the constructs in our measurement model exceeded the established criteria for AVE. We conducted two tests for discriminant validity. First, we calculated each indicator’s loading on its own construct as well as its cross-loading on all other constructs (Appendix B). The loadings for the indicators for each construct are higher than the cross-loadings for other constructs’ indicators. Additionally, going across the rows, each indicator has a higher loading with its construct than a cross-loading with any other construct. This provides good evidence of discriminant validity (Chin, 1998, p. 321).\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "629\n",
              "\n",
              "As a second test of discriminant validity, we considered whether the square roots of the AVEs of the latent constructs were greater than the correlations among the latent constructs. When this is true, more variance is shared between the latent construct and its block of indicators than with another construct (Chin, 1998). As can be seen by reading across the rows of Appendix C, our measures passed this test, thus providing additional evidence of discriminant validity.\n",
              "\n",
              "Hypotheses testing The explanatory power of a structural model can be evaluated by looking at the R2 value (variance accounted for) of the ﬁnal dependent construct. The ﬁnal dependent construct in this study (winner’s regret) has an R2 value of 0.229, indicating that the model accounts for 22.9% of the variance in the dependent variable. As shown in Figure 2, the path between impulsiveness and winner’s regret (β = 0.276, t = 3.846) and the path between sunk cost and winner’s regret (β = 0.266, t = 3.628) were both signiﬁcant at p < 0.01. These results provide strong support for H1 and H2. None of the control variables were found to be signiﬁcant. While some have suggested that gender differences do exist when it comes to regret, we may not have observed this because of the fact that there were far fewer females than males in our study.\n",
              "\n",
              "Moderating effects of competition intensity In order to test the moderating effect of competition intensity (H3–H4) on the relationship between our two independent variables (trait impulsiveness and sunk cost) and our\n",
              "Table 4. Descriptive statistics and reliability of constructs Total sample group (n = 301) Competition intensity Trait impulsiveness Winner’s regret Sunk cost High competition intensity group (n = 155) Trait impulsiveness Winner’s regret Sunk cost Low competition intensity group (n = 146) Trait impulsiveness Winner’s regret Sunk cost\n",
              "SD, standard deviation.\n",
              "\n",
              "Mean 4.60 4.21 3.96 4.35 Mean\n",
              "\n",
              "SD 1.17 1.22 1.25 1.30 SD\n",
              "\n",
              "Cronbach’s alpha 0.92 0.86 — 0.96 Cronbach’s alpha 0.89 — 0.94 Cronbach’s alpha 0.80 — 0.95\n",
              "\n",
              "Composite reliability 0.95 0.91 — 0.97 Composite reliability 0.92 — 0.96 Composite reliability 0.86 — 0.97\n",
              "\n",
              "AVE 0.87 0.71 — 0.92 AVE\n",
              "\n",
              "4.43 4.23 4.93 Mean\n",
              "\n",
              "1.29 1.38 1.16 SD\n",
              "\n",
              "0.75 — 0.89 AVE\n",
              "\n",
              "3.98 3.66 3.73\n",
              "\n",
              "1.05 1.03 1.15\n",
              "\n",
              "0.61 — 0.91\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "630\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Figure 2. Path analysis.\n",
              "\n",
              "dependent variable (winner’s regret), we performed a subgroup analysis as explained in the succeeding text. Before embarking on a subgroup analysis for competition intensity (CI), we ﬁrst needed to determine whether CI acts as a moderator, and if so, what type of moderator it is. In order to investigate the moderating role of CI on the relationship between our two predictors (trait impulsiveness and sunk cost) and our criterion variable (winner’s regret), we followed the moderated regression analysis (MRA) procedure recommended by Sharma et al. (1981). Using MRA, one can determine the type of moderator based on a few simple rules. If there is an interaction effect and no direct effect with criterion or predictor variables, we can conclude that the variable is a pure moderator. If there is an interaction effect and a direct relationship with the predictor, the criterion variable or both, we can conclude that the variable is a quasimoderator. If there is neither a direct effect nor a moderation effect but the detected interaction derives from unequal measurement errors across subsamples, we can conclude that the variable is a homologiser. Based on the MRA procedure, and applying a strict p < 0.05 signiﬁcance threshold, we concluded that CI was a moderator, but that it is neither a pure moderator, nor a quasi-moderator. Instead, CI acts as a homologiser (Appendix D). A homologiser Z acts as moderator in that it inﬂuences the strength of the relationship between X (an independent variable) and Y (a dependent variable) but is not itself related to X or Y and does not interact with X. Under such circumstances, Z exerts its inﬂuence through the error term, and the appropriate way of analysing the moderating effect of Z is by partitioning the dataset and performing a subgroup analysis (Sharma et al., 1981; Allison et al., 1992). In order to do this, we split the sample into high competition intensity and low competition intensity subgroups. This was performed by splitting the sample at the mean value of CI (4.60), after which, we also tested both reliability and validity for each subgroup. Appendix B and Appendix C show that all items in the CI subgroup (n = 155) demonstrate acceptable loadings (0.788–0.950), as do all items in the low-CI subgroup (n = 146) (0.779–0.969). In addition, the reliability indicators are all well above accepted thresholds, and the AVEs are greater than 0.5.\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "631\n",
              "\n",
              "Following Carte & Russell’s (2003) suggestion, we assessed whether the latent constructs were perceived in a similar fashion between the high-CI and low-CI subgroups. An examination of Appendix B suggests that the loading patterns are very similar, thus suggesting that meaningful comparisons can be made between groups. In addition, a measurement invariance analysis was performed to further validate the similarity of measurement models between the two subgroups (Cheung & Rensvold, 2002). Appendix E provides support for measurement invariance, and on that basis, we concluded that meaningful path coefﬁcient comparisons could be made across subgroups. With the measurement model appearing to be stable and adequate across the subgroups, we proceeded to analyse the structural model for each subgroup. Consistent with the Sharma et al. (1981) approach for analysing a homologiser, we tested the moderating effect of competition intensity by estimating two separate models in PLS, namely, the high-CI subgroup and the low-CI subgroup. This approach allowed us to examine the moderating effect of CI by looking at the differences in the magnitude of the path coefﬁcient from impulsiveness to winner’s regret across groups using the approach suggested by Chin et al. (2003). This involved computing a t-statistic2 as follows: ﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ È É ½ðN À 1Þ=ðN 1 þ N 2 À 2ÞÂ½ðN 2 À 1Þ=N 1 þ N 2 À 2ÂSE 2 2 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃÃ Â t ¼ ðPC 1 À PC 2 Þ= S pooled Â ð1=N 1 Þ þ ð1=N 2 Þ\n",
              "\n",
              "S pooled ¼\n",
              "\n",
              "As shown in Table 5, comparison of the path coefﬁcient from impulsiveness to winner’s regret is larger for the high-CI subgroup (β = 0.313) than for the low-CI subgroup (β = 0.184), whereas the path coefﬁcient from sunk cost to winner’s regret is larger for the low-CI subgroup (β = 0.337) than for the high-CI subgroup (β = 0.168). In other words, trait impulsiveness has a greater impact on winner’s regret when the level of competition is high, thus supporting H3, whereas sunk cost has a greater impact on winner’s regret when competition intensity is low, thus supporting H4. These ﬁndings indicate that the impact of trait impulsiveness as well as sunk cost on winner’s regret differs depending on the level of competition intensity. As indicated in Table 6, all of our hypotheses were supported.\n",
              "\n",
              "C O N C L U S I O N S A N D I M P L I C AT I O N S\n",
              "\n",
              "In this study, we applied the automatic thinking perspective as a meta-theoretic lens to explain why online auction bidders succumb to both trait impulsiveness and sunk cost, ultimately leading them to experience winner’s regret. This perspective allowed us to generate insights into one possible mechanism underlying winner’s regret, by focusing our attention on both a cognitive trigger (i.e. sunk cost) and an emotional trigger (i.e. trait impulsiveness) of automatic thinking. To add further richness to our research model, we also considered a situational moderator (i.e. competition intensity). Our results show that both impulsiveness and sunk cost can promote winner’s regret and that the\n",
              "2 where, Spooled: the pooled estimator of the variance; PCi: path coefﬁcient in structural model of competition intensity group i; Nj: sample size of dataset for competition intensity i; SEi: standard error of path in structural model of competition intensity i; and tij: t-statistic with N1 + N2 À 2 degrees of freedom.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "632\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Table 5. Comparisons of paths in each group (competition intensity) High CI (n = 155) From ➔ to Trait impulsiveness ➔ winner’s regret Sunk cost ➔ winner’s regret\n",
              "SE, standard error.\n",
              "\n",
              "Low CI (146) Path coefﬁcient 0.184 0.337 SE 0.098 0.077 R\n",
              "2\n",
              "\n",
              "Path coefﬁcient 0.313 0.168\n",
              "\n",
              "SE 0.101 0.102\n",
              "\n",
              "t-statistic 11.235 16.149\n",
              "\n",
              "0.168 0.196\n",
              "\n",
              "Table 6. Summary of hypotheses testing results # 1 2 3 4 Hypotheses Trait impulsiveness will be positively related to winner’s regret. Sunk cost will be positively related to winner’s regret. The strength of relationship between trait impulsiveness and winner’s regret will be greater when competition intensity is high. The strength of relationship between sunk cost and winner’s regret will be greater when competition intensity is low. Results Supported Supported Supported Supported\n",
              "\n",
              "relationship between these triggers and winner’s regret is moderated by competition intensity. Speciﬁcally, we found that competition intensity strengthens the relationship between impulsiveness and winner’s regret, but that it weakens the relationship between sunk cost and winner’s regret. Before turning to the implications of our study, it is appropriate to consider its limitations. First, we relied on a survey-based approach and did not collect actual bidding data. This means that our measures are subjective and open to potential recall bias. To minimise the risk of recall bias, we asked participants to recall an online auction that they had participated in within the last month and to complete the survey based on that auction experience. One beneﬁt of our approach is that we were able to gather data in an unobtrusive manner that did not risk interfering with the decisionmaking of the participants as they engaged in the auction process (Todd & Benbast, 1987). Second, we used a single-item measure for our dependent variable that cannot be assessed for reliability. Although we suggest that the use of a single-item measure is justiﬁed in this context due to the unidimensional nature of our focal construct, future research should be conducted with a multi-item measure that can be assessed for reliability to test the robustness of our ﬁndings. Third, although we employed automatic thinking as a meta-theoretic perspective to guide our research, we did not gather the kinds of physical measurements (e.g. functional Magnetic Resonance Imaging (fMRI)) that would allow us to conﬁrm the proposed mechanism and the brain activity associated with it. Additional research is needed in order to conﬁrm the underlying mechanism posited here and to probe other factors (both cognitive and emotional) that may also promote or impede winner’s regret. In spite of the aforementioned limitations, we believe that our work has important implications for both research and practice.\n",
              "\n",
              "Implications for research and practice This research makes several important contributions to both research and practice. In this paper, we draw upon the automatic thinking perspective and consider both cognitive and\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "633\n",
              "\n",
              "emotional factors in order to better understand bidding behaviour in online auctions. Speciﬁcally, we provide empirical evidence that both trait impulsiveness and sunk cost inﬂuence winner’s regret. Further, we show that competition intensity has a moderating role on these relationships. Speciﬁcally, trait impulsiveness has a greater impact on winner’s regret when the level of competition is high, whereas sunk cost has a greater impact on winner’s regret when competition intensity is low. These ﬁndings indicate that the impact of trait impulsiveness as well as sunk cost on winner’s regret differs depending on the level of competition intensity. Our ﬁndings also have practical implications for online auction participants as well as auction site operators. From the perspective of both online auction participants as well as online auction providers, it is important to know that there can be negative repercussions (i.e. winner’s regret) that result from both the time and the energy that an individual invests in an auction as well as the degree of impulsiveness that an individual brings to the auction. For online auction participants, minimising winner’s regret requires getting in the habit of honouring one’s initial reservation price and resisting the temptation to obsessively check the status of the auction and to incrementally increase one’s bidding limit when one is outbid. Auction participants should be especially wary about falling into a trap that will lead to winner’s regret when competition intensity is high. Under these conditions, participants need to be very careful not to succumb to their own impulsiveness even if it means losing the auction. For auction site operators, our ﬁndings create the possibility of predicting which individuals will be more likely to experience winner’s regret. Speciﬁcally, auction site users could be given a survey prior to participating in online auctions to determine their trait impulsiveness and susceptibility to the sunk cost effect. The results could be used to gauge how likely it is that individuals will experience winner’s regret and auction site users could be warned beforehand. In cases where the risk of winner’s regret is high, the auction site could even recommend that individuals use the ‘buy it now’ feature instead of online bidding. Our research also provides a tool for online auction sites to survey online auction winners to determine the extent to which they experienced winner’s regret. This can be an informative diagnostic tool for helping auction sites to gauge whether winner’s regret is experienced by a handful of users or whether it is something that is experienced more broadly. Once this is known, site management can determine whether any corrective action is needed.\n",
              "\n",
              "REFERENCES\n",
              "Adam, M.T.P., Krämer, J., Jähnig, C., Seifert, S. & Weinhardt, C. (2011) Understanding auction fever: a framework for emotional bidding. Electronic Markets, 21, 197–207. Allison, D.B., Heshka, S., Pierson, R.N.J., Wang, J. & Heymsﬁeld, S.B. (1992) The analysis and identiﬁcation of homologizer/moderator variables when the moderator is continuous: an illustration with anthropometric data. American Journal of Human Biology, 4, 775–782. Amyx, D.A. & Luehlﬁng, M.S. (2006) Winner’s curse and parallel sales channels-online auctions linked within e-tail websites. Information and Management, 43, 919–927. Andrade, E.B. & Ariely, D. (2009) The enduring impact of transient emotions on decision making. Organizational Behavior and Human Decision Processes, 109, 1–8. Angst, C.M., Agarwal, R. & Kuruzovich, J.H. (2008) Bid or buy? individual shopping traits as predictors of strategic\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "634\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "exit in online auctions. International Journal of Electronic Commerce, 13, 59–84. Ariely, D. (2008) Predictably Irrational – The Hidden Forces that Shape our Decisions. Harper Collins Publisher, Hammersmith, London, UK. Ariely, D. & Simonson, I. (2003) Buying, bidding, playing, or competing? value assessment and decision dynamics in online auctions. Journal of Consumer Psychology, 13, 113–123. Arkes, H.R. & Blumer, C. (1985) The psychology of sunk cost. Organizational Behavior and Human Decision Processes, 35, 124–140. Bapna, R., Goes, P., Gupta, A. & Jin, Y. (2004) User heterogeneity and its impact on electronic auction market design: an empirical exploration. MIS Quarterly, 28, 21–43. Bazerman, M.X. & Moore, D.A. (2009) Judgment in Managerial Decision Making. John Wiley & Sons, Inc., NY. Bearden, W.O., Netemeyer, R.G. & Mobley, M.F. (1993) Handbook of Marketing Scales: Multi-Item Measures for Marketing and Consumer Behavior Research. Sage Publications, Newbury Park, CA. Beatty, S.E. & Ferrell, M.E. (1997) Impulse buying: modeling its precursors. Journal of Retailing, 74, 169–191. Bikhchandani, S., Hirshleifer, D. & Welch, I. (1992) A theory of fads, fashion, custom, and cultural change as informational cascades. Journal of Political Economy, 100, 992–1026. Bonabeau, E. (2004) The perils of the imitation age. Harvard Business Review, 82, 45–54. Carmon, Z. & Ariely, D. (2000) Focusing on the foregone: how value can appear so different to buyers and sellers. Journal of Consumer Research, 27, 360–370. Carte, T.A. & Russell, C.J. (2003) In pursuit of moderation: nine common errors and their solutions. MIS Quarterly, 27, 479–501. Cheung, G.W. & Rensvold, R.B. (2002) Evaluating goodness-of-ﬁt indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255. Chin, W.W. (1998) The partial least square approach to structural equation modeling. In: Modern Method for Business Research, Marcoulides, G.A. (ed.), pp. 150–170. Lawrence Erlbaum, Mahwah, NJ. Chin, W.W., Marcolin, B.L. & Newsted, P.R. (2003) A partial least squares latent variable modeling approach for measuring interaction effects: results from a Monte Carlo simulation study and an electronic-mail emotion/adoption study. Information Systems Research, 14, 189–217. Dholakia, U.M., Basuroy, S. & Soltysinski, K. (2002) Auction or agent (or both)? a study of moderators of the herding bias in digital auctions. International Journal of Research in Marketing, 19, 115–130.\n",
              "\n",
              "Doran, N., Spring, B., McChargue, D., Pergadia, M. & Richmond, M. (2004) Impulsivity and smoking relapse. Nicotine & Tobacco Research, 6, 641–647. Foreman, P. & Murhighan, J.K. (1996) Learning to avoid the winner’s curse. Organizational Behavior and Human Decision Processes, 67, 170–180. Fornell, C. & Larcker, D.F. (1981) Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18, 39–50. Fuchs, C. & Diamantopoulos, A. (2009) Using single-item measures for construct measurement in management research. Die Betriebswirtschaft, 69, 195–210. Gefen, D. & Straub, D. (2005) A practical guide to factorial validity using PLS-graph: tutorial and annotated example. Communications of the Association for Information Systems, 16, 91–109. Gilkeson, J.H. & Reynolds, K. (2003) Determinants of Internet auction success and closing price: an exploratory study. Psychology & Marketing, 20, 537–566. Hair, J.F. Jr., Anderson, R.E., Tatham, R.L. & Black, W.C. (1998) Multivariate Data Analysis, 5th edn. Prentice-Hall, Upper Saddle River, NJ. Hardin, A. & Looney, C. (2012) Myopic loss aversion: demystifying the key factors inﬂuencing decision problem framing. Organizational Behavior and Human Decision Processes, 117, 311–331. Hausman, A. (2000) A multi-method investigation of consumer motivations in impulse buying behavior. Journal of Consumer Marketing, 17, 403–419. Heyman, J.E., Orhun, Y. & Ariely, D. (2004) Auction fever: the effect of opponents and quasi-endowment on product valuations. Journal of Interactive Marketing, 18, 7–21. Hofmann, W., Friese, M. & Strack, F. (2009) Impulse and self-control from a dual-systems perspective. Perspective on Psychological Science, 4, 162–176. Johns, C.L. & Zaichkowsky, J.L. (2003) Bidding behavior at the auction. Psychology & Marketing, 20, 303–322. Kahneman, D. (2003) A perspective on judgment and choice: mapping bounded rationality. American Psychologist, 58, 697–720. Kahneman, D. (2011) Thinking, Fast and Slow. Farrar, Straus and Giroux, New York, NY. Kahneman, D. & Tversky, A. (1979) Prospect theory: an analysis of decision under risk. Econometrica, 47, 263–291. Kahneman, D., Knetsch, J.L. & Thaler, R.H. (1990) Experimental tests of the endowment effect and the Coase theorem. Journal of Political Economy, 98, 1325–1348.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "Klaczynski, P.A. & Cottrill, J.I.M. (2004) A dual process approach to cognitive development: the case of children’s understanding of sunk cost decision. Thinking and Reasoning, 10, 147–174. Ku, G., Malhotra, D. & Murnighan, J.K. (2005) Towards a competitive arousal model of decision-making: a study of auction fever in live and Internet auctions. Organizational Behavior & Human Decision Processes, 96, 89–103. Lerner, J.S. & Tiedens, L.Z. (2006) Portrait of the angry decision maker: how appraisal tendencies shape anger’s inﬂuence on cognition. Journal of Behavioral Decision Making, 19, 115–137. Lindell, M.K. & Whitney, D.J. (2001) Accounting for common method variance in cross-sectional research designs. Journal of Applied Psychology, 86, 114–121. Looney, C. & Hardin, A. (2009) Decision support for retirement portfolio management: overcoming myopic loss aversion via technology design. Management Science, 55, 1688–1703. Malhotra, D. (2010) The desire to win: the effects of competitive arousal on motivation and behavior. Organizational Behavior and Human Decision Processes, 111, 139–146. Malhotra, N.K., Kim, S.S. & Patil, A. (2006) Common method variance in IS research: a comparison of alternative approaches and reanalysis of past research. Management Science, 52, 1865–1883. Mitchell, S.H. (1999) Measures of impulsivity in cigarette smokers and non-smokers. Psychopharmacology, 146, 455–464. Moon, H. (2001) Looking forward and looking back: integrating completion and sunk cost effect within an escalation of commitment progress decision. Journal of Applied Psychology, 86, 104–113. Moors, A. & De Houwer, J. (2006) Automaticity: a theoretical and conceptual analysis. Psychological Bulletin, 132, 297–326. Oh, W. (2002) C2C versus B2C: a comparison of the winner’s curse in two types of electronic auctions. International Journal of Electronic Commerce, 6, 115–138. Park, S.C., Keil, M., Kim, J.U. & Bock, G.W. (2012) Understanding overbidding behavior in C2C auctions: an escalation theory perspective. European Journal of Information Systems, 21, 643–663. Peters, C. & Bodkin, C.D. (2007) An exploratory investigation of problematic online auction behaviors: experiences of eBay users. Journal of Retailing & Consumer Services, 14, 1–16.\n",
              "\n",
              "635\n",
              "\n",
              "Petrescu, M. (2013) Marketing research using single-item indicators in structural equation models. Journal of Marketing Analytics, 1, 99–107. Podsakoff, P.M. & Organ, D.W. (1986) Self-reports in organizational research: problems and prospects. Journal of Management, 12, 531–554. Ringle, C.M., Wende, S. & Will, A. (2005) SmartPLS 2.0 (M3) beta, Hamburg, Germany. URL http://www. smartpls.de accessed on July 06, 2012. Robert, F.E., Charles, A.W. & Sharad, S. (2011) Bidding patterns, experience, and avoiding the winner’s curse in online auctions. Journal of Management Information Systems, 27, 241–268. Robins, R.W., Hendin, H.M. & Trzesniewski, K.H. (2001) Measuring global self-esteem: construct validation of a single-item measure and the Rosenberg self-esteem scale. Personality and Social Psychology Bulletin, 27, 151–161. Rook, D.W. (1987) The buying impulse. Journal of Consumer Research, 14, 189–199. Rook, D.W. & Fisher, R.J. (1995) Trait and normative aspects of impulsive buying behavior. Journal of Consumer Research, 22, 305–313. Schneider, W. & Shiffrin, R.M. (1977) Controlled and automatic human information processing. I: detection, search and attention. Psychological Review, 84, 1–66. Sharma, S., Durand, R.M. & Gur-Arie, O. (1981) Identiﬁcation and analysis of moderator variables. Journal of Marketing Research, 18, 291–300. Sloman, S.A. (1996) The empirical case of two systems of reasoning. Psychological Bulletin, 119, 3–22. Slovic, P., Finucane, M.L., Peters, E. & MacGregor, D.G. (2007) The affect heuristic. European Journal of Operational Research, 177, 1333–1352. Stafford, M.R. & Stern, B. (2002) Consumer bidding behavior on Internet auction sites. International Journal of Electronic Commerce, 7, 135–150. Strack, F. & Deutsch, R. (2004) Reﬂective and impulsive determinants of social behavior. Personality and Social Psychology Review, 8, 220–247. Straub, D., Boudreau, M.C. & Gefen, D. (2004) Validation guidelines for IS positivist research. Communications of AIS, 14, 380–427. Thaler, R.H. (1980) Toward a positive theory of consumer choice. Journal of Economic Behavior and Organization, 1, 39–60. Thaler, R.H. & Sunstein, C.R. (2009) Nudge: Improving Decisions about Health Wealth and Happiness. Penguin Books, New York, NY.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "636\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Todd, P. & Benbast, I. (1987) Process tracing methods in decision support systems research: exploring the black box. MIS Quarterly, 11, 493–512. Turel, O., Serenko, A. & Giles, P. (2011) Investigating technology addiction and use: an empirical investigation of online auction users. MIS Quarterly, 35, 1043–1061. Tversky, A. & Kahneman, D. (1981) The framing of decisions and the psychology of choice. Science, 211, 453–458. VandenBos, G.R. (2007) APA Dictionary of Psychology. American Psychological Association, Washington, DC. Wally, M.J.C. & Fortin, D.R. (2005) Behavioral outcomes from online auctions: reserve price, reserve disclosure, and initial bidding inﬂuences in the decision process. Journal of Business Research, 58, 1409–1418. Wanous, J.P. & Hudy, M.J. (2001) Single-item reliability: a replication and extension. Organizational Research Methods, 4, 361–375. Wanous, J.P., Reichers, A.E. & Hudy, M.J. (1997) Overall job satisfaction: how good are single item measures? Journal of Applied Psychology, 82, 247–252. Whyte, G. (1986) Escalating commitment to a course of action: a reinterpretation. Academy of Management Review, 11, 311–321. Wilcox, R.T. (2000) Experts and amateurs: the role of experience in Internet auctions. Marketing Letters, 11, 363–374.\n",
              "\n",
              "Business at Georgia State University. His research focuses on IT project management and includes work on preventing IT project escalation, identifying and managing IT project risks, and improving IT project status reporting. His interests also include IT implementation and use. Keil has published more than 100 refereed publications including papers that have appeared in MIS Quarterly, Journal of Management Information Systems, Decision Sciences, Strategic Management Journal and many other journals. He currently serves as a Senior Editor for Information Systems Research and is a member of the editorial board for the Journal of Management Information Systems. He has also served on the editorial boards of MIS Quarterly, Decision Sciences, IEEE Transactions on Engineering Management and The DATA BASE for Advances in Information Systems. Gee-Woo Bock received his PhD in Management Engineering from the Korea Advanced Institute of Science and Technology, Seoul, Korea, in 2001 after 7 years of working at the Department of Strategic Planning in Samsung Economic Research Institute. After acquiring his PhD, he joined the School of Computing at the National University of Singapore as an Assistant Professor in 2002. He is currently a Professor at the School of Global Business, Sungkyunkwan University in Korea. He has served Information and Management as a member of Associate Editors since 2012. His current research interests include Knowledge Management, Social Network Services, Personalization, IOS/SCM, Service Systems in Healthcare & Tourism. His papers have been published in MIS Quarterly, JAIS, EJIS, DSS, JGIM, IEEE TEM, CACM, I&M, IJEC, The Data Base: Advances in Information Systems and among others. Jong Uk Kim is currently Professor at the Business School of Sungkyunkwan University in Korea. He received his PhD in Computer Information Systems from Georgia State University. His research focuses on the areas of knowledge transfer in IT project, online consumer Behaviour, IT outsourcing and decision support systems. His papers have been published in European Journal of Information Systems, Computers in Human Behavior, International Journal of Human-Computer Studies and among others.\n",
              "\n",
              "Biographies\n",
              "Sang Cheol Park is currently an Assistant Professor of the Department of Business Administration at Daegu University in Korea. He received his PhD in MIS from Sungkyunkwan University in Korea. His research focuses on the areas of escalation of commitment, online bidding behaviour, cloud computing, knowledge transfer in IT project and so on. His papers have been published in the European Journal of Information Systems, Journal of Global Information Management, Computers in Human Behavior, Journal of Computer Information Systems and among others. Mark Keil is a John B. Zellars Professor of Computer Information Systems in the J. Mack Robinson College of\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "637\n",
              "\n",
              "APPENDIX A. MEASUREMENT ITEMS FOR KEY CONSTRUCTS\n",
              "Constructs Impulsiveness No. IMP1 IMP2 IMP3 IMP4 SC1 SC2 SC3 Winner’s regret Competition intensity WR CI1 CI2 CI3 Measures I usually do things on impulse. I often behave without thinking of the consequences. I often say the ﬁrst thing I think. I often act on the spur of the moment. I could not stop bidding because I had already spent too much effort in the process. I could not stop bidding because I had already spent too much time in the process. Overall, it would have been a waste of time and effort if I stopped bidding. After purchasing the item, I regretted that I had overpaid. The bidding competition was ﬁerce. There were many people who participated in the bidding process. Compared with other auctions, there were many bidders who competed in the auction. Sources Rook & Fisher (1995)\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "Ku et al. (2005), Park et al. (2012)\n",
              "\n",
              "Developed for this study Park et al. (2012)\n",
              "\n",
              "Strongly disagree/agree (1–7 scale). IMP, impulsiveness; WR, winner’s regret; SC, sunk cost.\n",
              "\n",
              "APPENDIX B. ITEM-FACTOR LOADINGS AND CROSS-LOADINGS FOR FULL SAMPLE AND FOR SUBGROUPS\n",
              "Full sample (n = 301) Competition intensity CI1 CI2 CI3 IMP 1 IMP 2 IMP 3 IMP 4 WR SC1 SC2 SC3 0.915 0.944 0.933 0.219 0.303 0.222 0.179 0.223 0.589 0.550 0.554 Impulsiveness 0.241 0.236 0.280 0.850 0.800 0.848 0.868 0.391 0.427 0.415 0.432 Winner’s regret 0.206 0.184 0.226 0.351 0.310 0.333 0.322 1.000 0.384 0.381 0.379 Sunk cost 0.546 0.507 0.583 0.358 0.386 0.403 0.347 0.397 0.966 0.968 0.942 High CI (n = 155) N/A Low CI (n = 146) N/A\n",
              "\n",
              "0.883 0.788 0.890 0.899 1.000 0.943 0.950 0.930\n",
              "\n",
              "0.779 0.817 0.773 0.767 1.000 0.969 0.970 0.922\n",
              "\n",
              "N/A, not applicable; IMP, impulsiveness; WR, winner’s regret; SC, sunk cost.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "638\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "APPENDIX C. CONSTRUCT CORRELATIONS AND SQUARE ROOT OF AVES (ON DIAGONAL)\n",
              "Total sample group (n = 301) Competition intensity 0.930 0.273 0.223 0.589 Impulsiveness 0.842 0.391 0.443 Winner’s regret Sunk cost\n",
              "\n",
              "Competition intensity Impulsiveness Winner’s regret Sunk cost High competition intensity group (n = 155) Competition intensity Impulsiveness Winner’s regret Sunk cost Low competition intensity group (n = 155) Competition intensity Impulsiveness Winner’s regret Sunk cost N/A, not applicable.\n",
              "\n",
              "0.909 0.397 0.959\n",
              "\n",
              "Competition intensity N/A N/A N/A N/A\n",
              "\n",
              "Impulsiveness 0.866 0.389 0.397\n",
              "\n",
              "Winner’s regret\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "1.000 0.294 0.941\n",
              "\n",
              "Competition intensity N/A N/A N/A N/A\n",
              "\n",
              "Impulsiveness 0.782 0.339 0.397\n",
              "\n",
              "Winner’s regret\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "1 0.412 0.954\n",
              "\n",
              "APPENDIX D. MRA ANALYSIS TO DETERMINE THE TYPE OF MODERATOR FOR COMPETITION INTENSITY\n",
              "Unstandardised coefﬁcients Model 1 Constant Impulsiveness (IMP) Sunk cost (SC) Constant Impulsiveness (IMP) Sunk cost (SC) Competition (CI) 3 intensity Beta 0033.957 0.329 0.280 3.957 0.330 0.293 0.073 3.961 0.333 0.291 À0.024 À0.013 Std error 0.064 0.071 0.057 0.064 0.071 0.068 À0.023 0.066 0.072 0.069 0.074 0.054 0.266 0.279 0.267 0.292 Standardised coefﬁcients Beta t 61.665 4.650 4.878 61.575 4.648 4.294 À0.356 59.686 4.614 4.235 À0.329 À0.244 Sig. 0.001 0.000 0.000 0.000 0.000 0.002 0.722 0.000 0.000 0.000 0.742 0.807 0.215 R\n",
              "2\n",
              "\n",
              "0.215\n",
              "\n",
              "2\n",
              "\n",
              "0.215\n",
              "\n",
              "Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*IMP\n",
              "\n",
              "0.269 0.291 À0.021 À0.013\n",
              "\n",
              "(Continues)\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "639\n",
              "\n",
              "Table . (Continued)\n",
              "Unstandardised coefﬁcients Model 4 Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*SC Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*SC CI*IMP Dependent variable: winner’s regret. *p < 0.001. Beta 4.008 0.336 0.290 À0.038 À0.065 4.007 0.329 0.294 À0.045 À0.077 0.037 Std error 0.071 0.071 0.068 0.073 0.038 0.071 0.072 0.069 0.074 0.043 0.061 Standardised coefﬁcients Beta t 56.522 4.748 4.257 À0.52 À1.697 56.413 4.578 4.293 À0.611 À1.784 0.607 Sig. 0.000 0.000 0.000 0.603 0.091 0.000 0.000 0.000 0.542 0.075 0.544 R\n",
              "2\n",
              "\n",
              "0.223\n",
              "\n",
              "0.272 0.289 À0.033 À0.088 0.266 0.294 À0.039 À0.104 0.036\n",
              "\n",
              "5\n",
              "\n",
              "0.224\n",
              "\n",
              "APPENDIX E. MEASUREMENT INVARIANCE ANALYSIS FOR GROUP COMPARISON\n",
              "Fit index Chisquare 77.019 Chisquare/df 1.605\n",
              "\n",
              "Model test Baseline model\n",
              "\n",
              "df 48\n",
              "\n",
              "GFI 0.959\n",
              "\n",
              "CFI 0.985\n",
              "\n",
              "NFI 0.962\n",
              "\n",
              "RMSEA 0.045\n",
              "\n",
              "ΔGFI —\n",
              "\n",
              "ΔCFI —\n",
              "\n",
              "ΔNFI —\n",
              "\n",
              "ΔRMSEA —\n",
              "\n",
              "Constrained models between Chisquare 77.019 77.019 77.252 144.729 146.093 149.698 152.398 Chisquare/df 1.605 1.605 1.577 2.895 2.865 2.879 2.875\n",
              "\n",
              "Model test IMP and RGT SC and RGT IMP, SC and RGT IMP, SC, OP and RGT IMP, SC, OP, NB and RGT IMP, SC, OP, NB, MP and RGT IMP, SC, OP, NB, MP, GD and RGT\n",
              "\n",
              "df 48 48 49 50 51 52 53\n",
              "\n",
              "GFI 0.959 0.959 0.959 0.950 0.949 0.945 0.943\n",
              "\n",
              "CFI 0.985 0.985 0.986 0.976 0.971 0.970 0.969\n",
              "\n",
              "NFI 0.962 0.962 0.962 0.929 0.928 0.926 0.925\n",
              "\n",
              "RMSEA 0.045 0.045 0.044 0.052 0.052 0.052 0.052\n",
              "\n",
              "ΔGFI 0.000 0.000 0.001 0.009 0.010 0.004 0.002\n",
              "\n",
              "ΔCFI 0.000 0.000 0.000 0.009 0.001 0.004 0.002\n",
              "\n",
              "ΔNFI 0.000 0.000 0.001 0.010 0.005 0.003 0.001\n",
              "\n",
              "ΔRMSEA 0.000 0.000 0.000 0.009 0.000 0.000 0.000\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "640\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "APPENDIX F. COMMON METHOD BIAS (CMB) ANALYSIS\n",
              "F1 Marker Variable Analysis to Evaluate Common Method Bias\n",
              "\n",
              "We followed the marker variable method described by Lindell & Whitney (2001) and used by Malhotra et al. (2006). We identiﬁed the lowest correlation marker variable collected during survey administration (RM1). We also identiﬁed the second lowest correlation marker variable (RM2). In Table , we present the correlations after correcting for RM1 and RM2: • Adjusting for RM1 and RM2, respectively, the correlations among the substantive variables dropped by 0.2 in maximum. All signiﬁcant correlations remained the same signiﬁcant level and insigniﬁcant correlations remained insigniﬁcant.\n",
              "RM1 = 0.008 Factors r(IP, SC) r(IP, WR) r(IP,CI) r(SC, CI) r(SC,WR) r(CI, WR) Uncorrected 0.444 0.390 0.271 0.585 0.398 0.221 M1 0.428 0.374 0.255 0.569 0.382 0.205 t 8.175 6.961 4.552 11.944 7.135 3.616 M2 0.426 0.372 0.253 0.567 0.380 0.203 RM2 = 0.009 t 8.128 6.918 4.514 11.883 7.092 3.579\n",
              "\n",
              "IMP, impulsiveness; SC, sunk cost; CI, competition intensity; WR, winner’s regret; M1, opening price; M2, perceived ease of use; RM1, correlation between M1 (marker variable) and WR; RM2, correlation between M2 (marker variable) and WR.\n",
              "\n",
              "F2 Correlation Tables of Marker Variable and Study Constructs\n",
              "\n",
              "Constructs SC CI IMP WR M1 M2\n",
              "\n",
              "SC 1.000 0.585 0.444 0.398 0.012 0.216\n",
              "\n",
              "CI\n",
              "\n",
              "IP\n",
              "\n",
              "WR\n",
              "\n",
              "M1\n",
              "\n",
              "M2\n",
              "\n",
              "1.000 0.271 0.221 0.056 0.294\n",
              "\n",
              "1.000 0.390 À0.145 0.205\n",
              "\n",
              "1.000 0.008 0.009\n",
              "\n",
              "1.000 0.131 1.000\n",
              "\n",
              "IMP, impulsiveness; SC, sunk cost; CI, competition intensity; WR, winner’s regret; M1, opening price; M2, perceived ease of use.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "metadata": {
        "id": "3mVvJC_4G-vj",
        "colab_type": "code",
        "outputId": "2435c080-350f-48e3-b5fb-d2e59c5f4c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc11.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[As shown in Figure 2, the path between impulsiveness and winner’s regret (β = 0.276, t = 3.846) and the path between sunk cost and winner’s regret (β = 0.266, t = 3.628) were both signiﬁcant at p < 0.01.,\n",
              " Based on the MRA procedure, and applying a strict p < 0.05 signiﬁcance threshold, we concluded that CI was a moderator, but that it is neither a pure moderator, nor a quasi-moderator.,\n",
              " *p < 0.001.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "metadata": {
        "id": "M1Pijgar8AqL",
        "colab_type": "code",
        "outputId": "87d858f7-4d11-4b03-c886-7195c1a0e726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "significant_Sents = [sent for sent in doc11.sents if 'signiﬁcant ' in sent.string]\n",
        "significant_Sents "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Our results show that both trait impulsiveness and sunk cost have signiﬁcant impacts on winner’s regret.,\n",
              " Indeed, numerous studies have demonstrated that strong emotions can have a signiﬁcant inﬂuence on decision-making behaviour (see, for example, Andrade & Ariely, 2009).,\n",
              " Cognitive Completion effect, selfjustiﬁcation, sunk cost and willingness to continue bidding (as a mediator) Overbidding behaviour signiﬁcant effects on a bidder’s decision-making process.,\n",
              " This result suggests that CMB was not a signiﬁcant threat in our study.,\n",
              " The results of our marker variable analysis (described in Appendix F) suggest that common method bias was likely not a signiﬁcant threat in this study.,\n",
              " As shown in Figure 2, the path between impulsiveness and winner’s regret (β = 0.276, t = 3.846) and the path between sunk cost and winner’s regret (β = 0.266, t = 3.628) were both signiﬁcant at p < 0.01.,\n",
              " All signiﬁcant correlations remained the same signiﬁcant level and insigniﬁcant correlations remained insigniﬁcant.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "metadata": {
        "id": "3NHx7yG1GRdk",
        "colab_type": "code",
        "outputId": "ab606641-8ee6-4c0b-a86d-eb4717b876a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17797
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc12 = nlp(open(u\"Pil HanS#ParkS#OhW_2016_Mobile App Analytics - A Multiple Discrete-Continuous Choice Framework_MIS Quarterly_4.txt\").read())\n",
        "doc12\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BIG DATA & ANALYTICS IN NETWORKED BUSINESS\n",
              "\n",
              "MOBILE APP ANALYTICS: A MULTIPLE DISCRETE-CONTINUOUS CHOICE FRAMEWORK1\n",
              "Sang Pil Han\n",
              "Department of Information Systems, W. P. Carey School of Business, Arizona State University, PO Box 874106, Tempe, AZ 85287 U.S.A. {shan73@asu.edu}\n",
              "\n",
              "Sungho Park\n",
              "Department of Marketing, W. P. Carey School of Business, Arizona State University, PO Box 874106, Tempe, AZ 85287 U.S.A. {spark104@asu.edu}\n",
              "\n",
              "Wonseok Oh\n",
              "College of Business, Korea Advanced Institute of Science and Technology, 85 Hoegiro Dongdaemoon-Gu, Seoul, KOREA 130-722 {wonseok.oh@kaist.ac.kr}\n",
              "\n",
              "The number of mobile apps launched in the market has exponentially grown to more than 2 million, but little is known about how users choose and consume apps of numerous categories. This study develops a utility theory-based structural model for mobile app analytics. We use the theoretical concepts of utility and satiation along with the factor analytic approach, as bases in simultaneously modeling the complex relationships among choice, consumption, and utility maximization for consumers of various mobile apps. Using a unique panel dataset detailing individual user-level mobile app time consumption, we quantify the baseline utility and satiation levels of diverse mobile apps and delineate how app preferences and consumption patterns vary across demographic groups affected by persistent use and time trends. The findings suggest that users’ baseline utility substantially diverges across app categories and that their demographic characteristics and habit formation explain the appreciable heterogeneity in baseline utility and satiation. These parameters also exhibit positive and negative correlations in mobile websites and app categories. Our modeling approaches and computational methods can unlock new perspectives and opportunities for handling large-scale, micro-level data, while serving as important resources for big data analytics and mobile app analytics. Keywords: Mobile analytics, mobile web and apps, time use modeling, satiation, interdependence, structural econometrics\n",
              "\n",
              "Introduction1\n",
              "The mobile revolution has stimulated a dramatic growth in communication, resulting in the production of data at a scale\n",
              "1\n",
              "\n",
              "Bart Baesens, Ravi Bapna, James R. Marsden, Jan Vanthienen, and J. Leon Zhao served as the senior editors for this paper. The appendices for this paper are located in the “Online Supplements” section of the MIS Quarterly’s website (http://www.misq.org).\n",
              "\n",
              "and pace never before seen in the history of digital technology. The plethora of different applications (“apps”) and consumers’ relentless use of such innovations generate voluminous data in the form of social media exchanges, purchase transactions, music downloads, car navigation requests, stock investment advice, location alerts, and search queries. In an app-based, networked socioeconomic environment, every text, transaction, digital procedure, virtually any tactile command, and other user input processed over apps can become a data point. The addictive nature of mobile plat-\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4, pp. 983-1008/December 2016\n",
              "\n",
              "983\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "forms has abetted continuous data production, even while consumers are asleep. The rise of mobile app technology and its attendant challenges have undoubtedly become a big deal in big data. In business communities, volume, velocity, and variety are often used to define the nature and varying dimensions of big data (hence the term “3Vs of big data”). Mobile apps provide an intriguing context in which the 3Vs perfectly account for the roles played by big data. As of July 2014, Google had 1.3 million apps available through its Android platform, while its competitor, Apple, offered a selection of 1.2 million apps through its iTunes app store.2 In addition to this tremendous number of apps, the portability and on-demand accessibility advanced by mobile platforms have accelerated data processing, thereby enabling real-time, streaming-based data consumption on the fly. Mobile apps considerably differ in terms of functionalities and users’ consumption preferences; certain apps are used with great frequency, whereas many others are deployed only once or twice in their lifespans before they are completely discarded. The exploding availability and the escalating complexity of dynamic and multimodal apps have thus presented formidable challenges to app developers and platform owners in understanding the business ramifications of placing such IT products at consumers’ immediate disposal. Mobile app analytics, in which app usage is monitored and measured to study the behavior of mobile app users, is at its nascent stage. The number of downloads was an initial key interest, but the direct, quantitative measurement of usage, habits, and engagement have increasingly elicited attention from businesses. For example, the solution accessible from Google’s Mobile App Analytics measures user behaviors ranging from app discovery and download to in-app purchase. It enables real-time reporting on where users take action, pause, or disappear through event tracking and flow visualization, thus providing insights into app users’ behaviors.3 Nevertheless, the methods and results derived from commercial mobile app analytics solutions (e.g., Google’s Mobile App Analytics, Yahoo’s Flurry Analytics, and Countly’s Mobile Analytics) require room for improvement given that they are generally descriptive in nature and neglect the complexity inherent to the usage of various mobile apps. An additional shortcoming is that these mechanisms offer a holistic view of app usage only at the aggregate level, thereby preventing analysts from comprehensively attending to the\n",
              "2\n",
              "\n",
              "individual user utility that originates from the choice and consumption of an assortment of apps. Furthermore, commercial analytics have a limited capacity to describe or predict interdependence and competition among a large number of apps. Some apps are often used in concert to enhance user experience and utility, but certain apps function only as substitutes for others. In a battle of substitutes, an app competes with another for volume and usage frequency. Although the number and diversity of apps are increasing at an unprecedented rate, validated empirical schemes, robust computational frameworks and analytics, and actual usage data in large quantities are lacking. Such deficiencies continue to impede our understanding of app usage patterns, as well as the interdependence and competition among such technologies. As mobile users are progressively confronted with an overabundance of apps, they adopt a range of choice mechanisms to sift through multiple options and effectively manage app consumption. These mechanisms allow users to maximize utility given time constraints. To systematically explore the patterns of consumption dependency for a wide selection of apps, this study builds on the work of Bhat (2005) in developing a utility theory-based structural model for multiple discrete/continuous choices in app use. An important component of our model is that it focuses on a user’s repeat decisions regarding which app to use and how extensive app use will be after download. The premise that underlies the model is that a consumer’s marginal utility diminishes as the level of consumption of any particular app increases—a phenomenon known as satiation. Another essential constituent of our utility-based choice paradigm is the use of a unique panel data set that details mobile app consumption at the individual user level. Guided by the capabilities and considerations of our paradigm, we pursue the following research questions: How do we quantify users’ intrinsic preferences for apps (i.e., baseline utility) and the marginal utility derived from app consumption (i.e., satiation)? How do the baseline utility and satiation levels of apps vary across different user demographic groups, and how are they affected by persistent use and time trends? Finally, how is a dimensionality issue addressed when estimating interdependence in unobserved attributes among app categories under conditions wherein numerous app categories exist? Resolving these matters on the basis of a rigorous theoretical framework with empirical validation can enrich our understanding of the baseline utility and satiation levels of numerous mobile apps in diverse categories. Further, unlike typical discrete-choice models in which a single option is chosen, our multiple discrete-continuous choice approach allows the simultaneous selection of various\n",
              "\n",
              "http://www.statista.com/statistics/276623/number-of-apps-available-inleading-app-stores/. http://www.google.com/analytics/mobile/.\n",
              "\n",
              "3\n",
              "\n",
              "984\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "options. Diminishing marginal utility horizontally motivates multiple discrete purchases. That is, if consumers experience diminishing marginal utility for each of several potential apps with comparable attractiveness, then they will evaluate and choose these options simultaneously. For example, users evenly distribute the time that they spend on various mobile apps across socializing, entertainment, and productivity tasks. Some of such co-choice incidences are, in reality, motivated by app design. A case in point is game apps that enable users to share their achievements with their Facebook friends by way of a “share” button that the users can click on after completing a level or reaching a new record. An additional potential cause of multiple choice incidences is data aggregation. In our analysis, we devote a week to extracting the nontrivial interdependency patterns regarding app choice and consumption. A narrow time slice (e.g., daily or hourly) may diminish the multi-choice feature. Moreover, our mobile analytics approach incorporates a factor analytic structure into the multiple discrete-continuous choice framework. Capturing correlations among options in their unobserved baseline utility and satiation is empirically challenging with the use of multiple discrete-continuous choice models. Covariance parameters increase in a quadratic fashion with an expanding number of options, thus resulting in prohibitive burdens in model estimation. Using the proposed factor analytic structure, we circumvent this empirical challenge. Finally, our analytical approach deals with the variety of app data that are characterized by individual heterogeneity, which complicates and constrains the understanding of consumers’ app time-use. In particular, the demographic characteristics of users affect their intrinsic preferences for apps and the marginal utility derived from app consumption. Because behavioral heterogeneity in IT usage has been frequently observed across different demographic groups (e.g., Wixom and Todd 2005), the patterns underlying app choice and consumption are expected to vary across age, gender, income, and educational groups. Researchers have yet to develop a comprehensive systematic analytics approach that explores the influence of demographic characteristics on the consumption of numerous mobile apps and their implications for individual utility trajectories. The methods and processes detailed in this work can serve as important resources for big data analytics and unlock new perspectives for mobile app analytics. To reiterate, the specific contributions of our proposed analytics frameworks are as follows: • We develop a utility theory-based structural model that simultaneously identifies consumer app choices and usage patterns within a single utility maximization framework.\n",
              "\n",
              "•\n",
              "\n",
              "To efficiently capture dependencies among alternative apps in their unobserved baseline utility and satiation, we propose a mobile analytics approach that incorporates a factor analytic structure into the multiple discretecontinuous choice framework. Our proposed analytics method offers a systematic approach that explores the influence of individual heterogeneity (i.e., demographic characteristics) on consumption patterns and choice preferences across numerous mobile apps.\n",
              "\n",
              "•\n",
              "\n",
              "The proposed mechanism of user app choice and consumption presents equally important implications for firms’ strategy building, particularly for mobile competitive analysis, mobile targeting, and mobile media planning. As app developers and publishers migrate from paid download models to in-app purchase and ad-supported business models, they steadily illuminate how much time consumers spend using their apps in comparison with competing apps. Increased time devoted to a focal app is likely to generate additional in-app advertising revenues and purchases and is likely to enhance consumer engagement with a brand, thereby endowing businesses with competitive advantage. We illustrate the pragmatic value of our proposed analytics approach by carrying out a competitive analysis of a focal app and selected competing apps. We also put forward managerial guidelines for new user prediction in mobile targeting. Finally, as advertisers raise spending on mobile advertising, they select optimal mobile media vehicles (i.e., mobile apps and websites) to maximize their advertising exposure and thereby stimulate desired consumer economic behaviors. We describe the strategic value of our quantitative paradigm and mechanism in maximizing the consumption of apps through mobile media planning in the app-based economy.\n",
              "\n",
              "Theoretical Background\n",
              "User Behavior on Mobile Platforms and Apps\n",
              "Research on mobile platforms and apps has proliferated in consonance with their increasing use. Ghose and Han (2011) investigate user behavior on the mobile Iternet by mapping the interdependence between the generation and usage of mobile content. The authors find negative temporal interdependence; the more frequent the consumption of mobile content in the previous period, the less the content generated in the current period or vice versa. Ghose et al. (2013) report that ranking and geographical proximity exhibit a more pronounced influence on mobile devices than on PCs. Einav\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "985\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "et al. (2014) find that the adoption of mobile shopping applications is associated with both an immediate and sustained growth in overall platform-based purchase. Xu et al. (2016) show that users’ adoption of tablets enhanced the overall growth of Alibaba’s e-commerce market, with an annual increase of approximately U.S. $923.5 million. Our study also builds on an evolving body of literature on mobile apps. Carare (2012), for instance, employs a reducedform model and Apple App Store data to delineate user behavior on mobile apps. The author reveals that app consumers are willing to pay an additional $4.50 for top- ranked apps, but that their preference for apps with bestseller status sharply declines for highly ranked products. A recent study by Xu et al. (2014) demonstrates that the introduction of a mobile app by a major national media company significantly increases demand on the corresponding mobile news website. Using publicly available data from Apple’s App Store, Garg and Telang (2013) discover that top-ranked paid apps available for the iPhone elicit 150 times more downloads than do other apps ranked in the top 200. Ghose and Han (2014) discover that app demand increases with the introduction of an in-app purchase option, which enables users to complete transactions within an app. A thorough review indicates that the current literature on mobile apps focuses exclusively on demand in the form of downloads or paid purchases, but pays scant attention to the actual choices and consumption patterns regarding mobile apps.\n",
              "\n",
              "Mobile users’ app usage decisions can be broadly broken down into two choices; which apps to select and how extensively to consume them. Because multiple discretecontinuous choice models tackle both problems within a single utility maximization framework, they are appropriate for analyzing our mobile app and web time-use data. Kim et al. (2002) propose a translated nonlinear, addictive utility model wherein a parsimonious specification provides both corner and interior solutions under the simultaneous purchase of multiple product varieties. A multiple discrete-continuous choice model formulated by Bhat (2005, 2008) extends single discrete-continuous frameworks (e.g., Chiang 1991; Chintagunta 1993; Dubin and McFadden 1984; Hanemann 1984) to include handling multiple discreteness in demand and resolve the heteroscedasticity and correlations that arise from unobserved characteristics. Among various applications, multiple discrete-continuous choice models have frequently been employed in analyzing time-use data. Bhat (2005) scrutinizes time-use allocation decisions among several discretionary activities on weekends. Drawing from Bhat’s multiple discretecontinuous time-use model, Spissu et al. (2009) probe into within-subject variations over a 12-week sample period for six broad activity categories along with another activity. Luo et al. (2013) incorporate dynamic components into a multiple discrete-continuous choice model to examine how consumers allocate time to a portfolio of leisure activities over time. In the present study, we develop a unique structural model of app selection and time-use decision by incorporating a factor analytic structure into a multiple discrete-continuous choice framework. The vectors of individual-level baseline utilities and satiation parameters are modeled as functions of observed mobile user characteristics and a small number of unobservable user-specific factors. In the literature, factor analytic structures are combined with probit or logit models, whose principal objective is to understand inter-brand competition by pictorially depicting the locations of competing brands on a perceptual map (Chintagunta 1994; Elrod and Keane 1995). Our approach methodologically extends the multiple discretecontinuous choice models by allowing for correlations in both the baseline utilities and satiation levels of various mobile app categories. The correlations are taken into account in a parsimonious manner by using factor analytic approaches.\n",
              "\n",
              "Multiple Discrete-Continuous Choice Models\n",
              "To capture the dependence between choice and quantity decisions, researchers have proposed several analytics approaches, including the structural single utility (e.g., Chiang 1991; Chintagunta 1993; Hanemann 1984; Kim et al. 2002) and the error-dependence (or reduced-form) methods (e.g., Krishnamurthi and Raj 1988; Tellis 1988; Zhang and Krishnamurthi 2004). The former specifies a utility function and assumes that the optimal choice and quantity are derived as an equilibrium solution from the utility function; that is, the dependence between choice and quantity decision is reflected by this function. The error-dependence technique handles dependence by allowing for correlations in the error terms of choice and quantity models. A major advantage of the single utility approach is that it enables researchers to estimate structural parameters and metrics of economic interest (e.g., compensating variation). The proposed model based on the multiple discrete-continuous choice method is categorized as a single utility approach.\n",
              "\n",
              "Empirical Background and Data Description\n",
              "Panel Data on Mobile App and Web Time-Use\n",
              "This section presents an overview of the empirical background behind our data. We collected large-scale panel data\n",
              "\n",
              "986\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "comprising the mobile app and website time-use histories provided by Nielsen KoreanClick, a market research company that specializes in consumers’ Internet and mobile usage. Audience measurement is designed to gauge the proportion of users in a given audience and how long they remain attentive, usually in relation to television viewership (e.g., Nielsen ratings), but also with respect to increasing traffic on websites and mobile application platforms. Nielsen KoreanClick maintains a panel of mobile app users with Android operating system-based devices in Korea. These users are aged 10 to 70 and are selected by stratified sampling.4 Android is the dominant operating system of mobile devices worldwide, accounting for 67.5% of the global market. In Korea, nearly 93% of smartphones are powered by the Android platform (Yonhap News 2014). After individuals voluntarily agree to serve in the panel, they are asked to download and install a meter application from Nielsen KoreanClick in their mobile devices.5 The participants are rewarded points for installing the meter app, and the incentive points can be accumulated and redeemed for gift cards. The app runs in the background and collects data on the panel members’ use of mobile apps and the mobile web, even when they are not connected to the Internet.6 The meter app regularly transmits encrypted log files to a server via a secure cellular connection or Wi-Fi. We also acquired individual user-level demographics, such as age, gender, monthly income, and education. Between March 5 and July 1, 2012 (17 weeks), we collected data on 1,366 panel members who used mobile apps and the web throughout the sampling period. Our data incorporates individual-level weekly information on the types and names of mobile apps and websites, as well as on duration of visits. Nielsen KoreanClick classified the mobile content under 14\n",
              "4\n",
              "\n",
              "activity categories; communication, game, map and navigation, entertainment, lifestyle, personal finance, music and radio, photo, portal, schedule and memo, social networking, utility, video, and combined mobile web activities.7 The panel members used 11,548 apps and visited 8,043 websites (see Appendix A for details).8 The apps in our sample include not only top global mobile apps such as Facebook, YouTube, Twitter, and Skype, but also top local mobile apps such as Kakao Talk, Naver, and Cyworld. Our data contain 23,222 (1,366 users × 17 weeks) app choice occasions. Column 1 of Table 1 shows that the apps most frequently used by the panel members are communication apps (99.4%), followed by schedule/memo apps (98.8%), utility apps (97.8%), and photo apps (90.0%). The least frequently accessed applications are the entertainment apps (38.0%). Column 2 of Table 1 reveals that the smartphone users in our sample devoted an average of 12 hours and 4 minutes every week (approximately 2 hours every day on average) to consuming content via their mobile devices.9 Weekly consumption on mobile apps surpasses that on the mobile web. A more specific illustration is provided in Figure 1. The users spent the largest amount of time (26%) on communication apps (e.g., mobile messengers), followed\n",
              "7\n",
              "\n",
              "To check the robustness of our results, we excluded users whose age is less than 20 years old or greater than 50 years old. The sample size reduced to approximately 85% of the original sample. Results show that our main results remain robust even when excluding the users in young or old groups. We did not include the results of this subsample analysis in the paper to conserve space, but they are available upon request from the authors.\n",
              "\n",
              "Google Play is a leading app market based on the Android operating system. Notably when publishing a new app or a new version of an existing app on Google Play, app developers self-select one or more appropriate app categories; however, they are not required to go through the verification process. In some cases, therefore, the app categories reported by app developers are incorrect or inconsistent. To address this issue, Nielsen KoreanClick performed a thorough, manual reclassification to ensure that a certain app is classified under a single, primary app category. We used Nielsen Korean Click’s categorization in our empirical analysis. If the categorization of apps were arbitrary, deriving meaningful results from the empirical analysis would be difficult. Our empirical analysis shows highly significant and meaningful results (refer to the “Results” section), thus endowing empirical validity to the categorization. An alternative categorization can be adopted in the proposed empirical framework. We performed an empirical analysis using the individual app-level data set in the “Applications of Proposed Approaches” section. The number of mobile websites visited was aggregated to the domain level. That is, multiple URLs with the same domain name were counted as one. For example, two different URLs with the common domain names (https://www. google.com/maps/preview and https://www.google.com/calendar/render) were treated as one website when we counted the number of websites visited. The mobile app and web usage amounts in our Korean sample resemble closely those in the U.S. market. According to Flurry (2013), a global leader in mobile analytics solutions, the average U.S. mobile consumer spends an average of 2 hours and 38 minutes per day on smartphones and tablets, and 80% of that time (2 hours and 7 minutes) is spent inside apps and 20% (31 minutes) is spent on the mobile web. This helps enhance the external validity of our substantive findings. However, our proposed framework can be readily applied to different data from other geographical users and different sample periods.\n",
              "\n",
              "8\n",
              "\n",
              "5 Nielsen’s metering app collects an exact start time when a user opens an app and measures how long s/he is actively using it until the app is closed or another app is activated. So it captures the engaged time in apps by measuring the amount of time a user actively consumes an app content. Further, a distinctive advantage of this measurement approach over the previous metering method is that the metering app can effectively measure the usage time of communication and news apps, which are often used for short bursts of time (e.g., 10 seconds). 6\n",
              "\n",
              "9\n",
              "\n",
              "Mobile web refers to the collective term for websites accessed from mobile devices by using browsers. Thus, this term is often used interchangeably used with “mobile browser.”\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "987\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 1. Choice and Time Use According to App Categories\n",
              "Time Use Categories Outside Option (other activities) Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music/Radio Photo Portal Schedule/Memo Social Network Utility Video Web Total Choice 100.0% 99.4% 63.9% 67.6% 38.0% 67.4% 60.2% 67.0% 90.0% 74.9% 98.8% 72.3% 97.8% 77.0% 88.3% Hours per Week 155.83 3.15 1.53 0.21 0.26 0.29 0.29 1.21 0.31 0.64 0.67 0.81 0.53 0.66 1.61 168.00 (24 hours × 7 days) Percent 92.8% 1.9% 0.9% 0.1% 0.2% 0.2% 0.2% 0.7% 0.2% 0.4% 0.4% 0.5% 0.3% 0.4% 1.0% 100.00%\n",
              "\n",
              "Figure 1. Time Use According to App Categories Excluding Outside Option\n",
              "\n",
              "by the mobile web (13%), game apps (13%), music and radio apps (10%), social apps (7%), and schedule/memo apps (6%). In the “Results” section, we show that our findings on app category baseline utilities and satiation levels, derived using the proposed model differ from those reported in Table 1 and Figure 1.\n",
              "\n",
              "Model-Free Evidence of Dependence Between Mobile Content Choice and Time-Use Decisions\n",
              "As mobile users increasingly access the mobile web and various kinds of apps, their choices become interdependent. Figure 2 indicates that all of the users accessed at least two\n",
              "\n",
              "988\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Figure 2. Distribution of the Number of Jointly Used App Categories\n",
              "\n",
              "categories of mobile content during a given week. Approximately 98.3% of the users accessed more than four categories of mobile content in a given week. These descriptive findings on the joint use of multiple mobile content categories strengthen the validity of our econometric model, in which we incorporated multiple-discrete choices into continuous timeuse decisions. Drawing inferences on time-use decisions across multiple app categories through simple methods is often challenging. We did not observe usage time on app categories that were not selected by the panel members. Our weekly data indicates that only 6.1% of usage was devoted to all 14 categories of mobile content. Employing simple metrics instruments (e.g., a correlation matrix) therefore necessitates aggregating the observed weekly data into a monthly or quarterly series or discard observations with zero-time use. To attribute zero time use to non-app use, a correlation matrix of app choice incidence can be employed using dummy variables that take the value of 1 if an app is used and 0 otherwise. Similarly, a correlation matrix of app time-use quantity can be established by assigning non-incidence with a zero value. These approaches are inferior to the proposed method because they do not fully leverage available information. The correlation matrix of app choice incidence variable and the correlation matrix of app time-use quantity variable (see Appendix B, Tables B1 and B2) indicate that the observed relationships among app use incidences and app use time are mostly positive but marginal. We found substantial positive relationships among communication, utility, and schedule/ memo categories. However, we could not find any substantial negative correlation. In the “Results” section, we demonstrate that the findings derived with the proposed model drastically\n",
              "\n",
              "differ from what the model-free correlation matrix reveals, indicating that simple methods can generate misleading results in the evaluation of correlation across mobile content categories. We discuss our modeling approach in detail below.\n",
              "\n",
              "Econometric Model\n",
              "In this section, we present our proposed model to estimate the baseline utility and satiation levels of different categories of mobile web and apps while allowing for user heterogeneity and cross-app use interdependence even when the number of app categories is large. We then discuss how we identify our demand system.\n",
              "\n",
              "Proposed Model\n",
              "Consumer Utility Function We observe which app categories are chosen and how much time is spent on each selected app category in our data. Accordingly, we describe a mobile user’s behavior by virtue of a multiple discrete-continuous choice process. Compared to discrete choice models (i.e., Logit or Probit models) and continuous dependent variable models, multiple discretecontinuous choice models can handle more effectively both discrete and continuous natures of observed data within a single utility-based framework. Because of this advantage, multiple discrete-continuous choice models have successfully been applied in several academic fields, including marketing, transportation, and economics (Bhat 2008; Hendel 1999; Kim\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "989\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "et al. 2002). In this paper, we extend Bhat’s (2008) multiple discrete-continuous extreme value (MDCEV) framework. We specify the latent utility of mobile content usage as follows:\n",
              "U ht =\n",
              "α0\n",
              "1\n",
              "\n",
              "α0 ⋅ exp(εh 0t ) ⋅ qh 0t +\n",
              "J 1 j = 1 α hjt\n",
              "\n",
              "\n",
              "\n",
              "⋅ μhjt ⋅ qhjt + 1\n",
              "\n",
              "{(\n",
              "\n",
              ")\n",
              "\n",
              "α hjt\n",
              "\n",
              "−1\n",
              "\n",
              "}\n",
              "\n",
              "(1)\n",
              "\n",
              "where h = 1, …, H denotes individual mobile users, j = 0 and j = 1, …, J denote an outside option (activities other than mobile content use) and mobile content use categories, respectively, and t = 1, …, T denotes time period (weeks). qhjt (j = 0, …, J) is time allocated to alternative j by user h in time period t. This specification is referred to as “alpha-profile” in the literature (for further details on model specification and other possible specifications, see Bhat 2008). εhjt is a user-, alternative-, and time-specific random term associated with the outside option.10 μhjt represents the “baseline marginal utility” (the marginal utility when none is consumed) of alternative j in time t by user h. When a user decides which category to use first, categories with large value of μhjt have higher probabilities of being selected compared to those with small μhjt. Also, it can be interpreted as a measure of “perceived quality” because higher values of μhjt mean that the alternative confers higher levels of utility from any level of consumption, all else equal. In addition, α0 and αhit are referred to as satiation parameters in that they determine how the marginal utility of alternative j changes as their consumption quantity increases. It is noteworthy that the satiation parameter captures the marginal rate of substitution among different options within a given time period and does not capture any dynamic effects.11 To illustrate the role of the satiation parameter, let us assume that there are only two choice options—the outside option (j = 0) and alternative 1 (j = 1) – and that the satiation parameters are fixed to one (i.e., α0 = 1 and αh1t = 1). Then, the utility function (1) becomes Uht = exp(εh0t) @ qh0t + μh1t @ qh1t. The marginal utilities of the outside option and the alternative 1 are equal to exp(εh0t) and μh1t, respectively. It should be noted that the marginal utility values do not change with consump10\n",
              "\n",
              "tion quantities, resulting in a linear indifference curve (see case (a) in Figure 3). In this case, user h allocates all her time to the category with the highest perceived quality. If exp(εh0t) > μh1t (exp(εh0t) < μh1t), as shown in Figure 3(a), the user can attain the highest level of utility by spending all her time for the outside option (alternative 1) and thus select the outside option (alternative 1) only. That is, qh0t = Q and qh1t = 0 (qh0t = 0 and qh1t = Q). Note that this linear indifference curve implies that the two alternatives are perfect substitutes. Now, let us consider a more realistic case where α0 < 1 and αhit < 1. Then the utility function (1) shows diminishing marginal utility. As αhit decreases, the utility function in alternative j shows more concave patterns, and higher satiation occurs at a lower value of qhit. Due to this diminishing marginal utility, multiple alternatives can become comparable to each other and they will be chosen together rather than only one option is selected (see case (b) in Figure 3). In this case, user h consumes the outside option and alternative 1 by qh0t = ˆ ˆ q h0t > 0 and qh1t = qh1t > 0, respectively. According to Bhat (2008), Uht becomes a proper utility function when uhjt > 0 and αhjt < 1for j = 1,…, J. To ensure that the baseline utility is nonnegative and the satiation parameter is less than 1 regardless of the model parameter values, we specify the baseline utility parameter μhjt and the satiation parameter αhjt as the following:\n",
              "\n",
              "α hjt = 1 − exp(λhjt ), for j = 1, , J\n",
              "\n",
              "μhjt = exp(βhjt + ε hjt ), for j = 1, , J\n",
              "\n",
              "(2)\n",
              "\n",
              "εhjt represents idiosyncratic elements in utility. Both εh0t and εhjt are known to decision makers, but unknown to researchers. We assume that these follow Type-I Extreme Value distribution. User Heterogeneity, Dynamics, and Correlation among Mobile Web and App Categories For user-, alternative-, and time-specific βhjt, we specify the factor analytic structure\n",
              "\n",
              "The utility function can be classified as a generalized variant of the translated constant elasticity of substitution (CES) utility function. This specification assumes additively separable utility. We discuss a limitation of this model specification in detail later in the “Conclusions.” Note that the utility specification of the outside option (the first term in the right hand side of (1)) is different from those of the other alternatives and this is for the utility function to capture that the outside option is always consumed.\n",
              "11 We capture dynamic effects in mobile app choice and usage with structural state dependence and common time trends. We elaborate on these in the next section.\n",
              "\n",
              "βht = β + Ξ β ⋅ t + Σ β ⋅ SDht\n",
              "+ Π β Dh + Γβψ h + Λ β υh\n",
              "\n",
              "(3)\n",
              "\n",
              "¯ where a (J × 1) vector βht = [βh1t, βh2t, …, βhJt]' and β is a (J × 1) constant vector. t is time index (t = 1, …, T) and Ξβ is a (J × J) vector of time trend coefficients. A (J × 1) vector SDht = [SDh1t, SDh2t, …, SDhJt]' and SDhjt takes the value of one if the alternative j is selected at t-1 and takes the value of\n",
              "\n",
              "990\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Figure 3. An Example of How Diminishing Marginal Utility (Satiation) Affects Multiple Discrete Choices\n",
              "\n",
              "zero otherwise. Gβ is a (J × J) diagonal matrix of the statedependence coefficients. Dh is the (K × 1) vector of observed demographic variables of user h and Πβ is a (J × K) coefficient matrix. Γβ is a (J × F) factor loading matrix and ψh is a (F × 1) vector of orgthogonal Gaussian factors (ψh ~ N(0, IF)). Λβ is a (J × J) diagonal matrix and υh is a (J × 1) vector of independent unit-variance Gaussian random variables (υh ~ N(0, IJ)). The proposed specification decomposes dynamics and heterogeneity in mobile content choice utility into five parts: (1) time trend; (2) structural state dependence; (3) demographic influence; (4) common factor; and (5) specific factor. The first is the category-specific time trend in baseline utility captured by Ξβ @ t. The impact of category-specific time trend is the same for all mobile users and thus this captures the influences of common demand shifter (e.g., common high demand for weather apps in storm seasons). A mobile user’s prior app choice and usage experience typically influence her decision in the future. In our model, we capture this using the lagged choices SDhjt. Unlike the time trend, this dynamic effect is individual specific and is referred to as “structural state dependence” in the literature. Structural state dependence can be positive or negative, in which cases they are called “inertia” (Jeuland 1979) and “variety seeking” (McAlister 1982), respectively. Variety seeking comes into play when consumers get bored with the same choice over time. Inertia could result from satisfaction obtained from the choice, learning and reinforcement effect, and/or decision makers’ need to routinize behavior so as to minimize the cost of thinking. Several empirical studies have reported that ignoring dynamics in consumer choice can result in biased estimates. Also, it is reported that structural state dependence, along with unobserved heterogeneity, captures most of the observed temporal dynamics in consumers’ product choices (Keane 1997; Seetharaman 2004). The influence of demographic variables on the app baseline utility is captured by ΠβDh in (3). The remaining variations in\n",
              "\n",
              "the baseline utility is captured by parsimonious common factors. In factor analysis literature, ψh is referred to as a “common factor.” F elements in ψh influence all J elements in βht. We can understand the main characteristics of the factors by interpreting the factor loading matrix Γβ. Also, it should be noted that, along with Σβ @ SDht and ΠβDh, these common factors generate correlations in βht. The last term in equation (3), Λβυh, is referred to as a “specific factor.” Unlike ψh, the jth element in υh influences βhjt only. The factor analytic structure is of interest in our empirical setting for a number of reasons. First, a factor model allows us to estimate category similarity in unobserved attributes (Elrod and Keane 1995). We can potentially interpret the factors as inherent mobile users’ traits. Moreover, the userspecific factor estimates can be used for targeting purposes. Second, the factor model introduces correlations in the latent baseline utilities across app categories with relatively few parameters. This characteristic is particularly useful in our context to alleviate the concern for dimensionality issues inherent in estimating unobserved heterogeneity and their interdependence when the number of app categories (i.e., alternatives) is large. We can reduce the number of parameters required to estimate a full covariance matrix in a linearly scalable manner while remaining highly flexible and minimizing loss of information. Because of these major advantages, researchers have applied factor analytic structure in random coefficient Logit or Probit models (Elrod and Keane 1995; Hansen et al. 2006; Singh et al. 2005). A key methodological contribution of our proposed model is that it extends factor analytic structure to multiple discrete-continuous choice models. From equation (3), we can derive the following covariance matrix of βht:\n",
              "Cov(β ht ) = Σ β Θ SD Σ ′ β + Πβ Ω DΠ′ β + Γβ Γβ′ + Λ β Λ ′ β\n",
              "\n",
              "(4)\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "991\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "where ΘSD and ΩD are covariance matrices of SDht and Dh, respectively. The variance decomposition of equation (4) allows us to quantify the relative contribution of each part. For example, the proportion of variation in βhjt explained by observed demographic variables is (jth diagonal element of Πβ ΩD Πβ ' )/(jth diagonal element of Cov(βht)). Further, the value of user-, alternative-, and time-specific satiation parameter αhjt is determined by λhjt. Similar to equation (3), we specify the following factor analytic model structure to λhjt:\n",
              "\n",
              "and exp(ε h 0t ) ⋅ q h 0t a −1 − δht = 0 since the outside option is always consumed (i.e., time used for activities other than mobile app and web uses is nonzero). We use the expression for δht from the first-order condition for the outside option to eliminate the Lagrange multiplier from (7) and then take log in both sides, so the Kuhn-Tucker conditions for the interior and corner solutions can be written, respectively, as\n",
              "0\n",
              "\n",
              "βhjt + (α hjt − 1) ⋅ ln(qhjt + 1) + ε hjt = βhjt + (α hjt − 1) ⋅ ln(qhjt + 1) + ε hjt <\n",
              "\n",
              "(α 0 − 1) ⋅ ln(qh0t ) + ε h0t , if qhjt > 0 (α 0 − 1) ⋅ ln(qh0t ) + ε h0t , if qhjt = 0\n",
              "\n",
              "λht = λ + Ξ λ ⋅ t + Σ λ ⋅ SDht + Π λ Dh + Γλ ϕ h + Λ λ ν h\n",
              "\n",
              "(9)\n",
              "\n",
              "(5)\n",
              "\n",
              "¯ where a (J × 1) vector λht = [λh1t, λh2t, …, λhJt ]' and λ is a (J ×1) constant vector. Ξλ, Gλ, and Πλ are (J × 1), (J × J), and (J ×K) coefficient matrices, respectively. Γλ is a (J × F) factor loading matrix and φh is a (F × 1) vector of orthogonal Gaussian factors (φh ~ N(0, IF)). Λλ is a (J × J) diagonal matrix and νh is a (J ×1) vector of independent Gaussian random variables (νh ~ N(0, IJ)). The covariance matrix of λht can be decomposed as follows:\n",
              "\n",
              "From this Kuhn-Tucker first order condition, we derive the following probability that any M of the J alternatives are chosen (for details, see Chapter 4 of Bhat 2008):\n",
              "Pht (qh 0t , qh1t ,, qhMt ,0,0, ,0)  M 1 − αhjt   M qhjt + τ j  =  M !⋅   ∏ j =0 q + τ   ⋅   j =0 1 − α   Θh  hjt j  hjt  ⋅\n",
              "\n",
              "(10)\n",
              "\n",
              "Cov(λht ) = Σ λ Θ SD Σ ′ λ + Π λ Ω DΠ′ λ + Γλ Γλ′ + Λ λ Λ ′ λ\n",
              "\n",
              "(6)\n",
              "\n",
              "∏ ( e\n",
              "\n",
              "M\n",
              "\n",
              "i =0\n",
              "\n",
              "eVhit\n",
              "\n",
              "J Vhjt j =0\n",
              "\n",
              ")\n",
              "\n",
              "M +1\n",
              "\n",
              "dF (Ψh )\n",
              "\n",
              "Model Estimation and Identification\n",
              "Constrained Utility Maximization and Estimation We derive our demand system by applying the Kuhn-Tucker method to the latent utility of mobile content use specified in equation (1). By solving the Kuhn-Tucker conditions for constrained utility maximization, we obtain demand functions wherein a mixture of corner solutions and interior solutions are a product of the underlying utility structure. The Lagrangian for the constrained utility maximization problem is given by\n",
              "\n",
              "Lht =\n",
              "\n",
              "{(q\n",
              "\n",
              "1 α0\n",
              "\n",
              "α0 1 ⋅ exp(εh 0t ) ⋅ qh 0 t +  j = 1 α hjt ⋅ μhjt ⋅\n",
              "J hjt\n",
              "\n",
              "+1\n",
              "\n",
              ")\n",
              "\n",
              "ahjt\n",
              "\n",
              "− 1 + δht Q −  j = 0 qhjt\n",
              "J\n",
              "\n",
              "} (\n",
              "\n",
              ")\n",
              "\n",
              "(7)\n",
              "\n",
              "where δht is the Lagrange multiplier, and Q denotes total amount of time given to each mobile user (i.e., 168 hours per week). The Kuhn-Tucker first order conditions can be derived as follows:\n",
              "\n",
              "where Vh0t = (α0 –1) @ ln(qh0t), Vhjt = βhj + (αhjt – 1) @ ln(qhjt + 1) for j = 1, ..., J, τ0 = 0, and τj =1 for j = 1, ..., J. Ψh is a vector of common factors and specific factors in αhjt and βhjt (ψh, υh, φh, and νh) and F(Ψh) is a joint distribution function of Ψh. We use Monte Carlo simulation methods to calculate the probability (10) and estimate the model parameters by maximizing a likelihood function derived from equation (10) (for details, see Keane 1993). To compute the integrals in the likelihood function, we generate random normal draws for ψh, υh, φh, and νh from their distributions, and then take averages of computed integrands. In our application, we use 100 Halton draws. The resulting estimator becomes a simulated maximum likelihood (SML) estimator. This procedure is the same as maximum likelihood except that simulated probabilities are used instead of the exact probabilities (see Appendix C for detailed instructions on model estimation). Researchers (e.g., Keane 1993) have articulated thoroughly the properties of SML (i.e., its consistency, efficiency, and asymptotic normality). Identification Bhat (2008) discusses the identification issues of general multiple discrete-continuous choice models and shows the\n",
              "\n",
              "μhjt ⋅ (qhjt + 1)\n",
              "\n",
              "μhjt ⋅ (qhjt + 1)\n",
              "\n",
              "α hjt − 1 α hjt − 1\n",
              "\n",
              "− δ ht = 0, if qhjt > 0 − δ ht < 0, if qhjt = 0\n",
              "\n",
              "(8)\n",
              "\n",
              "992\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "empirical identifiability of “alpha-profile” specification, which is adopted in the proposed model. A distinctive feature of our model is that it incorporates the Gaussian factor analytic structure into a multiple discrete-continuous choice model. It should be noted that our Gaussian factor specification is the most general and widely used specification (Elrod and Keane 1995; Hansen et al. 2006; Singh et al. 2005), but other distributional assumptions also can be used. For the identification of model parameters, appropriate restrictions on factor loading matrices are required. Following Singh et al. (2005) and Hansen et al. (2006), we impose a standard triangular restriction on a loading matrix. It is widely known that this approach imposes the minimum restriction for the parameter identification. Specifically, with two factors, one of the elements in the second column of factor loading matrix is restricted to be equal to zero. With three factors, a 3 × 3 submatrix of factor loading matrix is lower triangular. We estimate zero-, one-, two-, three-, and four-factor versions of the model specified in equations (3) and (5).12 The log-likelihood values for the zero-, one-, two-, three-, and four-factor models are, -176,152, -174,366, -173,784, -172,662, and -172,589, respectively. To determine the number of factors, we use the Bayesian Information Criterion (BIC). The BIC values for the zero-, one-, two-, three-, and four-factor models are 355,452, 352,120, 351,195, 349,192, and 349,265, respectively. In our case, the threefactor model is chosen.13 The estimation results of the other specifications are available upon request. Accordingly, we report and discuss the results for the three-factor model below.\n",
              "\n",
              "and their baseline utility for personal finance apps is the lowest among the various mobile web and app categories. The highest baseline utility for utility apps can be attributed to the routine use of utility apps such as alarm clock apps. The lowest baseline utility for personal finance apps suggests ¯ that they remain a niche market. It should be noted that as β ¯ increases the baseline utility increases. The sign for β estimates for all types of mobile content is negative due to the relatively higher utility of outside options (e.g., mobile users spend more than 155 hours weekly—approximately 22 hours daily—engaging in activities other than mobile content use), which is normalized to zero for model identification (βh0 = 0). We can also interpret the degree of baseline utility in terms of ¯ because the sign of the estimates is reversed. However, the μ relative magnitude of estimates remains unchanged after the exponential transformation specified in equation (2). Furthermore, among several demographic variables, age, gender, and education account for substantial heterogeneity in baseline utilities across mobile users. For example, senior users show a lower intrinsic preference for entertainment, social network, photo, and game apps in comparison with young users. In addition, women exhibit a higher intrinsic preference for photo and communication apps and a lower intrinsic preference for entertainment apps in comparison with men. Users with high education exhibit lower baseline utilities in most categories. Moreover, although users exhibit inertia in their app choices in all categories, they show stronger tendencies of inertial use of personal finance, game, and social network apps than other app categories, indicating that users have routinized their choices for these app categories. Finally, despite their small effect size, there is an increasing time trend of baseline utility for all app categories. Table 3 suggests that the satiation level is the highest in the portal search app category and the lowest in the communi¯ cation app category. As λ increases, the satiation effect also increases. The highest satiation for portal search apps implies that users access these apps quickly (e.g., having a quick search using mobile apps). In contrast, the lowest satiation for communication apps suggests that users tend to continue communicating via mobile apps without growing tired of them. Further, there exists substantial user heterogeneity in terms of satiation levels. For example, as age increases, satiation with music/radio and communication apps increases, while satiation with personal finance, map/navigation, and schedule/memo apps simultaneously decreases. In addition, women show significantly lower satiation levels for photo, social network, and communication apps than men do. Users with high education show significantly higher satiation levels regarding entertainment, communication, and social networking apps. Moreover, although users exhibit inertia in their app consumption in all categories, they show stronger tendencies of inertial use of portals search and game apps and\n",
              "\n",
              "Results\n",
              "Baseline Utility and Satiation Levels for Mobile Web and App Categories Tables 2 and 3 show the estimates for the baseline utility ¯ ¯ , Ξβ, Gβ, and Πβ) and the satiation parameters (λ , parameters (β Ξλ, Gλ, and Πλ), respectively. The results in Table 2 indicate that mobile users’ baseline utility for utility apps is the highest\n",
              "The zero-factor has no common factor and thus the correlations among random shocks are all set to zero. Unfortunately, we were unable to estimate a model with the full variancecovariance matrix and use it as an additional benchmark. The estimation of the variance-covariance of dimension of 14 in random coefficients seems almost impossible because of empirical difficulties. As the number of factors increases, the estimated variance-covariance matrix approaches the unstructured full variance-covariance matrix. We observe that the BIC of four-factor model is higher than that of three-factor model. This indicates that the selected three-factor model is arguably better than a full variance-covariance model in describing the data adequately without using too many parameters.\n",
              "13 12\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "993\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 2. Estimates for Baseline Utility Parameters\n",
              "Demographic Variables( Πβ) Education Education HighIncome Income University School UpperMidGraduates Graduates class class 0.30 -0.02 0.05 -0.05 -0.09 (0.03) (0.03) (0.03) (0.05) (0.04) 0.02 -0.13 -0.20 -0.04 -0.17 (0.03) (0.03) (0.03) (0.05) (0.04) 0.01 -0.03 0.02 -0.22 -0.13 (0.02) (0.03) (0.03) (0.05) (0.04) -0.11 -0.09 -0.15 -0.20 -0.31 (0.03) (0.03) (0.04) (0.06) (0.04) 0.13 -0.02 -0.08 -0.32 -0.22 (0.02) (0.03) (0.03) (0.05) (0.04) 0.12 0.00 0.01 -0.02 0.03 (0.03) (0.03) (0.03) (0.05) (0.04) 0.10 -0.03 -0.06 -0.19 -0.33 (0.02) (0.03) (0.03) (0.05) (0.04) 0.33 -0.06 -0.04 -0.24 -0.25 (0.02) (0.03) (0.03) (0.05) (0.04) 0.13 -0.07 -0.06 -0.17 -0.19 (0.02) (0.03) (0.03) (0.05) (0.04) 0.13 -0.10 -0.03 -0.02 -0.02 (0.03) (0.03) (0.03) (0.05) (0.04) 0.18 -0.05 -0.03 -0.14 -0.19 (0.02) (0.03) (0.03) (0.05) (0.04) 0.04 -0.07 -0.10 -0.06 -0.24 (0.02) (0.03) (0.03) (0.05) (0.04) 0.04 -0.05 -0.07 -0.06 -0.17 (0.02) (0.03) (0.03) (0.05) (0.04) 0.03 -0.06 -0.08 -0.08 -0.17 (0.02) (0.03) (0.03) (0.05) (0.04) Female State Dependence 0.82 (0.12) 1.80 (0.03) 1.06 (0.02) 1.70 (0.03) 1.35 (0.02) 1.87 (0.03) 1.22 (0.02) 0.77 (0.03) 1.18 (0.03) 0.59 (0.08) 1.76 (0.03) 0.52 (0.06) 0.85 (0.02) 1.11 (0.03)\n",
              "\n",
              "Constant ¯ (β ) -20.05 Communication (0.29) -21.87 Game (0.27) -21.17 Map/Navigation (0.27) -21.94 Entertainment (0.27) -21.44 Lifestyle (0.27) -22.20 Personal Finance (0.27) -21.15 Music/Radio (0.27) -20.46 Photo (0.27) -21.38 Portal (0.27) -20.07 Schedule/Memo (0.28) -21.69 Social Network (0.27) -19.79 Utility (0.27) -21.03 Video (0.27) -20.96 Web (0.27)\n",
              "\n",
              "Age 30's -0.31 (0.04) -0.15 (0.03) -0.38 (0.03) -0.40 (0.04) -0.25 (0.03) -0.29 (0.03) -0.51 (0.03) -0.29 (0.03) -0.14 (0.03) -0.20 (0.03) -0.33 (0.03) -0.36 (0.03) -0.15 (0.03) -0.23 (0.03)\n",
              "\n",
              "Age 40's & over -0.21 (0.04) -0.35 (0.04) -0.42 (0.04) -0.58 (0.04) -0.30 (0.04) -0.36 (0.04) -0.61 (0.04) -0.45 (0.04) -0.22 (0.04) -0.11 (0.04) -0.57 (0.04) -0.35 (0.04) -0.24 (0.04) -0.27 (0.04)\n",
              "\n",
              "0.013 (0.003) 0.011 (0.003) 0.008 (0.002) 0.015 (0.003) 0.009 (0.003) 0.016 (0.003) 0.010 (0.003) 0.014 (0.002) 0.013 (0.002) 0.008 (0.003) 0.023 (0.003) 0.008 (0.003) 0.016 (0.002) 0.006 (0.002)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. Following Bhat (2008), α0 is bounded between 0 and 1. The estimated value α0 of is -5.77 with a standard error of 0.02. To code discrete demographic variables, we use the following as a reference level: we use age 20’s or less for age and male for gender, respectively. For monthly income, we create two dummy variables, mid-class ($3,000–$5,000) and upperclass ($5,000 or over), and we use lower-class ($3,000 or less) as a reference level. Similarly, for education, we create two dummy variables, high school graduates and university graduates, and we use students (elementary, middle-and-high school students, university students) as a reference level.\n",
              "\n",
              "994\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "Time Trend\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 3. Estimates for Satiation Parameters\n",
              "Demographic Variables (Πβ) Education Education HighIncome Income University School UpperMidGraduates Graduates class class -0.22 -0.03 -0.03 0.24 0.23 (0.01) (0.02) (0.02) (0.02) (0.02) 0.03 0.14 0.07 0.02 0.09 (0.02) (0.02) (0.03) (0.04) (0.03) 0.20 -0.08 -0.04 0.17 0.02 (0.02) (0.03) (0.03) (0.04) (0.03) 0.14 0.04 -0.04 0.25 0.36 (0.03) (0.04) (0.04) (0.06) (0.05) 0.03 0.04 0.12 -0.02 -0.05 (0.02) (0.03) (0.03) (0.04) (0.03) 0.05 0.00 -0.19 0.08 0.05 (0.02) (0.03) (0.03) (0.04) (0.04) 0.03 0.00 -0.06 -0.13 0.02 (0.02) (0.03) (0.03) (0.04) (0.04) -0.47 0.06 0.15 0.19 0.18 (0.02) (0.02) (0.02) (0.03) (0.02) -0.01 -0.08 -0.07 0.18 -0.02 (0.02) (0.03) (0.03) (0.05) (0.04) 0.09 0.01 0.04 -0.03 0.02 (0.01) (0.02) (0.02) (0.03) (0.02) -0.39 0.03 0.20 0.15 0.19 (0.02) (0.02) (0.02) (0.04) (0.03) 0.09 0.05 0.14 0.14 -0.01 (0.02) (0.02) (0.02) (0.03) (0.03) 0.06 0.03 -0.14 -0.12 0.04 (0.02) (0.03) (0.03) (0.04) (0.04) -0.09 -0.06 -0.08 0.19 0.12 (0.02) (0.02) (0.02) (0.04) (0.03) Female State Dependence Time Trend -0.013 (0.001) -0.012 (0.002) -0.016 (0.002) -0.019 (0.003) -0.013 (0.002) -0.005 (0.002) -0.026 (0.003) -0.012 (0.002) -0.029 (0.002) -0.012 (0.001) -0.010 (0.002) -0.004 (0.002) -0.026 (0.002) -0.010 (0.002)\n",
              "995\n",
              "\n",
              "Constant ¯ (λ ) 2.38 Communication (0.09) 3.34 Game (0.04) 4.80 Map/Navigation (0.04) 4.15 Entertainment (0.06) 5.03 Lifestyle (0.04) 4.87 Personal Finance (0.05) 3.68 Music/Radio (0.05) 4.44 Photo (0.04) 5.56 Portal (0.05) 4.15 Schedule/Memo (0.07) 3.68 Social Network (0.04) 4.19 Utility (0.06) 4.43 Video (0.05) 3.97 Web (0.05)\n",
              "\n",
              "Age 30's 0.42 (0.02) -0.06 (0.03) -0.05 (0.03) 0.30 (0.04) 0.02 (0.03) -0.40 (0.03) 0.59 (0.03) 0.19 (0.02) 0.23 (0.03) -0.08 (0.02) 0.30 (0.02) 0.14 (0.02) 0.18 (0.03) 0.13 (0.03)\n",
              "\n",
              "Age 40's & over 0.56 (0.02) 0.02 (0.03) -0.21 (0.03) 0.11 (0.05) -0.14 (0.03) -0.46 (0.03) 0.67 (0.04) 0.30 (0.02) 0.41 (0.04) -0.18 (0.02) 0.49 (0.03) 0.24 (0.02) 0.19 (0.04) 0.50 (0.03)\n",
              "\n",
              "-0.70 (0.09) -1.57 (0.03) -0.80 (0.03) -1.22 (0.04) -0.96 (0.03) -1.04 (0.03) -1.45 (0.03) -0.72 (0.03) -2.55 (0.03) -0.81 (0.07) -1.19 (0.03) -0.94 (0.06) -1.27 (0.03) -1.87 (0.04)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. To code discrete demographic variables, we use the following as a reference level: we use age 20’s or less for age and male for gender, respectively. For monthly income, we create two dummy variables, mid-class ($3,000–$5,000) and upper-class ($5,000 or over), and we use lower-class ($3,000 or less) as a reference level. Similarly, for education, we create two dummy variables, high school graduates and university graduates, and we use students (elementary, middle-and-high school students, university students) as a reference level.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Figure 4. Mapping App Categories According to Baseline Utility and Satiation\n",
              "\n",
              "mobile web than other app categories, indicating that users have formed their consumption habit for these app categories. Finally, despite their small effect size, there is a decreasing time trend of satiation towards all app categories. We can ¯ also interpret the degree of satiation in terms of α . However, ¯ ¯ ¯ unlike μ, α decreases as λ increases after the exponential ¯ transformation in equation (2). Consequently, a lower α represents a higher satiation level. In Figure 4, we map mobile web and app categories according ¯ ¯ to their mean baseline utility and satiation levels, μ and α . The four quadrants of the scatterplot provide intriguing insights into the multiplicity of mobile content in terms of its choice and time use. For example, the top-right, quadrant I, represents mobile content that is used widely as well as extensively, including utility, communication, and schedule apps. Furthermore, we can distinguish among the apps with the similar level of baseline utility: low satiation apps from high satiation counterparts. For example, baseline utility levels are similar to each other for apps in quadrant III (portal search and life style apps) and quadrant IV (photo, map/navigation, personal finance, social, music, video, entertainment, game apps, and mobile web). However, satiation levels are greater in the former than in the latter.\n",
              "\n",
              "addition, in the last four columns of Tables 4 and 5, we decompose the overall variations in baseline utilities and satiations for mobile web and app categories into variations attributed to the state dependence, demographic variables, common factors, and specific factors. Interpretation of the factors is based on estimated factor loading matrices and variance decompositions. For the baseline utility, the first factor loads with positive signs for all app categories. In contrast, the second factor loads with positive signs for communication, game, map/navigation, entertainment, and schedule/memo apps, and a large positive factor score indicates high baseline utility for these app categories. Finally, the third factor loads with negative signs for communication, game, map/navigation, utility, and video apps, and a large negative factor score indicates low baseline utility for these app categories. The variance decomposition reveals that, on average, 34% of variation in the baseline utility can be explained by state dependence and 1% by demographic variables while 41% by common factors and remaining 23% by specific factors. In marketing literature, researchers assert that demographic variables often serve as poor predictors of consumer brand preferences that are estimated on scanner panel data (Singh et al. 2005). Echoing this, our result indicates that the proportion of variation explained by demographic variables is small. For the satiation parameters, we find that the first factor loads strongly with positive signs for most app categories except for game and entertainment apps. The second factor loads with negative signs for most app categories except communication, photo, and social network apps, capturing uniformly lower\n",
              "\n",
              "Variance Decomposition for Mobile Web and App Categories\n",
              "Tables 4 and 5 present the estimation results for common factors (Γβ and Γλ) and specific factors (Λβ and Λλ). In\n",
              "\n",
              "996\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 4. Baseline Utility Factor Estimates and Variance Decomposition\n",
              "Estimates Common Factor 0.07 0.02 (0.01) (0.01) 0.05 0.03 (0.01) (0.01) 0.05 0.02 (0.01) (0.01) 0.05 0.03 (0.01) (0.01) 0.04 0.01 (0.01) (0.01) 0.07 0.00 (0.01) – 0.05 0.02 (0.01) (0.01) 0.06 -0.01 (0.01) (0.01) 0.05 0.02 (0.01) (0.01) 0.04 0.02 (0.01) (0.01) 0.06 0.00 (0.01) (0.01) 0.04 0.01 (0.01) (0.01) 0.06 0.00 (0.01) (0.01) 0.03 0.02 (0.01) (0.01) Specific Factor 0.03 (0.01) 0.59 (0.02) 0.82 (0.01) 1.14 (0.02) 0.76 (0.03) 0.76 (0.01) 1.17 (0.02) 0.00 (0.03) 1.10 (0.02) 0.15 (0.01) 0.06 (0.02) 0.16 (0.01) 0.14 (0.02) 0.80 (0.01) Variance Decomposition State Demo- Common Specific Dependence graphic Factor Factor 0.04 0.65 0.25 0.33 0.39 0.58 0.18 0.32 0.18 0.12 0.85 0.06 0.72 0.16 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.04 0.01 0.02 0.03 0.00 0.01 0.31 0.69 0.62 0.55 0.39 0.74 0.00 0.80 0.44 0.00 0.27 0.11 0.80 0.87 0.04 0.06 0.05 0.05 0.02 0.08 0.66 0.02 0.40 0.14 0.65 0.14 0.04\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music/Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "-0.02 (0.01) -0.04 (0.01) -0.02 (0.01) 0.00 (0.01) 0.02 (0.01) 0.00 – 0.00 – -0.01 (0.01) -0.01 (0.01) 0.00 (0.01) 0.00 (0.01) -0.03 (0.01) -0.03 (0.01) 0.01 (0.01)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. The second common factor estimate for personal finance apps and the third common factor estimates for personal finance and music/radio apps are set to zero for the purposed of identification.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "997\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 5. Satiation Factor Estimates and Variance Decomposition\n",
              "Estimates Common Factor 0.32 0.07 (0.01) (0.01) 0.01 -0.37 (0.01) (0.01) 0.32 -0.18 (0.01) (0.01) 0.01 -0.56 (0.01) (0.01) 0.45 -0.59 (0.01) (0.01) 0.49 0.00 (0.01) – 0.28 -0.29 (0.01) (0.01) 0.56 0.18 (0.01) (0.01) 0.48 -0.16 (0.01) (0.01) 0.36 -0.11 (0.01) (0.01) 0.68 0.29 (0.01) (0.01) 0.41 -0.33 (0.01) (0.01) 0.45 -0.89 (0.01) (0.01) 0.21 -0.20 (0.01) (0.01) Specific Factor 0.05 (0.06) 0.21 (0.07) 0.05 (0.07) 0.08 (0.09) 0.03 (0.07) 0.03 (0.07) 0.11 (0.07) 0.28 (0.06) 0.25 (0.07) 0.04 (0.06) 0.05 (0.07) 0.08 (0.06) 0.03 (0.09) 0.15 (0.06) Variance Decomposition State DemoCommon Dependence graphic Factor 0.01 0.88 0.39 0.68 0.23 0.51 0.66 0.08 0.70 0.04 0.30 0.06 0.42 0.72 0.49 0.01 0.07 0.10 0.01 0.10 0.12 0.19 0.02 0.04 0.13 0.05 0.01 0.11 0.49 0.04 0.53 0.20 0.76 0.39 0.20 0.59 0.24 0.91 0.56 0.87 0.57 0.12 Specific Factor 0.01 0.07 0.01 0.01 0.00 0.00 0.02 0.14 0.04 0.01 0.00 0.02 0.00 0.04\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music/Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "-0.15 (0.01) -0.17 (0.01) -0.23 (0.01) -0.03 (0.01) -0.17 (0.01) 0.00 – 0.00 – -0.05 (0.01) 0.12 (0.01) -0.40 (0.01) 0.16 (0.01) -0.44 (0.01) 0.52 (0.01) 0.17 (0.01)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. The second common factor estimate for personal finance apps and the third common factor estimates for personal finance and music/radio apps are set to zero for the purposed of identification.\n",
              "\n",
              "(higher) levels of usage with a large positive (negative) factor score. The third factor also loads with negative signs for most app categories except for portal search, social network, video apps, and mobile web. The variance decomposition denotes that 41% of variation in satiation parameters is explained by state dependence. In contrast, 10% of the variation is accounted for by demographic variables and 46% by common factors while the remaining 3% by specific factors. While only a small variation is accounted for by demographic variables, a large variation is explained by state dependence, which indicates that satiation is a routinized behavior. Thus it is difficult to predict a user’s app satiations based on her\n",
              "\n",
              "demographic information. Instead, individual level historical information may be required for reliable prediction.\n",
              "\n",
              "Correlation across Mobile Web and App Categories\n",
              "Tables 6 and 7 present the correlation matrices for the baseline utility and satiation parameters across mobile web and app categories. Table 6 hints that people who use communication apps (e.g., mobile messengers) also frequently use photo apps (e.g., Instagram). This result indicates that com-\n",
              "\n",
              "998\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 6. Correlation Matrix for Baseline Utility Parameters\n",
              "Personal Finance Schedule/Memo Communication Map/Navigation Social Network Entertainment\n",
              "\n",
              "Music/Radio\n",
              "\n",
              "Lifestyle\n",
              "\n",
              "Photo\n",
              "\n",
              "Portal\n",
              "\n",
              "Utility\n",
              "\n",
              "Game\n",
              "\n",
              "Video 0.33 0.17 0.15 0.15 0.16 0.12 0.16 0.41 0.12 0.27 0.26 0.35 0.11 Video 0.22 0.23 0.50 0.33 0.70 0.47 0.45 0.16 0.42 0.53 0.10 0.68 0.33\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music/Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "0.14 0.14 0.20 0.13 0.19 0.15 0.22 0.74 0.14 0.65 0.32 0.66 0.33 0.15 0.07 0.12 0.09 0.05 0.08 0.19 0.06 0.12 0.14 0.17 0.17 0.06\n",
              "\n",
              "0.20 0.07 0.08 0.11 0.10 0.09 0.22 0.06 0.15 0.12 0.19 0.15 0.06\n",
              "\n",
              "0.13 0.12 0.08 0.11 0.06 0.10 0.17 0.06 0.12 0.14 0.18 0.15 0.06\n",
              "\n",
              "0.19 0.09 0.11 0.11 0.12 0.09 0.24 0.07 0.16 0.16 0.19 0.16 0.06\n",
              "\n",
              "0.15 0.05 0.10 0.06 0.12 0.06 0.17 0.06 0.12 0.13 0.13 0.12 0.05\n",
              "\n",
              "0.22 0.08 0.09 0.10 0.09 0.06 0.24 0.06 0.17 0.15 0.23 0.16 0.06\n",
              "\n",
              "0.74 0.19 0.22 0.17 0.24 0.17 0.24 0.16 0.51 0.42 0.60 0.41 0.17\n",
              "\n",
              "0.14 0.06 0.06 0.06 0.07 0.06 0.06 0.16 0.11 0.10 0.12 0.12 0.02\n",
              "\n",
              "0.65 0.12 0.15 0.12 0.16 0.12 0.17 0.51 0.11 0.24 0.51 0.27 0.12\n",
              "\n",
              "0.32 0.14 0.12 0.14 0.16 0.13 0.15 0.42 0.10 0.24 0.30 0.26 0.09\n",
              "\n",
              "0.66 0.17 0.19 0.18 0.19 0.13 0.23 0.60 0.12 0.51 0.30 0.35 0.16\n",
              "\n",
              "0.15 0.06 0.06 0.06 0.06 0.05 0.06 0.17 0.02 0.12 0.09 0.16 0.11\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "Table 7. Correlation Matrix for Satiation Parameters\n",
              "Personal Finance Schedule/Memo Communication Map/Navigation Social Network Entertainment\n",
              "\n",
              "Music/Radio\n",
              "\n",
              "Lifestyle\n",
              "\n",
              "Photo\n",
              "\n",
              "Portal\n",
              "\n",
              "Utility\n",
              "\n",
              "Game\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music/Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "0.00 0.00 0.30 0.10 0.42 0.18 0.15 0.21 -0.19 0.16 0.32 0.16 0.87 -0.02 0.37 0.13 0.49 0.09 0.75 -0.01 0.57 0.16 0.22 0.23 0.34 0.12\n",
              "\n",
              "0.30 0.10 0.39 0.57 0.38 0.27 0.35 0.43 0.75 0.29 0.70 0.50 0.21\n",
              "\n",
              "0.42 0.18 0.39 0.33 0.12 0.32 0.36 0.32 0.44 0.35 0.46 0.33 0.20\n",
              "\n",
              "0.15 0.21 0.57 0.33 0.58 0.42 0.09 0.41 0.60 0.03 0.73 0.70 0.30\n",
              "\n",
              "-0.19 0.16 0.38 0.12 0.58 0.21 -0.19 0.23 0.31 -0.20 0.38 0.47 0.15\n",
              "\n",
              "0.32 0.16 0.27 0.32 0.42 0.21 0.18 0.29 0.25 0.16 0.43 0.45 0.28\n",
              "\n",
              "0.87 -0.02 0.35 0.36 0.09 -0.19 0.18 0.36 0.57 0.82 0.51 0.16 0.25\n",
              "\n",
              "0.37 0.13 0.43 0.32 0.41 0.23 0.29 0.36 0.47 0.31 0.52 0.42 0.19\n",
              "\n",
              "0.49 0.09 0.75 0.44 0.60 0.31 0.25 0.57 0.47 0.47 0.86 0.53 0.23\n",
              "\n",
              "0.75 -0.01 0.29 0.35 0.03 -0.20 0.16 0.82 0.31 0.47 0.40 0.10 0.19\n",
              "\n",
              "0.57 0.16 0.70 0.46 0.73 0.38 0.43 0.51 0.52 0.86 0.40 0.68 0.38\n",
              "\n",
              "0.34 0.12 0.21 0.20 0.30 0.15 0.28 0.25 0.19 0.23 0.19 0.38 0.33\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "Web\n",
              "999\n",
              "\n",
              "Web\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "munication apps and photo apps might be frequently used together as complements.14 Table 7 shows correlations in satiation levels. People who spend a great deal of time on social apps (e.g., Facebook) also spend a significant amount of time on communication apps (e.g., WhatsApp) and photo apps (e.g., Instagram), which suggests that social, communication, and photo apps might be economic complements to each other. For example, many users take a picture or video, use Instagram to choose a filter to transform its look and feel, and post it to Facebook or share it on WhatsApp. In contrast, people who use social network apps infrequently use personal finance apps, which indicates that social network and personal finance apps might be economic substitutes.\n",
              "\n",
              "selected 10 apps with their key descriptive statistics. Column 2 of Table 8 shows that users accessed Kakao Talk (a communication app) most frequently (95.83%), while they accessed Rule the Sky (a game app) least frequently (3.78%). Column 5 of Table 8 shows that, excluding outside options, users spent the most time (18.38%) using Kakao Talk, followed by Naver (3.64%), Kakao Story (3.00%), Tiny Farm (1.66%), and Facebook (1.57%). In addition, the top-nine apps account for 33% of the total mobile content use and the \"Others\" category accounts for the remaining 67%. We use the same specification for the model estimation as in the app category-level analysis. Results exhibited in Appendix D show the estimates for the baseline utility and satiation parameters, respectively. Table 9 shows the correlation matrix for the baseline utility for the top nine apps and the remaining, combined other apps. We find that people who use the Naver app also frequently use its competing portal search Daum’s app. Moreover, people who use the Kakao Talk app also frequently use Kakao Story, a social networking app developed by Kakao Talk. Table 10 shows the correlation in the satiation levels. We find that people who spend a great deal of time on Facebook also spend a lot of time on Kakao Talk, which is similar to our empirical findings based on the category-level analysis. Now we illustrate the strategic value of our findings summarized in the correlation matrices in developing optimal mobile content portfolios. Companies can build and maintain an optimal mobile content portfolio by making internally and/or buying external mobile websites and apps. The results of the correlations for the app choices and app time use exhibited in Tables 9 and 10 provide useful and actionable guidelines and insights for this fundamental issue. The results of the correlation in app time use satiation reported in Table 10 demonstrate that some apps are used more extensively when they are used together rather than separately. Therefore, a company should acquire apps that complement its focal app in terms of time use to maximize the consumption of the combined apps. For example, due to strong positive correlations in consumption among photo, communication, and social network apps, a company can maximize the customers’ time spent on its portfolio of apps by combining these app categories together. Facebook’s recent acquisition of WhatsApp and Instagram is a good manifestation of this case.\n",
              "\n",
              "Applications of Proposed Approaches\n",
              "In this section, we illustrate the applications of our proposed model and findings in several areas, including mobile competitive analysis, mobile user targeting, and mobile media planning.\n",
              "\n",
              "Mobile Competitive Analysis\n",
              "Our proposed model provides a general empirical framework to analyze the usage data on various emerging IT artifacts as well as the app time usage data. To illustrate this vital strength, we conduct an app-level analysis using the proposed model to investigate mobile users’ popular app choices and time usage decisions. We consider the most popular nine apps (in terms of total usage time) and all of the other mobile content use excluding the top-nine apps as a single combined, “Others,” option. A similar specification can be applied for a mobile competitive analysis when a manager wants to understand the nature of competition between her/his company and key competitors in the mobile market. We chose a total of 10 options for the sake of the tractability of the model; however, we can increase the number of options at the expense of the computational time required to identify and to estimate the parameters of the model. Table 8 lists the\n",
              "\n",
              "14\n",
              "\n",
              "The positive correlation in baseline utility can be explained by not only inherent complementary relationship among apps but also some external reasons for simultaneous use. For example, in the case of product purchase decisions, bundle promotions (e.g., buy one get one free) can result in correlated demand. When mobile users decide which app to use, the decision is less likely to be influenced by external factors compared to other purchase or choice decisions. Thus, we believe that the inherent complementarity plays the major role in our context.\n",
              "\n",
              "Mobile User Targeting: App Time-Use Prediction\n",
              "Previous research has frequently used demographics or preference estimates based on individual-level panel data for new\n",
              "\n",
              "1000\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 8. Selected Apps and Summary Statistics\n",
              "Time Use App Outside option (other activities) KakaoTalk Naver Kakao Story RuleTheSky Facebook TinyFarm Mellon Daum YouTube Others Total Category Communication Portal Search Social Game Social Game Music and Radio Portal Search Video Choice 100.00% 95.83% 46.35% 59.14% 3.78% 27.07% 5.51% 11.03% 13.84% 29.21% 100.00% Hour 155.67 2.27 0.45 0.37 0.16 0.19 0.21 0.17 0.14 0.08 8.29 168.00 Percent 92.66% 1.35% 0.27% 0.22% 0.10% 0.12% 0.12% 0.10% 0.08% 0.05% 4.94% 100.00% Percent excld. outside option — 18.38% 3.64% 3.00% 1.31% 1.57% 1.66% 1.41% 1.13% 0.64% 67.25% 100.00%\n",
              "\n",
              "Table 9. Correlation Matrix for Baseline Utility Parameters: App-Level Analysis\n",
              "Mellon (Music/Radio) Kakao Story (Social) RuleTheSky (Game) Kakao Talk (Comm.)\n",
              "\n",
              "Facebook (Social)\n",
              "\n",
              "TinyFarm (Game)\n",
              "\n",
              "YouTube (Video)\n",
              "\n",
              "Naver (Portal)\n",
              "\n",
              "Daum (Portal)\n",
              "\n",
              "Kakao Talk Naver Kakao Story RuleTheSky Facebook TinyFarm Mellon Daum YouTube Others 0.12 0.26 0.11 0.19 0.13 0.19 0.04 0.20 0.23\n",
              "\n",
              "0.12 0.11 0.04 0.13 0.05 0.07 0.27 0.07 0.01\n",
              "\n",
              "0.26 0.11 0.06 0.09 0.06 0.08 0.04 0.15 0.07\n",
              "\n",
              "0.11 0.04 0.06 0.05 0.11 0.03 0.01 -0.01 -0.12\n",
              "\n",
              "0.19 0.13 0.09 0.05 0.06 0.20 0.05 0.09 0.09\n",
              "\n",
              "0.13 0.05 0.06 0.11 0.06 0.12 0.01 0.01 0.03\n",
              "\n",
              "0.19 0.07 0.08 0.03 0.20 0.12 0.02 0.10 0.18\n",
              "\n",
              "0.04 0.27 0.04 0.01 0.05 0.01 0.02 0.09 0.12\n",
              "\n",
              "0.20 0.07 0.15 -0.01 0.09 0.01 0.10 0.09 0.30\n",
              "\n",
              "0.23 0.01 0.07 -0.12 0.09 0.03 0.18 0.12 0.30\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "customer prediction (e.g. Rossi et al. 1996). In addition, researchers and practitioners have developed regression-based scoring models to predict customers’ future behaviors (e.g. Bolton 1998; Malthouse and Blattberg 2005). The purpose of these approaches is to use the measurements of customers’ prior behavior as predictors of their future behavior. Similarly, in direct marketing literature, it is a common practice to summarize customers’ prior behavior in terms of their recency (time of most recent purchase), frequency (number of prior purchases), and monetary value (average purchase amount per\n",
              "\n",
              "transaction) (RFM method; Fader et al. 2005). However, when there are no detailed information on prior transactions or behaviors, all these established methods are difficult to apply. In such a case, an average consumer’s category usage pattern over multiple categories can be used instead. We can obtain this information from the estimated correlation matrix of the proposed model. To add a specific context to our application, we aimed to identify a segment with extensive social network app use. We\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "Others\n",
              "1001\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 10. Correlation Matrix for Satiation Parameters: App-Level Analysis\n",
              "Mellon (Music/Radio) Kakao Story (Social) RuleTheSky (Game) Kakao Talk (Comm.)\n",
              "\n",
              "Facebook (Social)\n",
              "\n",
              "TinyFarm (Game)\n",
              "\n",
              "YouTube (Video)\n",
              "\n",
              "Naver (Portal)\n",
              "\n",
              "Daum (Portal)\n",
              "\n",
              "Kakao Talk Naver Kakao Story RuleTheSky Facebook TinyFarm Mellon Daum YouTube Others\n",
              "\n",
              "0.22 0.22 0.48 -0.28 0.53 -0.20 0.17 0.25 0.15 0.46 0.07 0.03 0.37 -0.01 0.01 0.33 0.10 0.37\n",
              "\n",
              "0.48 0.07 -0.23 0.13 -0.01 0.12 -0.01 0.06 0.04\n",
              "\n",
              "-0.28 0.03 -0.23 0.00 0.34 -0.10 0.11 -0.01 0.01\n",
              "\n",
              "0.53 0.37 0.13 0.00 -0.12 0.03 0.62 0.23 0.85\n",
              "\n",
              "-0.20 -0.01 -0.01 0.34 -0.12 -0.12 0.03 -0.04 -0.12\n",
              "\n",
              "0.17 0.01 0.12 -0.10 0.03 -0.12 -0.05 0.00 -0.02\n",
              "\n",
              "0.25 0.33 -0.01 0.11 0.62 0.03 -0.05 0.19 0.70\n",
              "\n",
              "0.15 0.10 0.06 -0.01 0.23 -0.04 0.00 0.19 0.24\n",
              "\n",
              "0.46 0.37 0.04 0.01 0.85 -0.12 -0.02 0.70 0.24\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "divided our 17 week sample data into an estimation period (first 9 weeks) and a holdout prediction period (last 8 weeks). We first estimated the proposed model using the estimation period data and obtained a correlation matrix of satiation parameters. The correlation matrix for satiation parameters is very similar to the pattern based on the full 17 week data (see Appendix B, Tables B1 and B2).15 The left panel of Figure 5 visualizes the correlation pattern in satiation parameters on two-dimensional space using a multidimensional scaling (MDS) technique.16 Using the same MDS technique, the right panel of Figure 5 depicts the correlation pattern in app use time as a benchmark against which the performance of the proposed approach is compared. Social network, photo, and communication apps are closely located in both plots of Figure 5. This indicates that heavy photo and communication app users are also characterized as heavy social network app users. Accordingly, we used the photo and communication app use time during the estimation period to infer the potential targets of extensive social network app use during the prediction period. It should be noted that we are not using past usage behavior of the focal category (i.e., social network) as\n",
              "15 The main findings from the estimation results using the first nine weeks are the same as those from the full data. The results are available on request from the authors. 16\n",
              "\n",
              "in a typical RFM approach. Our strategy was to use the photo and communication app usage histories instead. Table 11 shows the performance of the proposed targeting strategy. First, we divided the app users by the photo app use time in the estimation period. The average social network app use time of the top 50% is 1.58 and the bottom 50% is 0.52 during the prediction period. The average use time for all users is 1.03 and this is what we can expect when we use a random targeting strategy. When we select 50% of the users based on their photo app use time during the estimation period, the average social network app use time during the prediction period is longer by 53% than that of a randomly selected group. That is, the gain is 53%. Similarly, when we select 50% of the users based on their communication app use time during the estimation period, the gain is 43% in comparison with a random targeting strategy. Additionally, the left panel of Figure 5 indicates that there exist strong correlations in satiation parameters between video and lifestyle apps and among schedule, utility, and map/navigation apps, respectively. However, these meaningful relationships do not show up on the right panel or the correlation matrix based on app use time. The last three columns of Table 11 show the performance of the same targeting applications based on these strong positive correlations in the aforementioned satiation parameter estimates. All of these applications show substantial gains in comparison with the random targeting strategy, which confirms the validity of the proposed targeting strategy. Moreover, this clearly illustrates a distinctive advantage of the proposed method over the simple model-free approaches.\n",
              "\n",
              "Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual objects (categories in our case) of a dataset. An MDS algorithm aims to place each object in N-dimensional space such that the between-object distances are preserved as well as possible. As in our application, two-dimensional (N = 2) is the most frequently used option since it allows an easy graphical interpretation. We use the SPSS Proxscal package.\n",
              "\n",
              "1002\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "Others\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Figure 5. A Two-Dimensional MDS Representation of Correlation Matrices of Satiation Parameters (Left) and App Use Time as a Benchmark (Right)\n",
              "\n",
              "Table 11. Result of Heavy User Targeting Application\n",
              "Target Category Sorting Variable Top 50% Use Time Bottom 50% Average Gain Social Network Photo 1.58 0.52 1.03 53% Social Network Communication 1.48 0.57 1.03 43% Lifestyle Video 0.27 0.17 0.21 30% Schedule Utility 0.94 0.68 0.81 17% Schedule Map/Navigation 0.92 0.72 0.81 14%\n",
              "\n",
              "Mobile Media Planning\n",
              "This section illustrates the usefulness of the proposed model in optimal mobile media planning. To be specific, we developed a method that optimally selects mobile advertising vehicles and determines the share of advertising impressions that should be purchased to maximize one’s objective function. We first applied the proposed model in predicting individual mobile users’ app consumption, after which we used the resultant usage distribution as a component in the broader problem of optimal advertising vehicle selection. To the best of our knowledge, the proposed media selection method is the first general framework for mobile media. The proposed framework is based on continuous app usage time and ad exposure time. By contrast, existing media planning models for online, TV, or print media are based on discrete counts of ad exposure (e.g., Danaher et al 2010). Continuous exposure time provides more precise information and is more appropriate for measuring emerging forms of mobile ads (e.g.,\n",
              "\n",
              "video ads with varying run times) than the discrete counting of ad exposure. The first step in any media scheduling method is to develop reliable estimates of potential audience size. For this purpose, we used the proposed multiple discrete-continuous choice model. In existing methods for online, TV, and print media, simple count data models, such as Poisson or negative binomial models, are adopted. Compared with these count data models, the proposed model enables us to easily incorporate a rich set of explanatory variables (i.e. demographic variables, state dependence, factor analytic structure). Resolution of a Mobile Advertising Planner’s Optimization Problem Here, we present our mobile media planning framework. We focused on mobile media advertising scheduling for display ads given that our objective is to reach a target group via\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "1003\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "visual advertisements. For expositional simplicity, we considered one period (one week) and drop time subscript t, but the proposed approach can be extended to a multi-period case in a straightforward manner. cj denotes the cost of advertising on app j; cj is the cost per-hour of exposure; qhj is the time spent (in hours) on app j by mobile user h in a week. We can compute qhj using our proposed model (Equations 1–4). Let sj(0 # sj # 1) denote the share of time purchased for an ad at app j. sj is the media planner’s decision variable. That is, a media planner’s goal is deciding optimal {sj}j=1, …, J with the aim of maximizing her objective function while remaining within budget. We formally write down the optimization problem as follows:\n",
              "\n",
              "the target market; thus, she knows the baseline utilities and satiation levels of these apps (summarized in Table 12). She also knows the per-hour cost of advertising on these apps. In this example, App1 can be characterized as a “high utility, low satiation” application, whereas App2 and App3 are “low utility, high satiation” apps. Using the given parameter values, we simulated 1,000 pseudo mobile users’ app usage for a week and then determine the optimal solution that maximizes the ad planner’s objective function. For numerical optimization, we used the Matlab’s constrained nonlinear optimization routine. Table 13 reports the optimal schedules for the three apps. We first assumed that no correlation exists among the baseline utility parameters and among the satiation parameters (first three rows in Table 13). The predicted total app usage times are 795, 141, and 138 hours for App1, App2, and App3, respectively. When the linear response function is used, buying a 44% share of time on App1 or spending the entire ad budget on App1 is an optimal decision. The linear response function simply maximizes the total exposure time. Given that App1 exhibits the lowest cost, this result is unsurprising. The next row in Table 13 lists the optimal schedule for the square-root response function, which assumes diminishing returns on exposure. Note that the costs of App2 and App3 are more expensive than that of App1. However, buying 28% and 27% shares of time on App2 and App3, along with App1, is the optimal schedule. This optimality is ascribed to the fact that under the square-root response function, short ad exposure to many audience members is better than long exposure to a small group of people. The optimal ad schedule implies that allocating the ad budget to the more expensive App2 and App3 enables us to reach more audiences and reduce the overexposure. To determine how correlation in utility influences the optimal media schedule, we assumed that the baseline utility of App1 is correlated with that of App2. Specifically, we added Corr(βhApp1, βhApp2) = 0.7 to the parameter values specified in Table 13 when we simulated the app use times. The correlation suggests that a mobile user who highly values App1 tends to accord App2 the same appreciation. This correlation generates marginal differences in app use times. When the linear response function is applied, spending all the ad budget on App1 is the optimal decision as in the no-correlation case. However, when the square-root response function is applied, the optimal schedule becomes the purchase of 35%, 15%, and 34% of time shares on App1, App2, and App3, respectively. This optimal schedule markedly differs from that under the square-root response function with no correlation. App2 and App3 are identical in terms of baseline utility and satiation, but the shares required for the optimal schedules on these apps are substantially dissimilar (15% versus 34%). This\n",
              "\n",
              "maximize subject to\n",
              "\n",
              " \n",
              "\n",
              "H h =1 J j=1\n",
              "\n",
              "R(qh1 ⋅ s1,,qhJ ⋅ sJ )   \n",
              "\n",
              "(\n",
              "\n",
              "H\n",
              "\n",
              "h = 1 hj\n",
              "\n",
              "q\n",
              "\n",
              ") ⋅ s ⋅ c  ≤ B,\n",
              "j j\n",
              "\n",
              "(11)\n",
              "\n",
              "and 0 ≤ s j ≤ 1\n",
              "where R(A) is a media response function, and B is an advertising budget. Several response functions have been proposed for media planning, but we focused on two popular response variants: a linear function and a square-root function. The linear function assumes a simple and naive response function as follows:\n",
              "\n",
              "R(qh1 ⋅ s1 , , qhJ ⋅ sJ ) = qh1 ⋅ s1 + qh 2 ⋅ s2 ++ qhJ ⋅ sJ\n",
              "\n",
              "(12)\n",
              "\n",
              "When this response function is used, a media planner simply maximizes the total exposure time. Alternatively, Rust and Leone (1984) propose the following square-root response function:\n",
              "\n",
              "R(qh1 ⋅ s1 , , qhJ ) =\n",
              "\n",
              "\n",
              "\n",
              "J j =1\n",
              "\n",
              "qhj ⋅ s j\n",
              "\n",
              "(13)\n",
              "\n",
              "This response function assumes that exposure suffers from diminishing returns. Illustration The proposed method is elucidated using a specific example. Let us assume that a mobile media planner has three available mobile apps (App1, App2, and App3) for her advertising campaign. Let us also suppose that the target market comprises 1,000 mobile app users and that the media planner’s budget for this campaign is $350. The ad planner estimated the proposed multiple discrete-continuous choice model by using a representative app usage panel data set derived from\n",
              "\n",
              "1004\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table 12. Baseline Utility, Satiation, and Cost of Considered Mobile Apps\n",
              "Apps App1 App2 App3 Others Baseline Utility βhApp1 ~ N(–4, 1) βhApp2 ~ N(–6, 1) βhApp3 ~ N(–6, 1) βhOthers ~ N(–3, 1) Satiation αhApp1 ~ N(–7, 1) αhApp2 ~ N(–9, 1) αhApp3 ~ N(–9, 1) αhOthers ~ N(–6, 1) App Characteristic High Utility / Low Satiation Low Utility / High Satiation Low Utility / High Satiation — Ad Cost ($/per hour) 1 1.1 1.1 —\n",
              "\n",
              "Table 13. Optimal Schedules for Three Available Apps\n",
              "App1 No Correlation Total App Use Time (hours) Optimal Schedule () Linear Response Function Square-Root Response Function Corr(βhApp1, βhApp2) = 0.7 Total App Use Time (hours) Optimal Schedule (sj) Linear Response Function Square-Root Response Function 795 44% 33% 791 44% 35% App2 141 0% 28% 143 0% 15% App3 139 0% 27% 141 0% 34% 350 530.6 350 531.8 Response Function Value\n",
              "\n",
              "variance is due to the correlation between App1 and App2. Because of such correlation, App1 users tend to frequently use App2, thereby causing overexposure, which is undesirable under the square-root response function. Consequently, App3 becomes a more appealing avenue for the ad campaign, and its share increases to 34%.\n",
              "\n",
              "Conclusion\n",
              "The dramatic growth of the mobile platform and device industry has resulted in an app-centric marketplace, ushering in the app-based mobile economy, which provides new opportunities and challenges for entrepreneurs, managers, and marketers across all online business segments. Although online consumers are increasingly migrating from web browsers to mobile apps for their commercial transactions and social interactions, little is known about how these users behave and make decisions with respect to their choices, consumption, and management of mobile apps in diverse categories. The absence of sound mobile app analytics based on a thorough methodological approach has impeded our comprehension of consumers’ behavioral responses to the choice and usage of mobile apps.\n",
              "\n",
              "This study developed a novel framework for mobile app analytics by using the theoretical concepts of utility and satiation, along with a structured method of factor analytic approach. Our work contributes to an emerging stream of literature on the economics of the mobile Internet and mobile marketing by being the first study to quantify the baseline utility and satiation levels of distinct mobile app categories. Using large-scale, individual-level panel data on mobile app and web time-use histories, we constructed a unique multiple discrete-continuous model of app selection and time-use decisions to more clearly elucidate usage interdependence and dynamics among diverse categories of mobile web and apps. The frameworks, mechanisms, and processes discussed in this study offer high flexibility and efficiency in estimating parameters, particularly when the number of choices is exceedingly large, a phenomenon that has increasingly become prevalent in big data analytics. Our analytical paradigm and comprehensive computational procedures can, therefore, enlighten developers and researchers as to the advancement of big data analytics in general and mobile app analytics in particular. Data availability issues suggest that some caution is warranted with regard to the estimation of the proposed model. In certain cases, some consumers implement app selection, consumption decisions, and utility maximization as frequently\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "1005\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "as they wish (i.e., on a daily or hourly basis). Only weekly usage data were available for the analysis, thereby preventing us from further enhancing the precision in estimation. Future researchers can apply our proposed model to high-frequency data on the usage patterns of individual users. Another limitation of this study is that we did not observe consumers’ app purchase behaviors. Future works should be directed toward an integrated model of app purchase and consumption, provided that they have individual user-level data on both app purchase and usage. We believe that consumers’ mobile app usage decisions are influenced by the context and social network variables measuring which users are connected to which other users. Once these variables are observed, these can enter into the baseline utility and the satiation parameters as the demographic variables are incorporated in the current specification. Furthermore, for the sake of tractability, we postulated that the utility function is additively separable; thus the model cannot explicitly partial out complementarity or substitution from other factors (Bhat 2008). Lee et al. (2013) recently developed a direct utility model that circumvents this problem, but the improved version remains restricted to accommodating a relatively small number of options. Future research can focus on addressing this limitation of multiple discrete-continuous choice models. Notwithstanding the limitations of the current work, our utility theory-based structural model of app selection and consumption remains relevant to the future direction of mobile analytics. Online businesses that depend heavily on apps for their growth are confronted with a new strategic and operational challenge that increases with the rising volume and complexity of users’ app consumption. The findings reported in this study may help managers alleviate the adverse effects of such challenges through an enhanced understanding of app selection and usage behaviors. Our study also sheds light on direct implications for management, specifically with respect to how managers should allocate their advertising resources across app categories, which often complement or compete with one another. Our empirical results suggest that both positive and negative correlations exist in the baseline utility and satiation levels of mobile web and app categories. We also found that users’ demographic variables and unobservable factors play important roles in explaining app selection and time-use decisions. These findings offer insights related to mobile user segmentation, targeting, and optimal media planning in the mobile app marketplace, thus empowering firms in identifying valuable consumers on the basis of differing needs, varying demographic characteristics, and idiosyncratic behavioral patterns. As we transition into the mobile economy, the adoption of effective mobile app analytics becomes a key basis for\n",
              "\n",
              "competition in mobile-based markets. The number and complexity of mobile apps will rise above and beyond our prediction, and the radical growth of apps will continue to confound the intellect and challenge the inventiveness of practitioners. The ability to develop strong analytical prowess and the capability to drive significant business value from the insights brought forth by mobile app analytics should occupy the core of a firm’s mobile strategy.\n",
              "\n",
              "References\n",
              "Bhat, C. 2005. “A Multiple Discrete-Continuous Extreme Value Model: Formulation and Application to Discretionary Time-Use Decision,” Transportation Research Part B (39), pp. 679-707. Bhat, C. 2008. “The Multiple Discrete-Continuous Extreme Value (MDCEV) Model: Role of Utility Function Parameters, Identification Considerations, and Model Extensions,” Transportation Research Part B (42), pp. 274-303. Bolton, R. N. 1998. “A Dynamic Model of the Duration of the Customer’s Relationship with a Continuous Service Provider: The Role of Satisfaction,” Marketing Science (17:1), pp. 45-65. Carare, O. 2012. “The Impact of Bestseller Rank on Demand: Evidence from the App Market,” International Economic Review (53:3), pp. 717-742 Chiang, J. 1991. “A Simultaneous Approach to Whether to Buy, What to Buy, and How Much to Buy,” Marketing Science (10:4), pp. 297-314. Chintagunta, P. K. 1993. “Investigating Purchase Incidence, Brand Choice and Purchase Quantity Decisions of Households,” Marketing Science (12:2), pp. 194-208. Chintagunta, P. K. 1994. “Heterogeneous Logit Model Implications for Brand Positioning,” Journal of Marketing Research (31:2), pp. 304-311. Danaher, P., Lee, J., and Kerbache, L. 2010. “Optimal Internet Media Selection,” Marketing Science (29:2), pp. 336-347. Dubin, J. A., and McFadden, D. L. 1984. “An Econometric Analysis of Residential Electric Appliance Holdings and Consumption,” Econometrica (52:2), pp. 345-362 Einav, L., Levin, J., Popov, I., and Sundaresen, N. 2014. “Growth, Adoption, and Use of Mobile Commerce,” American Economic Review: Papers & Proceedings (104:5), pp. 489-494. Elrod, T., and Keane, M. P. 1995. “A Factor-Analytic Probit Model for Representing the Market Structure in Panel Data,” Journal of Marketing Research (32:1), pp. 1-16. Fader, P. S., Hardie, B., and Lee, K. L. 2005. “‘Counting Your Customers’ the Easy Way: An Alternative to the Pareto/NBD Model,” Marketing Science (24:2), pp. 275-284. Flurry. 2013. “Flurry Five-Year Report: It’s an App World. The Web Just Lives in It” (http://flurrymobile.tumblr.com/post/ 115188952445/flurry-five-year-report-its-an-app-world-the). Garg, R., and Telang, R. 2013. “Inferring App Demand from Publicly Available Data,” MIS Quarterly (37:4), pp.1253-1264. Ghose, A., Goldfarb, A., and Han, S. 2013. “How Is the Mobile Internet Different? Search Costs and Local Activities,” Information Systems Research (24:3), pp. 613-631.\n",
              "\n",
              "1006\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Ghose, A., and Han, S. 2011. “An Empirical Analysis of User Content Generation and Usage Behavior on the Mobile Internet,” Management Science (57:9), pp. 1671-1691. Ghose, A., and Han, S. 2014. “Estimating Demand for Mobile Applications in the New Economy,” Management Science (60:6), pp. 1470-1488 Hanemann, W. M. 1984. “The Discrete/Continuous Model of Consumer Demand,” Econometrica (52:3), pp. 541-561. Hansen, K., Singh, V., and Chintagunta, P. 2006. “Understanding Store-Brand Purchase Behavior across Categories,” Marketing Science (25:1), pp. 75-90. Hendel, I. 1999. “Estimating Multiple-Discrete Choice Models: An Application to Computerization Returns,” Review of Economic Studies (66), pp. 423-446. Jeuland, A. P. 1979. “Brand Choice Inertia as One Aspect of the Notion of Brand Loyalty,” Management Science (25:4), pp. 671-682. Keane, M. P. 1993. “Simulated Estimation for Panel Data Models with Limited Dependent Variables,” in The Handbook of Statistics and Econometrics, C. R. Rao, G. S. Maddala, and D. Vinod (eds), Amsterdam: Elsevier Science Publishers, pp. 545-572. Keane, M. P. 1997. “Modeling Heterogeneity and State Dependence in Consumer Choice Behavior,” Journal of Business and Economic Statistics (15:3), pp. 310-327. Kim, J., Allenby, G. M., and Rossi, P. E. 2002. “Modeling Consumer Demand for Variety,” Marketing Science (21:3), pp. 229-250. Krishnamurthi, L., and Raj, S. P. 1988. “A Model of Brand Choice and Purchase Quantity Price Sensitivities,” Marketing Science (7:1), pp. 1-20. Lee, S., Kim, J., and Allenby, G. M. 2013. “A Direct Utility Model for Asymmetric Complements,” Marketing Science (32:3), pp. 454-470. Luo, L., Ratchford, B. T., and Yang, B. 2013. “Why We Do What We Do: A Model of Activity Consumption,” Journal of Marketing Research (50:1), pp. 24-43 Malthouse, E. C., and Blattberg, R. C. 2005. ‘‘Can We Predict Customer Lifetime Value?,’’ Journal of Interactive Marketing (19:1), pp. 2-16. McAlister, L. 1982. “A Dynamic Attribute Satiation Model for Choices Made Across Time,” Journal of Consumer Research (9:3), pp. 141-150. Rossi, P. E., McCulloch, R. E., and Allenby, G. M. 1996. “The Value of Purchase History Data in Target Marketing,” Marketing Science (15:4), pp. 321-340. Rust, R. T. 1985. “Selecting Network Television Advertising Schedules,” Journal of Business Research (13:6), pp. 483-494. Rust, R. T., and Leone, R. P. 1984. “The Mixed Media DirichletMultinomial Distribution: A Model for Evaluating TelevisionMagazine Advertising Schedules,” Journal of Marketing Research (21:1), pp. 89-99. Seetharaman, P .B. 2004. “Modeling Multiple Sources of State Dependence in Random Utility Models: A Distributed Lag Approach,” Marketing Science (23:2), pp. 263-271. Singh, V., Hansen, K., and Gupta, S. 2005. “Modeling Preferences for Common Attributes in Multicategory Brand Choice,” Journal of Marketing Research (42:2), pp. 195-209.\n",
              "\n",
              "Spissu, E., Pinjari, A. R., Bhat, C. R., Pendyala, R. M., and Axhausen, K. W. 2009. “An Analysis of Weekly Out-of-Home Discretionary Activity Participation and Time Use Behavior,” Transportation (36:5), pp. 483-510. Tellis, G. J. 1988. “The Price Sensitivity of Competitive Demand: A Meta-Analysis of Sales Response Models,” Journal of Marketing Research (25:4), pp. 331-341. Wixom, B., and Todd, P. 2005. “A Theoretical Integration of User Satisfaction and Technology Acceptance,” Information Systems Research (16:1), pp.85-102. Xu, J., Forman, C., Kim, J., and van Ittersum, K. 2014. “News Media Channels: Complements or Substitutes? Evidence from Mobile Phone Usage,” Journal of Marketing (78:4), pp. 97-112. Xu, K., Chan, J., Ghose, A., and Han, S. 2016. “Battle of the Channels: The Impact of Tablets on Digital Commerce,” Management Science, Forthcoming. Yonhap News. 2014. “Android Rules S. Korean Market in 2013” (http://english.yonhapnews.co.kr/business/2014/01/21/30/ 0501000000AEN20140121001000320F.html). Zhang, J., and Krishnamurthi, L. 2004. “Customizing Promotions in Online Stores,” Marketing Science (23:4), pp. 561-578.\n",
              "\n",
              "About the Authors\n",
              "Sang Pil Han is an assistant professor of information systems in the W. P. Carey School of Business at Arizona State University. Prior to joining Arizona State University, he was an assistant professor of Information Systems in the College of Business at the City University of Hong Kong. He received his Ph.D. in Management Engineering from Korea Advanced Institute of Science and Technology (KAIST). Han is interested in studying how firms gain business insights from big data and business analytics. He is especially interested in topics related to mobile analytics, mobile apps, mobile marketing, and social media. Han’s recent research focuses on addiction to mobile social apps, mobile commerce, ebook consumption modeling and mobile media planning. In his research, he relies on empirical research methods including econometric analyses, hierarchical Bayesian modeling, dynamic structural modeling and randomized field experiments. His papers have been published in top-tier journals such as Management Science, MIS Quarterly, and Information Systems Research, among others. Sungho Park is an associate professor of marketing at W. P. Carey School of Business at Arizona State University. He has been with Arizona State University since 2010. He holds a Ph.D. in Management (Marketing) from Cornell University, an MS in management engineering from KAIST, and a BA in linguistics from Seoul National University. He is interested in studying consumers’ shopping behaviors in various retail settings. His research also includes developing new statistical models which measure the effectiveness of marketing actions and account for consumers’ decision making behaviors. Currently, he is studying consumers’ mobile applications and digital goods usage behaviors. He has published papers in premier academic journals such as Journal of Marketing Research, Marketing Science, Management Science, and Journal of Business and Economic Statistics. Sungho served as the corresponding author on this paper.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "1007\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Wonseok Oh is the KAIST C. B. Chair Professor of Information Systems in the College of Business at Korea Advanced Institute of Science and Technology (KAIST). He received his Ph.D. in Information Systems from the Stern School of Business at New York University. His research interests include economics of information systems, mobile app consumption and addiction, e-book pricing, and social media. His research has been published in Infor-\n",
              "\n",
              "mation Systems Research, International Journal of Electronic Commerce, Journal of the Association for Information Systems, Journal of Management Information Systems, Journal of Strategic Information Systems, MIS Quarterly, Management Science, and Production and Operations Management. He served as an associate editor for MIS Quarterly from 2007 to 2009 and currently is an associate editor of Information Systems Research.\n",
              "\n",
              "1008\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4/December 2016\n",
              "\n",
              "\n",
              "BIG DATA & ANALYTICS IN NETWORKED BUSINESS\n",
              "\n",
              "MOBILE APP ANALYTICS: A MULTIPLE DISCRETE-CONTINUOUS CHOICE FRAMEWORK\n",
              "Sang Pil Han\n",
              "Department of Information Systems, W. P. Carey School of Business, Arizona State University, PO Box 874106, Tempe, AZ 85287 U.S.A. {shan73@asu.edu}\n",
              "\n",
              "Sungho Park\n",
              "Department of Marketing, W. P. Carey School of Business, Arizona State University, PO Box 874106, Tempe, AZ 85287 U.S.A. {spark104@asu.edu}\n",
              "\n",
              "Wonseok Oh\n",
              "College of Business, Korea Advanced Institute of Science and Technology, 85 Hoegiro Dongdaemoon-Gu, Seoul, KOREA 130-722 {wonseok.oh@kaist.ac.kr}\n",
              "\n",
              "Appendix A\n",
              "Mobile Apps Used and Websites Visited by Panel Members in the Data\n",
              "Table A1. Mobile Content Categories: Mobile Apps and Mobile Websites\n",
              "Category Communication Entertainment Game Mobile Messengers, Mobile Internet Phone, Email Book, Cartoon, Adults, Sports, Humor, Magazine Action, Adventure, Board, Puzzle, Racing, Role Playing, Shooting, Simulation, Sports Map, Navigation Weather, News, Education, Restaurants, Job, Health, Religion, Fashion Banking, Stocks, Finance, Real Estate, Ecommerce Radio, Music Photo Gallery, Camera Portal Site, Search Engine Count 241 1,054 2,870 Example Kakao Talk, Mypeople Messenger, Phone, GO SMS Pro, LINE, NateOn UC, LightSMS, Gmail, LightSMS, Viber, Skype Naver Webtoon, Live Scores, TIViewer, T store Book, jjComics Viewer, Naver Books, Score Center Rule The Sky, TinyFarm, Smurfs' Village, Shoot Bubble Deluxe, Hangame, 2012 Baseball Pro, Angry Birds Space, Jewels Star T map, Google Maps, SeoulBus, Naver Maps, Olle Navi, Subway Navigation, YTN News, MK News, SBS News, Weather, Bible, Newspapers, JobsKorea Smart Trading, M-Stock Smart, KB Star Banking, Auction Mobile, Coupang, Gmarket Mobile Music Player, SKY Music, PlayerPro, Mnet, MyMusicOn, FM Radio, Soribada Gallery, Camera, Cymera, Photo Editor, Instagram Naver, Daum, NATE, Google Search, Junior Naver\n",
              "\n",
              "Map and Navigation Lifestyle Personal Finance Music and Radio Photo Portal Search\n",
              "\n",
              "340 1,811 414 341 297 74\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "A1\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table A1. Mobile Content Categories: Mobile Apps and Mobile Websites (Continued)\n",
              "Category Schedule Social Utilities Video Mobile Web Sum Scheduler, Memo, Alarm Clock Social Networking Service, Board, Blog, Microblog Productivity, Decoration, Webhard, Widget, Firewall Multimedia, Broadcasting, Movie Websites Count 1,389 165 2,125 290 8,043 19,591 Example Address book, Alarm/Clock, Calendar, Memo, Polaris Office, Polaris Office, Docviewer Kakao Story, Facebook, Twitter, Naver Café, Daum Café, Cyworld, Naver Blog, Me2day Alarm Clock, Calculator, Voice Recorder, HD Browser, Dropbox, Battery Widget TV, Youtube, MX Video Player, SKY Movie, Afreeca TV, TDMB, PandoraTV Naver.com, Daum.net, Nate.com, Google.co.kr, ppomppu.co.kr, dcinside.com, facebook.com\n",
              "\n",
              "Appendix B\n",
              "Correlation Matrices of App Choice Incidence and App Use Time\n",
              "Table B1. Correlation Matrix of App Choice Incidence Map/ Navigation Communication Schedule/ Memo Entertainment Personal Finance Lifestyle Social Network Music & Radio\n",
              "\n",
              "Game\n",
              "\n",
              "Photo\n",
              "\n",
              "Portal\n",
              "\n",
              "Video\n",
              "0.11 0.13 0.13 0.13 0.14 0.11 0.15 0.22 0.16 0.15 0.15 0.16 0.10\n",
              "\n",
              "Utility\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music & Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "0.07 0.07 0.10 0.05 0.09 0.08 0.00 0.20 0.10 0.51 0.11 0.36 0.11 0.02 0.06 0.17 0.10 0.05 0.08 0.10 0.09 0.10 0.10 0.12 0.13 0.06\n",
              "\n",
              "0.10 0.06 0.11 0.18 0.17 0.13 0.14 0.14 0.12 0.08 0.14 0.13 0.06\n",
              "\n",
              "0.05 0.17 0.11 0.16 0.08 0.15 0.09 0.12 0.06 0.12 0.08 0.13 0.05\n",
              "\n",
              "0.09 0.10 0.18 0.16 0.17 0.11 0.16 0.15 0.12 0.13 0.15 0.14 0.07\n",
              "\n",
              "0.08 0.05 0.17 0.08 0.17 0.06 0.11 0.13 0.11 0.11 0.14 0.11 0.06\n",
              "\n",
              "0.00 0.08 0.13 0.15 0.11 0.06 0.14 0.11 0.05 0.13 0.07 0.15 0.05\n",
              "\n",
              "0.20 0.10 0.14 0.09 0.16 0.11 0.14 0.18 0.23 0.22 0.24 0.22 0.08\n",
              "\n",
              "0.10 0.09 0.14 0.12 0.15 0.13 0.11 0.18 0.15 0.13 0.14 0.16 -0.03\n",
              "\n",
              "0.51 0.10 0.12 0.06 0.12 0.11 0.05 0.23 0.15 0.11 0.36 0.15 0.03\n",
              "\n",
              "0.11 0.10 0.08 0.12 0.13 0.11 0.13 0.22 0.13 0.11 0.12 0.15 0.05\n",
              "\n",
              "0.36 0.12 0.14 0.08 0.15 0.14 0.07 0.24 0.14 0.36 0.12 0.16 0.05\n",
              "\n",
              "0.02 0.06 0.06 0.05 0.07 0.06 0.05 0.08 -0.03 0.03 0.05 0.05 0.10\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "A2\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "Web\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table B2. Correlation Matrix of App Use Time Map/ Navigation Communication Schedule/ Memo Entertainment Personal Finance Lifestyle Social Network Music & Radio\n",
              "\n",
              "Game\n",
              "\n",
              "Photo\n",
              "\n",
              "Portal\n",
              "\n",
              "Video\n",
              "0.01 0.01 0.01 0.02 0.09 0.10 0.09 0.02 0.11 0.06 0.04 0.09 0.09\n",
              "\n",
              "Utility\n",
              "\n",
              "Communication Game Map/Navigation Entertainment Lifestyle Personal Finance Music & Radio Photo Portal Schedule/Memo Social Network Utility Video Web\n",
              "\n",
              "-0.02 -0.02 0.01 0.02 0.04 0.00 0.13 0.37 0.10 0.11 0.27 0.12 0.01 0.05 -0.01 0.03 0.04 -0.01 0.01 -0.02 0.01 0.00 -0.01 0.06 0.01 -0.01\n",
              "\n",
              "0.01 -0.01 -0.01 0.03 0.01 0.00 0.00 0.00 0.05 -0.01 0.04 0.01 -0.01\n",
              "\n",
              "0.02 0.03 -0.01 0.00 -0.02 0.04 -0.02 0.08 0.00 0.00 0.02 0.02 0.00\n",
              "\n",
              "0.04 0.04 0.03 0.00 0.06 0.07 0.01 0.08 0.07 0.01 0.10 0.09 0.05\n",
              "\n",
              "0.00 -0.01 0.01 -0.02 0.06 0.00 -0.01 0.08 0.04 -0.02 0.05 0.10 0.05\n",
              "\n",
              "0.13 0.01 0.00 0.04 0.07 0.00 0.07 0.06 0.03 0.08 0.05 0.09 0.06\n",
              "\n",
              "0.37 -0.02 0.00 -0.02 0.01 -0.01 0.07 0.09 0.04 0.31 0.09 0.02 0.09\n",
              "\n",
              "0.10 0.01 0.00 0.08 0.08 0.08 0.06 0.09 0.04 0.09 0.06 0.11 0.09\n",
              "\n",
              "0.11 0.00 0.05 0.00 0.07 0.04 0.03 0.04 0.04 0.01 0.09 0.06 0.01\n",
              "\n",
              "0.27 -0.01 -0.01 0.00 0.01 -0.02 0.08 0.31 0.09 0.01 0.06 0.04 0.08\n",
              "\n",
              "0.12 0.06 0.04 0.02 0.10 0.05 0.05 0.09 0.06 0.09 0.06 0.09 0.07\n",
              "\n",
              "0.05 -0.01 -0.01 0.00 0.05 0.05 0.06 0.09 0.09 0.01 0.08 0.07 0.09\n",
              "\n",
              "Note: Bold: significant at the .01 level.\n",
              "\n",
              "Appendix C\n",
              "Details on the Estimation and the Scalability of the Proposed Model\n",
              "In this appendix, we provide a guideline for the estimation of the proposed model. The estimation follows the general steps of simulated maximum likelihood estimation. Below we outline each step of the procedure. 1. 2.\n",
              "¯ ¯ Determine initial values for all model parameters (α0, β , Ξβ, Gβ, Πβ, Γβ, Λβ, λ , Ξλ, Gλ, Πλ, Γλ, Λλ).\n",
              "\n",
              "Draw H random values for each of ψh, υh, φh, and νh from the standard normal distribution. Note that H is the number of panel members in the data. Statistical packages usually provide random number generators and these can be used. However, we use the Halton draws to generate random numbers because it is known to improve the simulation performance. See Chapter 9 of Train (2009) for detailed discussion on this. Using the parameter values and random numbers drawn in the previous step, calculate the following:\n",
              "\n",
              "3.\n",
              "\n",
              " M 1 − α hjt   M qhjt + τ j  M !⋅   ∏ j=0 q + τ   ⋅   j=0 1 − α  ⋅  hjt j  hjt \n",
              "Note that this is the integrand in (10). 4.\n",
              "\n",
              "∏ ( e\n",
              "J\n",
              "\n",
              "M\n",
              "\n",
              "i=0\n",
              "\n",
              "eVhit\n",
              "\n",
              "Vhjt\n",
              "\n",
              "j=0\n",
              "\n",
              ")\n",
              "\n",
              "M +1\n",
              "\n",
              "Repeat steps 2 and 3 many times and take averages of computed integrands. This average is the simulated likelihood for the given parameter values. In the model estimation, we repeat the process 100 times using the Halton draws to obtain the simulated likelihood. Maximize the simulated likelihood value (computed through step 1 to step 4) by updating model parameters. GAUSS package is used for\n",
              "\n",
              "5.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "Web\n",
              "A3\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "this numerical maximization. Specifically, we use a quasi-Newton numerical optimization algorithm developed by Broyden, Fletcher, Goldfarb, and Shanno (BFGS). The BFGS algorithm is an iterative method for solving unconstrained nonlinear optimization problems. The BFGS method is one of the most popular quasi-Newton numerical optimization algorithms. Moreover, this algorithm is suited to problems with large numbers of variables and has proven to have good performance even for non-smooth optimization problems. We have 23,222 observations (1366 users × 17 weeks) and 363 parameters to be estimated. The computation time of likelihood value (step 1 through step 4) is 6.82 seconds with a desktop PC running at 2.8 GHz. Note that, like many other packages, GAUSS supports multi-thread computing and we calculate the likelihood using eight threads with eight-core processor. Multi-threading can reduce the computing time almost linearly. If one use 16 thread with a 16-core processor running at the same speed, the computation time will be reduced by a half. To see how computation time changes as the number of observations changes in our model, we reduce the number of observation by a half (n = 11,611). We find that the computation time is 2.97 second (44% of 6.82 seconds). Currently, we consider 15 alternatives in the main model. The total estimation time of this model is about seven days. If the number of alternatives increases, the model parameters will increase almost linearly and we can expect the similar increase in the likelihood computation time. However, the number of iterations needed for the numerical optimization (step 5) will increase due to the elevated problem complexity, resulting in substantial increase in total estimation time. It seems that there are no general results specifying the relationship between the number of alternatives (or model parameters) and nonlinear optimization time. We also want to direct interested researchers to Chandra Bhat’s web page (http://www.caee. utexas.edu/prof/bhat/FULL_CODES.htm) where his GAUSS and R codes for MDCEV model and its variants are available along with technical notes.\n",
              "\n",
              "Appendix D\n",
              "Model Estimation Results in Mobile Competitive Analysis\n",
              "The results in Table D1 show that mobile users’ baseline utility for Kakao Talk (a communication app) is the highest and that their baseline utility for Rule the Sky (a game app) is the lowest among the top nine apps. Consistent with the descriptive findings, the baseline utility for Kakao Talk is the highest. Demographic variables account for the substantial heterogeneity in the baseline utilities for all mobile users. For example, older users exhibit a lower intrinsic preference for Kakao Talk (a communication app), Facebook (a social network app), Mellon (a music/radio app), and Naver (a portal search app). Moreover, although users exhibit inertia in their app choices in all apps, they show stronger tendencies of inertial use of game apps (Rule the Sky and TinyFarm). Finally, the baseline utilities for Naver, Kakao Story, and YouTube significantly increase over the sample period. The results in Table D2 show that the satiation level is the highest for YouTube and the lowest for Tiny Farm among the popular apps. A game app (TinyFarm) shows the lowest satiation, and this finding is consistent with our results based on the app category-level data. We find that there is a substantial user heterogeneity in terms of satiation levels. For example, older users show significantly lower satiation levels for TinyFarm, but exhibit a significantly higher satiation level for Kakao Talk and Mellon. The state dependence coefficients are all significantly negative and this indicates that users show notably lower satiation levels (or longer usage time) this week if the app was used last week. This can be arguably interpreted as the learning reinforcement effect, and/or habituation in app use. The estimated time trend coefficient of TinyFarm is significant and positive, capturing mobile users’ decreasing time allocation for TinyFarm over time. This might be explained by boredom with the same game over time.\n",
              "\n",
              "A4\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table D1. Estimates for Baseline Utility Parameters: App-Level Analysis\n",
              "Demographic Variables Age 40's & Over -0.41 (0.05) -0.31 (0.05) -0.41 (0.05) -0.09 (0.18) -0.69 (0.06) -0.55 (0.13) -0.90 (0.08) -0.10 (0.09) -0.09 (0.05) -0.21 (0.06) Income Midclass -0.13 (0.04) -0.08 (0.04) -0.06 (0.04) -0.29 (0.13) 0.04 (0.05) -0.05 (0.09) 0.10 (0.06) -0.09 (0.06) 0.00 (0.04) -0.12 (0.05) Income Upperclass -0.12 (0.05) -0.04 (0.05) -0.12 (0.04) -0.31 (0.15) 0.03 (0.05) -0.15 (0.11) 0.17 (0.07) -0.09 (0.07) -0.12 (0.04) -0.11 (0.05) Education High School Graduate -0.07 (0.07) -0.27 (0.07) 0.18 (0.06) 0.11 (0.21) -0.45 (0.08) 0.13 (0.14) -0.24 (0.10) -0.30 (0.11) -0.28 (0.07) -0.38 (0.08) Education University Graduate -0.04 (0.06) -0.12 (0.06) 0.05 (0.05) -0.08 (0.18) -0.32 (0.06) -0.01 (0.10) -0.40 (0.07) -0.23 (0.09) -0.26 (0.05) -0.25 (0.06)\n",
              "\n",
              "KakaoTalk Naver Kakao Story RuleTheSky Facebook TinyFarm Mellon Daum YouTube Others\n",
              "\n",
              "Constant -19.88 (0.40) -22.04 (0.39) -21.21 (0.39) -25.14 (0.44) -21.88 (0.39) -24.32 (0.41) -22.12 (0.39) -23.10 (0.40) -20.97 (0.39) -16.84 (0.39)\n",
              "\n",
              "Age 30's -0.30 (0.05) -0.21 (0.05) -0.06 (0.04) 0.11 (0.14) -0.73 (0.06) -0.17 (0.10) -0.63 (0.07) 0.01 (0.08) -0.07 (0.05) -0.09 (0.05)\n",
              "\n",
              "State Dependence\n",
              "2.29 (0.07) 3.80 (0.04) 2.65 (0.03) 6.49 (0.15) 3.66 (0.05) 5.66 (0.10) 3.17 (0.05) 4.52 (0.06) 1.29 (0.03) – –\n",
              "\n",
              "Femal e\n",
              "\n",
              "0.36 (0.04) 0.12 (0.04) 0.22 (0.03) 0.34 (0.12) 0.10 (0.04) 0.21 (0.08) 0.12 (0.05) 0.00 (0.06) 0.17 (0.03) 0.05 (0.04)\n",
              "\n",
              "Time Trend 0.007 (0.004) 0.010 (0.004) 0.007 (0.003) 0.014 (0.013) 0.003 (0.005) 0.014 (0.010) -0.001 (0.006) 0.007 (0.006) 0.009 (0.004) 0.002 (0.002)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. The estimated value of α0 is -5.37 (standard error: 0.19). The state dependence parameter of “Others” cannot be separately identified from its constant because the “Others” option is always selected by all users.\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "A5\n",
              "\n",
              "\n",
              "Han et al./A Multiple Discrete-Continuous Choice Framework\n",
              "\n",
              "Table D2. Estimates for Satiation Parameters: App-Level Analysis\n",
              "Age 40's & Over 0.79 (0.03) 0.11 (0.04) 0.40 (0.04) -0.56 (0.14) 0.40 (0.05) -1.04 (0.11) 0.80 (0.14) -0.04 (0.08) 0.23 (0.07) 0.16 (0.02) Demographic Variables Education Income Income High MidUpperSchool class class Graduate -0.04 -0.08 0.44 (0.02) (0.03) (0.04) -0.13 -0.03 0.22 (0.03) (0.04) (0.06) 0.06 0.24 -0.22 (0.03) (0.03) (0.05) 0.10 -0.22 0.28 (0.10) (0.11) (0.15) 0.03 -0.11 0.39 (0.04) (0.04) (0.07) -0.02 -0.02 -0.01 (0.07) (0.08) (0.10) -0.18 -0.31 0.55 (0.10) (0.12) (0.19) -0.19 -0.18 0.19 (0.06) (0.07) (0.11) 0.05 -0.03 -0.11 (0.05) (0.06) (0.09) 0.00 0.02 -0.04 (0.02) (0.02) (0.03)\n",
              "\n",
              "Age Constant 30's 2.83 0.57 KakaoTalk (Communication) (0.08) (0.03) 4.15 0.14 Naver (Portal) (0.07) (0.04) 4.48 0.02 Kakao Story (Social) (0.05) (0.03) 2.74 -0.19 RuleTheSky (Game) (0.20) (0.11) 3.92 0.36 Facebook (Social) (0.07) (0.05) 2.06 -0.43 TinyFarm (Game) (0.12) (0.08) Mellon 4.46 0.50 (Music/Radio) (0.13) (0.12) Daum 4.23 -0.27 (Portal) (0.11) (0.08) YouTube 4.89 -0.03 (Video) (0.07) (0.06) 1.25 0.14 Others (0.02) (0.02)\n",
              "\n",
              "-0.31 (0.02) -0.11 (0.03) -0.71 (0.03) -0.07 (0.09) -0.28 (0.04) 0.00 (0.06) -0.17 (0.08) 0.02 (0.06) -0.14 (0.05) 0.01 (0.02)\n",
              "\n",
              "Education University Graduate 0.42 (0.03) 0.11 (0.04) -0.24 (0.04) -0.20 (0.12) 0.51 (0.04) 0.01 (0.08) 0.22 (0.12) 0.36 (0.09) 0.06 (0.07) 0.04 (0.02)\n",
              "\n",
              "State Dependence\n",
              "-0.80 (0.07) -1.18 (0.05) -0.55 (0.04) -1.14 (0.17) -1.01 (0.05) -0.47 (0.09) -1.62 (0.09) -1.13 (0.08) -0.90 (0.04) – –\n",
              "\n",
              "Femal e\n",
              "\n",
              "Time Trend -0.010 (0.002) -0.014 (0.003) -0.002 (0.003) 0.011 (0.009) -0.004 (0.004) 0.014 (0.007) -0.017 (0.010) -0.023 (0.006) -0.013 (0.005) 0.002 (0.002)\n",
              "\n",
              "Note: Standard errors in parentheses. Bold: significant at the .05 level. The state dependence parameter of “Others” cannot be separately identified from its constant because the “Others” option is always selected by all users.\n",
              "\n",
              "References\n",
              "Train, K. 2009. Discrete Choice Methods with Simulation (2nd ed.), New York: Cambridge University Press.\n",
              "\n",
              "A6\n",
              "\n",
              "MIS Quarterly Vol. 40 No. 4—Appendices/December 2016\n",
              "\n",
              "\n",
              "Copyright of MIS Quarterly is the property of MIS Quarterly and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "FSP3VnTQG_9n",
        "colab_type": "code",
        "outputId": "f0f3154f-2720-4f64-916c-1da6088a6e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc12.sents if 'significant' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2014) demonstrates that the introduction of a mobile app by a major national media company significantly increases demand on the corresponding mobile news website.,\n",
              " Our empirical analysis shows highly significant and meaningful results (refer to the “Results” section), thus endowing empirical validity to the categorization.,\n",
              " In addition, women show significantly lower satiation levels for photo, social network, and communication apps than men do.,\n",
              " Users with high education show significantly higher satiation levels regarding entertainment, communication, and social networking apps.,\n",
              " Bold: significant at the .05 level.,\n",
              " Bold: significant at the .05 level.,\n",
              " Bold: significant at the .05 level.,\n",
              " Bold: significant at the .05 level.,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " People who spend a great deal of time on social apps (e.g., Facebook) also spend a significant amount of time on communication apps (e.g., WhatsApp) and photo apps (e.g., Instagram), which suggests that social, communication, and photo apps might be economic complements to each other.,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " The ability to develop strong analytical prowess and the capability to drive significant business value from the insights brought forth by mobile app analytics should occupy the core of a firm’s mobile strategy.\n",
              " ,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " Bold: significant at the .01 level.\n",
              " ,\n",
              " Finally, the baseline utilities for Naver, Kakao Story, and YouTube significantly increase over the sample period.,\n",
              " For example, older users show significantly lower satiation levels for TinyFarm, but exhibit a significantly higher satiation level for Kakao Talk and Mellon.,\n",
              " The state dependence coefficients are all significantly negative and this indicates that users show notably lower satiation levels (or longer usage time) this week if the app was used last week.,\n",
              " The estimated time trend coefficient of TinyFarm is significant and positive, capturing mobile users’ decreasing time allocation for TinyFarm over time.,\n",
              " Bold: significant at the .05 level.,\n",
              " Bold: significant at the .05 level.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "cRfV8nF_GPc4",
        "colab_type": "code",
        "outputId": "a172dae6-16c7-4dcb-9aba-bd656bc7a1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10010
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc13 = nlp(open(u\"ParkS#KeilM#BockG#KimJ_2016_Winner's regret in online C2C Auctions - an automatic thinking perspective_Information Systems Journal_6.txt\").read())\n",
        "doc13\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doi: 10.1111/isj.12075 Info Systems J (2016) 26, 613–640 613\n",
              "\n",
              "Winner ’s regret in online C2C Auctions: an automatic thinking perspective\n",
              "Sang Cheol Park,* Mark Keil,† Gee-Woo Bock‡ & Jong Uk Kim‡\n",
              "*Department of Business Administration, Daegu University, Jillyang, Gyeongsan, Gyeongbuk, 712-714, Korea, email: sangch77@gmail.com, †John B. Zellars Professor of Computer Information Systems, J. Mack Robinson College of Business, Georgia State University, P.O. Box 4015, Atlanta, Georgia, 30302-4015, USA, email: mkeil@gsu.edu and ‡Sungkyunkwan University, Jongno-gu, Seoul 110-745, Korea, email: gwbock@skku.edu, jukim@skku.ac.kr\n",
              "\n",
              "Abstract. While human beings embody a unique ability for planned behaviour, they also often act automatically. In this study, we draw on the automatic thinking perspective as a meta-theoretic lens to explain why online auction bidders succumb to both trait impulsiveness and sunk cost, ultimately leading them to experience winner’s regret. Based on a survey of 301 online auction participants, we demonstrate that both trait impulsiveness as an emotional trigger and sunk cost as a cognitive trigger promote winner’s regret. By grounding our research model in the automatic thinking view, we provide an alternative meta-theoretical lens from which to view online bidder behaviour, thus bolstering our current understanding of winner’s regret. We also investigate the moderating effects of competition intensity on the relationships between the triggers of automatic thinking and winner’s regret. Our results show that both trait impulsiveness and sunk cost have signiﬁcant impacts on winner’s regret. We also found that the relationship between these two triggers and winner’s regret is moderated by competition intensity. Keywords: winner’s regret, automatic thinking, trait impulsiveness, sunk cost\n",
              "\n",
              "INTRODUCTION\n",
              "\n",
              "Online consumer-to-consumer (C2C) auction sites have become increasingly popular since the founding of eBay in 1995. Millions of consumers visit online auction sites, and these sites have become an important channel for acquiring goods. While consumers can sometimes save money by using online auction sites, participating in online auctions often comes at a price. For example, consumers may ﬁnd themselves spending more time than they would like to admit obsessively tracking the status of an auction. Worse yet, consumers may experience ‘auction fever’ and get so caught up with winning the auction that they end up experiencing ‘winner’s regret’ (Ku et al., 2005; Peters & Bodkin, 2007), deﬁned here as winning the auction but with the subjective emotional assessment of having overpaid for the item. Prior research on auctions has often referred to winner’s regret as the winner’s curse (Foreman & Murhighan, 1996; Amyx & Luehlﬁng, 2006;\n",
              "© 2015 Blackwell Publishing Ltd\n",
              "\n",
              "\n",
              "614\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Malhotra, 2010; Adam et al., 2011). In this paper, we use the term winner’s regret rather than winner’s curse because winner’s curse usually implies that the winning bidder pays more than an auction item is worth. The term winner’s regret does not carry this connotation and is deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). In order to gain a better understanding of why winner’s regret occurs in online auctions, we introduce the perspective of automatic thinking as a meta-theoretical frame. For a long time, economists have maintained that human behaviour is best described by the rational economic model, which basically holds that human beings are self-interested and capable of perfectly weighing the costs and beneﬁts in every decision, thus enabling optimal choices (Ariely, 2008). Although human beings do, in fact, frequently make rational decisions, this does not necessarily mean that they do this all, or even most, of the time. In line with this argument, psychologists distinguish between two modes of thinking, one that is intuitive and automatic, and another that is reﬂective and rational (Thaler & Sunstein, 2009; Kahneman, 2011). Thaler & Sunstein (2009) refer to the ﬁrst mode as automatic thinking. Automatic thinking provides a useful perspective for understanding the problem of winner’s regret, because automatic thinking is rapid and instinctual (Bazerman & Moore, 2009). For example, when people get nervous during a ﬂight that experiences turbulence or smile when they see a cute baby, they are using automatic thinking. In the online auction context, automatic thinking may help to explain why individuals experience auction fever and get so caught up in the auction process (Heyman et al., 2004; Ku et al., 2005). Despite often-voiced concerns regarding the pitfalls of auction fever, the problem of winner’s regret and why it occurs has not been examined from the perspective of automatic thinking. There are two types of triggers that result in automatic thinking: emotional triggers (Strack & Deutsch, 2004; Slovic et al., 2007; Hofmann et al., 2009) and cognitive triggers (Sloman, 1996; Kahneman, 2003; Klaczynski & Cottrill, 2004; Hofmann et al., 2009). Therefore, we believe that it is important to consider both emotional and cognitive triggers of automatic thinking in order to obtain a more complete understanding why winner’s regret occurs. In this study, we thus explore two factors related to the automatic thinking that may inﬂuence winner’s regret; trait impulsiveness, which is believed to be more emotional in nature, and sunk cost, which is believed to be more cognitive in nature. We chose these two factors because they are known to inﬂuence behaviour in related contexts. For example, escalation of commitment scholars have long suspected that sunk cost can create the kind of loss framing that is believed to promote escalation behaviour, and marketing researchers have pointed to impulsiveness as a trait that can inﬂuence retail buying behaviour. In addition to these two factors, competition intensity has been shown to affect bidding behaviour in online auctions, and we include it in our study to determine if it moderates the relationship between these automatic thinking triggers and winner’s regret. In summary, our aim is to better understand winner’s regret by considering both emotional and cognitive triggers of automatic thinking and the role of competition intensity in this context. In doing so, we seek to answer two research questions: 1 To what extent do emotional and cognitive triggers of automatic thinking help us to predict winner’s regret?\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "615\n",
              "\n",
              "2 To what extent is the relationship between these automatic thinking triggers and winner’s regret moderated by competition intensity? While prior research has examined the impact of certain escalation drivers such as sunk cost on willingness to continue bidding (Park et al., 2012) and how this can result in overbidding behaviour, we know of no research that has examined the effect of both cognitive and emotional triggers on winner’s regret.1 Thus, by addressing the previous research questions, we contribute to the current body of knowledge regarding individuals’ behaviour in online auctions. From the standpoint of theoretical contribution, ours is the ﬁrst study to draw on the lens of automatic thinking to examine two kinds of triggers (one cognitive and one emotional) that may contribute to winner’s regret. Speciﬁcally, by investigating the impact of sunk cost and impulsiveness as well as the moderating role of competition intensity, we shed new light on the phenomenon of winner’s regret in online auctions.\n",
              "\n",
              "B AC K G R O U N D\n",
              "\n",
              "Online auctions and winner’s regret Online auctions are conducted over the internet and differ from traditional auctions in some important respects. First, online auctions remove the geographical constraints of traditional auctions, thus enabling worldwide participation (Ariely & Simonson, 2003). Second, online auctions can last for several days and can allow for asynchronous bidding, which makes them more ﬂexible than traditional auctions and easier for people to participate in. While online auctions are attractive in several respects, prior research has documented a number of problems associated with participating in them. These include psychological distress (i.e. anxiety, aggression, anger and depression), habitual usage, negative impacts on ﬁnances or social relations and dependency and withdrawal symptoms (Peters & Bodkin, 2007). One problem, which is the focus of our research, is winner’s regret, deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). The emotion of regret stems from the comparison of an actual outcome with a better outcome that might have resulted. In an online auction context, individuals may experience regret when they compare the actual price that they paid with their reservation price (i.e. the highest price a buyer is willing to pay). While consumers may participate in online auctions to obtain a bargain, the reality is that in many instances they end up either overpaying for what they purchase or experience regret associated with the subjective emotional assessment of having overpaid for an item. Based on an analysis of 500 online auctions for compact discs and digital video discs, Ariely & Simonson\n",
              "1\n",
              "\n",
              "Overbidding generally refers to situations in which individuals bid beyond their reservation price. Thus, overbidding need not necessarily result in winning an auction or in winner’s regret, because many auction participants may engage in overbidding but only one person will win the auction. The distinction is important because winner’s regret could inﬂuence an individual’s willingness to use online auctions in the future.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "616\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "(2003) reported that 98% of all winning bidders overpaid. Overpayment can be reduced by providing bidders with an easy means of checking the retail prices of goods. Based on a sample of 416 online auctions, Amyx & Luehlﬁng (2006) found that only 8.7% of the winning bidders overpaid when they were using auction sites that provided links to websites that allowed bidders to check the reference price of identical retail merchandise available for sale at the same website. Naturally, when a bidder believes that he or she has paid too much for an item of uncertain value (Ku et al., 2005; Robert et al., 2011), regret can occur. A rational explanation for such regret can be formulated based on three assumptions: (1) while the average bidder may accurately estimate the value of the item up for sale in an auction, some bidders will underestimate this value and others will overestimate it; (2) the bidder who most greatly overestimates the value of the item will typically win the auction; and (3) the amount of overestimation will often be greater than the difference between the winning bidder’s estimate of the value of the item and what s/he ultimately had to bid in order to acquire it (Amyx & Luehlﬁng, 2006). Under this view, regret occurs as a result of uncertainty regarding an object’s value, and thus, bidders can be economically rational and still suffer regret when their information is poor. Empirical evidence suggests that regret can arise even when bidders have perfect information (Ariely & Simonson, 2003; Amyx & Luehlﬁng, 2006), and thus, the behaviour that leads up to this can be viewed as irrational. Oh (2002) examined regret in consumer-to-consumer auctions and concluded that bidders do not necessarily behave in an economically rational way. Speciﬁcally, they tend to bid on items for the sheer enjoyment that is intrinsic in an online competition, rather than on the basis of utility in pure monetary terms. Furthermore, online auctions can produce an emotionally charged climate in which individuals try to outbid one another in the hopes of acquiring a product (Turel et al., 2011). Given that competition intensity can be high in some auctions and that things typically become more intense as the auction nears completion, the longer an individual remains engaged in the bidding process the more likely it is that s/he will experience strong emotions (Adam et al., 2011). Those who experience auction fever may ﬁnd that they have little control over their bidding and buying behaviour, ultimately spending more than they anticipated and experiencing negative feelings such as regret as a result. Thus, whether bidding behaviour is seen as rational or irrational, and whether overpayment is real (in an objective sense relative to a reference price that represents an item’s actual worth) or not the subjective emotional assessment of having overpaid for an item is a problem that is relevant for both research and practice, as winner’s regret can cause customer dissatisfaction (Amyx & Luehlﬁng, 2006). Customers who are dissatisﬁed with their online auction experience are less likely to return to the auction site, and this can be damaging to online auction service providers. Unfortunately, little is known about the factors that lead to winner’s regret, deﬁned here as regret associated with the subjective emotional assessment of having overpaid for an item (regardless of whether the amount paid actually exceeds what the item is objectively worth). Park et al. (2012) investigated how key escalation drivers (e.g. completion effect, selfjustiﬁcation and sunk cost) affect an individual’s willingness to continue bidding, which in turn leads to overbidding behaviour (i.e. bidding in excess of one’s reservation price regardless of whether or not one wins the auction). Their study dealt with overbidding behaviour and did not consider examine winner’s regret per se. Moreover, they did not examine any emotional\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "617\n",
              "\n",
              "triggers that might be associated with this phenomenon. In this paper, we explore the problem of winner’s regret using the automatic thinking perspective as a meta-theoretical lens. Automatic thinking Automatic thinking is one of two information processing approaches that guide human perception, memory, decision and attention. Automatic thinking involves rapid and parallel processing and requires little effort. Schneider & Shiffrin (1977) distinguish automatic thinking from reﬂective thinking, which is much slower. Automatic thinking does not require higher-order mental operations, which include executive functions such as making deliberate judgments and evaluations (Strack & Deutsch, 2004). In other words, automatic thinking tends to be involuntary and requires no attention, whereas reﬂective thinking is voluntary and requires attention. Moors & De Houwer (2006) also reviewed the characteristics that distinguish automatic thinking from reﬂective thinking, concluding that one of the most important distinctions between the two is the degree to which actions are subject to conscious control. When peoples’ activities are automatic, they tend to be more likely to occur autonomously, i.e. they appear to occur on their own in the absence of central control. A third characteristic of automatic thinking is its inherent attentional efﬁciency. Generally speaking, activities associated with automatic thinking occur with a minimum of attentional capacity, which leaves more capacity for the performance of other tasks. Finally, automatic thinking can be quite difﬁcult to stop or modify, because it involves relatively little in the way of conscious monitoring. In the context of online auctions, automatic thinking may explain why individuals are prone to overpayment (either real or perceived) and to experience regret as a result. Ariely & Simonson (2003) demonstrated previously that overpayment in online auctions can be conceptualised as a form of automatic thinking, in which bidders lose their self-control and get caught up in the bidding process. The key features of automatic thinking and how they apply to online auction behaviour are shown in Table 1. Both cognitive and emotional factors can trigger automatic thinking. While cognitive factors often connote reﬂective thinking, this need not necessarily be the case. In other words, there may be aspects of cognition that remain somewhat opaque to reﬂective processes or that occur with such frequency that they become automatic over time. An example of the latter would be when driving a car and we come to a red light, we automatically know to stop and we engage the brake on the automobile. Clearly, there is cognition taking place in this action, but it is not something that we consciously think about unless we are a new driver. This example illustrates how something that at one time required reﬂection can become automatic with sufﬁcient practice. There\n",
              "Table 1. Applications of automatic thinking processes to online bidding behaviour Automatic decision process Unreﬂective Effortless Fast Application of the automatic process to online bidding behaviour Bidders may not control their bidding behaviour during the bidding stage. Bidders tend to automatically make bids without making the effort to compare auction price against retail price. Bidders tend to make decisions more quickly.\n",
              "\n",
              "Source: Adapted based on Thaler & Sunstein (2009) applied to our study context.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "618\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "are also instances in which cognitive factors inﬂuence our decision-making without our being fully aware of their impact. One example of this is sunk cost. Arkes & Blumer (1985) showed that when individuals invest in season tickets, they attend more shows. Presumably, this is because they have incurred sunk cost in buying the season tickets. However, if you were to ask these individuals why they chose to attend more shows, they might not even be aware that sunk cost was a factor inﬂuencing their attendance decisions. In this sense, sunk cost can be viewed as a cognitive factor that may operate at a subconscious level, resulting in automatic thinking. While sunk cost should not inﬂuence decisions from a rational economic perspective, numerous studies (e.g. Arkes & Blumer, 1985) have found that individuals ﬁnd it difﬁcult to ignore sunk cost when they make decisions. Moreover, the concept of sunk cost is not limited to ﬁnancial investments but also extends to investments of time and effort. Consistent with prospect theory, the effect of sunk cost on decision-making is believed to arise from the manner in which decisions are framed (Kahneman & Tversky, 1979; Tversky & Kahneman, 1981). Speciﬁcally, prior research has shown that individuals are more risk-seeking when decisions are framed as a choice between losses, and sunk cost can induce such a framing. Essentially, sunk costs evoke a loss framing, and this triggers risk-seeking behaviour in accordance with prospect theory. In the online auction context, ‘previous bids and/or time invested in the auction represent sunk costs’ (Ku et al., 2005, p. 92). Consistent with Park et al. (2012), we posit that individuals perceive sunk costs as losses that can only be recouped if the individual wins the auction. It is also possible that sunk cost (in the form of time and effort) may lead to the endowment effect, which refers to the fact that people tend to ascribe more value to things merely because they own them. Research on the endowment effect has shown that owners often value an item at more than twice the level that an average buyer is willing to pay (Thaler, 1980; Kahneman et al., 1990). Carmon & Ariely (2000) suggest that this disparity can be interpreted as form of loss aversion on the part of owners. Speciﬁcally, when one owns an item, giving it up is viewed as a loss, and it is well known that individuals exhibit loss aversion (Carmon & Ariely, 2000; Looney & Hardin, 2009; Hardin & Looney, 2012). In an online auction context, sunk costs associated with the bidding process may cause bidders to become so attached to what they are bidding on they begin to develop a sense of ownership over the item. If this occurs, the endowment effect may result in bidders’ overestimating the value of the item just as owners tend to overvalue their possessions. Indeed, prior research on online auctions has offered some empirical support for product attachment as a cause of overpayment (e.g. Carmon & Ariely, 2000; Ariely & Simonson, 2003). At some point, the idea of ‘losing’ the item by not winning the auction creates behaviour that is consistent with loss aversion. Loss aversion can be explained by the value function of prospect theory and the fact that individuals weigh losses roughly two and a half times greater than equivalent gains (Looney & Hardin, 2009). In sum, sunk costs (in the form of time and effort) may produce a loss framing or create an endowment effect that leads to loss aversion, either of which could theoretically result in bidding behaviour that can result in winner’s regret. Regardless of the exact mechanism through which sunk cost inﬂuences bidding behaviour, we suggest that it serves as cognitive trigger for automatic thinking in the online auction context and that it may lead to winner’s regret. In addition to cognitive factors, emotional factors can also trigger automatic thinking. Prior research suggests that people often make snap decisions based on their emotional state (Ariely &\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "619\n",
              "\n",
              "Simonson, 2003; Amyx & Luehlﬁng, 2006). Thus, emotions can inﬂuence an individual’s decision-making without the individual even being aware of the impact. For example, the angrier one feels, ‘the more one perceives others as responsible for a negative event’ (Lerner & Tiedens, 2006, p. 118). Indeed, numerous studies have demonstrated that strong emotions can have a signiﬁcant inﬂuence on decision-making behaviour (see, for example, Andrade & Ariely, 2009). In this research, we posit that emotional as well as cognitive factors can inﬂuence online bidding behaviour. One emotional factor that can trigger automatic thinking in the online auction context is trait impulsiveness. Trait impulsiveness involves a tendency to act on a whim, displaying behaviour characterised by little or no forethought, reﬂection or consideration of consequences (VandenBos, 2007). Trait impulsiveness has been linked to automatic thinking (Hofmann et al., 2009), and in the marketing literature, it has been shown to inﬂuence buying behaviour (Rook, 1987; Rook & Fisher, 1995). Individuals who are impulsive are more likely to acquire products when presented with the opportunity (Rook & Fisher, 1995). Presumably, this is because trait impulsiveness promotes automatic thinking, causing individuals to make decisions without consciously weighing costs and beneﬁts. An example of such impulsiveness is when an individual enters a supermarket with a list of groceries to buy and encounters a display case of candy bars in a prominently placed location near the checkout. Without necessarily weighing the costs and beneﬁts associated with the purchase, individuals may be inclined to make an impulse purchase and buy themselves a treat that they had not intended to buy. When this occurs, it is often because the individual has an emotional reaction to seeing the product. In this sense, impulsiveness can be viewed as an emotional factor that operates at a subconscious level, resulting in automatic thinking. In this research, we investigate trait impulsiveness as an emotional trigger for automatic thinking in the online auction context, which may lead to winner’s regret. While the impact of trait impulsiveness has not been explored in the online auction context, prior research does suggest that emotional factors (e.g. competitive arousal) can play a role in bidding behaviour (Ku et al., 2005). Although prior research has suggested that both cognitive and emotional factors may affect online bidding behaviour, the combined effect of these two types of factors has not been examined within the conﬁnes of a single study. Table 2 provides a representative list of 11 studies that have sought to identify cognitive or emotional factors that affect online auction outcomes. The list is not intended to be exhaustive, but it contains the major studies that have been published in this area, and we believe it is representative of online auction studies that have focused particular attention on bidding behaviour. As shown in Table 2, the studies that have been conducted to date invariably focus on either the cognitive aspects of why bidders decide to make bids or the emotional aspects that drive bidding behaviour. In order to better understand bidding behaviour in online auctions, however, we believe that it is important to consider both cognitive and emotional factors in researching this phenomenon, and this is the approach taken here.\n",
              "\n",
              "R E S E A R C H M O D E L A N D H Y P OT H E S E S\n",
              "\n",
              "Drawing on the meta-theoretical perspective of automatic thinking, we explore sunk cost as a cognitive trigger and trait impulsiveness as an emotional trigger that can lead to winner’s regret. Figure 1 illustrates our proposed research model. As shown in the model, both sunk cost and\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Table 2. Previous studies on bidder’s behaviours (chronological order) Variables\n",
              "\n",
              "620\n",
              "\n",
              "Focus\n",
              "\n",
              "Authors Cognitive Past auction experience Likelihood of a bidder bidding in the ﬁnal moments of an auction Likelihood of herding bias\n",
              "\n",
              "Bidding behaviour from emotional or cognitive perspective Independent variable(s) Dependent variable(s) Key contribution\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Wilcox (2000)\n",
              "\n",
              "Dholakia et al. (2002)\n",
              "\n",
              "Emotional\n",
              "\n",
              "More experienced bidders are more likely to bid according to theoretical predictions Examined herding bias in the psychological processes of bidders in online auctions\n",
              "\n",
              "Oh (2002)\n",
              "\n",
              "Emotional\n",
              "\n",
              "Auction attributes (volume of available listings, and posted reservation price) and agent attributes (buyer and seller experience) Product value, retail price dispersion and auction type Winner’s curse\n",
              "\n",
              "Examined the differences between C2C and B2C auctions by comparing the likelihood and magnitude of the winner’s curse\n",
              "\n",
              "Stafford & Stern (2002)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "Afﬁnity with the computer, intention to use, ease of use, perceived usefulness and involvement\n",
              "\n",
              "Bid/did not bid\n",
              "\n",
              "Gilkeson & Reynolds (2003)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "Opening price, number of unexpected bids, bidder experience, seller reputation and auction length Time of entry, time of exit and number of bids Reserve price value, research price disclosure and bidding history\n",
              "\n",
              "Auction success and ﬁnal closing price\n",
              "\n",
              "Examined consumer bidding behaviour on online auction sites by illuminating three different theories such as technology acceptance model, the afﬁnity theory and the involvement theory Investigated how the characteristics of online auctions impact outcomes such as auction success and ﬁnal closing price Bidding strategy properties\n",
              "\n",
              "Bapna et al. (2004)\n",
              "\n",
              "Cognitive\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640 Cognitive Auction interests\n",
              "\n",
              "Wally & Fortin (2005)\n",
              "\n",
              "Identiﬁed heterogeneous bidder strategies and the implications for auction design Found that reserve price, reserve disclosure and the initial bidding process had\n",
              "\n",
              "(Continues)\n",
              "\n",
              "\n",
              "Table 2. (Continued)\n",
              "Variables\n",
              "\n",
              "Focus\n",
              "\n",
              "Authors\n",
              "\n",
              "Bidding behaviour from emotional or cognitive perspective Independent variable(s) Dependent variable(s)\n",
              "\n",
              "Key contribution\n",
              "\n",
              "Peters & Bodkin (2007) Problematic online auction behaviours Price\n",
              "\n",
              "Emotional\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640 Emotional Compulsive consumption, compulsive gambling and internet addiction Trait competitiveness, impulse buying, hedonic need and strategic exit (mediator) Cognitive Completion effect, selfjustiﬁcation, sunk cost and willingness to continue bidding (as a mediator) Overbidding behaviour signiﬁcant effects on a bidder’s decision-making process. A combination of these three factors inﬂuences a bidder’s auction interest Explored bidders’ behaviour that could lead to an online auction addiction Investigated the effects of impulsive-buying tendencies, trait competitiveness and hedonic need fulﬁlment of strategic exit, and moderating role that hedonic need fulﬁlment on the relationship between impulse-buying tendencies and strategic exit Examined overbidding behaviour, deﬁned as making a bid that exceeded one’s reservation price, by drawing on escalation theory\n",
              "\n",
              "Angst et al. (2008)\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "Park et al. (2012)\n",
              "\n",
              "621\n",
              "\n",
              "\n",
              "622\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "impulsiveness are posited to have direct effects on winner’s regret. Both of these relationships, however, are posited to be moderated by competition intensity, which serves as a situational moderator. Trait impulsiveness is the tendency to act on a whim, with little or no planning or reﬂection. This construct has been studied extensively by both marketing researchers and clinical psychologists. For example, in the marketing area, Rook & Fisher (1995, p. 306) conceptualised impulsiveness as a consumer trait and deﬁned it as the tendency to buy ‘spontaneously, unreﬂectively, immediately, and kinetically’. In the context of retail shopping, individuals who rated higher on trait impulsiveness have been found to be more likely to experience powerful and persistent urges to buy something immediately and to act on these urges (Beatty & Ferrell, 1997). Marketing researchers agree that impulsive buying involves an instant-gratiﬁcation component (Rook & Fisher, 1995; Hausman, 2000) and that such behaviour occurs in the spur of the moment (Angst et al., 2008). Just as trait impulsiveness can inﬂuence purchase behaviour in retail settings, we believe that it can inﬂuence behaviour in an online auction context. Specifically, we posit that impulsive individuals will be more likely to repeatedly engage in ‘spur of the moment’ bidding decisions, making them more likely to bid past their reservation price without really thinking about the consequences of their behaviour. Further, evidence from clinical psychology research suggests that trait impulsiveness is associated with an inability to suppress emotional urges. For example, Doran et al. (2004) examined that the inﬂuence of trait impulsiveness on the smokers’ ability to maintain abstinence following a 1 day smoking cessation workshop. They found that higher levels of trait impulsiveness were predictive of a more rapid return to smoking following 48 h of nicotine abstinence. In line with this, Mitchell (1999) also found that smokers with high trait impulsiveness have greater difﬁculty inhibiting smoking than other smokers. Based on the preceding text, it appears that individuals with high trait impulsiveness have difﬁculty inhibiting their emotional urges and are thus more likely to experience ‘auction fever’. In other words, individuals who are impulsive are more likely to get emotionally caught up in the dynamics of the bidding process. Thus, we suspect that individuals who have high trait impulsiveness are more likely to continue bidding without careful consideration of whether their bid\n",
              "\n",
              "Figure 1. Research model. © 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "623\n",
              "\n",
              "exceeds their reservation price, ultimately leading to a greater likelihood of experiencing winner’s regret. Based on the preceding text, we hypothesise the following:\n",
              "\n",
              "H1: Trait impulsiveness will be positively related to winner’s regret.\n",
              "The sunk cost effect, which has been explained from a prospect theory perspective (Whyte, 1986), occurs when an individual’s decision-making is inﬂuenced by prior investments of time, effort or money that are not recoverable. In the online auction context, ‘previous bids and/or time invested in the auction represent sunk costs’ (Ku et al., 2005, p. 92). As the auction progresses, individuals will tend to perceive sunk cost to be greater as their investment of time and effort becomes higher. Consistent with Park et al. (2012), we posit that sunk costs are perceived as losses that can only be recouped if the individual continues to participate in the auction. Thus, in an illusory attempt to recover their sunk cost, individuals may actually bid beyond their reservation price, ultimately leading to winner’s regret should they win the auction. As noted earlier, it is also possible that sunk cost (in the form of time and effort) may lead to the endowment effect and that this results in overestimation of value, leading to overbidding and subsequently winner’s regret. Both theoretical mechanisms suggest the following hypothesis:\n",
              "\n",
              "H2: Sunk cost will be positively related to winner’s regret.\n",
              "In the online auction context, individuals are faced with the need to make rapid decisions and are often unable to reﬂect and conduct research that would guide their bidding behaviour. As a result, people often imitate others (Bonabeau, 2004). Such imitative behaviour can lead to the formation of informational cascades (Bikhchandani et al., 1992). Informational cascades occur when individuals follow the previous behaviour of others and disregard their own information. Previous studies on informational cascades have highlighted the importance of social inﬂuence in decision-making. On the basis of prior research (Gilkeson & Reynolds, 2003; Johns & Zaichkowsky, 2003; Ku et al., 2005), it is reasonable to assume that decision dynamics can be impacted by competition intensity. Park et al. (2012) found that the strength of the relationship between bidders’ willingness to continue bidding and overbidding behaviour was greater when competition intensity was higher. Ariely & Simonson (2003) suggest that most auction participants perceive other bidders as ‘competitors’ and associate auction outcomes with ‘winning’ and ‘losing’. Ku et al. (2005) suggest that competition intensity produces ‘competitive arousal’, an emotional state that causes individuals to shift from a motivation to acquire a product for a reasonable price to a motivation to win the auction at any cost (Ku et al., 2005). Their competitive arousal model places special emphasis on two antecedents of competitive arousal: heightened perceptions of rivalry and increasing time pressure (which characteristically occurs as an auction nears completion). Based on the preceding text, we expect there to be an interaction between competition intensity and trait impulsiveness such that a more intense competitive environment can stimulate those with high trait impulsiveness to be even more likely to bid past their reservation price, thus increasing the chances that they will experience winner’s regret. Thus, we offer the following hypothesis:\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "624\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "H3: The strength of the relationship between impulsiveness and winner’s regret will be greater when competition intensity is high.\n",
              "In the online auction context, bidders invest their time and effort. These investments represent sunk costs and can only be recovered if the auction is eventually won. Ku et al. (2005) reported that high sunk costs lead to increased self-reported levels of arousal and higher bidding activity. They argued that if bidding itself is arousing, this can ‘feed a vicious cycle of bidding’. In line with this, Park et al. (2012) found that auction participants’ willingness to continue to bidding is inﬂuenced by sunk cost. Interestingly, their results also suggest that competition intensity moderates the relationship between sunk cost and willingness to continue bidding, such that the effect of sunk cost is weakened under conditions of high competition intensity. They also reported that sunk cost had a direct effect on overbidding when competition intensity was low, but not when competition intensity was high. Together, these results indicate that competition intensity can play an important moderating role in this context, weakening the effect of sunk cost. Moon (2001) suggests that the effect of sunk cost differs depending on whether an individual is focused on the past or the future, and this provides a possible theoretical explanation as to why the sunk cost effect may be weakened when competition intensity is high. Speciﬁcally, the effect of sunk cost is theorised to be less potent when an individual is thinking about the future as opposed to the past. As Moon (2001) suggests, sunk cost causes decision-makers to think about the past, which leads them to try and recover monies already spent by continuing a previously chosen course of action. However, as competition intensity increases, we theorise that individuals become more absorbed in the auction dynamics and begin to envision a future state in which the auction is over and they also begin to realise that they may or may not win the auction. Moreover, as competition intensity increases, individuals are more likely to experience the effects of competitive arousal, which is an adrenaline-laden emotional state that can arise during highly competitive bidding (Ku et al., 2005). According to the competitive arousal model of Ku et al. (2005), there are two key antecedents of competitive arousal: heightened rivalry and increasing time pressure. These conditions can create an environment in which the desire to win the auction becomes so strong that it can lead to dysfunctional behaviour (Malhotra, 2010). We propose that when competition intensity is high, the combined effects of heightened rivalry and time pressure are likely to overshadow any effects of previous investments in time and effort (i.e. sunk cost). If competition intensity is high enough, competitive arousal could even create an environment in which bidders are willing to pay more than item is worth just to deprive other bidders from obtaining the item. If this occurs, the gratiﬁcation that comes from winning the auction at any cost may diminish the negative feelings that would normally be associated with winner’s regret. Thus, the relationship between sunk cost and winner’s regret may be weakened under conditions of high competition intensity. Thus, we expect that high competition intensity causes individuals not to think about the past but rather to focus on the future, and can even lead to situations where there is gratiﬁcation associated with depriving others from winning the auction, thereby weakening the effect of sunk cost on winner’s regret. Therefore, we expect that under conditions of high competition intensity, the effect of sunk cost on bidding behaviour is weakened and the predictive value of sunk cost on winner’s regret is reduced. Based on this logic, we advance the following hypothesis:\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "625\n",
              "\n",
              "H4: The strength of the relationship between sunk cost and winner’s regret will be greater when competition intensity is low.\n",
              "\n",
              "METHODOLOGY\n",
              "\n",
              "Research approach and construct operationalisation We employed a survey approach in order to test our research model (see Appendix A for our measures, which were all based on self-reports). Impulsiveness was operationalised using a four-item scale (IMP1–IMP4) adapted and modiﬁed from Rook & Fisher (1995), which was designed to assess the extent to which an individual acts spontaneously without thinking of the consequences. Sunk cost was operationalised using a three-item scale (SC1–SC3) adapted from Park et al. (2012), which captured the extent to which the bidder perceived that it would be difﬁcult to stop bidding due to prior investment of time or effort in the auction process. Competition intensity was operationalised using a three-item scale from Park et al. (2012). These measures were designed to tap into the number of people competing in the auction and how ﬁerce the competition was perceived to be (CI1–CI3). Our dependent variable, winner’s regret, was assessed using a single-item measure designed to capture regret associated with the subjective emotional assessment of having overpaid for an item. Single-item scales are frequently seen as being less reliable than multiitem scales, but ‘as far as internal consistency reliability is concerned, there is substantial evidence indicating acceptable reliability values for single-item scales’ (Fuchs & Diamantopoulos, 2009, p. 201). Single-item scales also tend to raise concerns regarding the assessment of convergent and discriminant validity, but again, the available evidence suggests that ‘single-item measures can be both reliable and valid’ (Wanous et al., 1997; Robins et al., 2001; Wanous & Hudy, 2001; Fuchs & Diamantopoulos, 2009, p. 203). The issue of whether or not to use a single-item measure depends on a variety of factors, but the nature of the construct is certainly an important consideration (Fuchs & Diamantopoulos, 2009; Petrescu, 2013). Petrescu (2013) notes that single-item measures can be used to assess concepts that are simple and easy to understand. This includes not only concrete concepts, such as sales or expenditures, but also behavioural constructs, such as repeat purchase intention. Fuchs & Diamantopoulos (2009) suggest that it is reasonable to employ single-item measures for unidimensional constructs where there is broad agreement as to what the construct means (e.g. favorability, price perception and buying intention). In our case, the winner’s regret construct was unidimensional and easy to understand, which made it possible to assess the construct with a single measurement item (‘After purchasing the item, I regretted that I had overpaid’). What makes the construct unidimensional is that it can be measured using a single ‘ruler’ that goes from low to high. While one could argue that the general concept of regret may have multiple dimensions (i.e. perhaps one may regret something for all kinds of different reasons), we are not attempting to capture the general concept of regret. Instead, we are focusing in on and measuring a very speciﬁc kind of regret (i.e. regret associated with the subjective\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "626\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "emotional assessment of having overpaid for an item). By restricting the concept in this way, we are able to treat it as a unidimensional construct. In addition to the previous constructs, we controlled for four variables that might inﬂuence bidding and perceptions in our research setting: (1) opening price on the focal item, (2) the total number of bids placed on the focal item, (3) maximum bidding price submitted by the bidder and (4) gender.\n",
              "\n",
              "Instrument validation and data collection An initial version of the questionnaire was developed with the idea that each subject would be asked to respond based on his or her most recent online auction experience, answering questions about regret after winning an auction. Four bilingual individuals with domain expertise in online auctions and experience with survey design provided feedback that was used to reﬁne the questionnaire. The survey was developed in English and translated into Korean by two individuals who were ﬂuent in both English and Korean. Two other individuals who were also ﬂuent in both English and Korean performed a backward translation to ensure consistency between the Korean version of the measurement items and the original English version. Minor adjustments were then made to eliminate any translation-related differences and to ensure that the meaning was equivalent. The questionnaire was then pilot tested with 196 undergraduate students, allowing us to check the psychometric properties of the scales (Straub et al., 2004). Convergent validity of each scale was assessed using a principal components factor analysis. A separate principal components factor analysis was run for each of the constructs. A single eigenvalue above 1 for each construct veriﬁed that the construct was unidimensional, hence, providing evidence of convergent validity for each scale. An exploratory factor analysis with all constructs revealed a clean factor structure and exhibited item-to-construct loadings that exceeded the desired threshold of 0.5. Cronbach’s alpha was used to assess the reliability of our measures in the pilot test, and all scales were judged to exceed the normal threshold of 0.7 for reliability (Hair et al., 1998). Subsequent to the pilot test, we administered a Web-based survey that targeted individuals who had participated in online auctions using one of Korea’s leading online auction sites. We contracted with a market research ﬁrm, which agreed to administer the survey to Koreans with prior experience using online auctions to acquire goods. Our aim was to obtain a representative sample of auction users that included participants from different age ranges. We instructed the market research ﬁrm to obtain 500 responses and to restrict the sample to those who indicated they had some actual experience of buying products in online auctions. Participants were asked to recall an online auction that they had participated in within the last month in which they had actually won the auction and to complete the survey based on that auction experience. Five dollars of cyber-money was provided to each survey recipient as an incentive to complete the survey. A total of 500 responses were obtained, but some had to be dropped because they were not fully completed, leaving us with 479 completed surveys. Because we were interested in studying winner’s regret in online auctions, we restricted our analysis to survey respondents who indicated that they felt regret after purchasing their item. A total of 301 survey respondents met this threshold and these cases were retained for further analysis. Of the 301 usable responses, 70 respondents purchased clothes (23.3%), 88 respondents purchased consumer electronics\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "627\n",
              "\n",
              "(e.g. digital camera, MP3 players and used computers) (29.2%) and 143 respondents (47.5%) purchased miscellaneous goods such as watch, shoes and wallets. Our survey respondents reported using the following online auction sites the most – Auction (http://www.auction.co.kr) (82%), G-market (http://www.gmarket.co.kr) (16%) and Onket (http://www.onket.com) (2%) – as these are the most popular online auction sites in Korea. All survey items for the constructs in our study were measured on a seven-point Likert scale, which ranged from strongly disagree (1) to strongly agree (7). Our data were analysed using partial least squares and regression using SmartPLS 2.0 (Ringle et al., 2005) and the SPSS 18.0, respectively. We used PLS for our analysis as it (1) enabled us to estimate the measurement model and the structural model simultaneously, (2) is suitable for exploratory models and (3) has fewer distributional assumptions (Gefen & Straub, 2005). We chose PLS over covariance-based Structural Equation Modeling (SEM) also because our emphasis is on prediction rather than model ﬁt.\n",
              "\n",
              "DATA A N A LY S I S A N D R E S U LT S\n",
              "\n",
              "Descriptive analysis Table 3 shows the demographic proﬁle of our respondents. 67.4% of our respondents were male and 32.6% were female. Most respondents (75.1%) were in the 21–40 years age group. Most respondents (46.2%) placed bids between one and three times and 27.2% of total respondents placed bids between four and six times. We employed two methods to test for common method bias (CMB) as it represents a potential threat to validity given our study design. First, we used Harman’s one-factor test (Podsakoff & Organ, 1986). According to Podsakoff & Organ (1986), if a single factor emerges from the factor analysis, this may be indicative of a serious CMB threat. In order to conduct this test, we entered all 11 measurement items into a principal component analysis and examined the results of the unrotated factor solution. Four factors were extracted, accounting for 26.39%, 24.38%, 24.21% and 8.90% of the variance, respectively. This result suggests that CMB was not a signiﬁcant threat in our study. Second, we conducted marker variable analysis per Lindell & Whitney (2001) in which unrelated constructs (termed marker variables) are used to adjust the correlations among the principle constructs. We identiﬁed two unrelated constructs (opening price and perceived ease of use), which were assessed as part of the survey. High correlations among any of the items of the study’s principal constructs and unrelated constructs would indicate common method bias as the constructs of opening price and perceived ease of use should be weakly related to the study’s principle constructs. The results of our marker variable analysis (described in Appendix F) suggest that common method bias was likely not a signiﬁcant threat in this study. Measurement model For the measurement model, each construct was modelled reﬂectively. The measurement model was tested by examining convergent and discriminant validity (Fornell & Larcker,\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "628\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Table 3. Sample demographics Items Gender Age (years) Category Men Women 10–19 20–29 30–39 40–49 Over 50 1–3 4–6 7–10 11–15 Over 16 1–3 4–6 7–10 11–15 Over 16 Frequency 203 98 8 92 134 57 10 139 82 31 24 25 136 116 32 15 5 Percentage (%) 67.40 32.60 2.70 30.60 44.50 18.90 3.30 46.20 27.20 10.30 8.00 8.30 45.2 38.5 10.6 4.0 1.7\n",
              "\n",
              "Number of Bids for a Product\n",
              "\n",
              "Number of visits to online auction site (monthly)\n",
              "\n",
              "1981). Two different assessments were made for convergent validity: (1) individual item reliability and (2) construct reliability. Individual item reliability was assessed by examining the item-toconstruct loadings for each construct that was measured with multiple indicators. In order for the shared variance between each item and its associated construct to exceed the error variance, the standardised loadings should be greater than 0.70. As can be seen in Appendix B, all of our item to-construct loadings exceeded the desired threshold. The next step in establishing measurement reliability was to examine the internal consistency for each block of measures (i.e. construct reliability). This was performed by examining the composite reliability, Cronbach’s alpha and the average variance extracted (AVE) for each block of measures, as shown in Table 4. Composite reliability and Cronbach’s alpha both measure the internal consistency within a given construct’s items. The threshold values for composite reliability and Cronbach’s alpha are not absolute ones, but our measures appear to be more than acceptable by established criteria. Bearden et al. (1993) claim that a score of 0.7 indicates ‘extensive’ evidence of reliability and a score of 0.8 or higher provide ‘exemplary’ evidence. As shown in Table 4, all of the constructs in our measurement model exhibited composite reliabilities of 0.86 or higher, and they all exhibited Cronbach’s alpha of 0.80 or higher. The guideline threshold for AVE is 0.5, meaning that 50% or more variance of the indicators is accounted for Chin (1998). As Appendix C indicates, all of the constructs in our measurement model exceeded the established criteria for AVE. We conducted two tests for discriminant validity. First, we calculated each indicator’s loading on its own construct as well as its cross-loading on all other constructs (Appendix B). The loadings for the indicators for each construct are higher than the cross-loadings for other constructs’ indicators. Additionally, going across the rows, each indicator has a higher loading with its construct than a cross-loading with any other construct. This provides good evidence of discriminant validity (Chin, 1998, p. 321).\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "629\n",
              "\n",
              "As a second test of discriminant validity, we considered whether the square roots of the AVEs of the latent constructs were greater than the correlations among the latent constructs. When this is true, more variance is shared between the latent construct and its block of indicators than with another construct (Chin, 1998). As can be seen by reading across the rows of Appendix C, our measures passed this test, thus providing additional evidence of discriminant validity.\n",
              "\n",
              "Hypotheses testing The explanatory power of a structural model can be evaluated by looking at the R2 value (variance accounted for) of the ﬁnal dependent construct. The ﬁnal dependent construct in this study (winner’s regret) has an R2 value of 0.229, indicating that the model accounts for 22.9% of the variance in the dependent variable. As shown in Figure 2, the path between impulsiveness and winner’s regret (β = 0.276, t = 3.846) and the path between sunk cost and winner’s regret (β = 0.266, t = 3.628) were both signiﬁcant at p < 0.01. These results provide strong support for H1 and H2. None of the control variables were found to be signiﬁcant. While some have suggested that gender differences do exist when it comes to regret, we may not have observed this because of the fact that there were far fewer females than males in our study.\n",
              "\n",
              "Moderating effects of competition intensity In order to test the moderating effect of competition intensity (H3–H4) on the relationship between our two independent variables (trait impulsiveness and sunk cost) and our\n",
              "Table 4. Descriptive statistics and reliability of constructs Total sample group (n = 301) Competition intensity Trait impulsiveness Winner’s regret Sunk cost High competition intensity group (n = 155) Trait impulsiveness Winner’s regret Sunk cost Low competition intensity group (n = 146) Trait impulsiveness Winner’s regret Sunk cost\n",
              "SD, standard deviation.\n",
              "\n",
              "Mean 4.60 4.21 3.96 4.35 Mean\n",
              "\n",
              "SD 1.17 1.22 1.25 1.30 SD\n",
              "\n",
              "Cronbach’s alpha 0.92 0.86 — 0.96 Cronbach’s alpha 0.89 — 0.94 Cronbach’s alpha 0.80 — 0.95\n",
              "\n",
              "Composite reliability 0.95 0.91 — 0.97 Composite reliability 0.92 — 0.96 Composite reliability 0.86 — 0.97\n",
              "\n",
              "AVE 0.87 0.71 — 0.92 AVE\n",
              "\n",
              "4.43 4.23 4.93 Mean\n",
              "\n",
              "1.29 1.38 1.16 SD\n",
              "\n",
              "0.75 — 0.89 AVE\n",
              "\n",
              "3.98 3.66 3.73\n",
              "\n",
              "1.05 1.03 1.15\n",
              "\n",
              "0.61 — 0.91\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "630\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Figure 2. Path analysis.\n",
              "\n",
              "dependent variable (winner’s regret), we performed a subgroup analysis as explained in the succeeding text. Before embarking on a subgroup analysis for competition intensity (CI), we ﬁrst needed to determine whether CI acts as a moderator, and if so, what type of moderator it is. In order to investigate the moderating role of CI on the relationship between our two predictors (trait impulsiveness and sunk cost) and our criterion variable (winner’s regret), we followed the moderated regression analysis (MRA) procedure recommended by Sharma et al. (1981). Using MRA, one can determine the type of moderator based on a few simple rules. If there is an interaction effect and no direct effect with criterion or predictor variables, we can conclude that the variable is a pure moderator. If there is an interaction effect and a direct relationship with the predictor, the criterion variable or both, we can conclude that the variable is a quasimoderator. If there is neither a direct effect nor a moderation effect but the detected interaction derives from unequal measurement errors across subsamples, we can conclude that the variable is a homologiser. Based on the MRA procedure, and applying a strict p < 0.05 signiﬁcance threshold, we concluded that CI was a moderator, but that it is neither a pure moderator, nor a quasi-moderator. Instead, CI acts as a homologiser (Appendix D). A homologiser Z acts as moderator in that it inﬂuences the strength of the relationship between X (an independent variable) and Y (a dependent variable) but is not itself related to X or Y and does not interact with X. Under such circumstances, Z exerts its inﬂuence through the error term, and the appropriate way of analysing the moderating effect of Z is by partitioning the dataset and performing a subgroup analysis (Sharma et al., 1981; Allison et al., 1992). In order to do this, we split the sample into high competition intensity and low competition intensity subgroups. This was performed by splitting the sample at the mean value of CI (4.60), after which, we also tested both reliability and validity for each subgroup. Appendix B and Appendix C show that all items in the CI subgroup (n = 155) demonstrate acceptable loadings (0.788–0.950), as do all items in the low-CI subgroup (n = 146) (0.779–0.969). In addition, the reliability indicators are all well above accepted thresholds, and the AVEs are greater than 0.5.\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "631\n",
              "\n",
              "Following Carte & Russell’s (2003) suggestion, we assessed whether the latent constructs were perceived in a similar fashion between the high-CI and low-CI subgroups. An examination of Appendix B suggests that the loading patterns are very similar, thus suggesting that meaningful comparisons can be made between groups. In addition, a measurement invariance analysis was performed to further validate the similarity of measurement models between the two subgroups (Cheung & Rensvold, 2002). Appendix E provides support for measurement invariance, and on that basis, we concluded that meaningful path coefﬁcient comparisons could be made across subgroups. With the measurement model appearing to be stable and adequate across the subgroups, we proceeded to analyse the structural model for each subgroup. Consistent with the Sharma et al. (1981) approach for analysing a homologiser, we tested the moderating effect of competition intensity by estimating two separate models in PLS, namely, the high-CI subgroup and the low-CI subgroup. This approach allowed us to examine the moderating effect of CI by looking at the differences in the magnitude of the path coefﬁcient from impulsiveness to winner’s regret across groups using the approach suggested by Chin et al. (2003). This involved computing a t-statistic2 as follows: ﬃ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ È É ½ðN À 1Þ=ðN 1 þ N 2 À 2ÞÂ½ðN 2 À 1Þ=N 1 þ N 2 À 2ÂSE 2 2 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃÃ Â t ¼ ðPC 1 À PC 2 Þ= S pooled Â ð1=N 1 Þ þ ð1=N 2 Þ\n",
              "\n",
              "S pooled ¼\n",
              "\n",
              "As shown in Table 5, comparison of the path coefﬁcient from impulsiveness to winner’s regret is larger for the high-CI subgroup (β = 0.313) than for the low-CI subgroup (β = 0.184), whereas the path coefﬁcient from sunk cost to winner’s regret is larger for the low-CI subgroup (β = 0.337) than for the high-CI subgroup (β = 0.168). In other words, trait impulsiveness has a greater impact on winner’s regret when the level of competition is high, thus supporting H3, whereas sunk cost has a greater impact on winner’s regret when competition intensity is low, thus supporting H4. These ﬁndings indicate that the impact of trait impulsiveness as well as sunk cost on winner’s regret differs depending on the level of competition intensity. As indicated in Table 6, all of our hypotheses were supported.\n",
              "\n",
              "C O N C L U S I O N S A N D I M P L I C AT I O N S\n",
              "\n",
              "In this study, we applied the automatic thinking perspective as a meta-theoretic lens to explain why online auction bidders succumb to both trait impulsiveness and sunk cost, ultimately leading them to experience winner’s regret. This perspective allowed us to generate insights into one possible mechanism underlying winner’s regret, by focusing our attention on both a cognitive trigger (i.e. sunk cost) and an emotional trigger (i.e. trait impulsiveness) of automatic thinking. To add further richness to our research model, we also considered a situational moderator (i.e. competition intensity). Our results show that both impulsiveness and sunk cost can promote winner’s regret and that the\n",
              "2 where, Spooled: the pooled estimator of the variance; PCi: path coefﬁcient in structural model of competition intensity group i; Nj: sample size of dataset for competition intensity i; SEi: standard error of path in structural model of competition intensity i; and tij: t-statistic with N1 + N2 À 2 degrees of freedom.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "632\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Table 5. Comparisons of paths in each group (competition intensity) High CI (n = 155) From ➔ to Trait impulsiveness ➔ winner’s regret Sunk cost ➔ winner’s regret\n",
              "SE, standard error.\n",
              "\n",
              "Low CI (146) Path coefﬁcient 0.184 0.337 SE 0.098 0.077 R\n",
              "2\n",
              "\n",
              "Path coefﬁcient 0.313 0.168\n",
              "\n",
              "SE 0.101 0.102\n",
              "\n",
              "t-statistic 11.235 16.149\n",
              "\n",
              "0.168 0.196\n",
              "\n",
              "Table 6. Summary of hypotheses testing results # 1 2 3 4 Hypotheses Trait impulsiveness will be positively related to winner’s regret. Sunk cost will be positively related to winner’s regret. The strength of relationship between trait impulsiveness and winner’s regret will be greater when competition intensity is high. The strength of relationship between sunk cost and winner’s regret will be greater when competition intensity is low. Results Supported Supported Supported Supported\n",
              "\n",
              "relationship between these triggers and winner’s regret is moderated by competition intensity. Speciﬁcally, we found that competition intensity strengthens the relationship between impulsiveness and winner’s regret, but that it weakens the relationship between sunk cost and winner’s regret. Before turning to the implications of our study, it is appropriate to consider its limitations. First, we relied on a survey-based approach and did not collect actual bidding data. This means that our measures are subjective and open to potential recall bias. To minimise the risk of recall bias, we asked participants to recall an online auction that they had participated in within the last month and to complete the survey based on that auction experience. One beneﬁt of our approach is that we were able to gather data in an unobtrusive manner that did not risk interfering with the decisionmaking of the participants as they engaged in the auction process (Todd & Benbast, 1987). Second, we used a single-item measure for our dependent variable that cannot be assessed for reliability. Although we suggest that the use of a single-item measure is justiﬁed in this context due to the unidimensional nature of our focal construct, future research should be conducted with a multi-item measure that can be assessed for reliability to test the robustness of our ﬁndings. Third, although we employed automatic thinking as a meta-theoretic perspective to guide our research, we did not gather the kinds of physical measurements (e.g. functional Magnetic Resonance Imaging (fMRI)) that would allow us to conﬁrm the proposed mechanism and the brain activity associated with it. Additional research is needed in order to conﬁrm the underlying mechanism posited here and to probe other factors (both cognitive and emotional) that may also promote or impede winner’s regret. In spite of the aforementioned limitations, we believe that our work has important implications for both research and practice.\n",
              "\n",
              "Implications for research and practice This research makes several important contributions to both research and practice. In this paper, we draw upon the automatic thinking perspective and consider both cognitive and\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "633\n",
              "\n",
              "emotional factors in order to better understand bidding behaviour in online auctions. Speciﬁcally, we provide empirical evidence that both trait impulsiveness and sunk cost inﬂuence winner’s regret. Further, we show that competition intensity has a moderating role on these relationships. Speciﬁcally, trait impulsiveness has a greater impact on winner’s regret when the level of competition is high, whereas sunk cost has a greater impact on winner’s regret when competition intensity is low. These ﬁndings indicate that the impact of trait impulsiveness as well as sunk cost on winner’s regret differs depending on the level of competition intensity. Our ﬁndings also have practical implications for online auction participants as well as auction site operators. From the perspective of both online auction participants as well as online auction providers, it is important to know that there can be negative repercussions (i.e. winner’s regret) that result from both the time and the energy that an individual invests in an auction as well as the degree of impulsiveness that an individual brings to the auction. For online auction participants, minimising winner’s regret requires getting in the habit of honouring one’s initial reservation price and resisting the temptation to obsessively check the status of the auction and to incrementally increase one’s bidding limit when one is outbid. Auction participants should be especially wary about falling into a trap that will lead to winner’s regret when competition intensity is high. Under these conditions, participants need to be very careful not to succumb to their own impulsiveness even if it means losing the auction. For auction site operators, our ﬁndings create the possibility of predicting which individuals will be more likely to experience winner’s regret. Speciﬁcally, auction site users could be given a survey prior to participating in online auctions to determine their trait impulsiveness and susceptibility to the sunk cost effect. The results could be used to gauge how likely it is that individuals will experience winner’s regret and auction site users could be warned beforehand. In cases where the risk of winner’s regret is high, the auction site could even recommend that individuals use the ‘buy it now’ feature instead of online bidding. Our research also provides a tool for online auction sites to survey online auction winners to determine the extent to which they experienced winner’s regret. This can be an informative diagnostic tool for helping auction sites to gauge whether winner’s regret is experienced by a handful of users or whether it is something that is experienced more broadly. Once this is known, site management can determine whether any corrective action is needed.\n",
              "\n",
              "REFERENCES\n",
              "Adam, M.T.P., Krämer, J., Jähnig, C., Seifert, S. & Weinhardt, C. (2011) Understanding auction fever: a framework for emotional bidding. Electronic Markets, 21, 197–207. Allison, D.B., Heshka, S., Pierson, R.N.J., Wang, J. & Heymsﬁeld, S.B. (1992) The analysis and identiﬁcation of homologizer/moderator variables when the moderator is continuous: an illustration with anthropometric data. American Journal of Human Biology, 4, 775–782. Amyx, D.A. & Luehlﬁng, M.S. (2006) Winner’s curse and parallel sales channels-online auctions linked within e-tail websites. Information and Management, 43, 919–927. Andrade, E.B. & Ariely, D. (2009) The enduring impact of transient emotions on decision making. Organizational Behavior and Human Decision Processes, 109, 1–8. Angst, C.M., Agarwal, R. & Kuruzovich, J.H. (2008) Bid or buy? individual shopping traits as predictors of strategic\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "634\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "exit in online auctions. International Journal of Electronic Commerce, 13, 59–84. Ariely, D. (2008) Predictably Irrational – The Hidden Forces that Shape our Decisions. Harper Collins Publisher, Hammersmith, London, UK. Ariely, D. & Simonson, I. (2003) Buying, bidding, playing, or competing? value assessment and decision dynamics in online auctions. Journal of Consumer Psychology, 13, 113–123. Arkes, H.R. & Blumer, C. (1985) The psychology of sunk cost. Organizational Behavior and Human Decision Processes, 35, 124–140. Bapna, R., Goes, P., Gupta, A. & Jin, Y. (2004) User heterogeneity and its impact on electronic auction market design: an empirical exploration. MIS Quarterly, 28, 21–43. Bazerman, M.X. & Moore, D.A. (2009) Judgment in Managerial Decision Making. John Wiley & Sons, Inc., NY. Bearden, W.O., Netemeyer, R.G. & Mobley, M.F. (1993) Handbook of Marketing Scales: Multi-Item Measures for Marketing and Consumer Behavior Research. Sage Publications, Newbury Park, CA. Beatty, S.E. & Ferrell, M.E. (1997) Impulse buying: modeling its precursors. Journal of Retailing, 74, 169–191. Bikhchandani, S., Hirshleifer, D. & Welch, I. (1992) A theory of fads, fashion, custom, and cultural change as informational cascades. Journal of Political Economy, 100, 992–1026. Bonabeau, E. (2004) The perils of the imitation age. Harvard Business Review, 82, 45–54. Carmon, Z. & Ariely, D. (2000) Focusing on the foregone: how value can appear so different to buyers and sellers. Journal of Consumer Research, 27, 360–370. Carte, T.A. & Russell, C.J. (2003) In pursuit of moderation: nine common errors and their solutions. MIS Quarterly, 27, 479–501. Cheung, G.W. & Rensvold, R.B. (2002) Evaluating goodness-of-ﬁt indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255. Chin, W.W. (1998) The partial least square approach to structural equation modeling. In: Modern Method for Business Research, Marcoulides, G.A. (ed.), pp. 150–170. Lawrence Erlbaum, Mahwah, NJ. Chin, W.W., Marcolin, B.L. & Newsted, P.R. (2003) A partial least squares latent variable modeling approach for measuring interaction effects: results from a Monte Carlo simulation study and an electronic-mail emotion/adoption study. Information Systems Research, 14, 189–217. Dholakia, U.M., Basuroy, S. & Soltysinski, K. (2002) Auction or agent (or both)? a study of moderators of the herding bias in digital auctions. International Journal of Research in Marketing, 19, 115–130.\n",
              "\n",
              "Doran, N., Spring, B., McChargue, D., Pergadia, M. & Richmond, M. (2004) Impulsivity and smoking relapse. Nicotine & Tobacco Research, 6, 641–647. Foreman, P. & Murhighan, J.K. (1996) Learning to avoid the winner’s curse. Organizational Behavior and Human Decision Processes, 67, 170–180. Fornell, C. & Larcker, D.F. (1981) Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18, 39–50. Fuchs, C. & Diamantopoulos, A. (2009) Using single-item measures for construct measurement in management research. Die Betriebswirtschaft, 69, 195–210. Gefen, D. & Straub, D. (2005) A practical guide to factorial validity using PLS-graph: tutorial and annotated example. Communications of the Association for Information Systems, 16, 91–109. Gilkeson, J.H. & Reynolds, K. (2003) Determinants of Internet auction success and closing price: an exploratory study. Psychology & Marketing, 20, 537–566. Hair, J.F. Jr., Anderson, R.E., Tatham, R.L. & Black, W.C. (1998) Multivariate Data Analysis, 5th edn. Prentice-Hall, Upper Saddle River, NJ. Hardin, A. & Looney, C. (2012) Myopic loss aversion: demystifying the key factors inﬂuencing decision problem framing. Organizational Behavior and Human Decision Processes, 117, 311–331. Hausman, A. (2000) A multi-method investigation of consumer motivations in impulse buying behavior. Journal of Consumer Marketing, 17, 403–419. Heyman, J.E., Orhun, Y. & Ariely, D. (2004) Auction fever: the effect of opponents and quasi-endowment on product valuations. Journal of Interactive Marketing, 18, 7–21. Hofmann, W., Friese, M. & Strack, F. (2009) Impulse and self-control from a dual-systems perspective. Perspective on Psychological Science, 4, 162–176. Johns, C.L. & Zaichkowsky, J.L. (2003) Bidding behavior at the auction. Psychology & Marketing, 20, 303–322. Kahneman, D. (2003) A perspective on judgment and choice: mapping bounded rationality. American Psychologist, 58, 697–720. Kahneman, D. (2011) Thinking, Fast and Slow. Farrar, Straus and Giroux, New York, NY. Kahneman, D. & Tversky, A. (1979) Prospect theory: an analysis of decision under risk. Econometrica, 47, 263–291. Kahneman, D., Knetsch, J.L. & Thaler, R.H. (1990) Experimental tests of the endowment effect and the Coase theorem. Journal of Political Economy, 98, 1325–1348.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "Klaczynski, P.A. & Cottrill, J.I.M. (2004) A dual process approach to cognitive development: the case of children’s understanding of sunk cost decision. Thinking and Reasoning, 10, 147–174. Ku, G., Malhotra, D. & Murnighan, J.K. (2005) Towards a competitive arousal model of decision-making: a study of auction fever in live and Internet auctions. Organizational Behavior & Human Decision Processes, 96, 89–103. Lerner, J.S. & Tiedens, L.Z. (2006) Portrait of the angry decision maker: how appraisal tendencies shape anger’s inﬂuence on cognition. Journal of Behavioral Decision Making, 19, 115–137. Lindell, M.K. & Whitney, D.J. (2001) Accounting for common method variance in cross-sectional research designs. Journal of Applied Psychology, 86, 114–121. Looney, C. & Hardin, A. (2009) Decision support for retirement portfolio management: overcoming myopic loss aversion via technology design. Management Science, 55, 1688–1703. Malhotra, D. (2010) The desire to win: the effects of competitive arousal on motivation and behavior. Organizational Behavior and Human Decision Processes, 111, 139–146. Malhotra, N.K., Kim, S.S. & Patil, A. (2006) Common method variance in IS research: a comparison of alternative approaches and reanalysis of past research. Management Science, 52, 1865–1883. Mitchell, S.H. (1999) Measures of impulsivity in cigarette smokers and non-smokers. Psychopharmacology, 146, 455–464. Moon, H. (2001) Looking forward and looking back: integrating completion and sunk cost effect within an escalation of commitment progress decision. Journal of Applied Psychology, 86, 104–113. Moors, A. & De Houwer, J. (2006) Automaticity: a theoretical and conceptual analysis. Psychological Bulletin, 132, 297–326. Oh, W. (2002) C2C versus B2C: a comparison of the winner’s curse in two types of electronic auctions. International Journal of Electronic Commerce, 6, 115–138. Park, S.C., Keil, M., Kim, J.U. & Bock, G.W. (2012) Understanding overbidding behavior in C2C auctions: an escalation theory perspective. European Journal of Information Systems, 21, 643–663. Peters, C. & Bodkin, C.D. (2007) An exploratory investigation of problematic online auction behaviors: experiences of eBay users. Journal of Retailing & Consumer Services, 14, 1–16.\n",
              "\n",
              "635\n",
              "\n",
              "Petrescu, M. (2013) Marketing research using single-item indicators in structural equation models. Journal of Marketing Analytics, 1, 99–107. Podsakoff, P.M. & Organ, D.W. (1986) Self-reports in organizational research: problems and prospects. Journal of Management, 12, 531–554. Ringle, C.M., Wende, S. & Will, A. (2005) SmartPLS 2.0 (M3) beta, Hamburg, Germany. URL http://www. smartpls.de accessed on July 06, 2012. Robert, F.E., Charles, A.W. & Sharad, S. (2011) Bidding patterns, experience, and avoiding the winner’s curse in online auctions. Journal of Management Information Systems, 27, 241–268. Robins, R.W., Hendin, H.M. & Trzesniewski, K.H. (2001) Measuring global self-esteem: construct validation of a single-item measure and the Rosenberg self-esteem scale. Personality and Social Psychology Bulletin, 27, 151–161. Rook, D.W. (1987) The buying impulse. Journal of Consumer Research, 14, 189–199. Rook, D.W. & Fisher, R.J. (1995) Trait and normative aspects of impulsive buying behavior. Journal of Consumer Research, 22, 305–313. Schneider, W. & Shiffrin, R.M. (1977) Controlled and automatic human information processing. I: detection, search and attention. Psychological Review, 84, 1–66. Sharma, S., Durand, R.M. & Gur-Arie, O. (1981) Identiﬁcation and analysis of moderator variables. Journal of Marketing Research, 18, 291–300. Sloman, S.A. (1996) The empirical case of two systems of reasoning. Psychological Bulletin, 119, 3–22. Slovic, P., Finucane, M.L., Peters, E. & MacGregor, D.G. (2007) The affect heuristic. European Journal of Operational Research, 177, 1333–1352. Stafford, M.R. & Stern, B. (2002) Consumer bidding behavior on Internet auction sites. International Journal of Electronic Commerce, 7, 135–150. Strack, F. & Deutsch, R. (2004) Reﬂective and impulsive determinants of social behavior. Personality and Social Psychology Review, 8, 220–247. Straub, D., Boudreau, M.C. & Gefen, D. (2004) Validation guidelines for IS positivist research. Communications of AIS, 14, 380–427. Thaler, R.H. (1980) Toward a positive theory of consumer choice. Journal of Economic Behavior and Organization, 1, 39–60. Thaler, R.H. & Sunstein, C.R. (2009) Nudge: Improving Decisions about Health Wealth and Happiness. Penguin Books, New York, NY.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "636\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "Todd, P. & Benbast, I. (1987) Process tracing methods in decision support systems research: exploring the black box. MIS Quarterly, 11, 493–512. Turel, O., Serenko, A. & Giles, P. (2011) Investigating technology addiction and use: an empirical investigation of online auction users. MIS Quarterly, 35, 1043–1061. Tversky, A. & Kahneman, D. (1981) The framing of decisions and the psychology of choice. Science, 211, 453–458. VandenBos, G.R. (2007) APA Dictionary of Psychology. American Psychological Association, Washington, DC. Wally, M.J.C. & Fortin, D.R. (2005) Behavioral outcomes from online auctions: reserve price, reserve disclosure, and initial bidding inﬂuences in the decision process. Journal of Business Research, 58, 1409–1418. Wanous, J.P. & Hudy, M.J. (2001) Single-item reliability: a replication and extension. Organizational Research Methods, 4, 361–375. Wanous, J.P., Reichers, A.E. & Hudy, M.J. (1997) Overall job satisfaction: how good are single item measures? Journal of Applied Psychology, 82, 247–252. Whyte, G. (1986) Escalating commitment to a course of action: a reinterpretation. Academy of Management Review, 11, 311–321. Wilcox, R.T. (2000) Experts and amateurs: the role of experience in Internet auctions. Marketing Letters, 11, 363–374.\n",
              "\n",
              "Business at Georgia State University. His research focuses on IT project management and includes work on preventing IT project escalation, identifying and managing IT project risks, and improving IT project status reporting. His interests also include IT implementation and use. Keil has published more than 100 refereed publications including papers that have appeared in MIS Quarterly, Journal of Management Information Systems, Decision Sciences, Strategic Management Journal and many other journals. He currently serves as a Senior Editor for Information Systems Research and is a member of the editorial board for the Journal of Management Information Systems. He has also served on the editorial boards of MIS Quarterly, Decision Sciences, IEEE Transactions on Engineering Management and The DATA BASE for Advances in Information Systems. Gee-Woo Bock received his PhD in Management Engineering from the Korea Advanced Institute of Science and Technology, Seoul, Korea, in 2001 after 7 years of working at the Department of Strategic Planning in Samsung Economic Research Institute. After acquiring his PhD, he joined the School of Computing at the National University of Singapore as an Assistant Professor in 2002. He is currently a Professor at the School of Global Business, Sungkyunkwan University in Korea. He has served Information and Management as a member of Associate Editors since 2012. His current research interests include Knowledge Management, Social Network Services, Personalization, IOS/SCM, Service Systems in Healthcare & Tourism. His papers have been published in MIS Quarterly, JAIS, EJIS, DSS, JGIM, IEEE TEM, CACM, I&M, IJEC, The Data Base: Advances in Information Systems and among others. Jong Uk Kim is currently Professor at the Business School of Sungkyunkwan University in Korea. He received his PhD in Computer Information Systems from Georgia State University. His research focuses on the areas of knowledge transfer in IT project, online consumer Behaviour, IT outsourcing and decision support systems. His papers have been published in European Journal of Information Systems, Computers in Human Behavior, International Journal of Human-Computer Studies and among others.\n",
              "\n",
              "Biographies\n",
              "Sang Cheol Park is currently an Assistant Professor of the Department of Business Administration at Daegu University in Korea. He received his PhD in MIS from Sungkyunkwan University in Korea. His research focuses on the areas of escalation of commitment, online bidding behaviour, cloud computing, knowledge transfer in IT project and so on. His papers have been published in the European Journal of Information Systems, Journal of Global Information Management, Computers in Human Behavior, Journal of Computer Information Systems and among others. Mark Keil is a John B. Zellars Professor of Computer Information Systems in the J. Mack Robinson College of\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "637\n",
              "\n",
              "APPENDIX A. MEASUREMENT ITEMS FOR KEY CONSTRUCTS\n",
              "Constructs Impulsiveness No. IMP1 IMP2 IMP3 IMP4 SC1 SC2 SC3 Winner’s regret Competition intensity WR CI1 CI2 CI3 Measures I usually do things on impulse. I often behave without thinking of the consequences. I often say the ﬁrst thing I think. I often act on the spur of the moment. I could not stop bidding because I had already spent too much effort in the process. I could not stop bidding because I had already spent too much time in the process. Overall, it would have been a waste of time and effort if I stopped bidding. After purchasing the item, I regretted that I had overpaid. The bidding competition was ﬁerce. There were many people who participated in the bidding process. Compared with other auctions, there were many bidders who competed in the auction. Sources Rook & Fisher (1995)\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "Ku et al. (2005), Park et al. (2012)\n",
              "\n",
              "Developed for this study Park et al. (2012)\n",
              "\n",
              "Strongly disagree/agree (1–7 scale). IMP, impulsiveness; WR, winner’s regret; SC, sunk cost.\n",
              "\n",
              "APPENDIX B. ITEM-FACTOR LOADINGS AND CROSS-LOADINGS FOR FULL SAMPLE AND FOR SUBGROUPS\n",
              "Full sample (n = 301) Competition intensity CI1 CI2 CI3 IMP 1 IMP 2 IMP 3 IMP 4 WR SC1 SC2 SC3 0.915 0.944 0.933 0.219 0.303 0.222 0.179 0.223 0.589 0.550 0.554 Impulsiveness 0.241 0.236 0.280 0.850 0.800 0.848 0.868 0.391 0.427 0.415 0.432 Winner’s regret 0.206 0.184 0.226 0.351 0.310 0.333 0.322 1.000 0.384 0.381 0.379 Sunk cost 0.546 0.507 0.583 0.358 0.386 0.403 0.347 0.397 0.966 0.968 0.942 High CI (n = 155) N/A Low CI (n = 146) N/A\n",
              "\n",
              "0.883 0.788 0.890 0.899 1.000 0.943 0.950 0.930\n",
              "\n",
              "0.779 0.817 0.773 0.767 1.000 0.969 0.970 0.922\n",
              "\n",
              "N/A, not applicable; IMP, impulsiveness; WR, winner’s regret; SC, sunk cost.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "638\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "APPENDIX C. CONSTRUCT CORRELATIONS AND SQUARE ROOT OF AVES (ON DIAGONAL)\n",
              "Total sample group (n = 301) Competition intensity 0.930 0.273 0.223 0.589 Impulsiveness 0.842 0.391 0.443 Winner’s regret Sunk cost\n",
              "\n",
              "Competition intensity Impulsiveness Winner’s regret Sunk cost High competition intensity group (n = 155) Competition intensity Impulsiveness Winner’s regret Sunk cost Low competition intensity group (n = 155) Competition intensity Impulsiveness Winner’s regret Sunk cost N/A, not applicable.\n",
              "\n",
              "0.909 0.397 0.959\n",
              "\n",
              "Competition intensity N/A N/A N/A N/A\n",
              "\n",
              "Impulsiveness 0.866 0.389 0.397\n",
              "\n",
              "Winner’s regret\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "1.000 0.294 0.941\n",
              "\n",
              "Competition intensity N/A N/A N/A N/A\n",
              "\n",
              "Impulsiveness 0.782 0.339 0.397\n",
              "\n",
              "Winner’s regret\n",
              "\n",
              "Sunk cost\n",
              "\n",
              "1 0.412 0.954\n",
              "\n",
              "APPENDIX D. MRA ANALYSIS TO DETERMINE THE TYPE OF MODERATOR FOR COMPETITION INTENSITY\n",
              "Unstandardised coefﬁcients Model 1 Constant Impulsiveness (IMP) Sunk cost (SC) Constant Impulsiveness (IMP) Sunk cost (SC) Competition (CI) 3 intensity Beta 0033.957 0.329 0.280 3.957 0.330 0.293 0.073 3.961 0.333 0.291 À0.024 À0.013 Std error 0.064 0.071 0.057 0.064 0.071 0.068 À0.023 0.066 0.072 0.069 0.074 0.054 0.266 0.279 0.267 0.292 Standardised coefﬁcients Beta t 61.665 4.650 4.878 61.575 4.648 4.294 À0.356 59.686 4.614 4.235 À0.329 À0.244 Sig. 0.001 0.000 0.000 0.000 0.000 0.002 0.722 0.000 0.000 0.000 0.742 0.807 0.215 R\n",
              "2\n",
              "\n",
              "0.215\n",
              "\n",
              "2\n",
              "\n",
              "0.215\n",
              "\n",
              "Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*IMP\n",
              "\n",
              "0.269 0.291 À0.021 À0.013\n",
              "\n",
              "(Continues)\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "Winner’s regret in online C2C auctions\n",
              "\n",
              "639\n",
              "\n",
              "Table . (Continued)\n",
              "Unstandardised coefﬁcients Model 4 Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*SC Constant Impulsiveness (IMP) Sunk cost (SC) Competition intensity (CI) CI*SC CI*IMP Dependent variable: winner’s regret. *p < 0.001. Beta 4.008 0.336 0.290 À0.038 À0.065 4.007 0.329 0.294 À0.045 À0.077 0.037 Std error 0.071 0.071 0.068 0.073 0.038 0.071 0.072 0.069 0.074 0.043 0.061 Standardised coefﬁcients Beta t 56.522 4.748 4.257 À0.52 À1.697 56.413 4.578 4.293 À0.611 À1.784 0.607 Sig. 0.000 0.000 0.000 0.603 0.091 0.000 0.000 0.000 0.542 0.075 0.544 R\n",
              "2\n",
              "\n",
              "0.223\n",
              "\n",
              "0.272 0.289 À0.033 À0.088 0.266 0.294 À0.039 À0.104 0.036\n",
              "\n",
              "5\n",
              "\n",
              "0.224\n",
              "\n",
              "APPENDIX E. MEASUREMENT INVARIANCE ANALYSIS FOR GROUP COMPARISON\n",
              "Fit index Chisquare 77.019 Chisquare/df 1.605\n",
              "\n",
              "Model test Baseline model\n",
              "\n",
              "df 48\n",
              "\n",
              "GFI 0.959\n",
              "\n",
              "CFI 0.985\n",
              "\n",
              "NFI 0.962\n",
              "\n",
              "RMSEA 0.045\n",
              "\n",
              "ΔGFI —\n",
              "\n",
              "ΔCFI —\n",
              "\n",
              "ΔNFI —\n",
              "\n",
              "ΔRMSEA —\n",
              "\n",
              "Constrained models between Chisquare 77.019 77.019 77.252 144.729 146.093 149.698 152.398 Chisquare/df 1.605 1.605 1.577 2.895 2.865 2.879 2.875\n",
              "\n",
              "Model test IMP and RGT SC and RGT IMP, SC and RGT IMP, SC, OP and RGT IMP, SC, OP, NB and RGT IMP, SC, OP, NB, MP and RGT IMP, SC, OP, NB, MP, GD and RGT\n",
              "\n",
              "df 48 48 49 50 51 52 53\n",
              "\n",
              "GFI 0.959 0.959 0.959 0.950 0.949 0.945 0.943\n",
              "\n",
              "CFI 0.985 0.985 0.986 0.976 0.971 0.970 0.969\n",
              "\n",
              "NFI 0.962 0.962 0.962 0.929 0.928 0.926 0.925\n",
              "\n",
              "RMSEA 0.045 0.045 0.044 0.052 0.052 0.052 0.052\n",
              "\n",
              "ΔGFI 0.000 0.000 0.001 0.009 0.010 0.004 0.002\n",
              "\n",
              "ΔCFI 0.000 0.000 0.000 0.009 0.001 0.004 0.002\n",
              "\n",
              "ΔNFI 0.000 0.000 0.001 0.010 0.005 0.003 0.001\n",
              "\n",
              "ΔRMSEA 0.000 0.000 0.000 0.009 0.000 0.000 0.000\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n",
              "\n",
              "640\n",
              "\n",
              "S C Park et al.\n",
              "\n",
              "APPENDIX F. COMMON METHOD BIAS (CMB) ANALYSIS\n",
              "F1 Marker Variable Analysis to Evaluate Common Method Bias\n",
              "\n",
              "We followed the marker variable method described by Lindell & Whitney (2001) and used by Malhotra et al. (2006). We identiﬁed the lowest correlation marker variable collected during survey administration (RM1). We also identiﬁed the second lowest correlation marker variable (RM2). In Table , we present the correlations after correcting for RM1 and RM2: • Adjusting for RM1 and RM2, respectively, the correlations among the substantive variables dropped by 0.2 in maximum. All signiﬁcant correlations remained the same signiﬁcant level and insigniﬁcant correlations remained insigniﬁcant.\n",
              "RM1 = 0.008 Factors r(IP, SC) r(IP, WR) r(IP,CI) r(SC, CI) r(SC,WR) r(CI, WR) Uncorrected 0.444 0.390 0.271 0.585 0.398 0.221 M1 0.428 0.374 0.255 0.569 0.382 0.205 t 8.175 6.961 4.552 11.944 7.135 3.616 M2 0.426 0.372 0.253 0.567 0.380 0.203 RM2 = 0.009 t 8.128 6.918 4.514 11.883 7.092 3.579\n",
              "\n",
              "IMP, impulsiveness; SC, sunk cost; CI, competition intensity; WR, winner’s regret; M1, opening price; M2, perceived ease of use; RM1, correlation between M1 (marker variable) and WR; RM2, correlation between M2 (marker variable) and WR.\n",
              "\n",
              "F2 Correlation Tables of Marker Variable and Study Constructs\n",
              "\n",
              "Constructs SC CI IMP WR M1 M2\n",
              "\n",
              "SC 1.000 0.585 0.444 0.398 0.012 0.216\n",
              "\n",
              "CI\n",
              "\n",
              "IP\n",
              "\n",
              "WR\n",
              "\n",
              "M1\n",
              "\n",
              "M2\n",
              "\n",
              "1.000 0.271 0.221 0.056 0.294\n",
              "\n",
              "1.000 0.390 À0.145 0.205\n",
              "\n",
              "1.000 0.008 0.009\n",
              "\n",
              "1.000 0.131 1.000\n",
              "\n",
              "IMP, impulsiveness; SC, sunk cost; CI, competition intensity; WR, winner’s regret; M1, opening price; M2, perceived ease of use.\n",
              "\n",
              "© 2015 Blackwell Publishing Ltd, Information Systems Journal 26, 613–640\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Btxj43iMHBRY",
        "colab_type": "code",
        "outputId": "1917530a-fc36-4060-d4dc-f45645ada950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc2.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " \n",
              " instantly affect ﬁrm labor demand., The difference in the coefﬁcients of the enterprise application use over two years is statistically signiﬁcant p < 0 01 ., p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " , p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " \n",
              " Notes., p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " \n",
              " as a piece of evidence that is consistent with the notion of SBTC, where IT complements skilled labor relatively more than unskilled labor., p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " , p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " , p < 0 1;, ∗∗ p < 0 05; ∗∗∗ p < 0 01.\n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "metadata": {
        "id": "3Os-7qG8_QTZ",
        "colab_type": "code",
        "outputId": "671ae50c-a536-4730-ed9e-cdb153581af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc13.sents if 'signiﬁcant' in sent.string]\n",
        "p_gleich_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Our results show that both trait impulsiveness and sunk cost have signiﬁcant impacts on winner’s regret.,\n",
              " Indeed, numerous studies have demonstrated that strong emotions can have a signiﬁcant inﬂuence on decision-making behaviour (see, for example, Andrade & Ariely, 2009).,\n",
              " Cognitive Completion effect, selfjustiﬁcation, sunk cost and willingness to continue bidding (as a mediator) Overbidding behaviour signiﬁcant effects on a bidder’s decision-making process.,\n",
              " This result suggests that CMB was not a signiﬁcant threat in our study.,\n",
              " The results of our marker variable analysis (described in Appendix F) suggest that common method bias was likely not a signiﬁcant threat in this study.,\n",
              " As shown in Figure 2, the path between impulsiveness and winner’s regret (β = 0.276, t = 3.846) and the path between sunk cost and winner’s regret (β = 0.266, t = 3.628) were both signiﬁcant at p < 0.01.,\n",
              " None of the control variables were found to be signiﬁcant.,\n",
              " All signiﬁcant correlations remained the same signiﬁcant level and insigniﬁcant correlations remained insigniﬁcant.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "metadata": {
        "id": "Nif-MMSLF6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc = nlp(open(u\"\").read())\n",
        "doc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyLVVXhzHIXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p_gleich_Sents = [sent for sent in doc2.sents if 'p =' in sent.string]\n",
        "p_groß_als_Sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RH7elaonGDw7",
        "colab_type": "code",
        "outputId": "cea99b6a-f365-4545-b173-65dd96113b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7557
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc14 = nlp(open(u\"ThiesF#WesselM#BenlianA_2016_Effects of Social Interaction Dynamics on Platforms_Journal of Management Information Systems_3.txt\").read())\n",
        "doc14\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Journal of Management Information Systems\n",
              "\n",
              "ISSN: 0742-1222 (Print) 1557-928X (Online) Journal homepage: http://www.tandfonline.com/loi/mmis20\n",
              "\n",
              "Effects of Social Interaction Dynamics on Platforms\n",
              "Ferdinand Thies, Michael Wessel & Alexander Benlian\n",
              "To cite this article: Ferdinand Thies, Michael Wessel & Alexander Benlian (2016) Effects of Social Interaction Dynamics on Platforms, Journal of Management Information Systems, 33:3, 843-873, DOI: 10.1080/07421222.2016.1243967 To link to this article: http://dx.doi.org/10.1080/07421222.2016.1243967\n",
              "\n",
              "View supplementary material\n",
              "\n",
              "Published online: 07 Dec 2016.\n",
              "\n",
              "Submit your article to this journal\n",
              "\n",
              "Article views: 72\n",
              "\n",
              "View related articles\n",
              "\n",
              "View Crossmark data\n",
              "\n",
              "Full Terms & Conditions of access and use can be found at http://www.tandfonline.com/action/journalInformation?journalCode=mmis20 Download by: [Bibliotheek TU Delft] Date: 14 February 2017, At: 09:21\n",
              "\n",
              "\n",
              "Effects of Social Interaction Dynamics on Platforms\n",
              "FERDINAND THIES, MICHAEL WESSEL, AND ALEXANDER BENLIAN\n",
              "FERDINAND THIES (thies@ise.tu-darmstadt.de) is a Ph.D. candidate in Information Systems at Technische Universität Darmstadt (TU Darmstadt), Germany. He holds a Masters’ degree in Business Administration from the University of Munich. His research interests include software platforms and crowdfunding. His work has been published in international journals such as Decision Support Systems as well as conferences such as the International Conference on Information Systems and the European Conference on Information Systems. MICHAEL WESSEL (wessel@ise.tu-darmstadt.de) is a Ph.D. candidate in Information Systems at Technische Universität Darmstadt (TU Darmstadt), Germany. He holds a Masters’ degree in Business Information Systems from the University of Amsterdam. His research interests include web personalization and crowdfunding. His work has been published in international journals such as Decision Support Systems as well as conferences such as the International Conference on Information Systems and the European Conference on Information Systems. ALEXANDER BENLIAN (benlian@ise.tu-darmstadt.de; corresponding author) is a chaired professor of Information Systems, especially electronic services, at Technische Universität Darmstadt (TU Darmstadt), Germany. He holds a Ph.D. in business administration and management information systems from the University of Munich. His research interests include web personalization in e-commerce, digital business models, software platforms, and software-as-a-service. His work has been published in journals such as Journal of Management Information Systems, Journal of the Association for Information Systems, MIS Quarterly Executive, Journal of Service Research, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, Decision Support Systems, International Journal of Electronic Commerce, and others, as well as in the proceedings of conferences such as the International Conference on Information Systems (ICIS) and the European Conference on Information Systems (ECIS). ABSTRACT: Despite the increasing relevance of online social interactions on platforms, there is still little research on the temporal interaction dynamics between electronic wordof-mouth (eWOM, a form of opinion-based social interaction), popularity information (a form of action-based social interaction), and consumer decision making. Drawing on a panel data set of more than 23,300 crowdfunding campaigns from Indiegogo, we investigate the dynamic effects of these social interactions on consumers’ funding decisions using the panel vector autoregressive methodology. Our analysis shows that both eWOM and popularity information are critical influencing mechanisms in crowdfunding. However, our overarching finding is that eWOM surrounding crowdfunding campaigns on Indiegogo or Facebook has a significant yet substantially weaker predictive power than\n",
              "Journal of Management Information Systems / 2016, Vol. 33, No. 3, pp. 843–873. Copyright © Taylor & Francis Group, LLC ISSN 0742–1222 (print) / ISSN 1557–928X (online) DOI: 10.1080/07421222.2016.1243967\n",
              "\n",
              "\n",
              "844\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "popularity information. We also find that whereas popularity information has a more immediate effect on consumers’ funding behavior, its effectiveness decays rather quickly, while the impact of eWOM recedes more slowly. This study contributes to the extant literature by (1) providing a more nuanced understanding of the dynamic effects of opinion-based and action-based social interactions, (2) unraveling both within-platform and cross-platform dynamics, and (3) showing that social interactions are perceived as quality indicators on crowdfunding platforms that help consumers reduce risks associated with their investment decisions. These results can help platform providers and complementors to stimulate contribution behavior and increase the prosperity of a platform. KEY WORDS AND PHRASES: crowdfunding, electronic word-of-mouth, eWOM, informational cascades, online platforms, panel vector autoregression, popularity information, platform success, reward-based crowdfunding.\n",
              "\n",
              "Consumers tend to consider other people’s opinions and actions when they make buying decisions. For instance, a person might choose to visit a restaurant based on a friend’s recommendation or on the restaurant’s observable popularity [11]. Given that online transactions restrict consumers’ ability to assess a product’s quality due to the lack of direct interaction with product and seller, these social interactions play a particularly critical role in electronic markets and have become a vital quality indication for consumers to use for decision support [28, 38]. Social interactions have been generally defined as “actions . . . taken by an individual not actively engaged in selling the product or service and that impact others’ expected utility for that product or service” [38, pp. 416–417]. Previous research distinguished between two distinct types of social interactions that have been shown to be particularly relevant in an online context, namely opinion-based or preferencebased and action-based or behavior-based social interactions (e.g., [21, 23, 72]). The former type is often referred to as electronic word-of-mouth (eWOM) communication in an online context and can be described as a statement by potential, actual, or former customers about a product or company [41]. The latter type, often facilitated through popularity information (PI), becomes relevant in situations in which individuals who face identical decisions under uncertainty can observe the actions of other consumers (e.g., in the form of aggregated statistics displaying the number of downloads or purchases of a product) who faced the same decisions earlier on, but not the motivation behind their actions [14, 15]. These situations can lead to informational cascades, an information-based explanation for herd behavior that occurs when individuals who face a certain decision choose to follow the actions of others instead of making a decision based on their own private information [14, 15, 31, 72]. The Internet offers consumers various opportunities to engage in online social interactions with their peers and other consumers via, for example, online review platforms, social networking websites, blogs, and online forums. These interactions help them to overcome the information asymmetry for products and services whose quality is difficult to ascertain before purchase. E-commerce vendors have also recognized the critical role of social interactions among consumers to influence their purchasing\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "845\n",
              "\n",
              "decisions, and platforms provide informational cues in order to facilitate these interactions. Amazon.com, for example, facilitates the dissemination of eWOM by encouraging consumers to publish product reviews, but also depicts popularity information by showing sales rankings and highlighting top selling products in each product category. Motivated by the practical relevance of online social interactions, researchers have dedicated a number of important studies to the phenomenon. For example, Luo and Zhang [56] have shown that consumer buzz and firm value not only affect one another over time, but that consumer buzz also has autoregressive carryover effects so that past buzz influences current buzz, highlighting the self-reinforcing nature of opinion-based social interactions. Duan et al. [31] empirically demonstrated the existence of complex informational cascades between other users’ software download behavior (as indicated by popularity information) and subsequent software downloads, revealing dynamic co-movements between user actions on platforms. While previous research has examined the effects of the two types of online social interactions separately in various settings (e.g., [30, 31, 35, 53]), only a few studies, such as Chen et al. [21] and Cheung et al. [23], have considered both types simultaneously and examined their relative impact on consumer decision making. However, there is still little understanding of how dynamic these effects are and how quickly or slowly they unfold and evolve. Such an assessment is crucial in order to understand at which point in time online social interactions have the biggest effect on consumer decision making, given the fast-paced speed of decisions in the online world. Thus, there is a clear need to examine how these mechanisms weigh up in their predictive ability as well as how they differentially affect one another over time. This study attempts to fill this research gap, guided by the following research question: Research Question: What are the effects of opinion-based and action-based online social interactions on consumer decision making and how do these effects build up and decay over time? To answer our research question, we focus on crowdfunding, a context in which social interactions play a particularly important role. Crowdfunding allows individuals or organizations to raise funds for diverse projects by receiving small financial contributions from a large number of individual investors [60]. As investments in crowdfunding campaigns can be considered risky for the investors due to limited information about the projects and uncertain outcomes, it becomes optimal for investors to infer the quality of campaigns from the opinions and actions of other consumers. We collected daily project-level data from Indiegogo, one of the largest reward-based crowdfunding platforms. Since its launch in 2008, more than 400,000 campaigns have run on Indiegogo and millions of dollars have been distributed to campaign creator [59]. We complemented this data set with corresponding eWOM data gathered from Facebook and Indiegogo itself. Using daily data on more than 23,300 crowdfunding campaigns that ran between November 2013 and June 2014, we analyze the dynamic effects of eWOM and PI on subsequent campaign funding decisions using the panel vector autoregressive (PVAR) methodology.\n",
              "\n",
              "\n",
              "846\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Our findings provide noteworthy contributions to theory and practice. First, we contribute to the software platform and social media literature by providing a more nuanced understanding of eWOM’s and PI’s dynamic impact. By examining buildup and decay effects [52], we show that PI has a more immediate predictive relationship with consumers’ funding behavior than eWOM. However, while the effects of PI diminish rather quickly, the effects of eWOM persist longer. Although previous studies also explored the differential effects of early versus late eWOM (e.g., [37, 55]), ours is among the first to disentangle and compare the dynamic relationships of opinion-based (eWOM) as well as action-based (PI) online social interactions over time. Second, our study also contributes to the burgeoning software platform literature because we unravel both within-platform and cross-platform dynamics (i.e., between crowdfunding and social media platforms). With rare exceptions (e.g., [20, 29, 56]), previous research has studied the dynamic effects of eWOM and/or PI on a single platform, overlooking the increasing relevance of cross-platform effects. By addressing these effects, we respond to calls in prior research that emphasize the importance of capturing and unpacking the evolution and interrelationships of multiple time series across information systems and platforms in an increasingly interconnected IT world [2]. Third, our study contributes to the emerging crowdfunding literature [4, 12, 18, 60] by showing that both eWOM and previous funding behavior by the crowdfunding community (as indicated by PI) are perceived as quality indicators that allow potential backers to reduce their own risk in the face of uncertainty. Finally, understanding the relative predictive value of eWOM and PI concerning the effect on critical consumer decisions over time can help platform providers and third-party complementors to monitor and analyze the echo of changes in eWOM and previous contribution behavior. They can then adapt their project campaign and communication with prospective consumers accordingly.\n",
              "\n",
              "Theoretical Background Dynamics on Reward-based Crowdfunding Platforms\n",
              "Crowdfunding, which builds on the broader concept of crowdsourcing (e.g., [10]), allows individuals or organizations to reach a monetary (project) goal by receiving small financial contributions from a large number of individuals instead of choosing the traditional approach and receiving large contributions from a small number of investors. Crowdfunding enables project creators (the fundraisers) to collect contributions (hereafter also referred to as backing or funding a campaign) from a large number of project backers (the funders) through an open call—mostly on the Internet [63]. The reasons project creators choose crowdfunding are not limited to financial aspects, as the success of the campaign also validates that there is a market for the respective product and the campaigns themselves can also have a certain marketing effect [18, 60]. Unlike equity-based and lending-based crowdfunding platforms, where backers can generate revenue through private equity in the respective company or through\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "847\n",
              "\n",
              "interest on the amount invested, reward-based crowdfunding platforms do not offer backers any financial benefits. Instead, they can expect to receive a “reward”—a nonfinancial tangible benefit for their investment (e.g., early and discounted purchase of the product or service). Rewards have a high level of uncertainty as specific conditions have to be met before backers receive their reward. A fundamental condition is that sufficient funds are raised within the prearranged campaign runtime. Even though project creators on most reward-based crowdfunding platforms (such as Indiegogo) receive funds regardless of whether the funding goal is reached, not collecting enough funds will make it difficult for most creators to implement their project ideas and deliver the rewards. Furthermore, the backer ’s investment cannot be equated to a purchase, since there is usually no legal obligation for the creator to produce and deliver the reward [60]. Backers can therefore be less certain that they will receive the return on their investment and have less information about the object they are investing in compared to a regular buying situation, in which the product or service already exists, and can be inspected thoroughly. Previous research has found these rewards to be a central reason for backers to participate in reward-based crowdfunding [49]. The primary source of information for a potential backer to use for decision support is the campaign description the creator published on the platform. This often includes a short video, showing the creator, possibly a prototype, the finished product, or other important aspects of the campaign. Even though this content allows the backer to develop an attitude toward the campaign and the offered rewards, this quality assessment is potentially biased because all information stems from a single source (the project creator). This means that the quality of the campaign is often relatively vague at the time prospective backers decide whether to pledge. Other evidence concerning the trustworthiness and quality of a campaign therefore becomes increasingly important for potential backers’ evaluation. The two most prominent and salient quality criteria that are increasingly available on these platforms are social information, mostly in the form of eWOM cues (e.g., social plugins that display the number of shares the campaign receives on Facebook or the number of direct consumer comments) and popularity information such as prior contribution behavior (e.g., the number of consumers who backed a project is prominently placed on each campaign’s dashboard), allowing consumers to observe other consumers’ actions (e.g., [31, 56, 64]). While previous studies have examined the effects of online social interactions on consumer decision making in settings where the products share characteristics with rewards in reward-based crowdfunding (e.g., the utility of the product is difficult to ascertain before purchase), the concept of reward-based crowdfunding offers unique characteristics that make studying the effects of online social interactions in this setting particularly interesting. First, in other settings, the decision making processes of consumers are often sequential in that consumers buy, experience, and review products and then influence other consumers. However, in reward-based crowdfunding, experiencing the product is a downstream process that occurs long after the campaign has ended. Therefore, backers influence potential backers with their\n",
              "\n",
              "\n",
              "848\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "opinions and actions without having any additional information about the reward. Second, many online platforms such as online auction websites encourage sellers to build and maintain a reputation on the platform as a quality indicator for subsequent buyers. As a crowdfunding campaign is most often a one-off process for the project creators,1 similar reputation mechanisms cannot be found on crowdfunding platforms, which increases the relevance of online social interactions in this setting.\n",
              "\n",
              "Electronic Word-of-Mouth\n",
              "Word-of-mouth (WOM) is opinion-based or preference-based social interaction between not commercially affiliated consumers about commercial content such as brands, products, or services [7, 21]. Previous research has found a significant influence of WOM on consumers’ information search, evaluation, and decision making, as it “influences attitudes during the pre-choice evaluation of alternative service providers” [19, p. 242]. Furthermore, it has been shown that WOM can be more relevant than traditional marketing channels, such as advertising, in raising awareness about innovation and in convincing the receiver to try out new products [19]. One of the central reasons for the success of WOM is increased perceived reliability, credibility, and trustworthiness compared to communication initiated by organizations themselves [7, 17]. The Internet drastically increased consumers’ options for exchanging opinions about products and services and offers them a large array of possibilities to engage in a specific form of WOM called electronic word-of-mouth (eWOM). While the Internet allows eWOM to spread at an unprecedented speed and scale compared to traditional (face-to-face) WOM, it brings new challenges, such as “the volatile nature of online identities and near complete absence of contextual cues” [46, p. 295]. Still, it has been argued that the consumer motives that have been identified as relevant for traditional WOM are also relevant for eWOM [41]. According to Hennig-Thurau et al., eWOM is “any positive or negative statement made by potential, actual, or former customers about a product or company, which is made available to a multitude of people and institutions via the Internet” [41, p. 39]. The opportunities that are available for consumers to share their opinions, preferences, or experiences online are manifold and a multitude of possible channels such as product review websites, blogs, and online communities are available. Due to their constant presence and accessibility, social networking websites such as Facebook in particular have been used to generate massive amounts of eWOM messages. The response of an individual to an eWOM message received via these channels depends on two sequential cognitive processes, namely, awareness and persuasiveness. The awareness effect can be explained by the sheer volume of eWOM, making it more likely for a receiver to be informed about the content [53]. Only after the receiver is aware of the content, a cognitive process starts evaluating the message’s credibility by examining its valence and the receiver ’s social ties with the sender. Several studies have shown that eWOM volume, rather than valence, is significantly associated with product sales (e.g., [26, 30]).\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "849\n",
              "\n",
              "Given the findings on the effects of eWOM on a firm’s equity value [56, 57] and on the decision making of consumers in different contexts such as movies [53], books [24], and video games [78], and taking into account the relatively high risk when investing in crowdfunding campaigns, eWOM can be expected to be very important to the success of a crowdfunding campaign. Spreading the word in social media raises awareness for the respective crowdfunding campaign without requiring financial investments and can be central in persuading prospective backers to invest. Without eWOM, the campaign description remains the central source of information for the backer who might be uncertain about the utility of the proposed project and its rewards. Although eWOM and its contagious effect on consumer decision making have been extensively investigated in online environments (e.g., [45]), previous research paid little attention to the fact that eWOM does not unfold in a vacuum. But it can be presumed to occur with parallel observational mechanisms on platforms—that is, besides spreading the word around crowdfunding campaigns, prospective backers can increasingly observe the dynamics of real funding behavior from other backers on the platform.\n",
              "\n",
              "Popularity Information and Informational Cascades\n",
              "Compared to eWOM, which is focused on the exchange of information in the form of opinions among consumers, PI facilitates action-based or behavior-based social interactions [21, 38]. Many e-commerce vendors use PI cues as an indicator of the choices earlier adopters made by displaying sales rankings or absolute sales in order to influence consumers’ choices and behavior. Such observable actions can help consumers to learn what the most appropriate response is in a given situation because people, in part, “determine what is correct by finding out what other people think is correct” [25, p. 152]. A possible outcome of such behavior is that many individuals start to behave identically. Ultimately, this behavior can lead to informational cascades, an information-based explanation for herd behavior [14, 44]. Informational cascades occur when individuals who face identical decisions under uncertainty, can observe the actions of other individuals who faced the same decisions earlier on, but not the motivation behind their actions [14, 15]. In these situations, individuals will consider their own private information as well as the inferences drawn from observing predecessors’ decisions. As soon as individuals consider the decisions of other individuals as more informative than their own private information, they will most likely disregard their own information and imitate predecessors’ actions in order to overcome uncertainty and to avoid blame from others for making a particular choice [68]. Any immediate successors will have even more reasons to disregard their own private information. Previous information systems (IS) research has shown that, due to large amounts of information available about the purchase decisions of consumers online, the Internet is the ideal environment for this type of herd behavior. Informational cascades have, for example, been found to arise in online microloan markets [77], when adopting software [31], and during online auctions [65].\n",
              "\n",
              "\n",
              "850\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Informational cascades can also be considered a central driving mechanism to explain the behavior of backers on reward-based crowdfunding platforms for the following reasons. First, backers on these platforms face decisions under uncertainty when they decide whether to pledge for a campaign. The uniqueness of the campaigns on these platforms stresses this point, as backers will rarely have to choose between two similar campaigns running simultaneously [49]. Second, the value of the promised reward remains relatively vague at the time the investment decision has to be made, so backers are unable to ascertain the true value until after the delivery when the campaign has ended—similar to experience goods [64]. Third, crowdfunding platforms are designed so that it becomes very convenient for potential backers to observe the level of funding by other backers at any time during the campaign runtime. Fourth, even though the pledge of any predecessor indicates his or her actions, the motives behind these actions are not revealed. Although both types of social interactions have been shown to affect consumers’ decision making processes (e.g., [21]), they can be considered distinct influencing mechanisms, as they differ strongly in respect to various characteristics (see Appendix 1 for a detailed overview of these characteristics). For instance, while eWOM messages largely contain qualitative information in the form of opinions about products and discrete buying recommendations, informational cues depicting a product’s popularity contain mostly quantitative information (e.g., the number of backers who invested in a specific crowdfunding campaign). Despite their lower information content, PI cues might be more relevant for consumers, as they typically depict definite and consequential actions [21]. As such, when in doubt about the quality of a product, one person who buys the product might send a stronger signal than a good friend who simply recommends the product without investing in it. However, even though the number of previous backers enables prospective backers to infer the success of the campaign directly, it does not offer any information about the prospective backer’s strength of relationship with previous backers. Therefore, for individuals who consider their social network when making an investment, it might be more appropriate to use eWOM for decision support. Given the distinct, yet complementary nature of both mechanisms in influencing consumers’ decision making processes, it is surprising to find that previous research has so far mainly focused on examining their effects in isolation without considering the mutual interdependencies over time. Particularly in multisided markets with high information transparency and a growing integration of social media cues, the effects of eWOM and PI are not detached from one another, but are interwoven and influence each other. Selfreinforcing as well as reciprocal time-varying effects are therefore worth investigating.\n",
              "\n",
              "Research Model and Hypotheses Development\n",
              "Against this backdrop, our research model (Figure 1) incorporates hypotheses that emphasize the dynamic relationships of opinion-based as well as behavior-based online social interactions over time. First, H1 and H3 focus on the self-reinforcing, intra-platform effects (i.e., on social media and crowdfunding platforms,\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "851\n",
              "\n",
              "Figure 1. Research Model\n",
              "\n",
              "respectively) that shed light on how previous eWOM surrounding a crowdfunding campaign drives present eWOM on the respective platform as well as how popularity information in the form of previous funding decisions affects current funding behavior (also called autoregressive carryover effects). Second, H2 captures potential cross-platform and intra-platform effects of eWOM on funding behavior. Finally, H4a and H4b are derived to theorize the time-varying (i.e., buildup and decay) effects of eWOM and popularity information on the decision making of backers. We derive the first sets of hypotheses, H1 and H2, drawing on theory related to eWOM effectiveness. We then develop H3 and H4a and H4b based on the literature related to informational cascades.\n",
              "\n",
              "Effects of Crowdfunding-Related eWOM\n",
              "The effectiveness of eWOM describes the ability of eWOM messages to influence a receiver ’s behavior, as simply receiving a persuasive message may not coincide with an actual response [22]. In this study, we distinguish between two outcomes of effective eWOM that could arise after receiving an eWOM message about a specific crowdfunding campaign. First, consumers might join the conversation around the campaign by retransmitting the message on social media or by writing comments about the campaign directly on the platform. Second, consumers might invest financially in the respective crowdfunding campaign. We expect these two outcomes to be sequential in their timing and to differ in their magnitude, due to different motives and risks associated with them. Generally, backers can be expected to have different motives for writing or sharing eWOM messages before and after investing in a crowdfunding campaign.\n",
              "\n",
              "\n",
              "852\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Before investing, backers are likely to seek peer evaluation by sharing or writing eWOM messages and by evaluating the responses in order to partially compensate for the uncertainty associated with the investment decision. This is consistent with the findings of King and Balasubramanian [47], showing that other-based preference formation is particularly important for goods whose value is difficult to ascertain before purchase [29], making peer evaluation a vital component for the decision making of a potential backer. After investing, backers can be expected to write or share eWOM messages for two reasons. First, for self-representation or self-enhancement purposes [74], where backers share content because it may reflect favorably on them as senders [13]. Projects on reward-based crowdfunding platforms are often technically innovative, socially responsible, or very creative, and are therefore ideal to reflect positively on the sender when shared via social media [1]. This motive will be particularly relevant for backers who share messages about their recent investments in crowdfunding campaigns on social media, where they usually have established social networks that might not exist between rather anonymous consumers on the crowdfunding platform. Second, as crowdfunding campaigns on Indiegogo will only have the chance to become successful if sufficient funds are raised, it is reasonable for backers to encourage their peers to also invest in the project by sharing the respective crowdfunding campaign via social media. These motives for writing and retransmitting messages can be expected to be critical for the diffusion of eWOM surrounding specific campaigns. Since it has been shown that a single eWOM message potentially influences a multitude of receivers [50], we expect that an increase in consumer comments on a crowdfunding platform and in the number of shares on social media will generate additional eWOM on the respective platform in a following period, creating positive feedback loops around a specific crowdfunding campaign.2 We therefore propose: Hypothesis 1: Past eWOM around a given crowdfunding campaign is positively associated with present eWOM on the respective platform. Previous research has highlighted the importance of eWOM in the diffusion of new products (e.g., [7, 58]). It has been argued that with higher perceived risk associated with the early adoption of new products, consumers tend to rely more on eWOM, as it is perceived to be more reliable, credible, and trustworthy compared to communication initiated by organizations themselves [7, 17]. As mentioned above, crowdfunding is different from a regular buying situation, as the investment is often required without an existing product or service, which increases perceived risk and the importance of eWOM messages. We therefore expect that increasing eWOM on both the crowdfunding platform (comments) and via social media (Facebook shares) will positively influence prospective backers’ contribution behavior and promote their funding decisions: Hypothesis 2: Past eWOM is positively associated with prospective backers’ present funding decisions for a given crowdfunding campaign.\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "853\n",
              "\n",
              "Informational Cascades on Crowdfunding Platforms\n",
              "Informational cascades may occur frequently on crowdfunding platforms, as the only available source of information is the campaign description the project creator published, which might be limited in scope, imperfect, or biased. Prospective backers thus often infer the product’s utility by observing prior contribution behavior, for example, based on popularity information displayed on the platform (such as in the form of the number of previous backers or the amount that has already been invested). Popularity information has been found to have a positive influence on consumer adoption decisions and subsequent sales performance (e.g., in the context of online software adoption [31] and for niche products [72]). In the crowdfunding context, previous research on the effect of prior contribution behavior on the decision making of prospective backers has found that for donationbased crowdfunding, the marginal utility gain from giving to a particular project is diminished through the contribution of other backers [18]. The reason is that potential backers see less “need” to contribute when others have already supported the campaign, leading to negative downward informational cascades and ultimately a stagnation of contribution. On the other hand, in equity-based and lending-based crowdfunding markets, backers rather invest in projects that already have a lot of support, which signals superior quality. Supporting an already successful project becomes a rational decision for backers in order to reduce their own risk [42, 77]. Already popular campaigns therefore receive an additional popularity boost, leading to positive informational cascades. To our best knowledge, this effect has not yet been empirically investigated in reward-based crowdfunding markets, and it remains unclear whether one can expect positive upward or negative downward informational cascades—or neither. We hypothesize that the intentions of backers in reward-based crowdfunding markets are more similar to those in equity-based and lending-based crowdfunding markets, as receiving the return on the investment is a primary objective in all three markets [49]. However, the risk of not receiving the reward in reward-based crowdfunding might be rather high, as the funds invested in an unsuccessful project are not reimbursed. Consequently, project creators whose campaigns do not reach the designated funding goal will still receive the funds, but might be unable to deliver the promised rewards to the backers due to a lack of funding. Hence, prospective backers are likely to minimize their risk of pledging without receiving a reward and to invest in campaigns that are already on their way to becoming successful in terms of the number of backers, leading to a reinforcement effect on the crowdfunding platform. We therefore expect to identify positive informational cascades and propose: Hypothesis 3: Past funding decisions of other backers as indicated by PI are positively associated with prospective backers’ present funding decisions for a given campaign. Although we expect that both past eWOM and PI will be positively associated with prospective backers’ present funding decisions, it can be assumed that the buildup and decay effects of both types of social interactions will differ strongly. Prior research suggests that decisions involving bandwagon or herding behavior are associated with\n",
              "\n",
              "\n",
              "854\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "short decision times rather than longer ones, which suggests that the propensity to herd is either an instinctive, emotional response or a well-practiced, automated decision making heuristic [8, 48]. Similarly, research on informational cascades found that the phenomenon can help to explain “rapid and short-lived fluctuations such as fads, fashions, booms, and crashes” [15, p. 994] and that, if informational cascades occur, consumers’ product choices exhibit significant jumps and drops that are directly associated with the product’s popularity [31]. The rapid and direct response of consumers to the changing popularity of products therefore suggests that an informational cascade, like herding behavior, is triggered by an affective response of the backer, which is usually evoked much more quickly than a cognitive response because feelings are often elicited immediately on exposure to a new stimulus [48, 61]. We therefore expect that the effects of popularity information on the funding decisions of prospective backers show a quick buildup but a short decay. In contrast, although eWOM like advertising messages can evoke affective responses through, for example, emotional content, these responses are combined with cognitive responses (i.e., rational evaluation) to form an attitude toward the message [9, 75]. Compared to the affective factors, cognitive factors also typically induce more lasting responses by consumers [34]. In addition, prior research has also highlighted that significant delays (wear-in effects) exist between the occurrence of an eWOM message and its impact on consumer decision making [55]. These delays can be explained with the information-gathering process that takes place when evaluating eWOM messages. While informational cascades lead to an immediate response because private information is ignored, eWOM messages trigger an information-gathering process, helping the individual to form an opinion about a specific crowdfunding campaign. Those who already possess a high degree of expertise would typically devote little effort to an information search prior to purchase [16], meaning that in our context, due to the little information available from other sources, we can expect backers to invest more effort in gathering information via eWOM messages. While this process takes more time, leading to a slow buildup of the effect of eWOM, the outcome is an informed decision that is likely to have a longer lasting effect. We therefore hypothesize: Hypothesis 4a: The impact of past funding decisions of other backers as indicated by PI on prospective backers’ present funding decisions has a faster buildup than that of past eWOM. Hypothesis 4b: The impact of past funding decisions of other backers as indicated by PI on prospective backers’ present funding decisions has a shorter decay than that of past eWOM.\n",
              "\n",
              "Research Methodology Data Set and Descriptive Statistics\n",
              "We collected daily project-level data from Indiegogo, which is among the largest and most prominent reward-based crowdfunding platforms on the web. Since its launch\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "855\n",
              "\n",
              "in 2008, more than 400,000 campaigns have run on the platform [59]. Indiegogo offers an appealing context in which to study our research model. The platform offers opportunities for consumers to create and share eWOM messages about the crowdfunding campaigns and reveals PI by prominently depicting the number of previous backers for any campaign on a visual dashboard. The transparent recording of eWOM and previous backers over time opens a window into the reciprocal and dynamic effects of these mechanisms, providing researchers an unobtrusive trace of these often hard-to-study activities. Our data cover 213 days from November 15, 2013, to June 16, 2014, resulting in 23,340 campaigns and approximately 464,000 data points. Data on every campaign that started and ended in this timeframe was gathered automatically in a daily routine with a self-developed web crawler to retrieve time-series data of all campaigns on the website. To account for potential deadline and commiseration effects, we only analyzed campaigns that were covered during their complete lifecycle [49]. Since our data set spans approximately seven months (including the holiday season), we also checked for seasonality effects, but did not find any significant deviations from the overall pattern. In addition to the number of backers and the specific eWOM volume for each campaign, we collected further campaign-related information (i.e., campaign category, number of campaigns in the respective categories, number of campaign updates, total funding amount in U.S. dollar, campaign runtime in days, and a dummy as to whether the campaign description contained a video) for robustness checks and ancillary post hoc subsample analyses. To operationalize PI, we chose the number of previous backers of the campaign. The metric is consistent with earlier studies of popularity information in IS and management research, where download numbers or the number of clicks were used as measurements [31, 72]. The number of previous backers is, however, also distinct from prior operationalizations because it reflects and indicates real financial involvement. We operationalize two types of eWOM: Facebook shares and comments about a project on the crowdfunding platform. As we focus on analyzing eWOM volume, rather than its valence,3 we counted the number of times a campaign was shared on Facebook and how many comments were written about it on Indiegogo. Consistent with the metric Tirunillai and Tellis [69] used, we employ the number of comments users posted about the focal campaign. This measure reflects the magnitude of eWOM received on the crowdfunding platform.4 As the eWOM volume for Facebook could not be extracted directly from the campaign webpage with our web crawler, we collected the data for the number of shares a specific campaign received via the application programming interface (API) of Facebook. Consistent with previous eWOM studies (e.g., [71]), we collected daily data (i.e., the recording interval is twenty-four hours [76]) on the number of Facebook shares and comments about the project. To measure eWOM volume on Facebook around crowdfunding campaigns correctly, we only considered shares that contained a direct hyperlink to the crowdfunding campaign on Indiegogo, as typically used in social media studies (e.g., [36]). To ensure that only genuine Facebook shares were considered for our analysis, we excluded campaigns that showed unnatural peaks in the number of shares on a single\n",
              "\n",
              "\n",
              "856\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "day, as they imply fraudulent behavior [33]. Even though peaks in these performance indicators are to be expected when a campaign receives major attention in other channels such as blogs or news websites, these natural peaks are followed by a gradual decline. On the contrary, unnatural peaks do not show these subsequent effects, implying fraudulent actions that would have distorted the results [33]. These unnatural peaks were identified if, on a single day, the number of additional shares exceeded the threefold standard deviation and at least 500 shares were added, which is usually the lowest quantity of fake shares that can be bought [66].5 The reversed procedure was applied again to ensure that a significant drop in the additional number of shares occurred. Only campaigns that showed an unnatural peak and decline afterward were dropped from the data set, resulting in the removal of 429 campaigns. Finally, consistent with standard outlier analysis procedures [3] and previous crowdfunding studies [73], we dropped the top 1 percent of campaigns with regard to the number of backers and eWOM, as extreme outliers and blockbuster campaigns are expected to show different patterns with regard to their funding process. Summary statistics are presented in Table 1.\n",
              "\n",
              "PVAR Model Specification\n",
              "We employed a panel vector autoregression (PVAR) model to capture the dynamic interdependencies and feedback effects among multiple time series [27]. PVAR models are particularly suitable for studying the relationships between a system of interdependent variables without imposing ad hoc model restrictions, including exogeneity of some of the variables, which other econometric model techniques require [2]. Further advantages of PVAR over alternative models are that it can explicitly “account for biases, such as endogeneity, autocorrelations, omitted variables, and reversed causality” [56, p. 223]. The endogenous treatment of the variables in PVAR models imply that eWOM and PI are explained by past variables of themselves (i.e., autoregressive carryover or self-reinforcing effects) as well as past variables of each other (i.e., cross or reciprocal effects) [57]. The PVAR model also allows the capture of complex feedback effects that may encompass the reverse effects of consumers’ funding behavior on future eWOM, revealing complex chained effects involving cyclical interactions within and between online platforms. PVAR models have been used particularly in marketing and finance studies (e.g., [54, 69]), whereas IS researchers have only recently adopted them (e.g., [20, 29]). The main challenges of our model setup are the simultaneous mutual influences of the different variables of interest, namely, the PI and the volume of eWOM. Consistent with Dewan and Ramaprasad [29], we distinguish the mutual effects by focusing on the orthogonalized impulse-response functions, which show the response of one variable of interest in the next period (e.g., number of backers) to an orthogonal shock of one standard deviation in another variable of interest (e.g., number of Facebook shares) in the current period. By orthogonalizing the response, we can identify the effect of one shock at a time, while holding other shocks constant. This technique combines the\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "857\n",
              "\n",
              "Table 1. Summary Statistics Variables\n",
              "Backers FacebookShares Comments CategoryCompetition (HHI) ProjectUpdates IndiegogoTweet FundingAmount ($) Video (dummy is 1 if the campaign contains a video) SuccessProbability\n",
              "\n",
              "Mean\n",
              "28.30925 192.1488 4.587832 0.0479004 1.663753 0.004156 2,167.6 0.43509 0.1721508\n",
              "\n",
              "SD\n",
              "39.75949 266.2012 4.862755 0.0704678 4.889731 0.064334 4,086.464 0.4957794 0.3775196\n",
              "\n",
              "Min\n",
              "2 0 0 0.0052532 0 0 0 0 0\n",
              "\n",
              "Max\n",
              "420 2,327 60 1 496 1 89,234 1 1\n",
              "\n",
              "Notes: N (projects) = 23,340; Entries are means at the end of the campaign runtime. IndiegogoTweet is a dummy of whether the platform tweeted about the campaign during the runtime.\n",
              "\n",
              "traditional VAR approach that treats all the variables in the system as endogenous with the panel-data approach that allows for unobserved individual heterogeneity [54]. When applying the VAR procedure to panel data, a specific restriction must be imposed. The underlying structure must be the same for each cross-sectional unit and since this constraint is likely to be violated in practice, fixed effects are typically introduced [6, 54]. As the fixed effects are correlated with the regressors due to the lags of the dependent variables, and consistent with previous PVAR studies [54], we use forward mean-differencing, also referred to as the Helmert procedure [6]. This procedure removes fixed effects by subtracting the mean of all future observations and preserves the orthogonality between transformed variables and lagged regressors. The lagged regressors can then be used as instruments in the PVAR model for the forwarddifferenced variables to address a possible simultaneity problem [29]. As previous studies showed that the within-group estimator (i.e., the least-squares estimator) for fixed-effects models is biased when applied to estimate a dynamic panel model, we use generalized method of moments (GMM) to estimate the PVAR model, allowing for error correlation across equations [54]. Our PVAR model is specified for each crowdfunding campaign as follows: 2 3 3 2 3 t Àj tÀj tÀj 2 β β β BackerstÀj εBackers;t j ¼ 1 2 3 12 13 X 6 11 Backerst t Àj tÀj tÀj 74 4 FacebookSharest 5¼ 4 β21 5 FacebookSharestÀj 5þ4 εFacebookShares;t 5 β22 β23 Commentst t Àj tÀj tÀj J CommentstÀj εComments;t β β β\n",
              "31 32 33\n",
              "\n",
              "(1) where Backerst, FacebookSharest, and Commentst are our endogenous variables and denote the number of backers of a project, the number of Facebook shares, and the number of comments on day t(t = 1, 2, . . ., T), respectively.\n",
              "\n",
              "\n",
              "858\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "In our PVAR analysis, today’s backers are a function of past shares on Facebook,6 past comments, past backers, and an error term. In the PVAR model, the β-coefficients represent the relationship between the lagged values of each variable and the variable on the left side of the equation. J represents the order or lag length of the model, which is usually determined using the Akaike information criterion (AIC) and the moment model selection criteria (MMSC) that Andrews and Lu [5] developed. Following the standard approach in the VAR literature [5, 43, 54], we calculated the AIC and the MMSC for each crosssection and took the modal value of the optimal lag among all cross-sections, leading to an optimal lag length of 1. We also controlled for a set of time-varying, exogenous control variables in our model, including the competition in campaign categories, the number of updates the project creator published during the campaign runtime and unobservable marketing efforts by the platform by monitoring their Twitter activity as a proxy. First, we controlled for the concentration of campaigns in each category to account for the competition (i.e., prospective backers’ attention allocation) that a project faces in its primary target category [40]. We used the following Herfindahl–Hirschman index (HHI) as a measure: HHI ¼\n",
              "N X i\n",
              "\n",
              "b2 it\n",
              "\n",
              "where bi is the fraction of a project’s number of backers in the ith project category on Indiegogo at time t. This measure ranges from 1/N to 1, where N is the number of projects in a given category. The HHI ranges from 0 to 1, were 1 represents a monopolistic market, and a decreasing index indicates stronger competition [40]. For example, if a project has all backers in the category Technology, this measure is 1 and it is maximally concentrated. A decreasing index therefore indicates stronger competition. Second, postings of project updates (e.g., progress updates, answers to questions, or appreciation messages) on the campaign website by the project creator may influence investor confidence. We therefore controlled for project updates measured as the cumulative number of updates posted on a project website since the start of the campaign [49]. Third, we monitored Indiegogo’s Twitter account to identify campaigns that benefited from potential marketing efforts by the platform providers. We used a dummy variable to mark the exact day when a campaign was mentioned on Indiegogo’s Twitter account.\n",
              "\n",
              "Results Test for Stationarity in Time Series and Granger Causality\n",
              "The procedure for estimating PVAR models typically starts with a unit-root test to assess whether variables are evolving or stationary [57]. Stationarity is an important\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "859\n",
              "\n",
              "Table 2. Granger Causality Tests Caused by Results\n",
              "Backers FacebookShares Comments\n",
              "\n",
              "Backers\n",
              "— 8.592 (0.00) 0.312 (0.576)\n",
              "\n",
              "FacebookShares\n",
              "88.776 (0.000) — 0.228 (0.633)\n",
              "\n",
              "Comments\n",
              "117.301 (0.000) 41.499 (0.000) —\n",
              "\n",
              "Notes: The results reported are Wald-χ2 statistics with p-values in parentheses. Granger causality tests are performed with 1 lag for consistency with the PVAR models (as selected by AIC).\n",
              "\n",
              "assumption to check in time-series and panel models in order to prevent spurious results. We employed a Phillips–Perron (PP) unit-root test, which is a common method in dynamic panel data analysis [62]. The PP test results suggest stationary time series (see Appendix 3). Next, we conducted pairwise Granger causality tests to understand the time-based causality in our PVAR model. Granger causality tests help to determine whether the lagged values of one variable help predict values of another variable in the PVAR system [39]. More specifically, if a lagged time series at–j (e.g., FacebookSharest−1) improves the accuracy to predict another time series bt (e.g., Backerst), then at–j Granger-causes bt. Table 2 provides the p-values and Wald-χ² values for all possible pairwise Granger causality tests related to our estimated PVAR model. Our results suggest that all three endogenous variables have significant temporal-based causal relationships with each other. The two eWOM volume metrics and the PI significantly “Granger-cause” each other, providing strong evidence of cross- and reverse causal effects and, ultimately, cyclical interactions between these variables. Solely, Comments appear to be unaffected by past Backers and FacebookShares. As a consequence, these results indicate that modeling the data set requires a dynamic system that can account for the complex relationships among multiple endogenous variables, supporting our approach of analyzing the variables through PVAR analysis.\n",
              "\n",
              "PVAR Model Results\n",
              "To test our research hypotheses, we estimated the coefficients of the endogenous system of variables given in the PVAR model, controlling for category competition, project updates, and Indiegogo Tweets. Results from the analysis are reported in Table 3.7 Before turning our attention to our first research hypotheses, we report the findings for the three control variables. First, we find that the coefficient estimates on category competition (measured by the HHI) are positive and statistically significant across our three dependent variables, which means that if fewer projects receive more attention, eWOM activity and backings increase. This result points toward the importance of a reinforcement effect.\n",
              "\n",
              "\n",
              "860\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Table 3. PVAR Model Results for Main Analysis Response of dependent variable Response to\n",
              "Endogenous variables Backerst–1 FacebookSharest–1 Commentst–1 Exogenous variables CategoryCompetitiont–1 Updatest–1 IndiegogoTweett–1\n",
              "\n",
              "Backerst\n",
              "0.911*** 0.00193*** 0.126*** 1.967*** –0.0624*** 2.381*\n",
              "\n",
              "FacebookSharest\n",
              "0.0257** 0.905*** 0.538*** 36.76*** –0.0841 2.857\n",
              "\n",
              "Commentst\n",
              "0.000152 0.0000226 0.912*** 1.178*** 0.00415 0.188\n",
              "\n",
              "Notes: The PVAR model is estimated by GMM. The reported numbers show the coefficients of regressing the column variables on lags of the row variables. ***, **, * denote significance at the 0.1 percent, 1 percent, and 5 percent levels, respectively. N = 23,430.\n",
              "\n",
              "The extreme case would be an evenly distributed number of backers among all projects, which would inhibit PI, as all campaigns are equally popular. Second, the coefficient estimates on project updates is negative and statistically significant for Backers, suggesting that posting project updates on a campaign website does not necessarily instantly increase the number of backers of a campaign, as previously stated [49]. Third, we checked whether marketing efforts by Indiegogo for specific campaigns improved their performance. We therefore analyzed all Tweets published by Indiegogo’s Twitter account that contained a link to a specific campaign. We find that these efforts by the platform provider might indeed increase the number of backers, even though the effect is only weakly significant. We posited in H1 that an increase in eWOM volume leads to additional eWOM volume on the respective platform. We find strong support for this hypothesis, as the coefficients for FacebookShares (0.905, p < 0.001) as well as Comments (0.912, p < 0.001) are strongly significant, implying a substantial reinforcement effect of eWOM. For H2, we highlight the effects of eWOM on prospective backers’ funding behavior. In our model, we are able to estimate the impact of today’s Comments and FacebookShares on tomorrow’s number of backers. Our results show that there is a significant and positive effect of present FacebookShares (0.00193, p < 0.001) and Comments (0.126, p < 0.001) on future Backers. We therefore find strong support of our hypothesis for Facebook-based as well as comment-based eWOM. This gives us further reason to believe that consumers trust recommendations and information from friends on Facebook as well as from an active discussion by more or less foreign commentators on the platform. Even though both eWOM forms appear to increase the number of backers, the reciprocal effect is different. While we observe an increased number of FacebookShares following an increase in\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "861\n",
              "\n",
              "backers, this effect does not occur for comments, meaning that backers tend to share their investment decision with their social network, but discontinue discussing it on the platform itself. Our third hypothesis was based on the argument of informational cascades and proposed that past funding decisions of other backers were positively associated with prospective backers’ present funding decisions. We observe a strong positive response of the number of backers (0.911, p < 0.001) to an increase of their own lagged value (i.e., the observation of the dynamics of other supporters’ backing behavior). This positive and significant response lends credence to the argument around positive informational cascades within platforms and suggests a strong selfreinforcement effect arising from observing other consumers’ choices, in support of H3.\n",
              "\n",
              "Error Variance Decomposition and Impulse Response Functions\n",
              "PVAR analyses are usually supplemented with generalized forecast error variance decomposition (GFEVD) and impulse response function (IRF) analyses [2, 56] to gain increased insights about the dynamics in the relationships of interest. GFEVD analysis provides insights into the relative power over time of shocks triggered by each endogenous variable in explaining other endogenous variables of interest in the PVAR model (e.g., prospective backers’ funding behavior). It is therefore comparable to a partial R2 that indicates the percentage of the variance of the error made in forecasting a variable because of specific shocks of all the variables in the system at a specified time horizon [67]. As such, GFEVD provides indications of the relative importance and magnitude of the effect of each endogenous variable in the PVAR model. The GFEVD analyses shows that for a forecast horizon of sixty days, 6.67 percent of the variation in the backer variable is explained by the eWOM variables, whereas in the short run with a ten-day horizon only 2.88 percent is explained. Accordingly, most of the variance in the endogenous variables is explained by their own lags, suggesting a strong feedback loop within each (crowdfunding and social media) platform rather than across them, which is in line with the coefficients from the PVAR analysis (for a more detailed GFEVD analysis, see Appendix 4). The variance decomposition further emphasizes the importance of action-based online social interactions (PI) compared to opinion-based online social interactions (eWOM), showing that the explanatory power of PI is generally stronger compared to eWOM regarding present backing behavior, although the effect of PI appears to be less persistent over time. We supplemented the PVAR estimates and the GFEDV analyses with an analysis of the corresponding impulse response functions (IRF) for our research model. For our last set of hypotheses (H4a and H4b), we turn to the analysis and interpretation of the IRFs. Figure 2 provides the nine possible IRFs for the estimated PVAR model. Each plot in Figure 2 can be interpreted as depicting the corresponding response\n",
              "\n",
              "\n",
              "862\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Backerst-1\n",
              "\n",
              "Backerst\n",
              "\n",
              "Backerst-1\n",
              "\n",
              "FacebookSharest\n",
              "\n",
              "Backerst-1\n",
              "\n",
              "Commentst\n",
              "\n",
              "FacebookSharest-1\n",
              "\n",
              "Backerst\n",
              "\n",
              "FacebookSharest-1\n",
              "\n",
              "FacebookSharest\n",
              "\n",
              "FacebookSharest-1\n",
              "\n",
              "Commentst\n",
              "\n",
              "Commentst-1\n",
              "\n",
              "Backerst\n",
              "\n",
              "Commentst-1\n",
              "\n",
              "FacebookSharest\n",
              "\n",
              "Commentst-1\n",
              "\n",
              "Commentst\n",
              "\n",
              "Figure 2. Impulse Response Functions (Impulse → Response) Notes: The x-axis is the forecast horizon (in days) and the y-axis is the forecasted response of the dependent variable to a unit shock in the impulse variable. Errors are 5 percent on each side generated by Monte Carlo simulations with 1,000 repetitions.\n",
              "\n",
              "(increase or decrease) of a dependent variable over time to a one standard deviation shock in another dependent variable in the preceding period, while keeping all other variables constant [2]. Using IRFs, we can visualize the dynamics of the pairwise relationships of our research model. The results of the IRF analysis support the results from the PVAR model and serve as an initial indication for our final hypotheses. All responses are positive and vary in the magnitude of their effects, as depicted in the position of the response function on the IRF plots. While the response of backers is strongest in magnitude when a shock in previous backers (compared to shocks in FacebookShares or Comments) is triggered (first column of Figure 2), the response in eWOM is greatest when a shock in previous eWOM on the corresponding platform occurs (second and third columns of Figure 2). This pattern of results underscores the strengths of self-reinforcing effects on the respective platforms and, in particular, the predominance of the underlying PI for affecting current backing behavior. In H4 we argued that PI has a fast buildup and a fast decay, compared to eWOM effects that require more time to be effective. We therefore interpret the buildup (i.e., how long it takes for each variable to reach the peak of the predictive relationship with the other variable) and decay (i.e., how long it takes for the\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "863\n",
              "\n",
              "predictive relationship to decrease from the peak impact point to nonsignificance) of the IRF. Additional important insights about the immediacy and persistence of the effects can also be derived and are presented in Table 4 [34]. The immediate effect denotes the magnitude of the response of backers on the first day after a shock. The cumulative effect is the sum of the response effects over sixty days. The peak effect is the largest effect over time and marks the point of inflection, which concludes the buildup. Most notably, while we can observe strong immediate effects in the Backerst–1 → Backerst plot that attenuate rather quickly, the effects in the FacebookSharest–1 → Backerst plot are less immediate (the buildup time is from day 1 to day 11), yet taper off more slowly, suggesting more persistent effects on consumers’ backing behavior. The effects in the Commentst–1 → Backerst plot also reach their peak impact point after around 11 days and diminish gradually thereafter. The relative fraction of the immediate effect is ten times higher for PI (7.35 percent) compared to the immediate eWOM effects (0.84 percent and 0.74 percent, respectively). The results also show that eWOM requires a significantly longer buildup time than PI (Kruskal–Wallis test for Comments = 4.1 and FacebookShares = 23.69, both p < 0.05). In sum, these results strongly support our final two hypotheses H4a and H4b.\n",
              "\n",
              "Post Hoc Subsample Analysis\n",
              "In addition to the full-sample analysis described above, we conducted two sets of post hoc subsample analyses in order to examine whether the dynamic relationships are consistent for campaigns that are successful versus unsuccessful,8 and have low versus high average investment/funding amounts.9 We chose to investigate how our main results vary for successful and unsuccessful campaigns and for campaigns with low and high average investments, because we expect the effects to be considerably different for successful compared to unsuccessful campaigns and because prospective backers are likely to go through different types of decision processes based on the level of financial commitment [32]. First, we can observe in Models 1 and 2 of the subsample analysis (Table 5) that the relationship regarding the effect of eWOM across and within platforms is much weaker for campaigns that fail to reach their funding goal compared to the successful ones, while the effect of PI within a platform still holds, regardless of campaign success. These results indicate that for crowdfunding campaigns, eWOM can be a crucial, if not decisive, success factor, while PI has consistent self-reinforcing effects across all campaigns. Second, with respect to our results regarding the average investment size depicted in Models 3 and 4 of the subsample analysis, we observe that eWOM effects give way to PI effects for higher spending amounts, while eWOM becomes more important for small contributions. This means that actionbased online social interactions become particularly important for investment decisions that involve more substantial amounts.\n",
              "\n",
              "\n",
              "864 THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Table 4. Timing and Effect Intensity on Backers\n",
              "\n",
              "Effect of\n",
              "26.633* (0.413) 3.999* (0.409) 8.546* (0.783) 7.35 0.84 0.74\n",
              "\n",
              "Immediate effect (SD) Cumulative effect (SD) Relative fraction of immediate effect (%) Peak effect (SD) Buildup time\n",
              "1.957* (0.003) 0.142* (0.024) 0.287* (0.029) 1 11 11\n",
              "\n",
              "Backers FacebookShares Comments\n",
              "\n",
              "1.957* (0.003) 0.0336* (0.003) 0.0630* (0.025)\n",
              "\n",
              "Notes: SD = Standard deviation; buildup time is measured in days; effects are calculated with Monte Carlo simulation and 1,000 repetitions; *p < 0.01.\n",
              "\n",
              "\n",
              "Table 5. PVAR Model Results for Subsample Analyses Model (2): Loser\n",
              "Response of dependent variable Backerst 0.886*** 0.00478*** 0.337*** Response of dependent variable FacebookSharest –0.0448** 0.916*** 0.770*** 0.0736*** 0.900*** 0.452*** 0.0365*** 0.906*** 0.549*** 0.00891 0.903*** 0.571*** 0.922*** 0.00128*** 0.0469*** 0.923*** 0.00156*** 0.0700*** 0.893*** 0.00230*** 0.228***\n",
              "\n",
              "Model (1): Winner\n",
              "\n",
              "Model (3): High spending\n",
              "\n",
              "Model (4) Low spending\n",
              "\n",
              "Response to\n",
              "\n",
              "Backerst–1 FacebookSharest–1 Commentst–1\n",
              "\n",
              "Backerst–1 FacebookSharest–1 Commentst–1\n",
              "\n",
              "Response of dependent variable Commentst –0.00156** 0.000195* 0.933*** 0.000791* 0.00000251 0.904*** 0.000136 0.0000641 0.919*** 0.00021 –0.0000175 0.905***\n",
              "\n",
              "Backerst–1 FacebookSharest–1 Commentst–1\n",
              "\n",
              "Response to (Controls) Included Included Included Included Included Included Included Included Included Included Included Included\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "CategoryCompetitiont–1 Updatest–1 IndiegogoTweett–1\n",
              "\n",
              "Notes: The PVAR model is estimated by GMM. The reported numbers show the coefficients of regressing the column variables on lags of the row variables. ***, **, * denote significance at the 0.1 percent, 1 percent, and 5 percent levels, respectively.\n",
              "\n",
              "865\n",
              "\n",
              "\n",
              "866\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "Discussion\n",
              "This study was motivated by the observation that—despite the increasing information availability about other consumers’ choices and opinions on consumer platforms —we know little about the reciprocal nature of influence among eWOM and PI, and how these mechanisms dynamically and differentially shape consumer decision making. This study offers insights into the dynamic interplay between eWOM, PI, and contribution behavior on consumer platforms in the context of crowdfunding. Our overarching finding is that an “action” in the form of a past contribution (actionbased social interactions) has stronger and more immediate effect on future contribution behavior, and thus speaks louder than spreading the “word” about project campaigns via eWOM (opinion-based social interactions). We also find that the impact of a positive shock in previous backing behavior abates relatively fast, while the effects of a positive shock in eWOM decrease at a slower pace. Two more specific findings in our study are noteworthy. First, we demonstrated the critical role of eWOM for the outcome of crowdfunding campaigns. As shown in our subsample analysis, subsequent effects of eWOM are weaker for campaigns that fail to reach the desired funding goal, while successful project creators can use the information distribution in social media in full capacity. Also, the relative predictive power of eWOM increases over time, indicating that social media interactions, within and beyond the platform, are a crucial discriminating factor for the success of crowdfunding campaigns. Second, our results based on the subsample analysis differentiating between high and low average funding amounts show that backers rely more heavily on PI when making higher investments. This suggests that backers strive for more informed and fact-based decision making when faced with a higher investment risk.\n",
              "\n",
              "Implications for Theory and Research\n",
              "Our study advances our understanding of the dynamic interplay among social media and critical consumer behavior within and across consumer platforms. First, while previous research, with a few exceptions [21, 23], has focused on examining the effects of eWOM and PI in isolation from one another, our study is among the first to show that both types of social interaction have to be considered together, as they dynamically influence each other and differentially affect consumer decision making over time. By measuring buildup and decay effects [52], we show that PI has a more immediate predictive relationship with consumers’ funding behavior than eWOM, but its effects diminish rather quickly, while the effects of eWOM require a longer time to build up their effectiveness but are more persistent over time. These results contribute to the software platform and social media literature by advancing our understanding of PI’s and eWOM’s temporal effect patterns. We also provide evidence of strong Granger-causal interdependencies that highlight the reciprocal and intertwined nature of influence among eWOM and PI over time, confirming that it is both theoretically and empirically valuable to examine these social interaction\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "867\n",
              "\n",
              "mechanisms in combination rather than in isolation. Second, while cross-platform effects have become increasingly important, previous research—with only rare exceptions (e.g., [20, 29, 56])—has tended to study the dynamic effects of eWOM and/or PI on a single platform (e.g., a software download platform). Our study is one of the first to focus on both within-platform and cross-platform dynamics (i.e., effects occurring between crowdfunding and a social media platform) and how they can affect critical consumer contributions involving real financial consequences. This study therefore answers calls for research that stresses the importance of tracking and unraveling the evolution and interrelationships of multiple time series across information systems and platforms in an increasingly interconnected IT world [2, 70]. Our study extends previous IS and social media research by going beyond one-directional relationships in fully accounting for the time-varying dynamic effects (i.e., self-reinforcing, cross- and reverse causal effects) in a system of mutually interdependent endogenous variables [57]. Finally, we contribute to the still nascent yet emerging crowdfunding literature (e.g., [4, 12, 18, 60]) by showing that in reward-based crowdfunding markets, prospective backers tend to contribute to projects that have already received a lot of attention on social media as well as on the crowdfunding platform itself. This suggests that, similar to equity-based and lending-based crowdfunding markets [42, 77], and in contrast to donation-based crowdfunding markets [18], prospective backers perceive prior contribution behavior by others as a quality indicator, which allows them to reduce their own risk in the face of uncertainty. However, as we showed in our split-sample analysis, the importance of these quality indicators varies with the investment amount. Furthermore, although herding behavior has been shown to be triggered by nonbiased quality indicators in lending-based crowdfunding markets [77], our study is the first to compare the time-varying effects of PI and eWOM in a crowdfunding context.\n",
              "\n",
              "Practical Implications\n",
              "Our findings offer various practical implications that should be considered, particularly by providers and third-party complementors (campaign/project creators) on crowdfunding platforms. First, project creators should bear in mind that prospective backers are responsive to changes in PI, which has comparatively stronger and more immediate effects than eWOM. As such, early support from the creator ’s own network is likely to increase the chances of success further down the road of the campaign. Second, project creators should be aware that eWOM—due to its persistent effect—could make the difference for their campaign’s success, as backers often learn about campaigns via their online social networks, spread the word about the campaign via social media, or request feedback on the campaign from peers prior to their own investment. Third, our findings suggest that the spread of eWOM on Indiegogo and Facebook is influenced differently by the decision making of backers. We find that backers tend to share their investment decision with their social network\n",
              "\n",
              "\n",
              "868\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "on Facebook, but are apparently not willing to discuss it afterward on Indiegogo. Consequently, it is advisable that project creators highlight the favorable aspects of their projects in the campaign descriptions, engage in social media marketing throughout the campaign runtime, and encourage backers to further share the campaign, comment on it on the platform, and keep an active discussion with prospective and past backers. Finally, understanding the relative predictive values of eWOM and PI for critical consumer contributions can help providers of consumer platforms monitor and analyze the impact of changes in eWOM and previous campaign backing behavior over time. Based on real-time evaluation of this information, providers are better able to adapt the salience and granularity of social media cues/metrics as well as information about other consumers’ past choices (popularity information) to stimulate contribution behavior and increase platform prosperity overall.\n",
              "\n",
              "Limitations, Future Research, and Conclusion\n",
              "While our study provides several contributions to research and practice, we acknowledge limitations that have to be considered when interpreting the results and implications. In calling attention to these limitations, we hope to suggest avenues for future research. First, although crowdfunding platforms share many characteristics with other multisided online consumer platforms, in particular the transparent recording of eWOM and previous consumer activities, caution should be exercised when drawing conclusions from our findings. Future research might attempt to confirm the interdependent and dynamic nature of relationships between eWOM, PI, and critical consumer behavior found in this study in other online platform settings (e.g., mobile app or media streaming platforms) besides crowdfunding. Second, we were unable to take into account all types of eWOM that might affect the outcome of crowdfunding campaigns and we have therefore limited our analysis to messages spread via Facebook and comments on the platform itself. Future research may benefit from including a broader set of social media (e.g., blogs or social news) and other nonbiased quality indicators available to potential backers, such as credit scores of creators that are available on other types of crowdfunding platforms. Third, our research design can show relationships among endogenous variables, but cannot assure causality. A fruitful avenue for future research is the use of laboratory or randomized field experiments. Our research design is also constrained by the recording unit of twenty-four hours, which might not perfectly capture the possible simultaneity of the variables. Another approach for future studies would be to reduce the recording unit from days to hours or minutes to better approximate those effects. Fourth, although our sentiment analysis of Facebook shares and comments revealed that eWOM messages with a negative sentiment are rare in the context of crowdfunding, it is worth exploring how eWOM valence interacts with PI and how they jointly affect critical consumer behavior on crowdfunding platforms. Finally, we did not\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "869\n",
              "\n",
              "focus on discriminating between different originators of eWOM on Facebook and Indiegogo. The characteristics of different eWOM originators (e.g., number of friends, commercial or private accounts, and experience in crowdfunding) might reveal additional insights. In conclusion, this study provides an initial step toward understanding the reciprocal and dynamic nature of effects among eWOM, PI, and consumer decision making. Our central finding is that previous consumer contributions have stronger predictive and more immediate effects than eWOM. Nonetheless, eWOM is an important complementary influencing mechanism, as its stimulating effects diminish slowly, while the effects of PI decay rather quickly. We hope that our results provide impetus for further analysis of the interdependencies between eWOM, PI, and contribution behavior, and give actionable recommendations to platform providers and project creators in the crowdfunding context.\n",
              "Acknowledgments: The research was partially supported by a grant from the Dr. Werner Jackstädt Foundation in Germany (Grant no. 010103/56300720).\n",
              "\n",
              "NOTES\n",
              "1. In our dataset, less than 7 percent of creators had set up more than two campaigns in the past. Out of the top twenty Indiegogo campaigns, seventeen were from first-time creators. 2. While the focus of our study is not on theorizing the differential effects of eWOM based on Facebook shares and comments on Indiegogo, we present notable differences in their effects in the results and discussion sections. 3. We argue that in reward-based crowdfunding, analyzing eWOM volume is more appropriate than eWOM valence, given that consumers write their eWOM messages before the reward has been received (e.g., the product being funded and still under development). It is therefore unlikely that the backer will have had a negative experience with the reward or the project before writing the message. The appearance of many and extremely negative eWOM messages is therefore unlikely. We checked and verified the important assumption that the valence of eWOM messages shared on crowdfunding platforms is mostly positive or neutral, such that we believe a focus on eWOM volume rather than valence is warranted in our study context (Appendix 2). Furthermore, as negative shares would not generate additional backers, our approach underestimates the true effect, as we treat all shares and comments equally. 4. As the average number of comments reported in Table 1 might be driven by many zeros in the data, we ran four robustness checks that excluded projects with less than 2, 3, 4, or 5 comments to account for possible skewness. The results did not significantly deviate from our main model. 5. As a robustness check, we conducted the outlier analysis with various alternative threshold values for identifying unnatural peaks, but the results remained qualitatively unchanged. 6. We deliberately chose the number of backers as the measure for backers’ decision making behavior instead of the funding amount, for a few reasons. First, using the number of backers more adequately reflects backers’ decision making behavior and the dynamic relationships among the endogenous variables in the PVAR model because funding amounts may be distorted in several ways (e.g., if backers who are closely related to the project creator donate excessive amounts to the project; individual funding amounts are also arguably driven by the distinct rewards a project offers). Second, in the long run, knowing how many individuals are interested in a specific crowdfunding project and the respective product or service could be more relevant to the creator of the project than reaching a short-term financial goal. Third, using backers instead of funding amounts ensures that all three variables in the\n",
              "\n",
              "\n",
              "870\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "PVAR model are measured on the same ordinal scale. Finally, as a robustness check, we conducted the PVAR analysis with funding amounts (in U.S. dollars) as a substitute for the number of backers and obtained qualitatively consistent results. 7. As a robustness check, we followed Lin et al. [51], who pointed out that studies with large sample sizes should not solely rely on p-values, as this might lead to a claim of support for hypotheses with no practical significance. We therefore followed their practice and provide coefficient/p-value/sample size (CPS) charts for the PVAR main analysis in Appendix 5 to illustrate that our results are not based on sample size but hold for random subsampling. 8. Campaigns tend to either surpass their funding goal (they are successful) or fail to do so (they are unsuccessful), by a large margin [60]. 9. In order to split the sample based on the average funding amount per campaign, we calculated the average funding amount per backer for each campaign. We then used a median split to turn the continuous variable (average funding amount per campaign) into a categorical one with the values “low” and “high” spending.\n",
              "\n",
              "ORCID\n",
              "Alexander Benlian http://orcid.org/0000-0002-7294-3097\n",
              "\n",
              "Supplemental File\n",
              "Supplemental data for this article can be accessed on the publisher ’s website at http://dx.doi.org/10.1080/07421222.2016.1243967\n",
              "\n",
              "REFERENCES\n",
              "1. Adler, C. How kickstarter became a lab for daring prototypes and ingenious products. from http://www.wired.com/2011/03/ff_kickstarter/all/1 (accessed on June 5, 2016). 2. Adomavicius, G.; Bockstedt, J.; and Gupta, A. Modeling supply-side dynamics of IT components, products, and infrastructure: An empirical analysis using vector autoregression. Information Systems Research, 23, 2 (2012), 397–417. 3. Aggarwal, C.C. Outlier Analysis. New York: Springer, 2013. 4. Agrawal, A.; Catalini, C.; and Goldfarb, A. Crowdfunding: Geography, social networks, and the timing of investment decisions. Journal of Economics and Management Strategy, 24, 2 (2015), 253–274. 5. Andrews, D.W.K., and Lu, B. Consistent model and moment selection procedures for GMM estimation with application to dynamic panel data models. Journal of Econometrics, 101, 1 (2001), 123–164. 6. Arellano, M., and Bover, O. Another look at the instrumental variable estimation of error-components models. Journal of Econometrics, 68, 1 (1995), 29–51. 7. Arndt, J. Role of product-related conversations in the diffusion of a new product. Journal of Marketing Research, 4, 3 (1967), 291–295. 8. Baddeley, M. Herding, social influence and economic decision-making: Socio-psychological and neuroscientific analyses. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 365, 1538 (2010), 281–290. 9. Batra, R., and Ray, M.L. Affective responses mediating acceptance of advertising. Journal of Consumer Research, 13, 2 (1986), 234–249. 10. Bayus, B.L. Crowdsourcing new product ideas over time: An analysis of the Dell IdeaStorm Community. Management Science, 59, 1 (2013), 226–244. 11. Becker, G.S. A note on restaurant pricing and other examples of social influences on price. Journal of Political Economy 99, 5 (1991), 1109–1116.\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "871\n",
              "\n",
              "12. Belleflamme, P.; Lambert, T.; and Schwienbacher, A. Crowdfunding: Tapping the right crowd. Journal of Business Venturing, 29, 5 (2014), 585–609. 13. Berger, J., and Milkman, K.L. What makes online content viral? Journal of Marketing Research, 49, 2 (2012), 192–205. 14. Bikhchandani, S.; Hirshleifer, D.; and Welch, I. Learning from the behavior of others: Conformity, fads and informational cascades. Journal of Economic Perspectives, 12, 3 (1998), 151–170. 15. Bikhchandani, S.; Hirshleifer, D.; and Welch, I. A Theory of fads, fashion, custom, and cultural change as informational cascades. Journal of Political Economy, 100, 5 (1992), 992– 1026. 16. Bloch, P.H.; Sherrell, D.L.; and Ridgway, N.M. Consumer search: An extended framework. Journal of Consumer Research, 13, 1 (1986), 119–126. 17. Brown, J.; Broderick, A.J.; and Lee, N. Word of mouth communication within online communities: Conceptualizing the online social network. Journal of Interactive Marketing, 21, 3 (2007), 2–20. 18. Burtch, G.; Ghose, A.; and Wattal, S. An empirical examination of the antecedents and consequences of contribution patterns in crowd-funded markets. Information Systems Research, 24, 3 (2013), 499–519. 19. Buttle, F.A. Word of mouth: Understanding and managing referral marketing. Journal of Strategic Marketing, 6, 3 (1998), 241–254. 20. Chen, H.; De, P.; and Hu, Y.J. IT-enabled broadcasting in social media: An empirical study of artists’ activities and music sales. Information Systems Research, 26, 3 (2015), 513–531. 21. Chen, Y.; Wang, Q.; and Xie, J. Online social interactions: A natural experiment on word of mouth versus observational learning. Journal of Marketing Research, 48, 2 (2011), 238–254. 22. Cheung, C.M.K., and Thadani, D.R. The impact of electronic word-of-mouth communication: A literature analysis and integrative model. Decision Support Systems, 54, 1 (2012), 461–470. 23. Cheung, C.M.K.; Xiao, B.S.; and Liu, I.L.B. Do actions speak louder than voices? The signaling role of social information cues in influencing consumer purchase decisions. Decision Support Systems, 65 (2014), 50–58. 24. Chevalier, J.A., and Mayzlin, D. The effect of word of mouth on sales: Online book reviews. Journal of Marketing Research, 43, 3 (2006), 345–354. 25. Cialdini, R.B. Influence: Science and Practice. Boston, MA: Pearson Education, 2009. 26. Davis, A., and Khazanchi, D. An empirical study of online word of mouth as a predictor for multi‐product category e‐commerce sales. Electronic Markets, 18, 2 (2008), 130–141. 27. Dekimpe, M.G., and Hanssens, D.M. The persistence of marketing effects on sales. Marketing Science, 14, 1 (1995), 1–21. 28. Dellarocas, C. The digitization of word of mouth: Promise and challenges of online feedback mechanisms. Management Science, 49, 10 (2003), 1407–1424. 29. Dewan, S., and Ramaprasad, J. Social media, traditional media, and music sales. MIS Quarterly, 38, 1 (2014), 101–121. 30. Duan, W.; Gu, B.; and Whinston, A.B. The dynamics of online word-of-mouth and product sales: An empirical investigation of the movie industry. Journal of Retailing, 84, 2 (2008), 233–242. 31. Duan, W.; Gu, B.; and Whinston, A.B. Informational cascades and software adoption on the Internet: An empirical investigation. MIS Quarterly, 33, 1 (2009), 23–48. 32. Engel, J.F., and Blackwell, R.D. Consumer Behavior. Chicago: Dryden Press, 1982. 33. Facebook, Inc. What are fake page likes? https://www.facebook.com/help/ 1540525152857028 (accessed on June 5, 2016). 34. Fang, Z.; Luo, X.; and Jiang, M. Quantifying the dynamic effects of service recovery on customer satisfaction evidence from Chinese mobile phone markets. Journal of Service Research, 16, 3 (2013), 341–355. 35. Forman, C.; Ghose, A.; and Wiesenfeld, B. Examining the relationship between reviews and sales: The role of reviewer identity disclosure in electronic markets. Information Systems Research, 19, 3 (2008), 291–313.\n",
              "\n",
              "\n",
              "872\n",
              "\n",
              "THIES, WESSEL, AND BENLIAN\n",
              "\n",
              "36. Galuba, W.; Aberer, K.; Chakraborty, D.; Despotovic, Z.; and Kellerer, W. Outtweeting the twitterers-predicting information cascades in microblogs. Proceedings of the Third Conference on Online Social Networks. Boston, MA: USENIX Association, 2010, pp. 1–9. 37. Godes, D., and Mayzlin, D. Using online conversations to study word-of-mouth communication. Marketing Science, 23, 4 (2004), 545–560. 38. Godes, D.; Mayzlin, D.; Chen, Y.; Das, S.; Dellarocas, C.; Pfeiffer, B.; Libai, B.; Sen, S.; Shi, M.; and Verlegh, P. The firm’s management of social interactions. Marketing Letters, 16, 3–4 (2005), 415–428. 39. Granger, C.W.J. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37, 3 (1969), 424–438. 40. Hansen, M.T., and Haas, M.R. Competing for attention in knowledge markets: Electronic document dissemination in a management consulting company. Administrative Science Quarterly, 46, 1 (2001), 1–28. 41. Hennig-Thurau, T.; Gwinner, K.P.; Walsh, G.; and Gremler, D.D. Electronic word-ofmouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the Internet? Journal of Interactive Marketing, 18, 1 (2004), 38–52. 42. Herzenstein, M.; Dholakia, U.M.; and Andrews, R.L. Strategic herding behavior in peer-to-peer loan auctions. Journal of Interactive Marketing, 25, 1 (2011), 27–36. 43. Holtz-Eakin, D.; Newey, W.; and Rosen, H.S. Estimating vector autoregressions with panel data. Econometrica, 56, 6 (1988), 1371–1398. 44. Huck, S., and Oechssler, J. Informational cascades in the laboratory: Do they occur for the right reasons? Journal of Economic Psychology, 21, 6 (2000), 661–671. 45. Jabr, W., and Zheng, Z. Know yourself and know your enemy: An analysis of firm recommendations and consumer reviews in a competitive environment. MIS Quarterly, 38, 3 (2014), 635–654. 46. Jensen, M.L.; Averbeck, J.M.; Zhang, Z.; and Wright, K.B. Credibility of anonymous online product reviews: A language expectancy perspective. Journal of Management Information Systems, 30, 1 (2013), 293–324. 47. King, M.F., and Balasubramanian, S.K. The effects of expertise, end goal, and product type on adoption of preference formation strategy. Journal of the Academy of Marketing Science, 22, 2 (1994), 146–159. 48. Kuan, K.K.Y.; Zhong, Y.; and Chau, P.Y.K. Informational and normative social influence in group-buying: Evidence from self-reported and EEG data. Journal of Management Information Systems, 30, 4 (2014), 151–178. 49. Kuppuswamy, V., and Bayus, B.L. Crowdfunding creative ideas: The dynamics Of project backers In Kickstarter. Working papers, UNC Kenan-Flagler Research Paper no. 2013–15 (2014). 50. Lau, G.T., and Ng, S. Individual and situational factors influencing negative word-ofmouth behaviour. Canadian Journal of Administrative Sciences/Revue Canadienne des Sciences de l’Administration, 18, 3 (2001), 163–178. 51. Lin, M.F.; Lucas, H.C.; and Shmueli, G. Too big to fail: Large samples and the p-value problem. Information Systems Research, 24, 4 (2013), 906–917. 52. Little, J.D. Aggregate advertising models: The state of the art. Operations Research, 27, 4 (1979), 629–667. 53. Liu, Y. Word of mouth for movies: Its dynamics and impact on box office revenue. Journal of Marketing, 70, 3 (2006), 74–89. 54. Love, I., and Zicchino, L. Financial development and dynamic investment behavior: Evidence from panel VAR. Quarterly Review of Economics and Finance, 46, 2 (2006), 190–210. 55. Luo, X. Quantifying the long-term impact of negative word of mouth on cash flows and stock prices. Marketing Science, 28, 1 (2009), 148–165. 56. Luo, X., and Zhang, J. How do consumer buzz and traffic in social media marketing predict the value of the firm? Journal of Management Information Systems, 30, 2 (2013), 213–238. 57. Luo, X.; Zhang, J.; and Duan, W. Social media and firm equity value. Information Systems Research, 24, 1 (2013), 146–163. 58. Mahajan, V.; Muller, E.; and Kerin, R.A. Introduction strategy for new products with positive and negative word-of-mouth. Management Science, 30, 12 (1984), 1389–1404.\n",
              "\n",
              "\n",
              "EFFECTS OF SOCIAL INTERACTION DYNAMICS ON PLATFORMS\n",
              "\n",
              "873\n",
              "\n",
              "59. Mearian, L. Indiegogo launches crowdsourcing for big businesses. http://www.computer world.com/article/3019349/emerging-technology/indiegogo-launches-crowdsourcing-for-bigbusinesses.html (accessed on June 5, 2016). 60. Mollick, E. The dynamics of crowdfunding: An exploratory study. Journal of Business Venturing, 29, 1 (2014), 1–16. 61. Pham, M.T. Representativeness, relevance, and the use of feelings in decision making. Journal of Consumer Research, 25, 2 (1998), 144–159. 62. Phillips, P.C.B., and Perron, P. Testing for a unit root in time series regression. Biometrika, 75, 2 (1988), 335–346. 63. Schwienbacher, A., and Larralde, B. Crowdfunding of small entrepreneurial ventures. In Cumming, D., (ed.), The Oxford Handbook of Entrepreneurial Finance. Oxford: Oxford University Press, 2012, pp. 369–391. 64. Shi, Z., and Whinston, A.B. Network structure and observational learning: Evidence from a location-based social network. Journal of Management Information Systems, 30, 2 (2013), 185–212. 65. Simonsohn, U., and Ariely, D. When rational sellers face nonrational buyers: Evidence from herding on eBay. Management Science, 54, 9 (2008), 1624–1637. 66. Steuer, E. How to buy friends and influence people on Facebook. http://www.wired. com/2013/04/buy-friends-on-facebook/(accessed on June 5, 2016). 67. Stock, J.H., and Watson, M.W. Vector autoregressions. Journal of Economic Perspectives, 15, 4 (2001), 101–115. 68. Sun, H. A longitudinal study of herd behavior in the adoption and continued use of technology. MIS Quarterly, 37, 4 (2013), 1013–1041. 69. Tirunillai, S., and Tellis, G.J. Does chatter really matter? Dynamics of user-generated content and stock performance. Marketing Science, 31, 2 (2012), 198–215. 70. Tiwana, A.; Konsynski, B.; and Bush, A.A. Research commentary. Platform evolution: Coevolution of platform architecture, governance, and environmental dynamics. Information Systems Research, 21, 4 (2010), 675–687. 71. Toubia, O., and Stephen, A.T. Intrinsic vs. image-related utility in social media: Why do people contribute content to Twitter? Marketing Science, 32, 3 (2013), 368–392. 72. Tucker, C., and Zhang, J. How does popularity information affect choices? A field experiment. Management Science, 57, 5 (2011), 828–842. 73. Wessel, M.; Thies, F.; and Benlian, A. The emergence and effects of fake social information: Evidence from crowdfunding. Decision Support Systems, 90, (2016), pp. 75–85. 74. Wojnicki, A.C., and Godes, D. Word-of-mouth as self-enhancement. HBS Marketing Research Paper, no. 06-01 (2008). 75. Wright, P.L. The cognitive processes mediating acceptance of advertising. Journal of Marketing Research, 10, 1 (1973), 53–62. 76. Zaheer, S.; Albert, S.; and Zaheer, A. Time scales and organizational theory. Academy of Management Review, 24, 4 (1999), 725–741. 77. Zhang, J.J., and Liu, P. Rational herding in microloan markets. Management Science, 58, 5 (2012), 892–912. 78. Zhu, F., and Zhang, X. Impact of online consumer reviews on sales: The moderating role of product and consumer characteristics. Journal of Marketing, 74, 2 (2010), 133–148.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "metadata": {
        "id": "7t1sz1MaHKHn",
        "colab_type": "code",
        "outputId": "e253a10d-28ab-4a28-9a03-2f4c177cb63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "p_kleiner_als_Sents = [sent for sent in doc14.sents if 'p <' in sent.string]\n",
        "p_kleiner_als_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[We find strong support for this hypothesis, as the coefficients for FacebookShares (0.905, p < 0.001) as well as Comments (0.912, p < 0.001) are strongly significant, implying a substantial reinforcement effect of eWOM.,\n",
              " Our results show that there is a significant and positive effect of present FacebookShares (0.00193, p < 0.001) and Comments (0.126, p < 0.001) on future Backers.,\n",
              " We observe a strong positive response of the number of backers (0.911, p < 0.001) to an increase of their own lagged value (i.e., the observation of the dynamics of other supporters’ backing behavior).,\n",
              " The results also show that eWOM requires a significantly longer buildup time than PI (Kruskal–Wallis test for Comments = 4.1 and FacebookShares = 23.69, both p < 0.05).,\n",
              " * (0.025)\n",
              " \n",
              " Notes: SD = Standard deviation; buildup time is measured in days; effects are calculated with Monte Carlo simulation and 1,000 repetitions; *p < 0.01.\n",
              " \n",
              " ]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "metadata": {
        "id": "KnOS62WoBPkC",
        "colab_type": "code",
        "outputId": "ad4cc4be-9e7b-41f8-8eb3-744d9011c4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "cell_type": "code",
      "source": [
        "significant_Sents = [sent for sent in doc14.sents if 'significant' in sent.string]\n",
        "significant_Sents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[However, our overarching finding is that eWOM surrounding crowdfunding campaigns on Indiegogo or Facebook has a significant yet substantially weaker predictive power than\n",
              " Journal of Management Information Systems / 2016, Vol.,\n",
              " Previous research has found a significant influence of WOM on consumers’ information search, evaluation, and decision making, as it “influences attitudes during the pre-choice evaluation of alternative service providers” [19, p. 242].,\n",
              " Several studies have shown that eWOM volume, rather than valence, is significantly associated with product sales (e.g., [26, 30]).\n",
              " \n",
              " ,\n",
              " Similarly, research on informational cascades found that the phenomenon can help to explain “rapid and short-lived fluctuations such as fads, fashions, booms, and crashes” [15, p. 994] and that, if informational cascades occur, consumers’ product choices exhibit significant jumps and drops that are directly associated with the product’s popularity [31].,\n",
              " In addition, prior research has also highlighted that significant delays (wear-in effects) exist between the occurrence of an eWOM message and its impact on consumer decision making [55].,\n",
              " Since our data set spans approximately seven months (including the holiday season), we also checked for seasonality effects, but did not find any significant deviations from the overall pattern.,\n",
              " These unnatural peaks were identified if, on a single day, the number of additional shares exceeded the threefold standard deviation and at least 500 shares were added, which is usually the lowest quantity of fake shares that can be bought [66].5 The reversed procedure was applied again to ensure that a significant drop in the additional number of shares occurred.,\n",
              " Our results suggest that all three endogenous variables have significant temporal-based causal relationships with each other.,\n",
              " The two eWOM volume metrics and the PI significantly “Granger-cause” each other, providing strong evidence of cross- and reverse causal effects and, ultimately, cyclical interactions between these variables.,\n",
              " First, we find that the coefficient estimates on category competition (measured by the HHI) are positive and statistically significant across our three dependent variables, which means that if fewer projects receive more attention, eWOM activity and backings increase.,\n",
              " Second, the coefficient estimates on project updates is negative and statistically significant for Backers, suggesting that posting project updates on a campaign website does not necessarily instantly increase the number of backers of a campaign, as previously stated [49].,\n",
              " We find that these efforts by the platform provider might indeed increase the number of backers, even though the effect is only weakly significant.,\n",
              " We find strong support for this hypothesis, as the coefficients for FacebookShares (0.905, p < 0.001) as well as Comments (0.912, p < 0.001) are strongly significant, implying a substantial reinforcement effect of eWOM.,\n",
              " Our results show that there is a significant and positive effect of present FacebookShares (0.00193, p < 0.001) and Comments (0.126, p < 0.001) on future Backers.,\n",
              " This positive and significant response lends credence to the argument around positive informational cascades within platforms and suggests a strong selfreinforcement effect arising from observing other consumers’ choices, in support of H3.\n",
              " ,\n",
              " The results also show that eWOM requires a significantly longer buildup time than PI (Kruskal–Wallis test for Comments = 4.1 and FacebookShares = 23.69, both p < 0.05).,\n",
              " The results did not significantly deviate from our main model.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "metadata": {
        "id": "ddLaz_O6HN5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4JYCQSUHNVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ypj1JWuZHTaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IUOtH5JHTt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jcptc_7nqBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for num, sentence in enumerate(doc1.sents):\n",
        "  print(sentence)\n",
        "  #doc1=nlp(u\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\"\n",
        "for num, sentence in enumerate(doc1.sents):\n",
        "  print(sentence)\n",
        " \n",
        "  \n",
        "#for word in doc1.ents:\n",
        "  #print(word.text, word.label_) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BpHWmo0Re8-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "displacy.render(doc1, style='ent', jupyter=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02QQ64WHqufx",
        "colab_type": "code",
        "outputId": "52021226-eef7-4fad-a570-621ab80946f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc1=nlp(u\"Apple is looking as buying U.K Startup for $1 Billion\")\n",
        "for word in doc1.ents:\n",
        "  print(word.text, word.label_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "U.K Startup PERSON\n",
            "$1 Billion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UsV1GuH0y-Kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xYuKFLuiHt6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10917
        },
        "outputId": "05c69105-9096-45da-acb7-b486591aab55"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc1=nlp(open(u\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\").read())\n",
        "#options={ent:}\n",
        "#for word in doc1.ents:\n",
        "  #print(word.text, word.label_)  \n",
        "doc1\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "J\n",
              "Editorial\n",
              "\n",
              "ournal of the\n",
              "\n",
              "A\n",
              "\n",
              "ssociation for\n",
              "\n",
              "I\n",
              "\n",
              "nformation\n",
              "\n",
              "S\n",
              "\n",
              "ystems\n",
              "ISSN: 1536-9323\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "Ahmed Abbasi\n",
              "McIntire School of Commerce, University of Virginia abbasi@comm.virginia.edu\n",
              "\n",
              "Suprateek Sarker\n",
              "McIntire School of Commerce, University of Virginia School of Business, Aalto University suprateek.sarker@comm.virginia.edu\n",
              "\n",
              "Roger H. L. Chiang\n",
              "Carl H. Lindner College of Business, University of Cincinnati\n",
              "\n",
              "roger.chiang@uc.edu\n",
              "\n",
              "Abstract:\n",
              "Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data’s characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data’s disruptive effects. Keywords: Big Data, Behavioral, Business Analytics, Business Intelligence, Design Science, Economics of IS, Information Value Chain, Research Directions.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "pp. i – xxxii\n",
              "\n",
              "February\n",
              "\n",
              "2016\n",
              "\n",
              "\n",
              "ii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "1\n",
              "\n",
              "Introduction\n",
              "\n",
              "As business processes become major differentiators for organizations in many industries, organizations are increasingly using analytics to “wring every last drop” of value from those processes (Davenport, 2006). Consequently, companies now view their data as a primary business asset (Redman, 2008). In organizational settings, the information technology (IT) function is tasked with managing and integrating data as an “enabler” of data-driven business processes and decision making (Chandler, Hostmann, Rayner, & Herschel, 2011; Lycett, 2013). Big data’s rise has further amplified the importance of IT in this role (Horan, 2011), resulting in important implications for IT managers and scholars within and beyond the information systems (IS) discipline. Recent editorials and special issues in top IS journals have discussed big data analytics from different vantage points. For example, Chen, Chiang, and Storey (2012) highlight the various domains and information sources associated with big data. They also touch on their vision for evolving analytics toward big data -- from the traditional structured relational database-driven paradigm to “business intelligence and analytics 2.0”, which leverages Web and unstructured content, and to “business intelligence and analytics 3.0”, which, in addition, encompasses mobile and sensor-based data. Goes (2014) presents valuable taxonomies for big data infrastructure and big data analytics. Agarwal and Dhar (2014) discuss challenges and opportunities pertaining to big data in information systems research and note that IS researchers are well positioned to take advantage of opportunities in this area. Sharma, Mithas, and Kankanhalli (2014) offer several research questions that IS researchers need to pursue. Adding to this conversation is the call for papers for MIS Quarterly’s upcoming special issue that emphasizes the need for IS researchers to examine and exploit big data’s disruptive nature (Baesens, Bapna, Marsden, Vanthienen, & Zhao, 2014). All of these commentaries, editorials, and/or special issue introductions highlight important facets of big data research in IS. In this editorial, we synthesize takeaways from prior expositions and expand on them by focusing on the interplay between big data’s unique characteristics, these characteristics’ implications for the information value chain, and potential areas of inquiry for the three major IS research traditions. We present a framework (Figure 1) that highlights this interplay and helps one to generate (and potentially refine) a set of meaningful research questions on this topic for the IS discipline. There has been a fair amount of IS research on the information value chain, which is the cycle of converting data to information to knowledge to decisions to actions and, thereby, generate additional data. Major bodies of IS research pertaining to the information value chain have examined the derivation and management of knowledge and decision making and actions; however, the effects of big data on the value chain remain relatively unexplored. Scholars and others often define big data by four key characteristics: volume, velocity, variety, and veracity; however, big data is not simply a matter of injecting additional scale, variation, speed, or noise to research data sets. As Chris Anderson of Wired famously said (2008, emphasis added), “Because in the era of big data, more isn’t just more. More is different”. In other words, these characteristics have the propensity to disrupt the traditional information value chain and result in a new “big data information value chain”. Such disruption not only presents opportunities for novel research from within or across the different IS research traditions (e.g., positivist, interpretive, and critical behavioral research, design research, and economics of IS-based research) but also raises epistemological questions and challenges for some of the IS research traditions. This editorial proceeds as follows. First, in Sections 2, 3, and 4, we provide an overview of the traditional information value chain and related IS research, big data’s disruptive characteristics, and the resulting big data information value chain. With these sections, we highlight the profound impact of big data on people, processes, technologies, and, consequently, on organizations, industries, and virtually every facet of the world we live in 1. Second, in Sections 5 to 11, we discuss possible research directions and implications for the three traditions of IS research alluded to earlier. In a way, we highlight the fact that we have much to learn about the big data phenomena and scholars in each tradition can and need to play a role in the research endeavor. We emphasize that that, by highlighting the three traditions and certain questions in our editorial, we do not wish to exclude other forms or areas of inquiry. In fact, we believe that we will see new traditions, genres, and areas of inquiry spring out of the IS community’s engagement with phenomena related to big data.\n",
              "\n",
              "1\n",
              "\n",
              "Those already familiar with the notion of “big data” and its impacts on the value chain may prefer to skim through this discussion or perhaps proceed directly to the discussion on research directions (starting in Section 5).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "iii\n",
              "\n",
              "Figure 1. Overview of the Big Data Information Value Chain Perspective 2\n",
              "\n",
              "2\n",
              "\n",
              "The Information Value Chain\n",
              "\n",
              "The information value chain is the cyclical set of activities necessary to convert data into information and, subsequently, to transform information into knowledge (Fayyad, Piatetsky-Shapiro, & Smyth, 1996a, 1996b; Han, Kamber, & Pei, 2006), which individuals use to make decisions and take action. The decisions and actions then result in outcomes such as business value and additional data (Sharma et al., 2014). Each stage of the value chain encompasses people, processes, and technologies (Chandler et al., 2011). Information value chains operate in a given context; for instance, at the enterprise level, in a centralized IT unit, or for a specific functional or business unit. Figure 2 illustrates the traditional information value chain prior to the era of big data. The list of people, processes, and technologies shown in Figure 2 are illustrative, not exhaustive. As a further caveat, one could place certain processes and technologies differently depending on how one interprets data, information, and knowledge and how one delineates between decisions and actions. Here, we present the examples to illustrate the interplay between people, processes, and technologies across the stages of the traditional information value chain (i.e., a pre-big data era baseline). In Section 3, we contrast this traditional value chain with the new information value chain resulting from big data’s disruption. One can broadly categorize the set of activities in the information value chain into two groups: knowledge derivation and decision making (Chandler et al., 2011; Goes, 2014; Sharma et al., 2014). In the traditional information value chain, structured data is predominantly stored on premises in organizations’ data centers that use relational database management systems (RDBMS). Many organizations integrate various structured data sources into data warehouses and/or data marts using extract, transform, and load (ETL) technologies. Database administrators, database managers, and ETL developers typically perform these tasks. Programmers or data analysts then process and analyze the data stored in these systems using structured query language (SQL) and use the resulting data as input for report generators, business intelligence (BI) tools, and/or analytical models incorporated in predictive technologies. In the value chain’s knowledge stage, the people tasked with deriving knowledge from the data intersect with the decision makers; in other words, in the knowledge stage, the enablers and producers interact with the consumers of information (Chandler et al., 2011). In this stage, technologies such as knowledge management systems, corporate wikis, reporting tools, BI dashboards, and expert systems preserve or present existing knowledge or facilitate the creation of new knowledge through processes such as forecasting or reporting. Technologies such as decision support systems (DSS), recommender systems, and collaboration tools support managers and analysts’ decision -making processes. The people, process, and technologies at various stages are also influenced by contextual factors such as the organizational/department/unit culture and IT governance. Traditionally, IS research on the information value chain has also focused on the two aforementioned areas: 1) deriving knowledge and 2) decision making (Sharma et al. 2014). Scholarship in the area of deriving knowledge encompasses areas such as large bodies of work on knowledge management, structured data mining, and database design (e.g., Alavi & Leidner 2001; Chiang, Barron, & Storey, 1994; Storey, Chiang, Dey, Goldstein, & Sudaresan, 1997). The IS body of literature on decision making\n",
              "2\n",
              "\n",
              "The arrow from the big data information value chain to the IS research traditions indicates the epistemological/paradigmatic considerations and challenges big data may bring to the way we conduct research and the types of criteria we might privilege in examining the big data phenomenon. The arrow between big data’s characteristics and the information value signifies the disruptive impact of the four Vs on the traditional value chain.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "iv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "is rich and extensive. Major areas of emphasis include research on designing decision support systems (DSS) and behavioral research on the effectiveness of IT artifacts or other decision aids for supporting decision making (Nunamaker, Chen, & Purdin, 1991; Wixom & Watson, 2001; Shim et al., 2002; Arnott & Pervan, 2008).\n",
              "\n",
              "Figure 2. The Traditional Information Value Chain and Examples of the Accompanying People, Processes, and Technologies\n",
              "\n",
              "In this era, academic scholars and practitioners have tended to use relatively scarce, largely static, and deliberately sampled and collected data (Kitchin, 2014a, 2014b). Having described the traditional information value chain, we now move on to describing the key characteristics of big data and elaborating on how these characteristics might disrupt the information value chain.\n",
              "\n",
              "3\n",
              "\n",
              "Enter Big Data: The Four Vs\n",
              "\n",
              "One can separate big data and “regular-sized” data based on the presence of a set of characteristics commonly referred to as the four Vs: volume, variety, velocity, and veracity (Schroeck, Shockley, Smart, Romero-Morales, & Tufano, 2012; Goes, 2014).\n",
              "\n",
              "3.1\n",
              "\n",
              "Volume\n",
              "\n",
              "The U.S. Library of Congress, which archives both digital and offline content, has collected hundreds of terabytes of data (Manyika et al., 2011). Interestingly, the average company in 15 of 17 industry sectors in the United States has more data stored than the Library of Congress (Manyika et al., 2011), which underscores the fact that big data is pervasive across industries including finance, manufacturing, retail, health, security, technology, and sports. For a detailed discussion of various applications domains for big data, see Chen et al. (2012). Furthermore, in the vocabulary of big data, petabytes and exabytes have now replaced terabytes. For instance, large retailers each collect tens of exabytes of transactional data every year (McAfee & Brynjolfsson, 2012). To put these volumes into perspective using the classic grains of sand\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "v\n",
              "\n",
              "analogy, if a megabyte is a tablespoon of sand, a terabyte is a sandbox two-feet wide and one-inch deep, a petabyte is a mile-long beach, and an exabyte is a beach extending from Maine to North Carolina.\n",
              "\n",
              "3.2\n",
              "\n",
              "Variety\n",
              "\n",
              "Organizations are now dealing with structured, semi-structured, and unstructured data from in and outside the enterprise (Schroeck et al., 2012). The variety includes traditional transactional data, user-generated text, images, and videos, social network data, sensor-based data, Web and mobile clickstreams, and spatial-temporal data (Chen et al., 2012; McAfee & Brynjolfsson, 2012). Effectively leveraging the variety of available data presents both opportunities and challenges.\n",
              "\n",
              "3.3\n",
              "\n",
              "Velocity\n",
              "\n",
              "The speed of data creation is a hallmark of big data. For instance, Wal-Mart collects over 2.5 petabytes of customer transaction data every hour (McAfee & Brynjolfsson, 2012). With respect to unstructured data, over one billion new tweets occur every three days, and five billion search queries occur daily (Abbasi & Adjeroh, 2014). Such information has important implications for “real-time” predictive analytics in various application areas, ranging from finance to health (Bollen, Mao, & Zeng, 2011; Broniatowski, Paul, & Dredze, 2014). Simply put, analyzing “data in motion” presents new challenges because the desired patterns and insights are moving targets, which is not the case for static data.\n",
              "\n",
              "3.4\n",
              "\n",
              "Veracity\n",
              "\n",
              "The credibility and reliability of different data sources vary. For instance, social media is plagued with spam, and Web spam accounts for over 20 percent of all content on the World Wide Web (Abbasi & Adjeroh, 2014). Similarly, clickstreams from website and mobile traffic are highly susceptible to noise (Kaushik, 2011). Furthermore, deriving deep semantic knowledge from text remains challenging in many situations despite significant advances in natural language processing.\n",
              "\n",
              "4\n",
              "\n",
              "The Big Data Information Value Chain\n",
              "\n",
              "In his seminal book The Innovator’s Dilemma, Christensen (1997) introduced and elaborated on the idea of disruption as relevant here. In organizational settings, disruptive phenomena significantly alter value chains. Indeed, big data and its four “V” characteristics have had a profound impact on the people, processes, and technologies related to the information value chain. As Mayer-Schönberger & Cukier (2013, p. 19) note, “The era of big data changes how we live and interact with the world”. Simply put, big data means big disruption (Newman, 2014). Figure 3 illustrates this disruption in three ways. First, the new value chain involves a different set of people, processes, and technologies. While IT is known to exist in a constantly changing landscape, we can clearly see the accompanying changes to the people and processes attributable to big data as a disruptor. Second, there is greater amalgamation of technologies into “platforms” and processes into “pipelines” in the value chain’s knowledge-derivation phase. Third, we see greater reliance on data scientists and analysts across all stages of the value chain to support self-service and realtime decision making. Big data’s four Vs clearly change how one stores and manages data. In terms of technical considerations, data’s volume, velocity, and variety in organizations have caused IT departments to consider distributed storage architectures capable of handling large quantities of unstructured data. In terms of the technology, NoSQL (“not only SQL”) systems, such as those leveraging Hadoop (Dean & Ghemawat, 2008) and/or Spark, have emerged as being better suited for the larger volumes and variety of unstructured data, while organizations commonly use in-memory databases to exploit velocity in real-time applications (Heudecker, 2013). In addition, firms are increasingly interested in collecting social media and sensor-based data (Chen et al., 2012) to supplement the internal data sources they have traditionally relied on, which contributes to the data’s variety. However, using such data sources comes with an array of data-quality and credibility concerns, which require appropriate data-management, data-preparation, and knowledge-management activities. Data’s volume and velocity have also caused IT departments to shift physical on-premises data centers to cloud-based infrastructure-as-a-service (IaaS), platform-as-a-service (PaaS), and database-asa-service (DBaaS) offerings better suited to meet organizations’ elastic computing and storage needs (Buytendijk, 2014). The interplay between traditional schema-based structured data storage and management, new schema-less big data technologies, and organizations’ increasing reliance on cloud\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "vi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "computing can create complex big data architectures. In some respects, the key data-management and storage questions that practitioners pose have shifted to “what other internal/external data sources can we leverage” and “what kind of enterprise data infrastructure do we need to support our growing needs?”.\n",
              "\n",
              "Figure 3. The Big Data Information Value Chain and Examples of Related People, Processes, and Technologies\n",
              "\n",
              "The shift towards schema-less data storage and management coupled with organizations’ increasing desire to leverage big data as a source of competitive advantage has brought about the rise of data scientists (Davenport & Patil, 2012; McAffe & Brynjolfsson, 2012). In the words of Davenport and Patil (2012, p. 73), “data scientists are the people who understand how to fish out answers to important business questions from today’s tsunami of unstructured information”. Data scientists and scriptingoriented programmers now perform (or at least complement those who do perform) many of the activities pertaining to deriving knowledge from internally and externally collected data sources that database managers and SQL programmers traditionally performed. Data scientists also work closely with analysts and management in the decision making phase (Davenport & Patil, 2012). Furthermore, data lakes, which are essentially data warehouses or data marts specifically intended to serve as “sand boxes” for data scientists to experiment in, are becoming increasingly pervasive (Buytendijk, 2014). While ETL developers still play an important role, data-integration tasks increasingly entail fusing various noisy structured and unstructured data sources. Consistent with the trend toward IaaS/PaaS/DBaaS, many technologies pertaining to the value chain’s information stage are also running in the cloud and accessed via software-as-as-service (SaaS) (Buyetdijk, 2014). Big data analytics allows data scientists and modelers to use enterprise machine learning— distributed scalable online algorithms running atop Hadoop platforms that can digest the volume and variety of information at unprecedented speeds. To offer an example, big data analytics running on Hadoop allowed Sears to push personalized promotions to customers more accurately and faster, which reduced the leadtime to one week compared to eight weeks in their previous data warehouse-based implementation (McAfee\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "vii\n",
              "\n",
              "& Brynjolfsson, 2012). Complex event-processing systems can analyze real-time sensor-based spatialtemporal data (Heudecker, 2013). For instance, the U.S. grocery chain Kroger has used overhead infra-red sensors to count customers and anticipate the number of currently needed checkout lanes and the number needed in 30 minutes, which has resulted in the company’s reducing average customer wait times from four minutes to 26 seconds (Coolidge, 2013). Big data’s velocity and the trend toward data driven decision making have created an exciting paradigm shift in how organizations create and leverage knowledge for decision making. The biggest shift is organization’s consuming analytics in real time with the rise of “self-service” BI/analytics (Chandler et al., 2011). This shift is attributable to the fact that “Big data, and the fast pace and complexity of today’s marketplace, require that leaders make decisions faster than ever before” (Kiron et al., 2011, p. 5). Self-service BI/analytics allows various employees in an organization, including managers and executives, to independently generate custom reports, run basic analytical queries, and access key performance indicators across various devices without relying on IT or decision analyst support. These factors help organizations avoid time-consuming hand-offs and make decisions in an agile manner (Chandler et al., 2011). The organizational context undoubtedly has impacts on the information value chain. Firms transformed by big data are often ones where a “strong top-line mandate to use analytics supports a culture open to new ideas” (Kiron et al., 2011, p. 5). Similarly, business units or departments with a longer-standing tradition of data-driven decision making, such as finance and operations, tend to leverage big data more relative to departments such as human resources (Kiron et al., 2011). Furthermore, big data governance practices have important implications for the availability, quality, maintenance, and security of the variety of novel structured and unstructured data sources that are part of the big data information value chain.\n",
              "\n",
              "5\n",
              "\n",
              "Big Data: Implications for IS Research\n",
              "\n",
              "The big data information value chain has several implications for IS research. The first set of issues relate to deriving knowledge from big data. In this area, prior editorials have examined the effects of “big research dataset” usage in academic research. These effects primarily pertain to sensemaking, which is the process of deriving knowledge based on information extracted from big data (Lycett, 2013). Below, we discuss the epistemological/paradigmatic issues, theoretical implications, and methodological challenges pertaining to “big research datasets”. However, we must emphasize that research implications of deriving knowledge from big data extend beyond the challenges of using “big research datasets”. The disruption to the traditional information value chain attributable to big data affords a plethora of research opportunities for the three IS research traditions. For instance, as organizations collect and store more customer data than ever before, privacy, security, and ethical considerations come to the forefront. With the proliferation of NoSQL and Hadoop systems, the advent of data lakes, and the popularity of in-memory databases, we need big data modeling formalisms and integration artifacts. Increased emphasis on complex event forecasting, credibility assessment, and social media analytics presents opportunities for novel prediction/description artifacts. Social listening platforms and the Internet of things allow novel forms of analysis pertaining to user-generated content and sensor-based data rich in knowledge, opinions, emotions, location, and geographic information. More broadly, with organizations treating data as a primary asset, assessing and, in some cases, quantifying the value of volume and variety relative to the costs of veracity becomes of paramount importance to evaluate the effectiveness of big data investments. We discuss some of these opportunities in Sections 6-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "viii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Figure 4. Toward a Big Data Research Agenda for IS\n",
              "\n",
              "The second set of implications pertains to big data’s impact on decisions and actions. This area of inquiry, which few prior IS papers have focused on, can potentially view big data and its characteristics as impacting IT artifact-related perceptions and behaviors in behavioral studies. We also believe that the four Vs of big data potentially change the very nature of IT artifacts, much like communication and collaboration technologies altered decision support systems and knowledge management, which gives rise to a new class of big data IT artifacts. IS research needs to not only contribute to the design but also examine the feasibility and effectiveness of such IT artifacts for different stakeholders. For instance, with the proliferation of real-time datadriven decision making and self-service analytics, executives, managers, and front-line employees are increasingly beginning to use big data to support timely decision making, which raises questions about the tension between data and intuition, implications for the nature of decision making, and appropriate ways to quantify the value/impact of the 4Vs on decisions. The above-mentioned issues also prompt researchers to think about the effects of cognition and usability and of the broader organizational context that includes but is\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "ix\n",
              "\n",
              "not limited to organizational culture and leadership, big data investment, and adoption outcomes. Real-time analytics pipelines built on big data platforms are augmenting or automating various business processes. A question that arises is: under what circumstances can (or should) one employ such alternatives for improving business processes? Big data also presents opportunities for designing and implementing novel decisionsupport artifacts and necessitates research on assessing the value of big data IT artifacts in different organizational contexts. In the ensuing sections, we elaborate on these and other opportunities. The key takeaway is that big data is not only different but also highly disruptive to the academic research process and to practice related to data, which requires that we re-assess our research assumptions, methodologies, and substantive questions. Furthermore, due to big data’s impact on people-processtechnology, these implications extend to behavioral science, design science, and the economics of IS. In their paper interestingly entitled “Why IT Fumbles Analytics”, Marchand and Pepper (2013) discuss the usability and cognitive challenges associated with big data analytics. They argue that scholars and practitioners have focused too much on big data’s technical facets and not enough on the people and their institutional and social environments, which has created a lack of socio-technical harmony that IS implementation initiatives often need to succeed. They note that “big data and other analytics projects require people versed in the cognitive and behavioral sciences, who understand how people perceive problems, use information, and analyze data in developing solutions, ideas, and knowledge” (p. 109), which appears to be well aligned with the core strengths of the IS discipline. Keeping in mind the aforementioned sets of implications from the perspective of the information value chain, Figure 4 outlines promising areas of research for big data research in the IS discipline. In Sections 6-8, we discuss possible research areas for the three IS traditions. We reiterate that the goal here is to maintain a broad and inclusive perspective and to be illustrative rather than exhaustive in our coverage.\n",
              "\n",
              "6\n",
              "6.1\n",
              "\n",
              "A Big Data Research Agenda for Behavioral IS Research\n",
              "Epistemological Concerns of Big Data and Behavioral IS Research\n",
              "\n",
              "The four Vs, particularly volume and variety, present challenges not only regarding the changing nature of big data phenomena as they exist in practice but also regarding how, and with what assumptions, research is conducted to investigate such phenomena. Some commentators have brought to question the value of the traditional scientific model of research (which often involves constructing hypotheses based on guesses and then deductively testing the hypotheses using carefully sampled data (e.g., Gregor & Klein, 2014)) in a data-abundant environment associated with big data (Anderson, 2008; Kitchin, 2014b). Will “machinegenerated correlations” on big data be enough to specify “inherently meaningful and truthful” patterns and relationships (Kitchin, 2014b, p. 135)? Can these correlations render front-end theorizing (before empirical analysis), which is at the heart of the scientific research model, meaningless (Kitchin, 2014b)? And, even if these patterns do not lead to understanding, will we be able to accurately predict behaviors, which is all that some important stakeholders may care about (Shmueli & Koppius 2010; Agarwal & Dhar, 2014)? Is an inductive “mode of science” in development, wherein algorithms “spot[s] patterns and generates theories” (Steadman, 2013, in Kitchin, 2014b, p. 131)? Are we nearing the “end of theory” (Anderson, 2008) given that “data tells the truth” under the assumption that studies have access to exhaustive data (that is, n = all), whereas “theory is merely spin” (Kitchin, 2014b, p. 135)?\n",
              "\n",
              "6.2\n",
              "\n",
              "Behavioral IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Many scholars, including Agarwal and Dhar (2014), do not perceive a fundamental transformation in the philosophy underlying research. Rather, they see potential for using a “guided knowledge-discovery” process that leverages big data in conjunction with traditional data-collection methods to generate insights and preliminary hypotheses worthy of further examination through a hybrid process of induction, deduction, and abduction. One perspective is that big data triggers a “paradigm shift towards computational social science” research (Chang, Kauffman, & Kwon, 2014, p. 67). In this perspective, scholars have argued that new “unobtrusive” big data information sources facilitate realism and generality with appropriate levels of control. Examples include using social media and Web clickstreams to enhance customer survey data to better understand the “voice of the customer” (Kaushik, 2011, p. 9), using large social networks to understand the dynamics of influence (Aral & Walker, 2012), and including mobile sensor-based data for enhanced spatial-temporal behavior analysis. Yet, other researchers note that such data is not “neutral, objective, and pre-analytic in nature” but reflects a certain underlying world-view, philosophy, or theory-in-\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "x\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "use that has informed the design of measurement instruments (e.g. Kitchin, 2014b, p. 2). Researchers and data scientists will need to reflect on this perspective on big data when making truth claims. Another major area in the broad realm of computational social science is workforce analytics (Davenport, Harris, & Shapiro, 2010). Today, many organizations use standard statistical techniques to combine employee perceptions derived from surveys with objective data from economic reports and/or measured through technology usage logs and sensors to make connections between employee satisfaction levels and sales, productivity, retention rates, and shrinkage levels (Coco, Jamison, & Black, 2011). For example, Best Buy found that a 0.1 percent increase in employee engagement resulted in a $100,000 annual increase in revenue per store (Davenport et al., 2010). Since workforce analytics examines the interplay between employee perceptions, technology usage, and business value-related outcomes, behavioral IS researchers should be well suited to lead research studies in this important area. While the preliminary results appear promising and present a great opportunity for the IS community to further develop and apply suitable data analytic techniques on large data sets, we need to critically reflect on several issues. First, we need to examine the assumptions underlying the data (e.g., how well the data represents the population and whose interests may be excluded or overrepresented) since the condition of “n = all” does not hold in most cases (Kitchin, 2014b). Second, we need to be aware of the assumptions underlying the analytic techniques (i.e., how value-free the techniques and algorithms are, and how compatible the assumptions underlying the techniques are with the nature of data). Third, and more importantly, we need to ensure that applying big data analytics actually leads to desirable economic and humanistic outcomes for relevant stakeholders. Clearly, both qualitative and quantitative researchers have an important role to play in rethinking and refining how big data is collected, prepared, analyzed, and presented and in investigating the actual processes and consequences of using big data analytics. For example, qualitative researchers, who one might not typically think of big data researchers, can seek to contribute to this arena by examining fundamental questions. These may include: “Are decision making processes at various levels of the organization being transformed due to big data, and, if so, how?” or “What kind of fit is needed (say, between the architecture/algorithms and the organizational structure/culture) for big data initiatives to be effective in organizations, and how can one cultivate such a fit?”. When using big data sources to derive insights, privacy considerations become paramount. In fact, Barocas and Nissenbaum (2014, p. 33) contend that “big data extinguishes what little hope remains for the notice and choice regime”, which provocatively points to the futility of organizations’ sharing their policies regarding collected data and the “opt-out options” they may provide. Analytics concerns how data from various sources is assembled and mined (Fayyad et al., 1996). However, what data actually gets mined, and using what techniques, is often emergent and not necessarily defined upfront, which makes individuals particularly vulnerable to privacy invasions. Interestingly, a poll conducted by KD Nuggets showed that “data mining” has remained the prevalent term in academia, while “analytics” has become more widely adopted in industry over the past decade (KDNuggets, 2011). It appears that “data mining” has a more invasive connotation, whereas industry has carefully marketed analytics as being progressive and associated with intelligence and business value. While analytics may have made data mining more socially acceptable, the underlying privacy concerns, which big data’s characteristics such as volume and variety amplify, remain. We know about many high-profile examples of privacy infringement in both industry and academic contexts, including one where the father of a teenage daughter discovered that she was pregnant owing to Target’s highly accurate marketing efforts (Hill, 2012). Similarly, Facebook’s intentionally manipulating users’ moods has raised many critical questions (Kramer, Guillory, & Hancock, 2014; Agarwal & Dhar, 2014). Often times, organizations tout “informed consent” and upfront notice as an answer to critics; yet, as Barocas and Nissenbaum (2014, p. 32) note, “upfront notice is not possible because new classes of goods and services reside in future and unanticipated uses”. Identifying acceptable levels of intrusion, and finding the right balance (and the principles of balancing) between insights obtained due to access to big data and the infringement such access results in is an important area of inquiry for IS scholars. Similarly, it is virtually unavoidable that big data will continue to see large breaches. With the rise of self-service analytics and with access to sensitive data more pervasive than ever in and across organizations, security behavior challenges regarding compliance, insider threats, and so on will grow as important areas of research. Big data analytics involves deriving knowledge and gaining insights. The pursuit of knowledge has always had a close relationship with ethics. One primary component of ethics already mentioned is privacy considerations. Beyond privacy, firms and researchers leveraging big data may consider deontological ethical criteria of consistency, equality, accountability, integrity, and an all-around conscientiousness (e.g.,\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xi\n",
              "\n",
              "Chatterjee, Sarker, & Fuller, 2009a, 2009b). These issues underlie much of the concerns related to the “transparency paradox 3” and the “tyranny of the minority 4”.\n",
              "\n",
              "6.3\n",
              "\n",
              "Behavioral IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "As industry moves towards self-service analytics using big data, several research questions arise, such as the impact of sources and collection methods on data credibility, the impact of access to knowledge on employee satisfaction and knowledge transfer, and organizational norms for knowledge access and transfer (Chatterjee & Sarker, 2013). Further, as big data introduces novel IT artifacts that support large-scale, selfservice, real-time analyses and decision making from vastly integrated enterprise-wide analytics, behaviors and perceptions remain critical to the process of effectively converting knowledge to appropriate decisions and actions. These emerging data sources, decision making processes, and IT artifacts present an opportunity to revisit questions related to constructs, such as trust, leadership, knowledge transfer, and decision making. Along these lines, Davenport and Harris (2007) analyze numerous successful organizations to derive key elements of the “anatomy of an analytical competitor”. Scholars have used these traits to categorize organizations’ capability maturity levels from aspirational to transformed (LaValle, Lesser, Shockley, Hopkins, & Kruschwitz, 2011). Two of the important differentiating elements they identify are people and culture. With respect to these two elements, Davenport and Harris wonder: how does analytical leadership emerge? What are the characteristics of analytical executives? What role do different c-level executives play? What if executive-level commitment is lacking? What traits of big data analysts make them effective? How do organizations transform from intuition-based decision making to a data-driven decision making paradigm? Edward Deming is often falsely attributed the famous quote “In God we trust, all others bring data”. As we mention earlier, it is often (naively) believed that data and algorithms result in what is necessarily just and true. However, the importance of intuition and judgment, especially in uncertain environments cannot be underestimated (Davenport, 2006; Yetgin, Jensen, & Shaft, 2015). Given that there is a natural tension between data and intuition (Davenport & Harris, 2007), what is the ideal balance between data and intuition? What kind of theory can inform this balancing act? What role does trust play? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? More broadly, we need to understand if and how we should revise existing decision making models (e.g., Hodgkinson & Starbuck, 2008) to reflect how decisions are actually made in organizations using big data and big data analytics. Also, given people and culture can potentially act as impediments to the adoption of big data analytics in organizational settings, what theories and models are appropriate for avoiding implementation failures due to human and cultural issues? From an HCI standpoint, as dashboard-based visualizations become the norm for “managerial cockpits”, we need to investigate what the implications of the four Vs are on how users handle cognitive loads resulting from big data. Finally, we need to conduct balanced assessments of outcomes of the implementation/adoption and use of big data (i.e., big data infrastructure and big data analytics). We need to conduct critical, intensive assessments of the actual impact of big data investment and use and understand if and how one can attain instrumental benefits (such as performance and profitability) and humanistic benefits (such as empowerment and freedom). In other words, big data offers the opportunity to re-examine some of the same phenomena pertaining to decisions and actions that behavioral IS researchers have examined over the years for different waves of technological innovations.\n",
              "\n",
              "6.4\n",
              "\n",
              "Summary of Sample Big Data Research Opportunities for Behavioral IS\n",
              "\n",
              "As Table 1 shows, behavioral researchers have numerous opportunities to contribute to scholarship on big data. Some potential areas include epistemological reflections and methodological development, contributions to computational social science, and reformulations of existing theories and development of new theories of human decision making/human behaviors for the new data abundant environments using traditional or newer computational approaches. In particular, privacy, security, and ethics of big data have significant implications\n",
              "\n",
              "3\n",
              "\n",
              "4\n",
              "\n",
              "This paradox is related to the observation that “datafication” (Lycett, 2013) is assumed to make processes transparent, yet the ways in which data is collected and mined remains unknown/inaccessible to the public (Richards & King, 2013). This is related to the idea that companies tend to make inferences and generalizations based on data from the minority who agree to share data and, thereby, silence (i.e., make irrelevant) the views of those who refuse to give consent (Barocas & Nissenbaum, 2014, p. 32).\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "and, hence, deserve special attention. Developing in-depth consultable case studies that capture the intricacies of applying big data approaches in complex social environments would also be of value. Note that the areas listed below in the table are not an exhaustive list of possible research avenues. Rather, they signify the breadth of possibilities and directions that have opened up from this interest in, and trend toward harnessing, big data.\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities Value chain stage(s) Possible research topics • • Deriving knowledge, decisions, and actions Epistemological concerns • • Possible areas of inquiry What implications does big data have for the traditional deductive scientific research model? Is an alternative big data-driven “inductive mode of science” in development or even feasible in the IS discipline? If so, how can we ensure the validity of such knowledge? If not, how can one use the inductive mode in conjunction with the traditional deductive model? What is the role of prediction versus explanation in big data research? How must one adapt traditional research methodologies/methods (qualitative and quantitative) to investigate phenomena of interest in big data environments? How can longitudinal big data channels, such as social media, mobile location, and Web clickstreams, enhance our understanding and explanation of user behaviors? How can sentiments and affects appearing in user-generated content, such as online word-of-mouth, inform our understanding of user behaviors and intentions? What can large online social networks reveal about patterns of influence and/or information propagation? What new insights can work logs and other unstructured sources reveal about relationships between employee actions and employee productivity, satisfaction, and/or customer-oriented outcomes? What are the threats to validity of knowledge computationally derived, and what are the ways to mitigate these threats? What is the nature of theory or theorizing that is consistent with this form of research (i.e., computational social science)? What are the principles by which one can manage invasiveness/infringement of privacy in business enterprises, in academia, and in wider society? How can big data contribute to a better understanding (and resolution) of the privacy paradox in which, on one hand, users desire personalization, innovative technologies, and novel communication channels, and, on the other hand, seek privacy and anonymity? Since informed consent is often seen as ineffective, what other controls/policies need to be put in place? How can we assess and ensure their effectiveness? What implications does big data have for consistency, equality, accountability, integrity and an all-around conscientiousness? What is the impact of sources and collection methods on data credibility? How might access to information and knowledge attained from big data affect employee satisfaction and performance? What should the organizational norms be for access and transfer of knowledge attained through big data analytics? What would be the key elements of an ethical code for organizations and analysts/data scientists using big data and big data analytics? How do organizations/individuals/groups actually make decisions in the big data environment? To what extent do traditional decision making models hold in the new environment?\n",
              "\n",
              "• • • • • • • • Privacy and security concerns •\n",
              "\n",
              "Computational social science\n",
              "\n",
              "Deriving knowledge\n",
              "\n",
              "• • • • • • Nature of decision making •\n",
              "\n",
              "Other ethical considerations\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xiii\n",
              "\n",
              "Table 1. Big Data and Behavioral Research: Sample Research Opportunities • • • • Organizational culture and governance • • How does analytical leadership emerge? What are the characteristics of analytical executives? What role does c-level leadership play in firms’ abilities to leverage and “compete on big data analytics?” How do organizations transform from an intuition-based decision making culture to a data-driven decision making culture? What is the role of IT departments in supporting big data analytics? What big data technology architectures and configurations (or analytics techniques) fit with different types of organizational cultures (and governance)? What are the capabilities and constraints of big data information sources in supporting cognition and decision making? As dashboard-based visualizations become the norm for managers, what are the implications of the four Vs on users’ cognitive load and decision making performance? What is the ideal balance between data and intuition? What does trust in big data mean? What role does trust (e.g., trust in big data, people, and processes) play in adopting and effectively using big data? How do the four Vs impact user perceptions and intentions to use big data IT artifacts? What are the key personality traits of good analysts and how do these traits impact data and technology usage and performance? How do we assess big data initiatives when considering the perspectives of relevant stakeholders and the potential instrumental and humanistic outcomes?\n",
              "\n",
              "Leadership\n",
              "\n",
              "• Cognition and usability •\n",
              "\n",
              "Trust and big data versus intuition\n",
              "\n",
              "• •\n",
              "\n",
              "• Adoption and adaptation of big data techniques • and technologies Big data outcomes •\n",
              "\n",
              "7\n",
              "7.1\n",
              "\n",
              "A Big Data Research Agenda for Design Science Research\n",
              "Paradigmatic Considerations for Design Science Research on Big Data\n",
              "\n",
              "We begin by discussing broader paradigmatic considerations of big data on design science; namely, 1) the effects of focusing on IT artifacts that emphasize information more than systems/technology and 2) implications of big data analytics artifacts on kernel design theories. Design is a product and a process (Walls, Widmeyer, & El Sawy, 1992; Hevner, March, Park, & Ram, 2004). The design product is a construct, model, method, and/or instantiation. The design process involves an iterative cycle of “test and learn” or “build and learn, evaluate and learn” (Simon, 1996; Nunamaker, 1992). In big data analytics, the process of deriving knowledge and insights from data is, in some ways, analogous to the design process. The most prevalent process for guiding analytics is the cross-industry standard process for data mining (CRISP-DM) (see Figure 6). CRISP-DM involves iteratively performing several phases 1) problem/business understanding, 2) data understanding, 3) data preparation, 4) modeling, 5) evaluation, and 6) deployment (Chapman et al., 2000). The first phase emphasizes the importance of tackling significant problems/opportunities and identifying data mining goals/success criteria. The second phase involves an inventory of available data sources, including assessing data quality. The third phase involves selecting appropriate sources and specific variables, and cleaning and reformatting the data. The fourth phase includes using predictive, descriptive, or prescriptive analytics method to analyze the data (Chandler et al., 2011). The fifth phase involves evaluating models and their connection to problem/business outcomes. Finally, the sixth phase involves analyzing the artifact in the field outside the lab/production environment. CRISP-DM is to data mining what the system development lifecycle (SDLC) was to traditional information systems development, with CRISP-DM similarly appearing in the introduction section of various data mining and business analytics textbooks. The commonalities between CRISP-DM and the traditional system design process (Simon, 1996; Nunamaker, 1992) presents great potential for design science research geared towards producing novel IT artifacts capable of deriving knowledge and insights from big data. For instance, following the CRISP-DM design process, new constructs, models, methods, and instantiations could enhance BI dashboards or predictive, descriptive, prescriptive, and/or diagnostic model-based technologies (Shmueli & Koppius, 2011; Chandler et al., 2011). Recently, we have already begun to see research in this\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "xiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "vein with new predictive IT artifacts developed using the design science tradition that espouses informal connections to elements of CRISP-DM (Abbasi, Zhang, Zimbra, Chen, & Nunamaker, 2010). Comparing and contrasting CRISP-DM with SDLC is also interesting for another reason. One can consider big data artifacts as a shift in the design science tradition toward more significantly emphasizing artifacts that support information relative to systems or technology. For instance, only one of the stages depicted in CRISP-DM pertains to actually deploying the artifact, whereas at least four stages relate to processing, modeling, and evaluating data/information. CRISP-DM barely emphasizes the key stages of SDLC: users’ system-related requirements, post-deployment system implementation, and system usage/evaluation. While the implications of this shift are not necessarily epistemological in nature, it does introduce paradigmatic considerations for design science. For instance, what is the appropriate balance between information and systems/technology in design research geared toward big data? Furthermore, how does research on the design of information-centric big data IT artifacts relate design science (in IS) to information and computer science? Our discipline has a clear need to have conversations on this issue.\n",
              "\n",
              "Figure 6. The CRISP-DM Analytics Process Embodies Similar Intuitions to the Iterative Design Process Advocated in the IS Design Science Tradition\n",
              "\n",
              "For predictive analytics, big data has also had an impact on the kernel theories providing design guidelines. Classical theories of statistics such as the information theory and Bayes theorem embody simple yet powerful knowledge that have guided the design and development of some of the commonly used predictive and descriptive analytics methods traditionally employed in industry and academia. However, with the growth of big data in recent years, the four Vs introduce various challenges for data mining methods grounded in these traditional statistical theories. The volume of data presents computational constraints. The sparsity and structural nuances of new varieties of data sources, such as text and multimedia, poses representational richness issues. Some have also found traditional statistical methods to be susceptible to veracity concerns. Data-mining methods grounded in the statistical learning theory overcome many of these limitations (Vapnik, 1998) and attain unprecedented results for tasks such as image recognition, text mining, phishing detection, and fraud classification. Consequently, despite appearing in the past 15 to 20 years, Vapnik’s research on the statistical learning theory has already garnered over 150,000 citations according to Google Scholar, with more than half of those citations appearing in the past five years. One can largely attribute the success of statistical learning theory-based methods to their having a strong theoretical foundation and robust analytical power that is well suited for big data. However, other big data analytics methods, such as deep learning (LeCun, Bengio, & Hinton, 2015), do not have significant theoretical underpinnings. Indeed, critics have argued that they provide power without the underlying statistical theory (Gomes, 2014). As design research continues to explore the dichotomy between prediction and explanation (Shmueli & Koppius, 2010), the precise role of kernel theories in big data IT artifacts will remain a central question. A key question is: should IS research adopt an “ends-justify-the-means” perspective in which predictive power trumps methodological\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xv\n",
              "\n",
              "transparency and explanatory potential? We discuss the broader “prediction versus explanation” issue in greater depth in Section 11.\n",
              "\n",
              "7.2\n",
              "\n",
              "Design Science Research on Deriving Knowledge from Big Data\n",
              "\n",
              "Setting aside the aforementioned paradigmatic considerations of designing big data IT artifacts, design science has much to offer in the burgeoning realm of predictive analytics, including novel constructs, models, methods, and instantiations leveraging big data. Scholars in the IS community have also applied predictive analytics at both “micro” and “macro” levels of granularity (Brown, Abbasi, & Lau, 2015). Chang et al. (2014) refer to this range of data granularities as the micro-meso-macro data spectrum. One of the most exciting opportunities pertains to predicting/analyzing micro-level outcomes (Agarwal & Dhar, 2014). Specifically, Brown et al. (2015, p. 6) note: Micro-level predictive analytics involves making inferences about future or unknown outcomes pertaining to individual firms, people, or instances. Micro-level prediction contains greater intraentity information, including perceptual constructs, [transactions/logs on] individual behavior, and spatial-temporal indicators. We have already begun to see some exciting, cutting-edge examples of micro-level prediction. For example, Wang and Ram (2015) predicted individuals’ sequential purchase patterns using spatial, temporal, and social relationship features on a test bed encompassing three million transactions initiated by thirteen thousand customers from nearly three hundred locations. This and other related studies highlight the “art of the possible” and the “art of the valuable” for IS research on novel predictive design artifacts. Future IS research could leverage objective (e.g., observed transactions and logs) and perceptual (e.g., survey, sentiment, voice transcript, and interview) data in conjunction with various intermediate decisions and actions to predict individuals’ behaviors with applications in marketing, e-commerce, security, health, and finance (Abbasi, Lau, & Brown, 2015). There is no doubt that user-generated content sources such as social media have enhanced our understanding of various micro and macro-level phenomena in recent years (Chen et al., 2012), which presents great opportunity for developing social media analytics artifacts for collecting, monitoring, analyzing, summarizing, and visualizing social media data (Zeng et al., 2011). However, a major challenge remains in ensuring high veracity of such data sources. As Zeng et al. (2011, p. 14) note: “issues such as semantic inconsistency, conflicting evidence, lack of structure, inaccuracies, and difficulty in integrating different kinds of signals abound in social media”. We need IT design artifacts capable of identifying, quantifying, accounting for, and alleviating veracity concerns in information sources such as social media by assessing key information quality dimensions such as usefulness, relevance, and credibility. Examples of preliminary research in this vein include work pertaining to online spam and deception detection (Zhang et al., 2014). Such artifacts can be potentially beneficial in various application areas including marketing, finance, public policy, and health (Zeng et al., 2011; Abbasi & Adjeroh, 2014). Big data presents numerous opportunities for new design-oriented work pertaining to the earlier stages of the value chain; namely, data and information. There is a long-standing tradition of design science work on modeling formalisms and ontologies (Wand & Weber, 2002). New forms of user-generated content present opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. A related area involves extending classification principles from conceptual modeling to modeling of information categories in big data, which forms the basis for many forms of predictive and descriptive analytics (Parsons & Wand, 2013). Embley and Liddle (2013) expect conceptual modeling to address big data challenges by adopting the perspective that the design activity related to big data is fundamentally about structuring information. Conceptual modeling research should make big data’s volume searchable, harness the variety uniformly, mitigate the velocity with automation, and check the veracity with application constraints. The IS research community needs to direct some attention to the tasks of investigating, addressing, and/or exploiting big data’s four Vs jointly and effectively. Similarly, database design and data integration have been a major area of focus for prior works (Storey et al., 1997), and there are opportunities for research on the integration of a variety of structured and unstructured data sources available in organizational settings. As we highlight earlier when discussing the big data information value chain, the four Vs have increased the complexity of managing, storing, and integrating data. The challenge remains in investigating and establishing an acceptable architecture to integrate, manage, and implement both structured and unstructured data under one unified platform. The\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "traditional relational data model and the corresponding relational database management systems (RDBMSs) cannot meet the heterogeneity (variety) challenge of big data. Many consider NoSQL as the potential data management solution for big data, but many architectural alternatives exist that range from Hadoop/Spark to Hadoop and RDBMS in parallel to Hadoop (for unstructured data) inputting into RDBMS (Heudecker, 2013; Buytendijk, 2014). We need research to examine the feasibility, fit, and business value of such alternatives and to provide guidelines for big data architectures based on organizational and industry-level contexts. For design research pertaining to knowledge derivation and representation, big data presents both advantages (i.e., volume and variety) and disadvantages (i.e., velocity and veracity). With the large volume and a variety of data sources, big data can certainly enhance ontology learning by automatically deriving domain knowledge by mining unstructured and semi-structured data (Buitelaar, Cimiano, & Magini, 2005). For example, user-generated content contributed freely in social media presents opportunities to enrich existing ontologies and develop new ones and to introduce new conceptual models and grammars. As we allude to earlier in this section, some IS scholars have recently suggested that domain ontologies may play a critical role in the conceptual model for managing and implementing big data (Embley & Liddle, 2013).\n",
              "\n",
              "7.3\n",
              "\n",
              "Design Science Research on Supporting Decisions and Actions from Big Data\n",
              "\n",
              "Scholars have long used design science to guide the design and development of various decision support systems, including group support systems, recommender systems, personalization, contextualization, and collaboration technologies (Nunamaker et al., 1991; Adomavicius & Tuzhilin, 2005; Arnott & Pervan, 2012). They have also used it to develop BI-related DSS artifacts (Chung, Chen, & Nunamaker, 2005). More recently, Lau et al. (2012) developed ABIMA, a big data business intelligence DSS for mergers and acquisitions. ABIMA integrates large volumes of financial metrics derived from structured databases with unstructured sources, such as financial news articles, search engine results, and documents crawled from the Web. The system, practitioners specializing in mergers and acquisitions evaluated, is the type of research that epitomizes how one can use design science for research on big data DSS. Other types of big data IT artifacts that support the decision making process could be ones designed to support real-time decision making that possibly incorporate user feedback-based or system-generated credibility assessments of underlying information sources (Jensen, Lowry, Burgoon, & Nunamaker, 2010; Jensen, Averbeck, Zhang, & Wright, 2013). Given that big data analytics significantly emphasizes enhancing business processes (Davenport, 2006), business process improvement driven by big data constitutes an important research area (Baesens et al., 2014). Potential avenues include developing automated artifacts for discovering and optimizing processes and employing analytics-driven methods in various internal-facing and external-facing business processes that range from operations and human resources to customer relationship management (Davenport & Harris, 2007). As an extreme example of automation, the use of big data analytics to replace human involvement from certain business processes has already begun to take shape (Davenport & Kirby, 2015); in Section 4, we mention real-time analytics pipelines that are replacing traditional business processes. However, in many contexts, big data analytics provides complementary “augmentation” to human-driven processes (Davenport & Kirby, 2015). Augmentation and automation signify a departure from the traditional human-centered computing paradigm toward autonomous computing albeit with varying degrees of separation depending on the level of human involvement. Nevertheless, this shift necessitates reconsidering guidelines for the design product and design process associated with such artifacts (e.g., requirements gathering in contexts where there are no users). When designing such artifacts, what role might theories and principles from, for example, the cognitive computing and artificial intelligence literature play? Given the dynamic and nascent technological and organizational environments related to big data, it would be interesting to see if (and how) one could productively use the action design research (ADR) method to develop robust big data IT artifacts (Sein et al., 2011). For instance, action design research incorporates provisions for varying levels of end user involvement that could provide the necessary flexibility for designing big data artifacts in contexts ranging from traditional user-centered decision support to business process augmentation/automation.\n",
              "\n",
              "7.4\n",
              "\n",
              "Summary of Design Science Research on Big Data\n",
              "\n",
              "Big data presents several wicked problems: how should IS researchers balance a big data-oriented design science research agenda with those being pursued in reference disciplines such as engineering, statistics, and computer science? Goes (2014, p. iv) touches on this challenge by noting that at least five different\n",
              "Volume 17 Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xvii\n",
              "\n",
              "departments at his institution were, in some way, explicitly related to big data research. Goes cautions us by noting that, without guidelines for shaping the IS big data research agenda, “each unit can contribute to the big data paradigm, but at present the approach resembles that well-known cartoon of making sense of an elephant by grabbing isolated parts of the animal”. In our assessment, we can say that, relative to other disciplines, IS design science researchers are uniquely positioned to provide the appropriate mix of rigor along with humanistic and instrumental relevance. Further, our research often seeks to offer generalizable design principles and guidelines abstracted from the development of contextualized big data IT artifacts that can potentially help address other important problems. The sample research opportunities in Table 2 reflect this view. We believe the design science perspective on big data analytics represents an important future area of emphasis for IS research.\n",
              "Table 2. Big Data and Design Science Research: Sample Research Opportunities Value chain stage(s) Possible research topics Possible areas of inquiry • What is the appropriate balance between information and systems/technology in design research geared toward big data in the IS discipline? • What implications does big data IT artifacts’ potential shift in focus from systems to information have for the design process? • How might the characteristics of big data affect the nature of kernel design theories that are potentially useful? • What is the IS “signature” for big data design research (i.e., what is the scope/nature of big data IS design artifacts relative to reference disciplines such as computer science, marketing, engineering, and statistics)?\n",
              "\n",
              "Deriving knowledge, decisions, and actions\n",
              "\n",
              "Paradigmatic considerations\n",
              "\n",
              "• How can one leverage the volume and variety of big data to develop novel artifacts for predicting/describing macro versus individual/micro-level phenomena or events? Novel artifacts for • How can design science research build novel artifacts for deriving knowledge prediction or from big data sources, such as user-generated content, to advance research in description other disciplines, including marketing, finance, and health? • How can design guidelines of big data analytics artifacts better compensate for the veracity of input data? What novel veracity-assessment artifacts can we develop to shed light on information relevance, usefulness, and credibility? Deriving knowledge Modeling formalisms and integration artifacts • Can new forms of user-generated content enrich existing ontologies, enable the development of new ones, and introduce new conceptual models and grammars? • What is the potential for extending classification principles from conceptual modeling to modeling of information categories in big data? • Can conceptual modeling address some of the challenges of big data by making the volume searchable, harnessing the variety uniformly, mitigating the velocity with automation, and/or checking veracity with application constraints? • How can design science inform the state-of-the-art integration, management, and implementation of organizational big data initiatives in light of the four V challenges? • What design theories do we need to guide big data architectures based on organizational and industry-level contexts?\n",
              "\n",
              "• How can IS contribute guidelines for design artifacts that support real-time Novel IT artifacts decision making from big data? for decision • What is the role of credibility assessment as a design guideline in big data support decision support systems? • What is the potential for automated process discovery and optimization? Decisions Business process • How can big data analytics improve business processes? and actions improvements • Are existing design theories sufficient for real-time big data analytics and automation environments in which run-time and autonomy considerations create nuanced design requirements? What are the alternative theories that may be valuable? Big data action design research • Given the emerging nature of big data, how can we use approaches such as action design research (ADR) to guide the development and harnessing of big data IT artifacts in organizational settings?\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "8\n",
              "8.1\n",
              "\n",
              "A Big Data Research Agenda for the Economics of IS\n",
              "Epistemological Concerns for Big Data and the Economics of IS\n",
              "\n",
              "The economics of big data has important implications for information systems. Just as scholars once used the “economics of information” to describe the value of information asymmetries in marketplaces (Stigler 1961), now, with firms competing on analytics, access to information that can enable enhanced analytical capabilities and insights facilitating differentiation has ushered a new era of “knowledge is power”. Quantifying this power is critical. Beyond some of the issues discussed in Section 6.1, the epistemological implications for the economics of IS community primarily relate to research methodology. These include the deflated p-value problem (Lin, Lucas, & Shmueli, 2013), increasing emphasis on prediction versus explanation (Shmueli & Koppius, 2010), and construct validity when using unstructured and log-based data sets; however, given that many of these issues are applicable to multiple IS research traditions, we discuss them in greater detail in Section 11.\n",
              "\n",
              "8.2\n",
              "\n",
              "Economics of IS Research on Deriving Knowledge from Big Data\n",
              "\n",
              "The value of information has been a longstanding area of inquiry in the economics of IS tradition (Banker & Kauffman, 2004). In the context of big data, assessing information’s value is more critical than ever. One research direction analyzes the relative value contributions of the four Vs (e.g., value of data volume and variety) for deriving knowledge from big data. As organizations treat data as an asset (and, in many Web 2.0 business models, as the primary asset), quantifying its value has become a major discussion topic both from a broad business value perspective (which includes the implications for third party data brokers and data markets) and from a more traditional accounting perspective. This emphasis on data as an asset has spurred infonomics: the theory, study, and discipline of assigning economic significance to information. For instance, a recent McKinsey report states that public data sources pertaining to education, energy, healthcare, transportation, and consumer finance (collectively dubbed “open data”) have the potential to create USD$3 trillion in annual business and/or societal value (Manyika et al., 2013). At a more micro level, individual organizations are routinely interested in identifying the most useful public/private data sources and quantifying the precise value of these sources. For example, Bardhan, Oh, Zheng, and Kirksey (2015) used demographic, clinical, and administrative data from 67 hospitals in northern Texas gathered over a four-year period to build models capable of predicting and describing congestive heart failure patient readmissions. Their model demonstrates the importance of health IT-related variables, which prior or baseline models have not considered impactful but that could help hospitals save millions of dollars by avoiding costly readmission-related penalties. Many other recent studies also suggest that using more data instances and variables can improve predictive capabilities (Junque de Fortuny et al., 2013). These findings raise questions regarding complexity, model management, and cost-benefit tradeoffs. Researchers wonder: if bigger is better, how much is too much? As Junque de Fortuny et al. (2013, p. 219) ask, “is it worth undertaking the investment to collect, curate, and model from larger data sets?” Prior to big data’s rise, firms were beginning to derive the last iota of predictive power from a set of data they had access to often by building predictive models that were increasingly complex. Netflix is one example. In 2006, Netflix offered a US$1 million prize to any team that could improve their existing movie recommendation models by 10 percent. The competition concluded on July 26, 2009. Over the final two years, the performance of the best predictive models improved by less than 2 percent, whereas the complexity of the solutions increased dramatically. The final winning model from a team appropriately named BellKor’s Pragmatic Chaos blended results from hundreds of underlying base models. Similarly, scholars commonly describe loan risk-assessment models at financial services firms developed in the past two decades, which largely leverage the same structured data sources, as complex, variegated black-box arrangements of models on top of models delicately combined (Derman, 2011). Former Goldman Sach’s Lead Quantitative Analyst Emmanuel Derman highlights the pitfalls of complexity and poor model management over time in his book Models Behaving Badly. He describes the role played by complex models in the 2008 financial crisis (Derman, 2011); the narrative reminds readers of the classic saying “complexity is death”. Big data creates an opportunity to not only enhance these models’ analytical capabilities but also reduce the inherent risks associated with using them. However, not all information sources are created equal. A key challenge facing organizations is finding a way to quantify the value of information that considers both insightfulness and risks.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xix\n",
              "\n",
              "Research on pricing for data sets/data sources in the booming data broker markets can help firms make more-informed decisions in the data marketplace. Here, all of the standard services management questions apply, including those that Rai and Sambamurthy (2006) articulate. What are the best strategies for bundling of data? Should firms pursue flat or usage-based monthly service rates versus one-time sales? How do alternative service rate plans impact data usage? Big data’s impact on data-based marketing and pricing, omni-channel marketing (Song, Sahoo, Srinivasan, & Chrysanthos, 2014), and attribution are other potential areas of inquiry. The value of information also raises questions about intellectual property rights, especially in the context of user-generated content. The cost of veracity can potentially offset the value of data volume and variety. The existing body of knowledge on data warehousing and business intelligence, which one can consider a close predecessor to big data, has emphasized the importance of data quality as an antecedent for the success of data warehousing initiatives (Wixom & Watson, 2001). Hence, quantifying the adverse effects of incomplete or inaccurate information is essential yet challenging for mitigating risk in the era of big data analytics. Such research could connect data quality to the effectiveness of business outcomes. The analysis of social media has garnered considerable attention from the economics of IS community in recent years with many outstanding avenues of inquiry. As organizations move towards the “socialecosystem” encompassing the use of social media for various employees and customer-oriented activities, we can ask how firms can leverage social media for internal communication and collaboration, external engagement, and listening/ideation (Zeng, Chen, & Lusch, 2010). What is the value of insights, engagement, and internal communication usage through social technologies? The interplay between social media channels and marketing effectiveness is another important area receiving considerable attention in the economics of IS community (Song et al., 2014). In that vein, questions include: what are the key factors impacting social media marketing effectiveness? How does peer influence impact social media marketing? What is the role of social media in viral marketing? A recent related stream of work examined the usefulness of location and geographic information for the analysis of choice, price, competition, and mobile marketing.\n",
              "\n",
              "8.3\n",
              "\n",
              "Economics of IS Research on Implications of Big Data for Decisions and Actions\n",
              "\n",
              "The economics of big data analytics extend further down the information value chain beyond knowledge acquisition to decisions, actions, and their ensuing consequences. In his book entitled The Value of Business Analytics, acclaimed business analytics guru Evan Stubbs talks about the challenges business analysts and data scientists face when attempting to quantify the value of an analytics project or portfolio of initiatives (Stubbs, 2011). For example, when should firms invest in big data, and what are the potential returns? As one answer to this question, Tambe (2014) found that firms with significant existing data sets who invested in Hadoop were associated with 3 percent faster productivity growth. A related question is: what implications does big data’s ushering in the increased usage of cloud-based SaaS and DBaaS have for platform economics and strategy? Research on quantifying the business value of big data analytics, including the implications of the four Vs, could inform the existing body of knowledge. Potential research directions include work measuring the return on investment for big data technologies, the impact of data variety and veracity on quality of decision making, and the economics of real-time decisions using big data.\n",
              "\n",
              "8.4\n",
              "\n",
              "Summary of Economics of IS Research on Big Data\n",
              "\n",
              "In some ways, the economics of IS tradition is ideally suited to tackle problems pertaining to deriving knowledge from big data. In particular, beyond certain methodological adjustments (see Section 11), the empirically driven, inductive reasoning-oriented applied econometrics approach is well aligned to addressing these types of questions. The economics of IS also has an essential role to play in examining the economic value of big data insights and big data analytics-driven decision making. Table 3 summarizes some of the important research opportunities.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Table 3. Big Data and Economics of IS: Sample Research Opportunities Value chain stage(s) Deriving knowledge, decisions, and actions Possible research topics Possible areas of inquiry\n",
              "\n",
              "• How must traditional research methods be adapted to investigate big data Epistemological environments? and/or • What is the role of prediction versus explanation in big data research? methodological • How can we ensure the validity of constructs derived from noisy unstructured concerns and log-based data sources? • What is the value of various data sources and channels in terms of quality of insights, enabling new capabilities, and quantifiable business value? • Regarding the impact of data volume on insights and business value, how much is enough and how much is too much? • What is the value of data volume and variety from a risk management perspective? • As data becomes an asset, what role can third party data brokers and data markets play? What are the pricing and market structure implications? • As firms monetize user-generated content, what are the implications for intellectual property? • How can the volume and variety of data inform data-based marketing and pricing? • What are the benefits and challenges of data variety for omni-channel marketing analysis and attribution? • How can we quantify the business impact of low veracity data? • Which types of data quality issues are the most impactful?\n",
              "\n",
              "Value of data, volume, and variety\n",
              "\n",
              "Deriving knowledge Cost of veracity\n",
              "\n",
              "• What is the role of social media in enterprises for internal communication, customer engagement, and listening/ideation? • Regarding the economics of social media, what is the value of insights, Social media and engagement, and internal communication usage through social technologies? economics of IS • What are the key factors that impact social media marketing effectiveness? • How does peer influence impact social media marketing? • What is the role of social media and incentive schemes in viral marketing? Impact of location and geography • How can location and geographic information impact research on choice, pricing, and competition? • What are the implications of geo-targeting in mobile marketing?\n",
              "\n",
              "Decisions and actions\n",
              "\n",
              "Quantifying value • What is the impact of data variety and veracity on the quality of decision and impact of four making? Vs on decision • What are the key factors influencing business value in the context of real-time making decision making using big data? • How do we measure the return on investment for big data technologies? Value of big data • When should firms invest in big data, and what are the potential returns? IT artifacts • As big data ushers in increased usage of cloud-based SaaS and DBaaS, what are the implications for platform economics and strategy?\n",
              "\n",
              "9\n",
              "\n",
              "Cross-tradition Research on Big Data\n",
              "\n",
              "There are many opportunities for research at the cross-sections of behavior, design, and economics of IS. In some respects, big data’s scale and complexity afford and encourage cross-tradition research projects. The work on human-computer systems design has traditionally been at the intersection of design science and behavior, such as cognitive psychology and decision science (Banker & Kaufmann, 2004). In this vein, one obvious direction is to design and develop big data IT artifacts that researchers may subsequently evaluate in terms of their positive impact on behavior (Zahedi, Abbasi, & Chen, 2015). Another connection between the design and behavioral traditions relates to research on IS strategy. Big data analytics ultimately focuses on improving business processes (Davenport & Harris, 2007) to attain a strategic competitive advantage, a perspective that is highly congruent with the resource-based view of the firm. Designing enterprise-wide big data analytics in a manner that maximizes the potential for competitive advantage in different types of industries and for different organizational cultures and governance archetypes is a potentially valuable direction. Related to this area, knowing how to align alternative big data architectures\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxi\n",
              "\n",
              "(such as ones based on Hadoop, traditional RDBMS, or hybrid models) with business strategy and a firm’s data environment and understanding important criteria and success factors in the decision making process remain important issues. Researchers can use established approaches in IS such as case studies, laboratory experiments, and survey studies in investigating such topics. Design science and the economics of IS also appear to have potential in the context of big data research. One example is cost-sensitive classification or regression in which one quantifies the values of true and false positives/negatives and incorporates them into the design of predictive artifacts (Bansal, Sinha, & Zhao, 2008; Zhao, Sinha, & Bansal, 2011). Here, existing work has mostly focused on the predictive artifact and less on the methodology for deriving cost matrix values. The value of information in big data IT artifacts represents another important area at the cross-section of design and economics of IS. Recently, many studies have designed novel IT artifacts focused on mining big data sources as decision-support aids (e.g., Lau, Liao, Wong, & Chiu, 2012). It remains unclear what the business value of such artifacts truly is with respect to key business performance indicators. Another already potent research area at the cross-section of design and economics of IS pertains to optimization (Banker & Kaufmann, 2004). Here, big data’s variety presents opportunities. For instance, online product reviews could possibly enrich product design optimization (e.g., Balakrishnan & Jacob, 1996). Similarly, consumer sentiments and demand forecasts based on user-generated content could enhance pricing optimization; however, in both of these examples, the tension between the value and veracity of social media and other user-generated data sources could present an interesting dynamic, worthy of an in-depth inquiry. Big data presents numerous opportunities at the intersection of economics of IS and behavior as well. For instance, behavioral economics is a well-established area (Tversky & Kahneman, 1974). In the context of big data, research could examine managers’ propensities to combine data of varying quality/credibility with, for instance, intuition in real-time versus non-real-time settings. Such work would borrow from theories in economics, cognitive psychology, decision making, and risk management (Banker & Kaufmann, 2004).\n",
              "\n",
              "10 Big Data: Implications for Theory\n",
              "Many scholars have reflected on big data’s possible implications on theory. One point of view, albeit extreme, is that big data renders the role of theory—sometimes seen as fictional and value-laden— unnecessary and obsolete, and replaces it with patterns derived directly from data that reflect nothing but the truth (e.g., Kitchin, 2014b). On the other hand, many scholars argue that, in the absence of theory, data lacks “order, sense and meaning” and that “theories without data are empty; data without theories are blind” (Harrington, 2005, p. 5, cited in Sarker et al., 2013, p. xiiii). While this debate is likely to continue without immediate resolution, we do not foresee theories disappearing or diminishing in importance because of research using big data. To the contrary, we foresee that some of the theories will become more robust because “researchers now have a medium for theory development through massive experimentation in the social, health, urban, and other sciences” (Agarwal & Dhar, 2014, p. 444). This is in part due to easier data collection and enhanced control and precision, realism, and generality associated with big data (Chang et al., 2014). At a broader level, we believe that scholars have the opportunity to reflect on the changing nature of the theorizing process and on the characteristics of theories developed in a data-abundant environment. From an information value chain perspective, some recent big data IS studies and editorials have touched on the role of theory when leveraging big data sources for discovering knowledge. As previously mentioned, we believe that, in addition to big data “information sources”, big data’s characteristics embodied in the four Vs afford important opportunities for research, that can both borrow from novel theories and contribute to existing theories, related to deriving knowledge as well as decisions and actions. Below, we outline a few examples. The impact of data variety and velocity on problem-solving accuracy and time constitutes an important research topic. The cognitive fit theory (CFT) provides an excellent and robust theoretical lens for examining this topic. Earlier work on CFT examined the importance of congruence between problem task and problem representation on users’ mental representation and overall problem-solving performance (Vessey, 1990). While initial studies focused on tables versus graphs, subsequent work extended CFT to other specialized representations such as maps (Dennis & Carte, 1998), and considered the impact of users’ prior domain knowledge and the effect of subtasks (Shaft & Vessey, 2006). All of these findings have important implications when examining the impact of big data characteristics for problem solving in general, and specifically in the context of dashboards depicting a variety of information in real-time. On the other hand, big data characteristics, such as variety and velocity, can also potentially offer theoretical extensions. Data\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "visualization dashboards often incorporate multiple tabs with coordinated views depicting real-time data (Andrienko & Andrienko, 2003). The effects of problem solving in such multi-representation, multi-subtask, real-time situations remain unclear, and this offers great potential to contribute to theory. CFT, with suitable adaptations, can also inform the design/construction of novel user interface artifacts (Vance, Lowry, & Egget, 2015) for presenting big data. Organizations using big data routinely ask managers and analysts to monitor and present key findings using reporting tools that integrate traditional structured data sources with novel social listening, web clickstream, sensor-based, and open data. Practitioner studies have suggested that analysts often do not perceive such tools to be useful, with obvious implications for the business value of such artifacts (Kaushik, 2011). Adoption models represent an excellent theoretical lens for examining the impact of perceived usefulness and ease of use on behavioral intention to use such reporting tools and actual use (Davis, 1986, 1989). The effects of mandatory versus voluntary reporting and trust are also important considerations (Brown, Massey, Montoya-Weiss, & Burkman, 2002; Gefen, Karahanna, & Straub, 2003). In turn, big data’s four Vs could provide important insights that can inform the extensive body of knowledge pertaining to technology adoption (Venkatesh, Morris, Davis, & Davis, 2003; Venkatesh, Thong, & Xu, forthcoming). For instance, the variety of data could have potentially contrasting effects on users’ perceptions of usefulness and ease of use, which users’ levels of experience may moderate. Further, as data veracity becomes increasingly relevant to big data IT artifacts, the implications for trust (in both the data and the artifact) and eventually for behavioral intention to use also present interesting issues to investigate. Chaos theory studies the behavior of dynamic systems that are highly sensitive to initial conditions, where small changes in initial conditions can yield widely diverging outcomes (Gleick, 1987; Sprott, 2003; Werndl 2009). Prior studies have already discussed the potential of big data for theory development via computational social science or massive experimentation (Agarwal & Dhar, 2014; Chang et al., 2014). Chaos theory could be beneficial in macro-level computational social science research, since it “appears to provide a means for understanding and examining many of the uncertainties, nonlinearities, and unpredictable aspects of social systems behavior” (Kiel & Elliott, 1996). Furthermore, as IS design science research explores novel predictive artifacts utilizing big data, chaos becomes an important consideration. The inclusion of big data should make predictive artifacts more accurate, stable, and valuable. However, for complex event forecasting in situations where chaos is present, prediction can be problematic since model assumptions, which are typically based on probabilities of various patterns (i.e., connections between observed initial conditions and eventual observed outcomes), may not hold true (Sprott 2003). Consequently, seemingly small errors in initial condition prediction probabilities can result in large errors between longer-term forecasts and actual outcomes (Werndl, 2009). We are already beginning to see Chaos theory concepts incorporated in problems such as weather forecasting and traffic prediction, where, in turn, the results are informing our understanding of chaos in these application areas. Somewhat related to chaos is black swan theory, which focuses on highly improbable or surprising, highimpact events that are often incorrectly rationalized in hindsight (Taleb, 2005, 2007). The key idea is that probability-centered analysis and thinking that diminishes the importance of outliers or the unobserved is problematic for appropriately managing the risks associated with black swan events. Taleb’s views on the limitations of statistics and his seemingly negative portrayal of statisticians has raised several (possibly valid) rebuttals from the statistics community (e.g., Westfall & Hilbe, 2007). Nevertheless, his central tenet appears to have merit. As big data further creates the shift towards data-driven decision making, risk management pitfalls such as attempting to predict extreme events, overreliance on the past, psychological biases against less likely outcomes, and overemphasis on standard deviations are likely to be exacerbated (Taleb, Goldstein, & Spitznagel, 2009). Black swan events also have important implications for the design of process automation relying on big data. We have already seen automated loan risk assessment and algorithmic trading engines fail miserably due to such events (Taleb, 2007; Derman, 2011). Black Swan Theory could shed light on studies examining risk considerations in data-driven decision making where traditional decision theories may be inadequate. Similarly, it can inform the design of process automation in big data environments such that the unknown unknowns are given proper consideration. In summary, big data has potentially important implications for theory. From an information value chain perspective, big data sources and associated IT artifacts have distinct implications for both knowledge acquisition and for decisions and actions (and related outcomes), which include system usage, performance, and satisfaction. The key nuances of big data artifacts stem from the four V characteristics. On one hand, these characteristics may simply inform well established IS theories by playing the role of antecedent constructs or of moderator/mediator variables. On the other hand, these characteristic can\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxiii\n",
              "\n",
              "introduce complexity and risk in IT artifacts increasingly relying on big data, thereby opening up exciting new possibilities for utilization of theories that have seen relatively limited usage in IS. Our final comment regarding theory and big data is while one cannot understate the role of “theory” in big data research, we do need to acknowledge that theory has different forms in different traditions of research, and, thus, as research community, we need to be open to different types of abstractions offered as theoretical contributions.\n",
              "\n",
              "11 Big Data: Implications for Methodology\n",
              "The characteristics of big data test beds have important implications for the norms of analyses. One significant implication that has recently garnered attention is the “deflated p-value” problem. In their Academy of Management Journal editorial, George et al. (2014) suggest that the statistical methods and metrics used to examine big data sets may need to incorporate alternative techniques from statistics, computer science, applied mathematics, and econometrics. They state (p. 323): “The typical statistical approach of relying on p values to establish the significance of a finding is unlikely to be effective because the immense volume of data means that almost everything is significant”. In addition to statistical significance and co-efficient signs, one may also need to consider effect sizes and variance when testing hypotheses on big data sets (Lin et al., 2013; George et al., 2014). As Chatfield (1995, p. 70) notes: “The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical significance, what is the practical significance of the results?”. To quantify the extensiveness of the problem, Lin et al. (2013) examined nearly 100 IS papers published between 2004 and 2010 with research test beds exceeding 10,000 instances and concluded that nearly half failed to discuss practical significance. As we note in Section 7 and as George et al. (2014) and others allude to, analyzing big data often requires using computer science-based methods grounded in machine learning and artificial intelligence rather than statistics. For instance, genetic algorithms are a computationally effective non-deterministic heuristic method for searching an NP-hard problem’s solution space and researchers/analysts have used them for variable feature selection in many predictive analytics problems involving thousands of input variables. Similarly, deep learning methods have enabled neural networks to attain impressive classification accuracies on large data sets (LeCun et al., 2015). Deep learning methods add additional layers of processes capable of learning or representing complex patterns at the expense of further degrees of separation between the model output and the underlying model intuition. Consequently, such methods also constitute a departure from traditional statistical methods, such as ordinary least squares or simple logistic regression analyses that prior IS studies have commonly used for both explaining and predicting. Shmueli and Koppius (2010) note that there is a difference between models geared toward prediction and those geared toward explanation. In many ways, big data amplifies this dichotomy as powerful non-deterministic and/or “black box” methods gain prominence for their predictive capabilities. In many cases, such methods produce answers to the “what” without the “why”. Another implication of big data sets is construct validity/credibility concerns pertaining to variables derived from user-generated, non-survey-based data sources, often characterized by low veracity. In addition to the spam and deception traits inherent to user-generated content sources (e.g., social media) and clickstreams’ data-tracking limitations, scholars operationalize many unstructured data sources as a few structured variables. We can see one prominent illustration of this point in the context of user sentiment polarity: whether the user is expressing a positive, negative, or neutral sentiment toward a given topic. Due to the volume of big data, one typically derives such constructs using software packages that rely on natural language processing methods as opposed to traditional manual coding methods. There have been thousands of studies published using social media sentiments in the recent years, including several in IS outlets. Such studies routinely make conclusions about the impact of user sentiment-related independent variables. However, scholars rarely report information on the suitability and accuracy of the underlying sentiment classification models used to operationalize the constructs, which happens despite the fact that benchmarking studies have found that many state-of-the-art sentiment analysis methods’ sentiment polarity classification performances are subpar, which affects the sentiment-related analysis and conclusions drawn from it (Hassan, Abbasi, & Zeng, 2014). Moving forward, we need research-based guidelines on how to validate variables derived from natural language, clickstream, sensor, and other big data sources.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxiv\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Another important issue is to consider how the penetrative, imaginative understanding of human meanings discerned by qualitative (particularly interpretive) researchers using small data can complement (rather than be substituted by) patterns derived from big data using machines/techniques/algorithms, to be able to offer a more complete picture of the phenomenon. Indeed, an emerging stream of work illustrates how findings based on qualitative “idiographic” approaches may mutually inform findings based on computational methods (Gaskin, Berente, Lyytinen, & Yoo, 2014). We are also aware that grounded theory researchers in IS (i.e., the SIG GTM community) are looking for ways in which the grounded theory methodology (GTM) principles can be effectively utilized in big data settings. Clearly, big data is requiring us to reexamine how we analyze and validate data and interpret and discuss the findings. We need further research to assess more thoroughly the pros and cons of different methods and metrics on various types of big data sets, and to provide meaningful guidelines. In particular, due to the volume, variety, and veracity dimensions, we need to be watchful about big data’s creating “false positives” in terms of statistical significance of independent variables or considerably altering the effect size. Finding ways and principles that can aid in effectively complementing and/or triangulating big data research with small data research is another issue we must take seriously. Achieving “consilience—that is, convergence of evidence from multiple, independent, and unrelated sources”— needs to be a matter of priority (George et al., 2014, p. 324).\n",
              "\n",
              "12 Closing Thoughts\n",
              "The arena of big data/big data analytics has captured the attention and imagination of both practitioners and academics in a variety of disciplines, not just in IS. Commentators have described the big data phenomenon as a “deluge” and as having the potential to cause long-lasting impacts on practice and academia (e.g., Anderson, 2008; Kitchin, 2014b). Indeed, thought leaders and editors of leading IS journals see much reason for optimism regarding big data’s impacts on the IS discipline. For example, Goes (2014, p. viii) has encouraged the field to “embrace the changes and provide leadership in the new environment… [for which we] are uniquely positioned” and to “claim our [rightful] territory”. Agarwal and Dhar (2014) view big data as an opportunity for ushering in a “golden age for IS researchers”. Yet, not all scholars from IS and other disciplines unquestionably accept projections of such promise (e.g., Buhl, Röglinger, Moser, & Heidemann, 2014). Some, including Professor Michael Jordan, a “machine-learning maestro”, predict the onset of “big data winter” if we continue to over-promise and overhype (Gomes, 2014) without addressing fundamental epistemological and methodological issues associated with big data (e.g., Kitchin, 2014b). Furthermore, we must address the concerns regarding the erosion of privacy and, consequently, the loss of human dignity in the face of economic imperatives (e.g., Barocas & Nissenbaum, 2014) that can lead to “digital colonization” (Buhl et al., 2014) and the “subjugation” of human interests by machines and algorithms, which socio-technical scholars have for long been wary of (e.g., Bjorn-Andersen, Earl, Holst, & Mumford, 1982). In addition, the rhetoric of making academic research relevant by helping solve immediate organizational problems through big data without adequate abstraction or without designing approaches or artifacts for addressing broad classes of problems raises questions regarding how academic research differs from practice. Also, in line with Benbasat and Zmud (2003), many scholars believe that routinely engaging in big data projects without a unique disciplinary “signature” could prove to be ominous for the IS discipline in the long run. Hence, our position is one of cautious optimism. While we undoubtedly see potential for big data in contributing to a stronger and more relevant IS discipline—one that would have a significant social impact— we do not take the benefits for granted. To help big data research achieve its potential, we invite IS scholars to: a) critically engage with fundamental issues, such as epistemology, methodology, ethics, and the design of novel artifacts; b) rethink decision models proposed in the era of scarce data and adapt them for use in the current era of abundant data; and c) assess economic and humanistic outcomes of big data in the form of systematic, multi-paradigmatic research initiatives across the information chain value. Further, to ensure a healthy development of scholarship in this area, we see the need to carefully balance “real-world” problemsolving using big data/big data techniques with reflective inquiry and scholarly abstraction of knowledge in this area. We also encourage big data researchers from the IS discipline to participate in boundary-spanning interdisciplinary big data projects, but to balance engagement with other disciplines with conscious development and nurturing of big data approaches and objectives that are somewhat consistent with the IS discipline’s socio-technical heritage. In this editorial, we do not intend to provide definite answers or directions but to encourage inquiry, reflection, and debate on this topic by IS scholars embedded in diverse theoretical and methodological traditions. We\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxv\n",
              "\n",
              "are hopeful that the framework (Figure 4) and the accompanying tables (Tables 1, 2, and 3), while preliminary, will help energize the conversation on big data in the broader IS community and provide a roadmap for advancing scholarship in the area. Our final comment is related to teaching, which we believe is our raison d'être. No other academic unit has the diversity of research traditions and understanding of the business, information, technology, and human issues that are essential to comprehending the various facets of the big data value chain (e.g., Agarwal & Dhar, 2014). This diversity places us in an excellent position to offer pedagogical leadership in teaching, developing curricula, and programs and to initiate industry outreach centers (Chiang, Goes, & Stohr, 2012). In summary, we are convinced that big data is here to stay. However, we can foresee a time when big data will not be at the forefront of our conversations, as we have seen in the cases of expert systems, BPR, ebusiness, ERP, and groupware. Yet, few will disagree that the ideas underlying these topics have continued, and will continue, to be important knowledge areas informing research and practice in IS. We expect that big data research will do the same. For now, big data offers a stage for learning new lessons, re-learning and refining old lessons, and reflecting on assumptions that underlie our research endeavors and the complex ways in which technology, information, and humans interact to shape the world we live in.\n",
              "\n",
              "Acknowledgments\n",
              "We thank Kenny Cheng, Dirk Hovorka, Steven Johnson, Vijay Khatri, Brent Kitchens, Sridhar Nerur, and Tony Vance for their constructive comments on an earlier version of the editorial. The reactions of TSWIM 2015 participants to a keynote talk by one of the authors also helped shape several ideas, for which we are grateful. Likewise, we greatly appreciate feedback from faculty and doctoral students that participated in the 2015 Antai Graduate Summer School at Shanghai Jiaotong University.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxvi\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "References\n",
              "Abbasi, A., & Adjeroh, D. (2014). Social media analytics for smart health. IEEE Intelligent Systems , 29 (2), 60-64. Abbasi, A., Zhang, Z., Zimbra, D., Chen, H., & Nunamaker, J. F., Jr. (2010). Detecting fake websites: The contribution of statistical learning theory. MIS Quarterly, 34(3), 435-461. Abbasi, A., Lau, R. Y. K., & Brown, D. E. (2015). Predicting Behavior. IEEE Intelligent Systems, 30(3), 35-43. Adomavicius, G., & Tuzhilin, A. (2005). Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6), 734-749. Agarwal, R., & Dhar, V. (2014). Editorial: Big data, data science, and analytics: The opportunity and challenge for IS research. Information Systems Research, 25(3), 443-448. Alavi, M., & Leidner, D. E. (2001). Review: Knowledge management and knowledge management systems: Conceptual foundations and research issues. MIS Quarterly, 25(1), 107-136. Anderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. Retrieved from http://www.wired.com/2008/06/pb-theory/ Andrienko, N., & Andrienko, G. (2003). Informed spatial decisions through coordinated views. Information Visualization, 2(4), 270-285. Aral, S., & Walker, D. (2012). Identifying influential and susceptible members of social networks. Science, 337(6092), 337-341. Arnott, D., & Pervan, G. (2008). Eight key issues for the decision support systems discipline. Decision Support Systems, 44(3), 657-672. Arnott, D., & Pervan, G. (2012). Design science in decision support systems research: An assessment using the Hevner, March, Park, and Ram Guidelines. Journal of the Association for Information Systems, 13(11), 923-949. Bardhan, I., Oh, C., Zheng, E., & Kirksey, K. (2015). Predictive analytics for readmission of patients with congestive heart failure: Analysis across multiple hospitals. Information Systems Research, 26(1), 19-39. Baesens, B., Bapna, R., Marsden, J. R., Vanthienen, J., & Zhao, J. L. (2014). Transformational issues of big data and analytics in networked business. MIS Quarterly, 38(2), 629-632. Balakrishnan, P. V., & Jacob, V. S. (1996). Genetic algorithms for product design. Management Science, 42(8), 1105-1117. Bansal, G., Sinha, A. P., & Zhao, H. (2008). Tuning data mining methods for cost-sensitive regression: A study in loan charge-off forecasting. Journal of Management Information Systems, 25(3), 315-336. Banker, R. D., & Kauffman, R. J. (2004). 50th anniversary article: The evolution of research on information systems: A fiftieth-year survey of the literature in management science. Management Science, 50(3), 281-298. Barocas, S., & Nissenbaum, H. (2014). Big data's end run around procedural privacy protections. Communications of the ACM, 57(11), 31-33. Benbasat, I., & Zmud, R. W. (2003). The identity crisis within the IS discipline: Defining and communicating the discipline's core properties. MIS Quarterly, 27(2), 183-194. Bjørn-Andersen, N., Earl, M., Holst, O., & Mumford, E. (1982). Information society: For richer, for poorer, Amsterdam: North-Holland. Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxvii\n",
              "\n",
              "Broniatowski, D., Paul, M. J., & Dredze, M. (2014). National and local influenza surveillance through Twitter: An analysis of the 2012-2013 influenza epidemic. PLoS One, 8, e83672. Brown, S. A., Massey, A. P., Montoya-Weiss, M. M., & Burkman, J. R. (2002). Do I really have to? User acceptance of mandated technology. European Journal of Information Systems, 11(4), 283-295. Brown, D. E., Abbasi, A., & Lau, R. Y. K. (2015). Predictive analytics: Predictive modeling at the micro level. IEEE Intelligent Systems, 30(3), 6-8. Buhl, H. U., Röglinger, M., Moser, F., & Heidemann, J. (2013). Big data—a fashionable topic with(out) sustainable relevance for research and practice? Business & Information Systems Engineering, 5(2), 65-69. Buitelaar, P., Cimiano, P., & Magnini, B. (Eds.). (2005). Ontology learning from text: Methods, evaluation and applications. Amsterdam: IOS Press. Buytendijk, F. (2014). Hype cycle for big data, https://www.gartner.com/doc/2814517/hype-cycle-big-data2014. Gartner. Available from\n",
              "\n",
              "Chandler, N., Hostmann, B., Rayner, N., & Herschel, G. (2011). Gartner’s business analytics framework. Gartner. Chang, R. M., Kauffman, R. J., & Kwon, Y. (2014). Understanding the paradigm shift to computational social science in the presence of big data. Decision Support Systems, 63, 67-80. Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R. (2000). CRISP-DM 1.0 Step-by-step data mining guide. SPSS. Retrieved from www.crisp-dm.org Chatfield, C. (1995). Problem solving: A statistician’s guide (2nd ed.). London: Chapman & Hall/CRC. Chatterjee, S., & Sarker, S. (2013). Infusing ethical considerations in knowledge management scholarship: Toward a research agenda. Journal of the Association for Information Systems, 14(8), 452-481. Chatterjee, S., Sarker, S., & Fuller, M. (2009a). A deontological approach to designing ethical collaboration. Journal of the Association for Information Systems, 10(3), 138-169. Chatterjee, S., Sarker, S., & Fuller, M. (2009b). Ethical information systems development: A Baumanian postmodernist perspective. Journal of the Association for Information Systems, 10(11), 787-815. Chen, H., Chiang, R. H., & Storey, V. C. (2012). Business intelligence and analytics: From big data to big impact. MIS Quarterly, 36(4), 1165-1188. Chiang, R. H., Barron, T. M., & Storey, V. C. (1994). Reverse engineering of relational databases: Extraction of an EER model from a relational database. Data & Knowledge Engineering, 12(2), 107-142. Chiang, R. H., Goes, P., & Stohr, E. A. (2012). Business intelligence and analytics education, and program development: A unique opportunity for the information systems discipline. ACM Transactions on Management Information Systems, 3(3), 1-13. Chung, W., Chen, H., & Nunamaker, J. F., Jr. (2005). A visual framework for knowledge discovery on the Web: An empirical study of business intelligence exploration. Journal of Management Information Systems, 21(4), 57-84. Christensen, C. (1997). The innovator's dilemma: When new technologies cause great firms to fail. Boston, MA: Harvard Business Review Press. Coco, C. T., Jamison, F., & Black, H. (2011). Connecting people investments and business outcomes at Lowe’s. People & Strategy, 34(2), 28-33 Coolidge, A. (2013). New technology helps Kroger speed up checkout times. The Cincinnati Inquirer. Retrieved from http://www.usatoday.com/story/money/business/2013/06/20/new-technology-helpskroger-speed-up-checkout-times/2443975/ Davenport, T. H. (2006). Competing on analytics. Harvard Business Review, 84(1), 98-107. Davenport, T. H., & Harris, J. G. (2007). Competing on analytics: The new science of winning. Boston, MA: Harvard Business Press.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxviii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Davenport, T. H., Harris, J., & Shapiro, J. (2010). Competing on talent analytics. Harvard Business Review, 88(10), 52-58. Davenport, T. H., & Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(10), 70-76. Davenport, T. H., & Kirby, J. (2015). Beyond automation. Harvard Business Review, 94(6), 59-65. Davis, F. D. (1986). A technology acceptance model for empirically testing new end-user information systems: Theory and results (Doctoral dissertation). Sloan School of Management, Massachusetts Institute of Technology. Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319-340. Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1), 107-113. Dennis, A. R., & Carte, T. A. (1998). Using geographical information systems for decision making: Extending cognitive fit theory to map-based presentations. Information Systems Research, 9(2), 194-203. Derman, E. (2011). Models. Behaving. Badly. New York, NY: Simon and Schuster. Embley, D. W., & Liddle, S. W. (2013). Big data—conceptual modeling to the rescue. In W. Ng, V. C. Storey, & J. C. Trujillo (Eds.), ER conference (vol. 8217, pp. 1-8). Springer. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996a). From data mining to knowledge discovery in databases. AI Magazine, 17(3), 37-54. Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996b). The KDD process for extracting useful knowledge from volumes of data. Communications of the ACM, 39(11), 27-34. Gaskin, J., Berente, N., Lyytinen, K., & and Yoo, Y. (2014). \"Toward Generalizable Sociomaterial Inquiry: A Computational Approach for Zooming In and Out of Sociomaterial Routines,\" MIS Quarterly, 38(3), 849-871. Gefen, D., Karahanna, E., & Straub, D. W. (2003). Trust and TAM in online shopping: An integrated model. MIS Quarterly, 27(1), 51-90. George, G., Haas, M. R., & Pentland, A. (2014). Big data and management. Academy of Management Journal, 57(2), 321-326. Gleick, J. (1987). Chaos. Making a new science. New York: Viking Penguin Inc. Goes, P. (2014). Big data and IS research. MIS Quarterly, 38(3), iii-viii. Gomes, L. (2014). Machine-learning maestro Michael Jordan on the delusions of big data and other huge engineering efforts. IEEE Spectrum. Gregor, S., & Klein, G. (2014). Eight obstacles to overcome in the theory testing Genre. Journal of the Association for Information Systems, 15(11), i-xix. Han, J., Kamber, M., & Pei, J. (2006). Data mining: Concepts and techniques. New York: Morgan Kaufmann. Harrington, A. (2005). Modern social theory. New York: Oxford University Press. Hassan, A., Abbasi, A., & Zeng, D. (2013). Twitter sentiment analysis: A bootstrap ensemble framework. In Proceedings of the IEEE International Conference on Social Computing (pp. 357-364). Heudecker, N. (2013). Hype cycle for big data, https://www.gartner.com/doc/2574616/hype-cycle-big-data2013. Gartner. Available from\n",
              "\n",
              "Hevner, A. R., March, S. T., Park, J., & Ram, S. (2004). Design science in information systems research. MIS Quarterly, 28(1), 75-105.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxix\n",
              "\n",
              "Hill, K. (2012). How Target figured out a teen girl was pregnant before her father did. Forbes. http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-waspregnant-before-her-father-did/ Hodgkinson, G. P., & Starbuck, W. H. (2008). The Oxford handbook of organizational decision making. Oxford, UK: Oxford University Press. Horan, J. A. (2011). The essential CIO. IBM CIO C-Suite Studies. Retrieved from http://www935.ibm.com/services/c-suite/cio/study/ Jensen, M. L., Lowry, P. B., Burgoon, J. K., & Nunamaker, J. F., Jr. (2010). Technology dominance in complex decision making: The case of aided credibility assessment. Journal of Management Information Systems, 27(1), 175-202. Jensen, M. L., Averbeck, J. M., Zhang, Z., & Wright, K. B. (2013). Credibility of anonymous online product reviews: A language expectancy perspective. Journal of Management Information Systems, 30(1), 293-323. Junqué de Fortuny, E., Martens, D., & Provost, F. (2013). Predictive modeling with big data: Is bigger really better? Big Data, 1(4), 215-226. Kaushik, A. (2011). Web analytics 2.0: The art of online accountability and science of customer centricity. Indianapolis, IND: Wiley. KDNuggets (2011). What do you call analyzing data? KDNuggets. http://www.kdnuggets.com/polls/2011/what-do-you-call-analyzing-data.html Retrieved from\n",
              "\n",
              "Kiel, L. D., & Elliott, E. W. (1996). Chaos theory in the social sciences: Foundations and applications. University of Michigan Press. Kiron, D., Shockley, R., Kruschwitz, N., Finch, G., & Haydock, M. (2012). Analytics: The widening divide. MIT Sloan Management Review, 53(2), 1-21. Kitchin, R. (2014a). Big Data, new epistemologies and paradigm shifts. Big Data & Society, 1(1), 1-12. Kitchin, R. (2014b). The data revolution. London: Sage. Kramer A. D. I., Guillory J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788-8790. Lavalle, S., Lesser, E., Shockley, R., Hopkins, M. S., & Kruschwitz, N. (2011). Big data, analytics, and the path from insights to value. Sloan Management Review, 52(2), 21-31. Lau, R. Y. K. Liao, S. Y., Wong, K. F., & Chiu, K. W. (2012). Web 2.0 environmental scanning and adaptive decision support for business mergers and acquisitions. MIS Quarterly, 36(4), 1239-1268. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521, 436-444. Lin, M., Lucas, H. C., Jr., & Shmueli, G. (2013). Research commentary-too big to fail: Large samples and the p-value problem. Information Systems Research, 24(4), 906-917. Lycett, M. (2013). “Datafication”: Making sense of (big) data in a complex world. European Journal of Information Systems, 22(4), 381-386. McAfee, A., & Brynjolfsson, E. (2012). Big data: The management revolution. Harvard Business Review. Retrieved from https://hbr.org/2012/10/big-data-the-management-revolution/ar Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation Manyika, J., Chui, M., Groves, P., Farrell, D., Van Kuiken, S., & Doshi, E. A. (2013). Open data: Unlocking innovation and performance with liquid information. McKinsey Global Institute. Retrieved from http://www.mckinsey.com/~/media/McKinsey/dotcom/Insights/Business%20Technology/Open%20d ata%20Unlocking%20innovation%20and%20performance%20with%20liquid%20information/MGI_ OpenData_Full_report_Oct2013.ashx\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxx\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "Marchand, D. A., & Pepper, J. (2013). Why IT fumbles analytics. Harvard Business Review. Retrieved from https://hbr.org/2013/01/why-it-fumbles-analytics Mayer-Schönberger, V., & Cukier, K. (2013). Big data: A revolution that will transform how we live, work, and think. Boston, MA: Houghton Mifflin Harcourt. Newman, D. (2014). Big Data Means Big Disruption. Forbes. Retrieved http://www.forbes.com/sites/danielnewman/2014/06/03/big-data-means-big-disruption/ from\n",
              "\n",
              "Nunamaker, J. F., Jr., Chen, M., & Purdin, T. (1991). Systems development in information systems research. Journal of Management Information Systems, 7(3), 89-106. Nunamaker, J. F., Jr. (1992). Build and learn, evaluate and learn. Informatica, 1(1), 1-6. Parsons, J., & Wand, Y. (2012). Extending classification principles from information modeling to other disciplines. Journal of the Association for Information Systems, 14(5), 245-273. Rai, A., & Sambamurthy, V. (2006). Editorial notes-the growth of interest in services management: Opportunities for information systems scholars. Information Systems Research, 17(4), 327-331. Redman, T. C. (2008). Data driven: Profiting from your most important business asset. Boston, MA: Harvard Business Press. Richards, N. M., & King, J. H. (2013). Three paradoxes of big data, Stanford Law Review Online, 66(41), 41-46. Sarker, S., Xiao, X., & Beaulieu, T. (2013). Qualitative studies in information systems: A critical review and some guiding principles. MIS Quarterly, 37(4), iii-xviii. Schroeck, M., Shockley, R., Smart, J., Romero-Morales, D., & Tufano, P. (2012). Analytics: The real-world use of big data. IBM Institute for Business Value. Sein, M., Henfridsson, O., Purao, S., Rossi, M., & Lindgren, R. (2011). Action design research. MIS Quarterly, 35(1), 37-56. Shaft, T. M., & Vessey, I. (2006). The role of cognitive fit in the relationship between software comprehension and modification. MIS Quarterly, 30(1), 29-55. Sharma, R., Mithas, S., & Kankanhalli, A. (2014). Transforming decision-making processes: A research agenda for understanding the impact of business analytics on organisations. European Journal of Information Systems, 23, 433-441. Shim, J. P., Warkentin, M., Courtney, J. F., Power, D. J., Sharda, R., & Carlsson, C. (2002). Past, present, and future of decision support technology. Decision Support Systems, 33(2), 111-126. Shmueli, G., & Koppius, O. (2011). Predictive analytics in information systems research. MIS Quarterly, 35(3), 553-572. Simon, H. A. (1996). The sciences of the artificial (3rd ed.). Cambridge, MA: MIT Press. Song, Y., Sahoo, N., Srinivasan, S., & Chrysanthos, D. (2014). Uncovering path-to-purchase segments in large consumer population. In Proceedings of the 24th Workshop on Information Technologies and Systems. Sprott, J. C. (2003). Chaos and time-series analysis (Vol. 69). Oxford: Oxford University Press. Steadman I. (2013). Big data and the death of the theorist. http://www.wired.co.uk/news/archive/2013-01/25/big-data-end-of-theory Wired. Retrieved from\n",
              "\n",
              "Stigler, G. (1961). The economics of information. The Journal of Political Economy, 69(3), 213-225. Storey, V. C., Chiang, R. H., Dey, D., Goldstein, R. C., & Sudaresan, S. (1997). Database design with common sense business reasoning and learning. ACM Transactions on Database Systems, 22(4), 471-512. Stubbs, E. (2011). The value of business analytics: Identifying the path to profitability. New York, NY: John Wiley & Sons. Taleb, N. (2005). Fooled by randomness: The hidden role of chance in life and in the markets (Vol. 1). Random House Incorporated.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Journal of the Association for Information Systems\n",
              "\n",
              "xxxi\n",
              "\n",
              "Taleb, N. N. (2007). The black swan: The impact of the highly improbable. Random House. Taleb, N. N., Goldstein, D. G., & Spitznagel, M. W. (2009). The six mistakes executives make in risk management. Harvard Business Review, 87(10), 78-81. Tambe, P. (2014). Big data investment, skills, and firm value. Management Science, 60(6), 1452-1469. Te’eni, D. (2006). Designs that fit: An overview of fit conceptualizations in HCI. In P. Zhang & D. Galletta (Eds.), Human computer interaction and management information systems: Foundations (pp. 205224). London, England: M. E. Sharpe. Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124-1131. Vapnik, V. N. (1998). Statistical learning theory (Vol. 1). New York: Wiley. Venkatesh, V, Thong, J. Y. L., & Xu, X. (Forthcoming). Unified theory of acceptance and use of technology: A synthesis and the road ahead. Journal of the Association for Information Systems. Venkatesh, V., Morris, M., Davis, G., & Davis, F. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 27(3), 425-478. Vessey, I. (1991). Cognitive fit: A theory‐based analysis of the graphs versus tables literature. Decision Sciences, 22(2), 219-240. Walls, J. G., Widmeyer, G. R., & El Sawy, O. A. (1992). Building an information system design theory for vigilant EIS. Information Systems Research, 3(1), 36-59. Wand, Y., & Weber, R. (2002). Research commentary: Information systems and conceptual modeling—a research agenda. Information Systems Research, 13(4), 363-376. Wang, Y., & Ram, S. (2015). Prediction of location-based sequential purchasing events using spatial, temporal and social patterns. IEEE Intelligent Systems, 30(3), 10-17. Werndl, C. (2009). What are the new implications of chaos for unpredictability? The British Journal for the Philosophy of Science, 60(1), 195-220. Westfall, P. H., & Hilbe, J. (2007). The black swan: Praise and criticism. The American Statistician, 61(3), 193-194. Wixom, B. H., & Watson, H. J. (2001). An empirical investigation of the factors affecting data warehousing success. MIS Quarterly, 25(1), 17-41. Yetgin, E., Jensen, M., & Shaft, T. (2015). Complacency and intentionality in IT use and continuance. AIS Transactions on Human-Computer Interaction, 7(1), 17-42. Zahedi, F. M., Abbasi, A., & Chen, Y. (2015). Fake-website detection tools: Identifying elements that promote individuals’ use and enhance their performance. Journal of the Association for Information Systems, 16(6), 448-484. Zeng, D., Chen, H., Lusch, R., & Li, S. H. (2010). Social media analytics and intelligence. IEEE Intelligent Systems, 25(6), 13-16. Zhao, H., Sinha, A. P., & Bansal, G. (2011). An extended tuning method for cost-sensitive regression and forecasting. Decision Support Systems, 51(3), 372-383. Zhang, W., Lau, R., & Li, C. (2014). Adaptive big data analytics for deceptive review detection in online social media. In Proceedings of the International Conference on Information Systems.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "xxxii\n",
              "\n",
              "Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
              "\n",
              "About the Authors\n",
              "Ahmed Abbasi is Murray Research Professor and Associate Professor of Information Technology in the McIntire School of Commerce at the University of Virginia. He is Director of the Center for Business Analytics and a member of the Predictive Analytics Lab. His research on online fraud and security, natural language processing, social media, and e-health has been funded through multiple grants from the National Science Foundation. Ahmed received the IBM Faculty Award and AWS Research Grant for his work on Big Data. He has published over fifty peer-reviewed papers in journals and conferences, including top-tier outlets such as MIS Quarterly, Journal of the AIS, Journal of MIS, ACM Transactions on IS, IEEE Transactions on Knowledge and Data Engineering, and IEEE Intelligent Systems. One of his papers was considered a top publication by the AIS. Ahmed also won best paper awards at MIS Quarterly and WITS. His work has been featured in various media outlets, including the Wall Street Journal, the Associated Press, and Fox News. Ahmed serves as an Associate Editor for Information Systems Research, Decision Sciences Journal, ACM Transactions on MIS, and IEEE Intelligent Systems, and as an editorial review board member for the Journal of AIS. He is a senior member of the IEEE and serves on program committees for various conferences related to computational linguistics, text analytics, and data mining. He is also a co-founder and/or advisory board member for multiple predictive analytics-related companies. Suprateek Sarker is a Professor of Information Technology at the McIntire School of Commerce, University of Virginia, USA. He also serves as Visiting Distinguished Professor at Aalto University School of Business, Helsinki, Finland. He is primarily a qualitative researcher, and his past work has been published in leading outlets including the MIS Quarterly, Information Systems Research, Journal of the AIS, Journal of MIS, European Journal of Information Systems, MIS Quarterly Executive, Journal of Information Technology, IEEE Transactions, ACM Transactions on MIS, Communications of the ACM, MIS Quarterly Executive, and Decision Sciences Journal. He is currently serving as the Editor-in-Chief of the Journal of the AIS, a Senior Editor of Decision Sciences Journal, a Senior Editor (Emeritus) of MIS Quarterly, a member of the Board of Editors of Journal of MIS, and an editorial board member of IEEE Transactions on Engineering Management. Some of his past work has been funded by the National Science Foundation (NSF) and the Institute for the Study of Business Markets (ISBM). He is also a past recipient (with S. Sahay) of the Stafford Beer Medal awarded by the Operational Research Society, UK. Roger H. L. Chiang is a Professor of Information Systems at Department of Operations, Business Analytics, and Information Systems, Carl H. Lindner College of Business, University of Cincinnati. Dr. Chiang’s research interests are in business intelligence and analytics, data and knowledge management, and intelligent systems, particularly in database reverse engineering, database integration, data and text mining, sentiment analysis, document classification and clustering, domain knowledge discovery, and semantic information retrieval. He has over fifty refereed papers published by conferences and journals including ACM Transactions on Database Systems, ACM Transactions on MIS, Communications of the ACM, The DATA BASE for Advances in Information Systems, Data & Knowledge Engineering, Decision Support Systems, Journal of American Society for Information Science and Technology, Journal of Database Administration, Journal of MIS, Marketing Science, MIS Quarterly, and Very Large Data Base Journal. He has served as the Senior Editor of The DATA BASE for Advances in Information Systems, Decision Sciences, and Journal of the AIS and the associate editor of Information & Management, Journal of the AIS, Journal of Database Management, International Journal of Intelligent Systems in Accounting, Finance and Management, and MIS Quarterly.\n",
              "\n",
              "Copyright © 2016 by the Association for Information Systems. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and full citation on the first page. Copyright for components of this work owned by others than the Association for Information Systems must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or fee. Request permission to publish from: AIS Administrative Office, P.O. Box 2712 Atlanta, GA, 30301-2712 Attn: Reprints or via e-mail from publications@aisnet.org.\n",
              "\n",
              "Volume 17\n",
              "\n",
              "Issue 2\n",
              "\n",
              "\n",
              "Copyright of Journal of the Association for Information Systems is the property of Association for Information Systems and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.\n",
              "\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "XRKv2IhZSQJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5P44m5CyBFiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "options = {'ents': ['PERSON', 'ORG', 'PRODUCT','MONEY'],\n",
        "        'colors': {'ORG': 'yellow'}}\n",
        "displacy.render(doc1, style='ent', options=options, jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAa-hA6EBHHA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "displacy.render(doc1, style=\"ent\", jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGhRqq_XK9KJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaIb1cRgUDmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
        "#options={'ents':['CARDINAL', 'PERCENT']}\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BoLKTBB14OfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u'Hello, world. Here are two sentences,I see .')\n",
        "print([t.text for t in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOaWBWZlK17t",
        "colab_type": "code",
        "outputId": "72a4c284-ad1d-4273-952f-8aff56049ba0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 41
        }
      },
      "cell_type": "code",
      "source": [
        "#To upload Local files from system to collab storage/directory.\n",
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "    _files = files.upload()\n",
        "    if len(_files) >0:\n",
        "       for k,v in _files.items():\n",
        "        open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-710b91c8-0af5-487a-9139-50d2dd77e0e7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-710b91c8-0af5-487a-9139-50d2dd77e0e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "L-iR5VWtKD1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en') \n",
        "text1=nlp(open(u\"AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt\").read())\n",
        "for ent in text1.ents:\n",
        "  print(ent.text, ent.label_)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M54_2VG7Wgew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "options = {'ents': ['PERCENT'],\n",
        "        'colors': {'PERCENT': 'yellow'}}\n",
        "displacy.render(text1, style='ent', options=options, jupyter=True)\n",
        "text1.remove(ents!='PERCENT')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O69AfXJyL1ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#enumerate sentence in a file\n",
        "for num, sentence in enumerate(doc.sents):\n",
        "  print(f'{num}:{sentence}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLkOsNUIR-a2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0yeEpXiDTz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4fNDynpdivw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "options={'ents':['CARDINAL', 'PERCENT']}\n",
        "      #  'colors': {'': 'yellow'}}\n",
        "displacy.render(doc1, style='ent', options=options, jupyter=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZmRglQVOzip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en') \n",
        "text1=nlp(open('AbbasiA#SarkerS#ChiangR_2016_Big Data Research in Information Systems - Toward an Inclusive Research Agenda_Journal of the Association for Information Systems_2.txt').read())\n",
        "for ent in text1.ents:\n",
        "  print(ent.text,ent.label_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwfvWhM9J-Fn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOMJF3jQEEh6",
        "colab_type": "code",
        "outputId": "6b7ef0d6-efbf-4218-e9b2-8660efb57d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk import Tree\n",
        "\n",
        "\n",
        "en_nlp = spacy.load('en')\n",
        "\n",
        "doc = en_nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        jumps                    \n",
            "  ________|______________         \n",
            " |        |             over     \n",
            " |        |              |        \n",
            " |       fox            dog      \n",
            " |    ____|_____      ___|____    \n",
            " .  The quick brown the      lazy\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "gSZCjxfHOznL",
        "colab_type": "code",
        "outputId": "f6ddcf31-7a35-4965-ff29-7e03666b5822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "nlp=spacy.load('en')\n",
        "doc = nlp(u'I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">I just bought \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    2\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " shares at \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    9 a.m.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " because the stock went up \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    30%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    just 2 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " according to the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    WSJ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FBi7ge4vIMly",
        "colab_type": "code",
        "outputId": "7a52b848-d501-410b-d36f-620f75a3866f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp=spacy.load('en')\n",
        "doc = nlp(u'Rats are various medium-sized, long-tailed rodents.')\n",
        "displacy.render(doc, style='dep', jupyter=True,options={'distance': 110})\n",
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"483-0\" class=\"displacy\" width=\"930\" height=\"357.0\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Rats</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">are</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">various</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">medium-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">sized,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">long-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">tailed</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">rodents.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,167.0 145.0,167.0 145.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-1\" stroke-width=\"2px\" d=\"M290,222.0 C290,57.0 815.0,57.0 815.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M290,224.0 L282,212.0 298,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-2\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-3\" stroke-width=\"2px\" d=\"M510,222.0 C510,112.0 810.0,112.0 810.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M510,224.0 L502,212.0 518,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-4\" stroke-width=\"2px\" d=\"M620,222.0 C620,167.0 695.0,167.0 695.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M620,224.0 L612,212.0 628,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-5\" stroke-width=\"2px\" d=\"M730,222.0 C730,167.0 805.0,167.0 805.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M730,224.0 L722,212.0 738,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-483-0-6\" stroke-width=\"2px\" d=\"M180,222.0 C180,2.0 820.0,2.0 820.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-483-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M820.0,224.0 L828.0,212.0 812.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rats are various medium-sized, long-tailed rodents."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "hD7aTntVLYxd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}