This article was downloaded by: [130.89.97.167] On: 17 February 2017, At: 08:09 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA

Information Systems Research
Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org

Research Note—Are Online Labor Markets Spot Markets for Tasks? A Field Experiment on the Behavioral Response to Wage Cuts
Daniel L. Chen, John J. Horton

To cite this article: Daniel L. Chen, John J. Horton (2016) Research Note—Are Online Labor Markets Spot Markets for Tasks? A Field Experiment on the Behavioral Response to Wage Cuts. Information Systems Research 27(2):403-423. http://dx.doi.org/10.1287/ isre.2016.0633 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

Information Systems Research
Vol. 27, No. 2, June 2016, pp. 403–423 ISSN 1047-7047 (print) ISSN 1526-5536 (online) http://dx.doi.org/10.1287/isre.2016.0633 © 2016 INFORMS

Research Note
Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Are Online Labor Markets Spot Markets for Tasks? A Field Experiment on the Behavioral Response to Wage Cuts
Daniel L. Chen
Institute for Advanced Study, Toulouse School of Economics, 31015 Toulouse Cedex 6, France, dchen@law.harvard.edu

John J. Horton
Stern School of Business, New York University, New York, New York 10012, john.joseph.horton@gmail.com

I

n some online labor markets, workers are paid by the task, choose what tasks to work on, and have little or no interaction with their (usually anonymous) buyer/employer. These markets look like true spot markets for tasks rather than markets for employment. Despite appearances, we ﬁnd via a ﬁeld experiment that workers act more like parties to an employment contract: workers quickly form wage reference points and react negatively to proposed wage cuts by quitting. However, they can be molliﬁed with “reasonable” justiﬁcations for why wages are being cut, highlighting the importance of fairness considerations in their decision making. We ﬁnd some evidence that “unreasonable” justiﬁcations for wage cuts reduce subsequent work quality. We also ﬁnd that not explicitly presenting the worker with a decision about continuing to work eliminates “quits,” with no apparent reduction in work quality. One interpretation for this ﬁnding is that workers have a strong expectation that they are party to a quasi-employment relationship where terms are not changed, and the default behavior is to continue working. Keywords : economics of IS; electronic commerce; ﬁeld experiments; IT and new organizational forms History : Chris Forman, Senior Editor; Jonathon Cummings, Associate Editor. This paper was received on June 28, 2014, and was with the authors 10 months for 2 revisions. Published online in Articles in Advance May 20, 2016.

1.

Introduction

According to Coase (1937), the boundary of the ﬁrm is determined by the relative costs and beneﬁts of organizing production through markets or through authority. For the labor input to production, this markets-versus-authority choice was conceptualized by Simon (1951) as the choice between what Simon called a “sales contract” and the conventional employment contract. In Simon’s model, the employee accepts a wage in exchange for giving the employer control over what precise task, from some set of tasks, is done in the future. Although employment is at will, the expectation is that the relationship will be ongoing and persist until explicitly dissolved. By contrast, a sales contract is narrower in scope, with the worker agreeing to complete a speciﬁc task for a speciﬁc price. When the task is completed and payment is made, the contract is dissolved. Simon’s labels are not always easy to apply (e.g., consider the salesperson who works on commission), but the number of edge cases seems to have grown dramatically in recent years with the emergence of new labor-intermediating platforms.
403

Platforms differ, but a number seem to create quasiemployment or quasisales contracts that share features of both types of relationships.1 One kind of new labor-intermediating platform of particular interest is the online labor market (Horton 2010). In the case of online labor markets, some create relationships that at least have some of the properties of traditional employment relationships: workers are paid by the hour, and projects can be substantial and may require a number of different tasks, with the work being closely monitored by the hiring ﬁrm. Examples from this mold include oDesk/Elance (now Upwork), Freelancer.com, Guru, and several other more specialized sites. However, other online labor markets are far more task focused and seem to create sales contract relationships.
1

An example of a technology-driven platform creating this kind of confusion is Uber. Uber sets ride prices and determines who is eligible to drive, but does not tell drivers which precise customer to pick up or when to work—but it does impose standards of performance. Perhaps unsurprisingly, a number of worker classiﬁcation lawsuits have sprung up around this industry.

404

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

The most extreme version of a task-focused market is Amazon Mechanical Turk (MTurk), where tasks often take seconds and pay pennies. “Employers” in this market cannot exercise Simon-style authority, because they do not communicate with workers (except initially, through the job description)—workers simply and immediately complete the task at the terms proposed by the buyer. Workers are free to take any task they want at the offered price. This market, at least based on its main characteristics, seems like a true spot market for tasks. If this is the case, MTurk and markets like it would offer ﬁrms a profoundly different way of obtaining labor inputs. With a spot market, ﬁrms could buy discrete chunks of labor from a global pool of workers at a market price, similar to how they obtain any other factor of production. Even if MTurk is conceptualized by employers and Amazon itself as a spot market in tasks, this does not imply that workers share this view or behave accordingly. In this paper, we test whether workers on MTurk react as parties to a sales contract or an employment contract when presented with a wage cut. We conducted an experiment in which we contracted with workers for a data-entry task, paid them a high piece rate, and then offered some treated workers the opportunity to keep working, albeit for a lower rate. Although in both the sales contract and employment characterizations fewer workers should be willing to accept the lower offer, the two views differ in how “justiﬁcations” for the new offer should affect worker uptake. For a party to a sales contract in a spot market, how price changes are justiﬁed is materially irrelevant and should not differentially affect uptake of the follow-on task. However, for a party to an employment contract, a lower offer would be perceived as a wage cut, which in turn could provoke retaliation from the worker, depending on the perceived justice of that offer. We designed the different framings and justiﬁcations so that they would provoke workers to view the cuts as either reasonable and fair or capricious and unjust.2 Consistent with the employment view, but not the spot market view, we ﬁnd that workers were more likely to reject low offers, but justiﬁcations we deemed “reasonable” largely nulliﬁed this effect. Not all justiﬁcations were effective—justifying the cut in terms of our proﬁts actually increased quits (i.e., individuals unwilling to perform a follow-on task at the new offer). A worker’s quality might suffer following a wage cut. We can only measure work quality for people
2

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

who actually accepted the wage cut and performed an additional task. As such, it is possible that different kinds of workers quit in different treatments, so differences in quality across treatments could simply reﬂect selection effects as well as treatment effects. However, when we control for work quality from the ﬁrst stage, we do ﬁnd evidence of lower quality in the “proﬁts” treatments, suggesting the possibility of retaliation. In each of the different testing justiﬁcations for the new offer, we still explicitly framed the decision about whether to continue working as a choice, with the new per-task payment made salient. However, in one cell, we simply presented another data entry task with the new wage cut price clearly labeled, but without an explicit framing of a new offer and the need for a worker decision. In this cell, 100% of workers simply completed the next paragraph—a higher percentage than when they were offered their previous wage. Worker mistake is a possibility—perhaps they believed that not completing our extra paragraph would jeopardize payment—but we ﬁnd no evidence of follow-on retaliation in this group. A parsimonious explanation could be that workers regard themselves as being in quasi-employment relationships and so the “default” is to simply keep working without the expectation of—or the need for—renegotiation. The evidence from the experiment suggests that even in the most “spot” of the online labor markets, workers still bring an employee-like behavior to their interactions. MTurk is, of course, only one market and the results of the experiment come from a selfselected sample of workers on that market. It is necessarily unclear whether these results would extend to other markets and other kinds of workers. However, given that MTurk is the most spot-market like in its observable characteristics, it seems probable that the employee-like behavior found on MTurk would generalize to other online labor markets.

2.

Conceptual Framework

Although we technically were offering a lower piece rate rather than a chance in time-based compensation, we refer throughout the text as a “wage cut.”

Developments in information and communications technology have made it possible for ﬁrms to obtain labor inputs from around the world. Given that labor markets have historically been strongly segmented by geography, it is unsurprising that an enormous literature sprang up around information technology (IT) offshoring and outsourcing. This literature has focused on the decision to offshore and the sourcing mechanism (Tanriverdi et al. 2007); the structure (Chen and Bharadwaj 2009); formality (Tanriverdi et al. 2007) and ﬂexibility of contracts (Gopal and Koka 2012); the interplay with the open source community (Ågerfalk and Fitzgerald 2008); and so on. In this work, labor may cross borders, but is still being mediated by arm’s

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

405

length contracts between ﬁrms. However, the emergence of online labor markets lets ﬁrms buy labor inputs from workers directly, regardless of geography. These markets raise the question, however, of whether ﬁrms are obtaining labor inputs as completed tasks, obtained in a spot market, are actually creating microemployment relationships. How can we distinguish a spot market for tasks from a market for employment? In this paper, we propose using the worker response to a proposed price change, i.e., a wage cut, as our distinguishing test. Our proposition is that if workers respond to wage cuts in an "employee-like" way, then we have evidence against the spot-market characterization. Of course, we must deﬁne what employee-like means. Our claim is that a party to a sales contract will respond to a proposed price change “economically,” considering only the amount proposed. By contrast, a party that views herself as party to an employment contract will consider the fairness of the offer, and this fairness judgment will take into account context. To the extent that we, as experimenters, can alter acceptance of the wage cuts by changing the context of the wage cuts, the more evidence we have in favor of the employment contract conception of how the workers view what they are doing. A distinguishing feature of employment contracts is that prices generally do not fall when demand falls (as in a recession). This fact has profound policy implications, because without falling wages, labor markets do not clear, and some workers are involuntarily unemployed. The most prominent explanation for why ﬁrms do not cut wages is that workers with employment contracts—unlike, say, vendors with sales contracts— care a great deal about the fairness of wages and any proposed wage changes (Bewley 1999). As such, ﬁrms worry that wage cuts could be perceived as unfair and workers would retaliate explicitly, such as by sabotaging the ﬁrm, or implicitly, by withholding their cooperation and “consummate performance.” What is considered “unfair” often seems to depend strongly on what occurred in the relationship before the cut: laboratory experiments support the proposition that past wage experiences can create wage reference points (e.g., Fehr et al. 2006). It is the workers’ expected reaction to the perceived unfairness of wage cuts that makes them relatively rare. When wages have been cut, workers seem to respond as the theory would predict.3
3

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

It is theoretically unclear why fairness considerations matter so much more in employment transactions compared to other kinds of commercial transactions. One possibility is that employment and sales contracts fundamentally differ in the expectations parties have about when a contract has ended. As a thought experiment, suppose we bought a single widget from a vendor for $1. If we returned the second day and were told that a widget was now $1.01 (and the price was clearly posted for everyone as such), we may not like the change, but it would be difﬁcult to argue the change was unfair or exploitative. If, by contrast, we agreed to a price of $1 but when we were rung up, we were told the price had risen to $1.01, we would likely be rightfully angry at the unfairness of the new “offer.” The key difference is that in the ﬁrst scenario, after the initial widget was bought, the original contract was over and both parties would expect an additional transaction to require a new contract. In the second scenario, the contract was not yet completed, and so the new offer was an illegitimate, exploitative attempt to renegotiate. To bring us back to the labor context, an employer proposing a lower wage for the same set of tasks is trying to renegotiate the contract: the employment contract already speciﬁes the payment if the employer wants more of some task performed—namely, the price they already agreed on. In the Simon view, this ceding of discretion over tasks in exchange for a ﬁxed wage is the essence of employment. By contrast, a buyer with a sales contract proposing a lower price for an additional unit of task is not trying to renegotiate— once the task was completed, the original contract was completed.4 Even this conceptualization of fairness as being about contract completeness or agreed-on conclusion is not quite right: it is not viewed as unfair for workers (the sellers in a labor market) to propose a new,
laboratory experiments, observational data from real, long-term employment scenarios would be more convincing, but the circumstances needed for causal inference—idiosyncratic factors leading to wage changes for one group of workers but not for another—are rare. Occasionally, this kind of scenario occurs, and the evidence from them is consistent with the view that negative reciprocity can be (but is not necessarily) long lasting and harmful to the organization. Mas (2006) found that New Jersey police forces losing arbitration closed fewer cases for several months after the decision; Lee and Rupp (2007) found that airline pilots subject to large, industry-wide wage cuts were late more often. In the airline example, the effects were modest and transitory, perhaps because many airlines were at or near bankruptcy when the cuts were made, thus muting any fairness judgments.
4

In a real effort ﬁeld experiment Kube et al. (2013) cut wages, or more accurately, frustrated wage expectations, by advertising the job using ambiguous language and then exploiting this ambiguity to pay some workers (college undergraduates) less than they expected, causing substantial outputs in reduction. Although ﬁeld experiments offer a more compelling test of gift exchange than the

This employment versus sales contract distinction is similar to the fundamental distinction Bajari and Tadelis (2001) draw between ﬁxed-price and cost-plus procurement contracts, with the former analogous to the sales contract and the cost-plus contract analogous to the employment contract.

406

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

higher wage. It is however, generally viewed as unfair for the employer to propose a new, lower wage. By contrast, it is not considered unfair in other kinds of markets for buyers to propose a new, lower price— this happens regularly in markets where bargaining is commonplace (and where buyers try to obtain a lower price or a lower price for purchasing additional units of some good). The one exception to the “lower prices in markets for employment are unfair” is that context can alter these judgments. Fairness has made a somewhat slow entry into economics, with purported general theories of fairness not appearing until the 1990s, with fairness being somewhat narrowly deﬁned as preferences over the allocations received by others (Fehr and Gächter 2000, Fehr and Schmidt 1999, Rabin 1993). A central theme of this work is that individuals dislike inequality. As it is, those extant theories and the empirical underpinnings are still rather controversial (Binmore and Shaked 2010). However, empirical work has emphasized that preferences are more complex, with individuals showing a taste for social welfare (maximizing total payoffs that are received even if allocations are unequal) and for justice (i.e., punishing people who act selﬁshly) (Charness and Rabin 2002). That preferences are more complex than some simple rule such as disliking inequality should not be too surprising: the identiﬁcation of fairness-related “anomalies” has an older history even in economics. For example, Kahneman et al. (1986), who surveyed people on their beliefs about a variety of economic factors, found particularly strong opinions about labor market matters: Kahneman et al. (1986) found that people (a) view wage cuts that keep the ﬁrm solvent differently from cuts that increase already positive proﬁts or exploit market changes, and (b) see wage reference points as attached to speciﬁc workers in speciﬁc jobs and not transferable to new employees or as having any relevance after sufﬁciently large reorganizations. The special importance of fairness in labor markets was emphasized by Rees (1993), who offers a number of examples of “real-life” wage setting where fairness considerations were paramount. A common theme in all of the examples from Rees (1993) and from Kahneman et al. (1986) is the importance placed on context. The research on organizational justice also highlights the importance of context. Colquitt et al. (2001), in a summarization of 25 years of research on organizational justice, trace the early theories of what is “fair” (for example, a ratio of inputs to outputs in social exchange theory) to elaborations that put emphasis on the importance of the procedure by which allocations are determined, the equality of those allocations, and perceived need of the various parties. Many of these additional considerations are not simply about which allocations a process generates, but

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

rather the process itself, and additional factors such as the situation of the various parties. To illustrate the importance of context, to paraphrase slightly an example from Rees (1993), an ofﬁce worker learning he got a $2/hour raise would be quite pleased, but he would be considerably less pleased if he learned every other worker in the ofﬁce got a $3/hour raise (or even a $2.01 raise). Similarly, procedural justice matters—if the same ofﬁce worker learns that raises were based on strict seniority and he happened to be the most junior worker, he might view the situation very differently than if he learned that the bigger raises went to the college friends of the human resources director. In short, context matters, though as far as we are aware, there has been no uniﬁed theory that can map all of these various contexts to different popularly held fairness judgments. Context can be manipulated, either by design or by chance, but the only study we are aware of that exploits variation in context is Greenberg (1990), who looked at employee theft in two factories following a temporary 15% pay cut. Employee theft rose in both plants, but in the plant where the CEO spent over an hour explaining the cuts and ﬁelding questions, theft rose less than in the plant where management provided only a cursory explanation. The two plants were randomly assigned to treatment by the experimenter, though arguably the sample size was just two plants. In our study, if context can reduce or exacerbate quits by workers, then the implication is that workers are behaving like employees. For our purposes, we do have some guides regarding different contexts to try creating for our wage cuts (such as Kahneman et al. 1986), but we are far away from having a tight connection between a theory of human fairness judgments and what contextual manipulations to “try.” We cut wages under different experimentally induced contexts and then measured whether workers quit and refused to work at the new lower rate or accepted the new rate and performed one more unit of the task for lower payment. If workers accepted the wage cut, we could observe the quality of their work on the subsequent task.5 Based on how workers react to those cuts, we can infer whether they perceive themselves as
5

We are not the ﬁrst researchers to reduce wages experimentally— in two psychology studies from the early 1970s, one by Pritchard et al. (1972) and one by Valenzi and Andrews (1971), experimenters hired workers and randomly manipulated their wages or their perception of underpayment or overpayment. Valenzi and Andrews (1971) found that wage cuts, which the subjects knew were randomly determined, caused quits, but did not affect productivity. Pritchard et al. (1972) found that switching people between a wage and a quasi piece rate lowered productivity when a subject’s past experience with the payment system led them to believe they were underpaid in the new system. Our study differs in that workers did not know wage cuts were random nor did workers switch between wages and piece rates.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

407

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

party to a sales contract—in which there is no expectation of continuation, contracts are complete, and only offered wages matter—or party to an employment contract with reference points about wages, an expectation to continue working at the “old” wage, and concern with the fairness of any offer. It is ambiguous ex ante whether these fairnessmediated reactions would be more or less important online. In our experimental setting, the short duration, anonymity, and “one shot” nature of interactions could encourage the sales contract view. Because interactions are one shot, workers have less fear of being seen as an easy mark, as they might in a repeated interaction. Furthermore, the limited time frame makes the formation of reference points presumably more difﬁcult. The very low-stakes cut has an ambiguous effect: on one hand, the wage cuts are absolutely small and thus perhaps less offensive, yet the small stakes make principled refusals less costly. In conventional jobs, quitting has far greater consequences, and we would expect workers to be less sanguine about losing their jobs, even after a wage cut. The higher stakes and longer duration of real jobs would probably make any fairness judgment more strongly felt. These more salient fairness judgments might magnify the response to wage cuts, though perhaps reactions with direct income consequences for the worker (such as quitting) are less likely and withdrawal of cooperation is more likely. Why Should We Care About Online Labor Markets? To date, there has been relatively little work examining the nature of online labor markets. Most studies have focused on using them as a testing domain for some question of broader interest, such as Pallais (2014) and Pallais and Sands (2016). However, exceptions include Horton (2010), Kittur et al. (2013), and Horton (2011). These markets are relatively new, which could explain the lack of research, but another factor is that they have not received much mainstream attention. Historically, total labor income in the United States has been about two-thirds of the gross domestic product, or on the order of $8 trillion. By this standard, online labor markets are minuscule. Furthermore, MTurk is not the largest of online markets (see Ipeirotis 2010 for statistics on MTurk; see Agrawal et al. 2013 for statistics on oDesk (now Upwork)). However, they are worthy of research attention for several reasons beyond just their current size and the ease with which research can be conducted (see Horton et al. 2011 for an elaboration on this argument). First, they are labor markets that are so comparatively simple that they can potentially give us insights into markets more generally: in the same way that studying gravity and classical mechanics is easier without 2.1.

friction, studying markets without geography, social networks, strong institutions, and so on, can be clarifying. Second, these markets are serving as a testing ground for novel combinations of human intelligence and machine intelligence—particularly in the case of MTurk. The ability to obtain labor inputs algorithmically as part of a larger system is something genuinely new in the world, and perhaps unsurprisingly, some of the heaviest users of MTurk are academic computer scientists.6 It seems plausible that this academic interest could eventually translate into widespread application and perhaps fundamentally change how labor is supplied and demanded. 2.2. What Is Different About Online Work? The most obvious distinguishing characteristic of online work is that it happens online rather than by workers that are physically colocated. What is important about this is not likely to be the “online” face-toface aspect per se; although a Skype call or a Google hangout are not yet substitutes for face-to-face interactions, as communications technology improves, the qualitative differences between physical colocation and remote collaboration will presumably decline. What matters about online work is that it removes the role of geography. Geography profoundly shapes labor markets, in that throughout history, workers have needed to live relatively “close” to the productive capital and their fellow workers. This fact had implications not only for what was made where and by whom but also the human capital decisions made by individuals and even the degree of specialization of workers. The differences in the returns to different skills based solely on geography—and the arbitrage opportunity this creates—have been vividly characterized as “trillion dollar bills on the sidewalk” (Clemens 2011, p. 1). In addition to these arbitrage opportunities, online labor markets change individual incentives to acquire skills. They might even allow for more specialization as the extent of the market grows—in an online labor market, a worker can reasonably specialize in some skill for which total global demand might only be 40 hours per week.
6

To make this point more concretely, in the recent proceedings of the “Human Computation” workshop, which is part of the AAAI (Association for the Advancement of Artiﬁcial Intelligence) annual conference—the premier AI conference—use of MTurk was dominant. Of the 25 papers from the 2014 session, 23 were empirical, of which 16 used a marketplace for workers (as opposed to volunteers or users on social networking sites, for example). Of the 16 making use of online workers, all but 4 used Mechanical Turk— or 75%. Computer scientists are primarily building and testing new systems that take advantage of the “labor as a service” aspect of these markets.

408

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

There are other distinctive features of online work. In contrast to conventional markets, it is commonplace for workers to work for many different “employers” within a short amount of time, even in markets like oDesk/Upwork that “look” more traditional. Because projects tend to be short lived, there is far more individual variability in hours worked. The shorter duration of projects makes both the search and screening process more important, because they must happen so much more frequently. At the same time, the short duration of projects makes the stakes for any particular hire that much lower. The case of MTurk—where there is no employer screening at all—is the exception that proves the rule, in that the stakes are so low for any “hire” that the default is for any worker to work on a task without approval from the employer. Unsurprisingly, all online labor markets have created features that try to make the search and matching process easier and faster: would-be ﬁrms can post jobs, get applicants, screen applicants, and make a hire in a matter of hours. In the case of MTurk, because workers search over the pool of tasks, employers need to attract workers to their tasks if they hope to get them completed. Chilton et al. (2010) show that employers deliberately add and remove their tasks from the marketplace at a high frequency so that their tasks appear higher in the search results shown to workers. This stands in sharp contrast to the “normal” way that ﬁrms get work done, which is simply to instruct their employees to perform the task. Firms competing to get workers to complete their tasks—rather than competing for employees— does suggest that online labor markets can at least look like true labor spot markets.

their attitudes toward us as their employer/trading partner. The goal of this phase was to measure any change in worker “consummate performance” caused by the various treatments (Hart and Moore 2008). These games revealed little, and we conﬁne the reporting of game outcomes to Appendix B. Before discussing the experimental design in more depth, it is useful to explain the testing domain— MTurk—and how the conditions for causal inference can be met in this domain. We will also describe how we recruited subjects, the nature of the real-effort task, and the games we used to try to measure consummate performance. 3.1. Subject Pool Recruitment We recruited experimental subjects from MTurk, an online labor market. Through an interface provided by MTurk, registered users perform tasks (posted by buyers) for money. The tasks are generally simple for humans to do, yet difﬁcult for computers— common tasks are captioning photographs, extracting data from scanned documents, and transcribing audio clips. Buyers control the features and contract terms of the tasks they post: they choose the design, piece rate, time allowed per task, how long each task will be available, and how many times they want a task completed.7 Workers, who are identiﬁed to buyers only by a unique string of letters and numbers, can inspect tasks and the offered terms before deciding whether to complete them. Buyers can require workers to have certain qualiﬁcations, but the default is that workers can “accept” a task immediately and begin work. Once the workers submit their work, buyers can approve or reject their submission. If the buyer approves, MTurk pays the worker with buyer-provided escrow funds; if the buyer rejects, the worker is paid nothing. Although most buyers post tasks directly to MTurk, it is possible to host tasks on an external site that workers reach by following a link. We used this external hosting method; we posted a single placeholder task containing a description of the work and a link to follow if subjects wanted to participate. When they completed all stages of the experiment on the external site, they received a completion code that they entered into the original MTurk interface. Figure A.1 shows the landing page that all subjects ﬁrst arrived at, regardless of assignment. 3.2. Real-Effort Task For the real-effort task, subjects transcribed (not translated) paragraph-sized chunks of Adam Smith’s The Wealth of Nations. A sample paragraph—and the interface subjects used to transcribe the paragraph—is
7

3.

Empirical Context, Conditions for Causal Inference, and the Real-Effort Task Employed

The experimental design was quite simple, even if the particular details of each cell—and the motivations for including that cell in the experiment—require more detailed explanation. We will ﬁrst describe the experiment in broad strokes and then introduce the details of cells when we present the results. The experiment had three phases: (1) the wage expectation building phase, (2) the treatment phase, and (3) the “games” phase. In phase (1), workers performed a real-effort task at a high piece rate. Then, in phase (2), they were assigned to different treatment groups and, in most cases, offered a new lower wage to perform an additional unit of the task. This wage cut was justiﬁed in different ways across cells. If workers accepted the offered wage, they performed one more unit of the task. Finally, in phase (3), all subjects played a series of contextualized games designed to measure

Tasks are often done multiple times by different workers for qualitycontrol purposes.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

409

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

shown in Figure A.2. This task was so sufﬁciently tedious that no one was likely to do it “for fun,” and it was sufﬁciently simple enough that all market participants could do the task. The source text was machine translated into Dutch and blurred, which increased the error rate of the transcriptions, thereby providing a more informative measure of work quality. Translating the text also prevented subjects from ﬁnding the text elsewhere on the Internet.8 Although this task might seem unusual and raise suspicions that the workers were in some kind of experiment, we view this as unlikely. Transcribing text from images is a commonplace use of MTurk. In fact, it is one of the few tasks that Amazon now supports with premade templates and pools of vetted workers (though this was not the case when our experiment was run). Figure A.9 shows the template and shows other, similar commonplace MTurk tasks. 3.3. Conditions for Causal Inference To identify causal effects from the treatments, we needed ways to (a) as good as randomly assign subjects to different groups, (b) keep subjects from changing groups once assigned, (c) keep subjects from participating multiple times, and (d) minimize nonrandom attrition. These problems are somewhat more challenging in an online setting; Horton et al. (2011) discuss these challenges and explain how they can be addressed, as well as discussing the many advantages of using online populations—particularly those in labor markets—as experimental subjects. Steelman et al. (2014) also highlight the usefulness of sampling from nontraditional populations accessed online. For (a), we sequentially assigned subjects to treatment groups as they accepted our task, stratifying on arrival order. For example, arrival 1 went to the ﬁrst cell, arrival 2 to the second cell, and so on, and when all cells were exhausted, the next subject would go to the ﬁrst cell again. Unbeknownst to subjects, clicking on our link initiated the experimental group assignment. Because subjects neither knew nor could control their relative arrival order, this assignment mechanism meets the unconfounded assignment condition necessary for estimating causal effects. In fact, this method provides more precise estimates of causal effects. The reason for the improved precision is that stratiﬁcation on arrival order improves balance on arrival times, which in turn are correlated with demographics (because of time zones) and behaviors
8

(early arrivals are more likely to be heavy users of the site). Our method for achieving balanced samples was effective: for the binary covariates of gender, resident of the United States or Canada, and whether the worker spends more than 10 hours a week online doing tasks for money, none of the p-values for a 2 test are conventionally signiﬁcant. The actual demographic survey used to collect these measures is shown in Figure A.8. Note that despite stratiﬁcation, because of attrition during the prerandomization phase, the realized sample size for each experimental group differs by more than one.9 For (b), if workers were aware of the different treatment groups, they would have an incentive to get into a group with a larger payoff. Even though subjects were unaware of the other treatments, to prevent the possibility of subjects hunting for the “best” treatment group, the software assigning subjects to groups tracked users’ IP addresses. This tracking prevented subjects from changing their assignments after their initial “assignment” clicks. For (c), workers with multiple online identities could in principle complete our task multiple times. However, this double-dipping is unlikely; because buyers often want the same task done multiple times by different workers, MTurk designed several policies and software features to prevent a worker from having more than one account.10 For (d), to prevent differential attrition driven by differences among treatments, all subjects had identical initial experiences during the wage expectation building phase. Subjects’ experiences differed by group only after already performing three transcriptions. Because of this investment, there was no attrition after subjects observed their own treatment. An important point is that although we assigned subjects to treatments at the moment they accepted the task, the conceptual point of randomization was after the wage expectation stage, but before the treatment stage. The prerandomization attrition is easy to spot in the data: attriting subjects just stopped working before the end of the paragraph, never get to the treatment stage, and never submit their completion code to receive payment. We dropped from the sample any worker making more than 100 errors on a paragraph. This occurred for a little less than 20% of all subjects who
9

If we had no attrition, then stratiﬁcation should lead to sample sizes differing by at most one, by experimental group.
10

Since the text was presented as images, subjects were unable to copy and paste the text into the text box on the form. It is irrelevant whether or not any subjects actually knew Dutch since, if anything, knowledge of the language might make the task more difﬁcult given the poor quality of the translation. One subject that apparently did speak Dutch sent an email warning us that the work was “grammatical gibberish.”

Workers must agree to have only one account—any detected attempt to have multiple accounts leads to a permanent ban. MTurk also requires browsers to accept cookies. If a person had two bank accounts or credit cards and two separate computers not sharing a network connection, they could in principle participate twice, but given the low stakes and risks involved (if this was detected, they would be banned from the site), we consider this possibility highly unlikely.

410
Figure 1 (Color online) Experimental Design

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

Pool of Subjects

CONTROL: Will you transcribe an additionAL paragraph for 10 cents? WAGECUT: Will you transcribe an additional paragraph for 3 cents? ONLYGAMES: (No 4th ¶—Straight to Games) PRODUCTIVITYWC: “We find that workers do this task much faster after doing three paragraphs, would you be willing to do an additional paragraph for 3 cents?” PROFITWC: “We are trying to get this work done for as little money as possible, would you be willing to do an additional paragraph for 3 cents?” NEIGHBORWC: “We find that some workers are willing to a fourth paragraph for 3 cents. Would you be willing to do an additional paragraph for 3 cents?” Subject asked “What is a fair wage?” Do 4th ¶? Trust Cooperation and Patience Games

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Worker agrees to perform 3 ¶ transcriptions at 10 cents per ¶ Sample paragraph is shown

UNFRAMEDWC: (Straight to 4th ¶)
FAIRNESSPRIMEDWC: Will you transcribe an additional paragraph for 3 cents?

Transcription

Note. This ﬁgure illustrates the ﬂow of subjects through the experiment.

started the paragraphs, but since these individuals were dropped prior to conceptual randomization, this kind of attrition is immaterial. 3.4. Measuring Outcomes Following the Wage Cut We can easily measure whether a worker was willing to transcribe a fourth paragraph following a wage cut. Measuring the quality of their work is somewhat more challenging. To measure work quality, we calculated the minimum number of “edits” that would be required to transform the provided transcription to a perfect transcription of the underlying text. For example, if the worker typed “infromation” instead of “information,” we could ﬁx the error with one simple transposition edit, namely, by switching the “r” and the “o.” Note that this would be fewer edits than the two-edit adjustment of changing the “r” to an “o” and the “o” to an “r.” This minimum-number-of-edits metric is called the edit distance, or the Levenshtein distance. In addition to output and work quality, we also had subjects play a series of contextualized games with us to measure their “consummate performance” on the task. The design of these games and the results from the experiment are in Appendix B.

phase. In the wage expectation building phase, subjects transcribed three paragraphs at a rate of 10 cents per paragraph.11 If subjects ﬁnished three paragraphs, they moved to the treatment phase. In the treatment phase, subject experiences differed based on group assignment. Generally, subjects were offered some new piece rate to transcribe a fourth paragraph. When a lower wage was proposed, it was always 3 cents—a 70% reduction from their previous wage. Subjects assigned to Control (n = 23) were given the option to transcribe another paragraph at their previous 10 cent piece rate. Figure 2 shows the interface used for Control and lists the exact language used for the group where subjects were asked if they wanted to transcribe an additional paragraph. Screenshots of all experimental interfaces and instructions are in Appendix A. Subjects in WageCut (n = 17) were offered the option to transcribe another paragraph, but at a rate of only 3 cents. In FairnessPrimedWC (n = 24), subjects were ﬁrst asked what they considered a fair wage for the task, before receiving the 3 cent offer. Subjects in OnlyGames (n = 21) were not given the option to transcribe an additional paragraph, but instead went
11

4.

Experiment Design and Methodology

Figure 1 shows how subjects ﬂowed through the experiment, from the expectation-building transcription phase, to the treatment phase, and ﬁnally, the “games”

A paragraph takes about 100 seconds to enter, so the offered payment of 10 cents per paragraph is equivalent to $3.60/hour ($28.80 per day). It is challenging to assess wages on MTurk, because prices are for tasks, not for time, but evidence suggests that the wage we paid is considerably higher than the median reservation wage on the platform. Horton and Chilton (2010) estimates that reservation wages are approximately log normally distributed and centered at $1.38.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

411

Figure 2

(Color online) Offer to Perform an Additional Transcription Presented to Subjects in Control

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Notes. After subjects completed the initial three paragraphs, they were, in most cases, presented the offer to do an additional paragraph. This is a screenshot for the offer presented to subjects assigned to Control.

straight to the games phase. We also had three groups that were given the option to continue working at the lower rate of 3 cents, but the offer was justiﬁed differently across groups. Justiﬁcations were based on the following: workers get faster over time, ProductivityWC (n = 25); our need (as employers) to pay as little as possible, ProﬁtWC (n = 23); and our observation that many other workers were willing to accept this wage cut, NeighborWC (n = 25). We also had an UnframedWC (n = 26) group, in which workers were simply brought to a fourth paragraph, without an explicit framing of a decision to keep or stop working, but with the fourth paragraph clearly labeled as only paying 3 cents. Following the treatment phase, regardless of whether they accepted the wage cut, all workers continued on to the games phase.

Column (1) of Table 1 reports an estimate of AddPara =
0

+

1 WageCut +

(1)

where AddPara is an indicator for whether the worker performed the additional paragraph transcription and WageCut is an indicator that the subject was assigned to that group (we will use this indicator/ group name convention throughout the paper). The comparison group in the regression is Control. In terms of magnitude, we can see that a 70% reduction in wages (10 cents to 3 cents) led to an approximately
Table 1 Effect of a Wage Cut on Willingness of the Worker to Perform an Additional Task Dependent variable Transcribe additional paragraph? (AddPara = 1) (1) Offered 3 cents for fourth transcription, WageCut From U.S./Canada Male Prior error Constant Comparison group Observations R2 0 739∗∗∗ 0 099 Control 40 0 109 −0 327∗∗ 0 152 (2) −0 437∗∗∗ 0 155 0 179 0 192 −0 296 0 185 −0 234 0 162 1 623∗∗ 0 640 Control 40 0 282

5.

Results

5.1. Accepting an Unexplained Wage Cut We ﬁrst test whether being offered a lower wage caused fewer workers to accept our offer to transcribe a fourth paragraph. At a lower offered piece rate, we expected fewer workers to accept the offer, since for some fraction, the marginal cost of doing another transcription outweighs the marginal beneﬁt. This test cannot distinguish between the sales contract and employment views, but it does let us verify that workers ﬁnd the task costly and that precise amount offered matters to workers. The two relevant experimental cells for this question are Control and WageCut. Control. Subjects were offered their previous high wage of 10 cents to perform an additional transcription. WageCut. Subjects were offered a lower wage of 3 cents to perform an additional transcription. No explanation was given for the lower wage offer. The precise language of the offer was “Will you transcribe an additional paragraph for 3 cents?” Table 1 shows that, as expected, substantially fewer workers are willing to transcribe a paragraph when the offered rate is 3 cents instead of 10 cents.

Notes. This table reports two OLS regressions where the dependent variable is an indicator for whether the subject was willing to transcribe a fourth paragraph. In the control (the omitted group), Control, the offer was 10 cents—the same piece rate as for the previous work. For the treated group, WageCut, the offer was 3 cents. In column (2) regressors are added for several pretreatment variables, including the indicators for whether the subject was from the United States or Canada, a self-reported male, and a continuous variable that is the log cumulative prior errors on the three previous paragraph transcriptions. Robust standard errors are reported. ∗ p ≤ 0 10; ∗∗ p ≤ 0 05; ∗∗∗ p ≤ 0 01.

412

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

30% reduction output, implying an elasticity of labor supply of about 0 42. In column (2), several pretreatment subject characteristics are added to the regression model. With these added covariates, the treatment effect increases slightly relative to column (1), but this estimate is well within the 95% conﬁdence interval for the column (1) treatment effect, as we would expect for a true experiment. There is some evidence that self-reported males were less likely to do the follow-on task, as were those that made a large number of prior errors during the expectation-building phase. However, none of the coefﬁcients are conventionally signiﬁcant. We can test for whether there are heterogeneous treatment effects by comparing the column (2) regression model with one in which the treatment indicator is interacted with all of the pretreatment covariates. When we perform this test, we fail to reject the null hypothesis that the coefﬁcients on the interaction terms are all zero, p = 0 414 , making substantial treatment effect heterogeneity unlikely, at least within the limits of the statistical power available. 5.2. Is Offer Rejection About Fairness? That fewer workers accept the offer to do an additional task is unsurprising—it is in a sense, overdetermined, because it is predicted by both the employment and sales contract views. However, we can partially test whether workers have an “employment-like” view that a lower wage offer is a contract breach by (1) getting subjects to think about fairness with respect to the task and then (2) presenting them the low wage offer. Some subjects were assigned to a cell FairnessPrimedWC, in which they were ﬁrst asked what would be a fair rate for the task before receiving the 3 cent offer. Figure A.3 shows the interface used to present the “fairness” question to users. FairnessPrimedWC. Subjects were offered a lower wage of 3 cents to perform an additional task. No explanation was provided, but we ﬁrst asked subjects what they thought would be a fair wage for doing an additional task. For the sake of brevity, we simply describe the results rather than present them in regression tables. Comparing acceptances in FairnessPrimedWC to our WageCut cells, acceptance of the offer was 41% in WageCut and 33% in FairnessPrimedWC, or an 8 percentage point difference. Despite this lower uptake when fairness was primed, the standard error for the differences in means is slightly more than 15 percentage points and the p-value of a t -test for a two-sided means comparison is p = 0 63. Clearly we cannot reject a null hypothesis of no effect from priming, but the direction of the effect is what we would expect if worker decision making was mediated by fairness concerns (assuming a 3 cent offer after a 10 cent offer would be viewed by most as unfair).

5.3. The Effects of Justifying the Wage Cut Providing some justiﬁcation for a wage cut might change a worker’s probability of accepting that wage cut. For example, a “reasonable” justiﬁcation might induce the worker simply to consider the offer relative to their reservation wage for the task. By contrast, “unreasonable” justiﬁcations might even increase the cost of accepting the lower offer. To return to the question of how we should characterize relationships in this market, in the sales contract view, justiﬁcations should be immaterial, because they do not change payoffs, but they would matter in the employment view. To test this notion that justiﬁcations can alter wage cut acceptance, we randomized subjects to cells with varying justiﬁcations. ProductivityWC. Subjects were offered a lower wage of 3 cents to perform an additional task, but we ﬁrst informed them that most workers can complete tasks more quickly after they gain some experience. The precise language was “We ﬁnd that workers do this task much faster after doing three paragraphs, would you be willing to do an additional paragraph for 3 cents?” ProﬁtWC. Subjects were offered a lower wage of 3 cents to perform an additional task, but we ﬁrst informed them that we are trying to get as much work done for as little money as possible. The precise language was “We are trying to get this work done for as little money as possible, would you be willing to do an additional paragraph for 3 cents?” NeighborWC. Subjects were offered a lower wage of 3 cents to perform an additional task, but we informed them that in our experience, other workers are willing to accept the lower offer.12 The precise language was “We ﬁnd that some workers are willing to do a fourth paragraph for 3 cents. Would you be willing to do an additional paragraph for 3 cents?” The motivations for ProductivityWC and ProﬁtWC were to contrast “reasonable” and “unreasonable” justiﬁcations for the wage cut. Although ProﬁtWC is unrealistic in the sense that no real ﬁrm would propose a wage cut in that manner, it is still a useful treatment in that it lets us test how workers react to an extreme, insulting justiﬁcation: ProﬁtWC tests whether any justiﬁcation—no matter how insulting—“works.” Perhaps any justiﬁcation would be effective—Langer et al. (1978) showed that asking someone to let you cut in line to make copies “because you need to make copies” dramatically increases compliance compared to just asking without the justiﬁcation. NeighborWC tests the notion that workers “learn” how to react from other workers. One reason why
12

This was a true statement. From earlier pilot experiments, we knew that some subjects were willing to work at this lower wage.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

413

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

NeighborWC might mitigate the behavioral response to a wage cut is that perhaps the psychic cost of taking work below one’s past wage comes not just from a fear of being exploited, but also from a fear of being singularly exploited. The notion that social comparisons could affect the response to wage cuts has some empirical support—Luttmer (2005) found that individuals feel worse off when others around them earn more. Consistent with the employment view but not the sale contract view, the empirical results in Table 2 show that justiﬁcations do matter, with “reasonable” justiﬁcations substantially increasing the fraction of workers willing to work at the new, lower piece rate. Compared to the unexplained cut in WageCut, however, the unreasonable explanation, ProﬁtWC, did not substantially decrease acceptance, at least within the limits of available statistical power. We ﬁrst consider how each justiﬁcation by itself affected a worker’s willingness to perform the fourth transcription at the new rate. Column (1) of Table 2 reports an estimate of AddPara =
0

+

1 ProductivityWC

+ +
Table 2

2 ProﬁtWC 3 NeighborWC +

(2)

where the sample consists of all of the justiﬁcation groups and the unexplained group, with WageCut being the comparison group. The coefﬁcient on the productivity justiﬁcation indicator is positive and conventionally signiﬁcant, with nearly a 30 percentage point increase in the fraction of workers accepting the lower offer. The coefﬁcient on the neighbor justiﬁcation is smaller, with the effect being about 23 percentage points. It is not conventionally signiﬁcant, though the difference between the neighbor and the productivity coefﬁcients would itself not be signiﬁcant. The proﬁt justiﬁcation is negative and nontrivial in magnitude—the effect is about an 11 percentage point reduction in task acceptance—but it is far from conventionally signiﬁcant. In column (2), the outcome is the same, but we pooled what we considered “reasonable” justiﬁcations, ProductivityWC and NeighborWC, and compared them to what we considered unreasonable or missing justiﬁcations, WageCut and ProﬁtWC. Here the difference is stark, with the reasonable justiﬁcations increasing output by nearly 30 percentage points. This greater effect is attributable to the roughly similar magnitudes for the coefﬁcients on NeighborWC and ProductivityWC and the negative and fairly large coefﬁcient on the proﬁt justiﬁcation indicator. The Effects of Explicitly Asking About Task Continuation One of the key differences between the sales and employment contract is that in the employment contract, the expectation is that the project will continue until explicitly dissolved. By contrast, the sales contract is ended when the speciﬁed task is completed. We wanted to test whether simply presenting workers the additional transcription task, labeled with the new 3-cent wage, would cause a different reaction from workers, relative to ﬂagging the proposed explicitly new wage and requiring workers to give a yes or no answer to whether they wanted to continue. To test whether this offer framing mattered, we created a cell UnframedWC. UnframedWC. Subjects were not explicitly offered a lower wage to perform an additional task. Rather, they were simply brought to the next task, which was clearly labeled with the new, lower wage of 3 cents. There was a button at the bottom of the screen that allowed them to quit and continue to the follow-on contextualized games. Consistent with the employment contract view, Table 3 shows that this “unframed” wage cut was remarkably effective, in that all subjects assigned to UnframedWC transcribed the fourth paragraph. First we consider output in the unframed cell relative to the explicit, unjustiﬁed wage cut of WageCut. Column (1) of Table 3 reports an estimate of AddPara =
0

5.4.

The Effects of Wage Cut Justiﬁcations on Worker Output Choice Following a Wage Cut Dependent variable Transcribe additional paragraph? (1) (2)

Neighbor justiﬁcation, 0 228 NeighborWC 0 152 Productivity justiﬁcation, 0 268∗ ProductivityWC 0 152 Proﬁt justiﬁcation, −0 107 ProﬁtWC 0 155 “Reasonable” justiﬁcations (NeighborWC + ProductivityWC) Constant 0 412∗∗∗ 0 118 Comparison group Observations R2 WageCut 90 0.101

0 310∗∗∗ 0 102 0 350∗∗∗ 0 076 WageCut + ProﬁtWC 90 0.095

Notes. This table reports two OLS regressions where the dependent variable is an indicator for whether the worker subject was willing to transcribe a fourth paragraph. Comparisons are being made against WageCut, in which the offer was 3 cents and no justiﬁcation was offered for the cut. In column (1), indicators for each justiﬁcation—productivity increases, employer proﬁts, and comparison to “neighbor” choices are used as regressors. In column (2), the “unreasonable” proﬁt justiﬁcation is pooled with the unexplained cut and the two “reasonable” justiﬁcations from NeighborWC and ProductivityWC are pooled as an indicator for reasonable justiﬁcations. Robust standard errors are reported. ∗ p ≤ 0 10; ∗∗ p ≤ 0 05; ∗∗∗ p ≤ 0 01.

+

1 UnframedWC +

(3)

414
Table 3

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

The Effect of Not Explicitly Framing the Wage Cut as a Decision to Continue Working Dependent variable Transcribe additional paragraph? (AddPara = 1)

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

(1) Unframed wage cut, UnframedWC Constant Comparison group Observations R2 0 588∗∗∗ 0 101 0 412∗∗∗ 0 078 WageCut 42 0.460

(2) 0 261∗∗∗ 0 090 0 739∗∗∗ 0 065 Control 48 0.155

These caveats aside, the evidence does suggest that explicitly ﬂagging a wage change has a different effect than imposing the same change surreptitiously. If workers regard their relationships with the buyer as an employment contract, this continuation of work—without any explicit discussion of prices or decisions—is what they would expect to happen normally. By contrast, a worker with the sales contract view who was told ex ante he would perform three paragraphs would be very surprised by an additional task, because the contract was at that point dissolved. Comparison of Output by Offer Size, Justiﬁcation, and Explicitness of Follow-On Offer The pattern of output results can be more clearly seen in Figure 3. We have divided reasons into reasonable and unreasonable with reasonable based on our own judgment. The plots show that reasonable justiﬁcations keep output levels quite close to those in the 10cent offer Control group. The remarkable uptake in UnframedWC is also apparent, as well as the much lower output in the cells with no justiﬁcation or an unreasonable justiﬁcation. 5.6. Follow-On Task Performance Workers might be willing to accept a wage cut and then retaliate by offering poor performance on the task, such as by introducing many errors in the transcription. This is the consensus view for why ﬁrms are so averse to cutting wages. In our experimental setting, detecting retaliation is challenging because of selection. We know that the different cells have different task acceptance rates. As such, any differences in follow-on error rate could reﬂect some combination of selection and treatment. We can potentially deal with this selection problem by controlling for transcription quality from the ﬁrst phase of the experiment. It is the case that error rates in the fourth paragraph are highly correlated with the error rate in the initial three paragraphs: when pooled, the point estimate of the correlation coefﬁcient is = 0 33 and a 95% conﬁdence interval of 0 14 0 56 .13 Table 4 shows that there is no evidence that workers willing to accept the wage cut were any worse overall, relative to Control. However, there is some evidence that workers assigned to the unreasonable ProﬁtWC produced worse transcriptions, even when controlling for output quality during the ﬁrst phase of the experiment. However, it is important to remember that even with our prior error controls, we cannot credibly estimate causal effects. First, we test whether workers assigned to any of the wage cut cells had worse performance compared to
13

5.5.

Notes. This table reports OLS regressions where the dependent variable is an indicator for whether the worker subject was willing to transcribe a fourth paragraph. In column (1), the independent variable is an indicator for assignment to UnframedWC, the unframed wage cut. The comparison group is WageCut. In column (2) the comparison group is instead Control, in which subjects received a 10 cent offer to perform an additional transcription. Robust standard errors are reported. ∗ p ≤ 0 10; ∗∗ p ≤ 0 05; ∗∗∗ p ≤ 0 01.

The comparison group is WageCut, the unexplained wage cut. The coefﬁcient on the “unframed” indicator shows that not explicitly framing work continuation as a choice leads to a very high acceptance of the wage cut: in fact, 100% of subjects in UnframedWC performed the follow-on paragraph transcription, compared to just a little more than 40% of subjects in WageCut. Column (2) reports the same regression as column (1), but with the comparison group being the Control group instead of WageCut. When we compare the unframed wage cut group to the subjects that had no wage cut at all, we can see that the unframed group had about a 25 percentage point increase over those not receiving a wage cut at all. In other words, not framing a decision to continue working led to far more output than even offering the previous rate. There are several possible interpretations of these results. It is possible that subjects failed to consider their option of refusing to do the task by simply entering a poor transcription (the most extreme form being leaving the text box blank). Another possibility is a worker might believe they were in error: if a worker thought she had read the instructions incorrectly, she might worry that a skipped transcription would jeopardize payment for work already completed. Beyond just payment, workers might worry about their reputation. MTurk does have a rudimentary reputation system, in that buyers can “reject” a worker’s submission and future buyers can screen workers ex ante based on their rejection rates, making rejections consequential. However, there is no rich reputation system with bilateral feedback and nonbinary rating options common in other electronic commerce sites (Dellarocas 2006), including in other online labor markets (Moreno and Terwiesch 2014).

Standard errors for the correlation coefﬁcient were calculated with one million bootstrap replications.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

415

Figure 3

Fraction of Subjects Accepting the Offer to Perform an Additional Transcription Task
Control 1.00 Reasonable Unframed Unreasonable/None

Fraction of subjects completing an additional paragraph

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

0.75

0.50

0.25

L

C

C

C

C

C

O

BO R

ED

O N

ED

VI T

FI

AM

IM

C

O

G H

D U C

N

N

PR

Experimental groups

Table 4

Transcription Errors in Fourth Paragraph Among Workers Willing to Accept the New Offer Dependent variable Log edit distance for the fourth paragraph (1) (2)

those who were offered their previous rate. Column (1) of Table 4 reports an estimate of log EditDist4 =
0

FA

IR

N ES

O

U

SP R

EI

FR

+

1 AnyWageCut

PR

+ log
k∈ 1 2 3

EditDistk +

W

TI

AG

EC U

YW

W

TW

W

TR

W

T

(4)

AnyWageCut FairnessPrimedWC NeighborWC ProductivityWC ProﬁtWC UnframedWC Prior error Constant Comparison group Observations R2

−0 060 0 098 0 020 0 192 0 068 0 170 −0 007 0 167 0 491∗∗ 0 206 0 087 0 159 0 290∗∗∗ 0 082 1 540∗∗∗ 0 319 Control 96 0.117 0 264∗∗∗ 0 092 1 500∗∗∗ 0 375 WageCut 79 0.168

Notes. This table reports OLS regressions where the dependent variable is the log edit distance for the fourth paragraph. In column (1), the independent variable is an indicator for assignment to any group where wages were cut. The comparison group is Control. In column (2) the comparison group is instead WageCut and regressors are indicators for the remaining cells in which wages were cut. Both regressions include controls for worker error in the ﬁrst phase of the experiment. This prior error control is the log of the sum of edit distances for the ﬁrst three paragraphs transcribed. Robust standard errors are reported. ∗ p ≤ 0 10; ∗∗ p ≤ 0 05; ∗∗∗ p ≤ 0 01.

where EditDist4 is the edit distance for the fourth paragraph and AnyWageCut is an indicator that the subject was assigned to any of the wage cut groups. Note that we control for the log cumulative error in the ﬁrst three paragraphs, which should at least partially address the possibility that selection effects drive differences in error rates. The coefﬁcient on AnyWageCut is negative—implying fewer errors—but is also imprecisely estimated and far from conventionally signiﬁcant. In column (2), we decompose AnyWageCut into the indicators for the different cells and use WageCut as the comparison group. This lets us test whether any of the framings or justiﬁcations among workers accepting the wage cut affected output. Here we see that the coefﬁcient on ProﬁtWC is large and positively signiﬁcant, with nearly a 50% higher error rate. This is suggestive evidence that workers in the proﬁts justiﬁcation retaliated, but there are two important caveats. First, this group had the lowest acceptance rate and so any selection effects would be particularly strong for this group. Second, we are looking at ﬁve different treatment cells and only one is conventionally significant. On the other hand, for theory reasons, we also have reason to believe that if there was retaliation, it would be more likely in ProﬁtWC.

416

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

6.

Conclusion

Our main positive ﬁnding is that framing can strongly affect the behavioral response to a wage change. Workers reduce output—and possibly work quality—when wages are cut for capricious or selﬁsh-seeming reasons, but workers do not reduce output as much and apparently are still willing to cooperate if the employer has seemingly valid reasons for her actions. Workers on MTurk exhibit the same fairness-mediated reactions to wage cuts thought to characterize worker reactions to wage cuts in conventional employment relationships. Furthermore, the results from the “unframed” wage cut experiment suggest that workers have a bias toward simply continuing to work rather than viewing the completion of the last agreedon task as the end of the contract. Although this is not the only interpretation, it is some evidence toward the employment characterization of the relationships that are created (at least from the worker’s perspective). As such, it seems unlikely that ﬁrms hiring in online labor markets can treat their interactions as spot-market transactions, but must instead consider the quasi-employment nature of the relationships they seem to create. An interesting question for future research is to what extent this employment-like characterization makes an economic difference in what can and cannot be done in which kinds of markets. The existence— and to date, persistence—of very different kinds of platforms for online work seems to indicate that having a variety of platforms is useful for different purposes, but each raises its own challenges. For example, Boudreau et al. (2011), looking at paid programming contests, illustrates the power of the “open call” format for certain kinds of problems, in that the probability of solution rises with more entrants, and yet the contest version of a wage cut—adding more competitors—can reduce incentives. They show that for highly uncertain problems, the payoff to adding more competitors dominates, but not for more wellspeciﬁed projects. If sufﬁciently well speciﬁed and certain, the innovation “contest” of one—a single hired worker might dominate. Going back to our MTurk domain, for certain kinds of problems, an employeelike mindset is desirable, as the high-powered incentives in a sales contract can motivate workers toward both cost savings (good) but also corner cutting and opportunism (bad). Our results come from MTurk, which is just one of several online labor markets.14 However, some of the characteristics that MTurk exhibits in the extreme—
14 Other markets include Upwork (the combined marketplace of the merged oDesk and Elance marketplaces), Freelancer.com, Fiver, 99Designs, and several others.

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

very short-lasting relationships, relatively small stakes, computer-mediated work relationships—are appearing in a variety of labor markets. In recent years, we have seen the rise of new forms of computermediated marketplaces, including Uber, Lyft, TaskRabbit, Handy, Instacart, and so on, that share some similarities. These markets are capturing so much attention because they raise so many questions. On the positive side, the greater ﬂexibility they offer workers seems valuable, expanding the set of options available to workers. It is an open research question as to how much value workers place on the ﬂexibility these kinds of markets afford them—the ability to work when they want, at terms that they set. On the negative side, these markets have received criticism for potentially transferring power away from individual workers and putting them more at the whim of the platform of would-be buyers. More research on the nature of the relationships created in these platforms and the larger implications of online work is needed.

Acknowledgments
The authors thank Kelsey Jack, Nolan Miller, Jeffrey Liebman, and Richard Zeckhauser for helpful discussions. Work on this project was conducted while the ﬁrst author received ﬁnancial support from the Institute for Humane Studies, the John M. Olin Foundation, and the Petrie-Flom Center. The second author received ﬁnancial support from the Harvard Multidisciplinary Program in Inequality and Social Policy [NST IGERT 0333403]. The authors gratefully acknowledge funding from the Berkman Center for Internet and Society at Harvard Law School. A draft of this paper previously circulated under the title “The Wages of Paycuts.”

Appendix A. Experimental Materials
The actual surveys used for each of the experimental groups are hosted at the following URLs: • Control: http://www.surveymonkey.com/s/R23DT7Y • WageCut: http://www.surveymonkey.com/s/R23FZHW • OnlyGames: http://www.surveymonkey.com/s/R2G53NM • ProductivityWC: http://www.surveymonkey.com/s/R2BXXM7 • ProﬁtWC: http://www.surveymonkey.com/s/R2H6QQ2 • NeighborWC: http://www.surveymonkey.com/s/R2HMWZR • UnframedWC: http://www.surveymonkey.com/s/R2699DW • FairnessPrimedWC: http://www.surveymonkey.com/s/R2Z2SH9

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

417

Figure A.1

(Color online) Initial Instructions Page

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Note. This was the ﬁrst page that all participants—regardless of experimental group—ﬁrst landed on.

Figure A.2

(Color online) Transcription Environment for All Paragraphs

Notes. All subjects transcribing paragraphs used this interface for both the initial three paragraphs and the additional paragraph. Note that the task value differed depending on the treatment assignment.

418
Figure A.3 Fairness Priming Question in FairnessPrimedWC

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

Note. Subjects in FairnessPrimedWC were asked what was a fair wage for a paragraph, using this dialog.

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Figure A.4

(Color online) “Trust Game” Dialog

Note. This screenshot shows the instructions and choices for subjects playing the contextualized “trust game.”

Figure A.5

(Color online) Follow-On Survey Dialog

Note. This screenshot shows the interface in which we presented subjects with the opportunity to take a follow-on survey at a later date.

Figure A.6

(Color online) “Patience” Dialog

Note. This screenshot shows the interface in which we presented subjects with the opportunity to delay getting paid in exchange for a larger payment at a later date.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

419

Figure A.7

(Color online) “Advice” Dialog

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Note. This screenshot shows the interface in which we asked subjects to provide advice to other workers.

Figure A.8

(Color online) Demographics Dialog

Notes. This screenshot shows the interface in which we asked subjects to provide some demographic information. It also shows the completion code link that subjects used to generate a code that could link their responses to their MTurk identities.

420
Figure A.9

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

(Color online) MTurk Tasks That Amazon Supports via Premade Templates and Vetted Workers

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Notes. This is a screenshot of the Amazon Mechanical Turk interface for starting a new project from a template. The column of text on the left side of the image shows the common use cases for MTurk. The “Transcription from an Image” task is selected and shown, which is the kind of task we used for our experiment.

Appendix B. Consummate Performance Measures
When we designed the experiment, we also wanted to look for other indications of work effort and cooperation beyond just acceptance of the wage cut. To that end, we had subjects play a number of contextualized games to measure “consummate performance.” This part of the experiment had generally uninteresting results—in part because of the relatively low power of the design to detect anything but very large effects. We do present the results here in an appendix for the beneﬁt of interested parties and for completeness. B.1. Consummate Performance Measures To measure consummate performance, subjects played a collection of contextualized games that we hoped would not seem unusual to the subjects. These games tried to measure worker patience, trust in us, and cooperation with us. To measure patience, we simply asked subjects whether they would be willing to forgo nearly immediate payment in exchange for a 30-cent bonus to be paid out in two weeks. We measured trust with a modiﬁed version of the trust game (Glaeser et al. 2000). Subjects were told to imagine they had $15 and that they could choose any fraction of that money to “send” to us. If we judged their work favorably, we would double what they sent and “send it back”; in the event of an unfavorable judgment, we would keep

whatever they sent.15 To save money, we told subjects that we would select one player at random and implement her choices. Although this game measures the worker’s trust in us (e.g., trust that we will follow our own protocol and judge their work fairly), it is confounded with the worker’s conﬁdence, risk aversion, and beliefs about our standards for the work. If these nontrust factors held constant across experimental groups, we could in principle detect changes in trust, though this assumption of constant effects is questionable. In any event, these factors should be unaffected by the treatment for the subjects in a cell we created in which subjects proceeded immediately to the games. We measured cooperation in two ways: (a) willingness to give free advice and (b) willingness to take a short survey at a later date. For the ﬁrst cooperation measure, subjects were asked to suggest ways to make the transcription task easier. The wording of the request made it clear that offering advice was voluntary and that no payment would be made for the offered advice.16 For a second cooperation measure, we offered subjects the opportunity to take a follow-on survey
15

The subject we randomly selected had chosen to send the full $15; we gave her $30.
16

The text of the actual question was, “Please help us make these tasks easier for other workers. Describe what steps workers can take to make doing these transcriptions easier.”

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

421

Table B.1

Correlation Matrix for Consummate Performance Measures Advice.z Trust.z Survey

which received a 10-cent offer, to the OnlyGames group. Column (1), Table B.2 reports an estimate of Survey =
0

+

1 Control +

(B1)

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Advice.z Trust.z Survey Patient
∗

0 17∗ 0 15∗ −0 07
∗∗

0.01 0.10
∗∗∗

0.07

p ≤ 0 10;

p ≤ 0 05;

p ≤ 0 01.

for an additional 15 cents.17 We also asked subjects if they would wait two weeks to be paid in exchange for additional payment, creating a “patient” measure. The instructions and interfaces for the trust, survey, patience, and advice measures are shown in Figures A.4–A.7, respectively.

B.2. Consummate Performance Results In addition to simply not accepting a wage cut or doing a poor job, workers who face wage cuts could withhold “consummate” performance. Consummate performance is difﬁcult to measure—at least in some models, the difﬁculty of measuring (or at least contracting on it) is deﬁnitional. These challenges aside, we attempted to measure consummate performance with the games phase of the experiment, which followed treatment. Each game was designed to give workers the chance to evince some changed attitude toward us that a real employer might ﬁnd consequential. To aid in the comparison across groups, we created a cell in which subjects were not exposed to an offer to perform an additional paragraph transcription. OnlyGames. Subjects were not offered an additional task. After the initial transcriptions, they went straight to the follow-on games. We had hoped that our consummate measures would all be highly correlated with each other, suggesting that the different measures were all manifestations of the same positive or negative change toward us as an employer/trading partner. However, there is only weak evidence that this hope was borne out: Table B.1 shows the correlation matrix for the four consummate performance measures, pooled across all experimental groups. We see that the patience measure had no strong relationship to any of the other measures and is even slightly negatively correlated with the advice measure. The advice and trust measures are positive related, as are survey and advice measures—but survey and trust are not related to each other. The ﬁrst question about any of the games is whether simply being asked to perform an additional paragraph transcription had any effect. For this, we can compare Control,
17 When you pay workers on MTurk, you can embed a short message about why you are paying them—we embedded the link to our survey in this message. Remarkably, not one person who agreed to do the survey actually did it after we sent them the link. We had intended to use completing the survey as another measure of cooperation and trustworthiness, but there was no variation in survey response. Given the universal rejection, we fear that some technical problem may have prevented completion.

where the sample consists of subjects in Control and the excluded group is OnlyGames. We can see that subjects that were presented an additional fourth paragraph were about 9 percentage points less likely to agree to a survey compared to those simply brought directly to the games. However, this estimate is not conventionally signiﬁcant. Note that among OnlyGames, agreement to do the survey was 100%. For the other games, we also ﬁnd no evidence of a differential performance: in two-sided t -tests for both the advice Z -score and the trust Z -score, we fail to reject the null hypothesis of no difference in group means, with p = 0 29 and p = 0 174, respectively. Furthermore, the Control coefﬁcient is positive for the trust and advice measures. Next we test whether any wage cut—regardless of how framed or justiﬁed—affected willingness to take the survey. Column (2) reports an estimate of Survey =
0

+

1 AnyWageCut +

(B2)

where AnyWageCut is an indicator that the subject was assigned to WageCut of any of the four wage cut groups. The comparison group is the Control group, to which the 10 cent offer was made. The coefﬁcient on AnyCut is very close to zero, implying no detectable effect on willingness to take the survey. As before, for the other games, we also ﬁnd no evidence of a differential performance: in two-sided t -tests for both the advice Z -score and the trust Z -score, we fail to reject the null hypothesis of no difference in group means, with p = 0 26 and p = 0 67, respectively. Despite no overall effect of a wage cut, we know that the different justiﬁcations strongly affected willingness to accept the wage cut. To test whether these justiﬁcations affected consummate performance, column (3) reports a regression where AnyWageCut is decomposed into each of the distinct justiﬁcations, with WageCut as the comparison group. In this regression, we see that the coefﬁcient on ProﬁtWC is negative and large, with a nearly 20 percentage point reduction in agreement to complete the survey. The effect is conventionally signiﬁcant. However, given the multitude of treatment cells, the ﬁnding of one signiﬁcant effect out of ﬁve could very likely be due to chance. On the other hand, ﬁnding the strongest negative effects in the cell with the most unreasonable justiﬁcation could indicate an actual behavioral response. Yet there is additional evidence for the argument that the seeming effect of the proﬁts treatment is spurious: in columns (4) and (5), we change the outcomes to the advice and trust Z -scores and ﬁnd no substantial differences across cells. For both outcomes, we would fail to reject the null hypothesis of zero coefﬁcients on all group indicators (p = 0 16 for the advice measure and p = 0 74 for the trust measure). However, for both the trust and advice measures, we are underpowered to detect anything but very large effects, with the standard errors being more than 3/10ths of a standard deviation in the outcome variable.

422
Table B.2

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

The Effects of Wage Cuts on Consummate Performance Measures from the “Games” Phase of the Experiment Dependent variable Agree to complete survey? (Survey = 1) (1) (2) (3) Advice Z -score (4) Trust Z -score (5)

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Control Any wage cut offer? FairnessPrimedWC NeighborWC ProductivityWC ProﬁtWC UnframedWC Constant Comparison group Observations R2

−0 087 0 063 −0 013 0 067 −0 101 0 092 −0 202∗∗ 0 094 0 059 0 092 0 020 0 091 −0 025 0 093 0 941∗∗∗ 0 071 WageCut 140 0 085 −0 420 0 312 −0 302 0 317 0 167 0 312 −0 400 0 309 −0 473 0 314 0 226 0 241 WageCut 140 0 057 −0 267 0 311 0 038 0 317 0 137 0 311 −0 076 0 309 −0 189 0 314 0 127 0 240 WageCut 140 0 020

1 000∗∗∗ 0 046 OnlyGames 44 0 043

0 913∗∗∗ 0 062 Control 163 0 0002

Notes. This table reports OLS regressions where the dependent variable is an indicator for whether the subject agreed to complete a short survey in the future, in columns (1) through (3) and a Z -score measure of the workers’ offered advice, in column (4), and their trust in us as employers/trading partners, in column (5). In column (1), the comparison group are those subjects assigned to OnlyGames, which were not offered the chance to do a fourth transcription. They are compared against Control, which was offered 10 cents. In column (2), all cells where a wage cut was proposed (i.e., 3 cents for a fourth paragraph) are compared to the Control to see if the wage cut collectively lowered willingness to take the survey. Finally, in column (3), each of the justiﬁed or primed cells in which 3 cents was offered is compared against WageCut, the unexplained wage cut. Robust standard errors are reported. ∗ p ≤ 0 10; ∗∗ p ≤ 0 05; ∗∗∗ p ≤ 0 01.

References
Ågerfalk PJ, Fitzgerald B (2008) Outsourcing to an unknown workforce: Exploring opensourcing as a global sourcing strategy. MIS Quart. 32(2):385–409. Agrawal A, Horton J, Lacetera N, Lyons E (2013) Digitization and the contract labor market: A research agenda. Goldfarb A, Greenstein SM, Tucker CE, eds. Economics of Digitization (University of Chicago Press, Chicago), 219–250. Bajari P, Tadelis S (2001) Incentives versus transaction costs: A theory of procurement contracts. RAND J. Econom. 32(3): 387–407. Bewley TF (1999) Why Wages Don’t Fall During a Recession (Harvard University Press, Cambridge, MA). Binmore K, Shaked A (2010) Experimental economics: Where next? J. Econom. Behav. Organ. 73(1):87–100. Boudreau KJ, Lacetera N, Lakhani KR (2011) Incentives and problem uncertainty in innovation contests: An empirical analysis. Management Sci. 57(5):843–863. Charness G, Rabin M (2002) Understanding social preferences with simple tests. Quart. J. Econom. 117(3):817–869. Chen Y, Bharadwaj A (2009) An empirical analysis of contract structures in IT outsourcing. Inform. Systems Res. 20(4):484–506. Chilton LB, Horton JJ, Miller RC, Azenkot S (2010) Task search in a human computation market. Proc. ACM SIGKDD Workshop Human Comput. (ACM, New York), 1–9. Clemens MA (2011) Economics and emigration: Trillion-dollar bills on the sidewalk? J. Econom. Perspectives 25(3):83–106.

Coase RH (1937) The nature of the ﬁrm. Economica 4(16):386–405. Colquitt JA, Conlon DE, Wesson MJ, Porter COLH, Ng KY (2001) Justice at the millennium: A meta-analytic review of 25 years of organizational justice research. J. Appl. Psych. 86(3):425–445. Dellarocas C (2006) Reputation mechanisms. Hendershott T, ed. Handbook on Economics and Information Systems (Elsevier, Amsterdam), 629–660. Fehr E, Gächter S (2000) Fairness and retaliation: The economics of reciprocity. J. Econom. Perspectives 14(3):159–181. Fehr E, Schmidt KM (1999) A theory of fairness, competition, and cooperation. Quart. J. Econom. 114(3):817–868. Fehr E, Falk A, Zehnder C (2006) Fairness perceptions and reservation wages—The behavioral effects of minimum wage laws. Quart. J. Econom. 121(4):1347–1381. Glaeser EL, Laibson DI, Scheinkman JA, Soutter CL (2000) Measuring trust. Quart. J. Econom. 115(3):811–846. Gopal A, Koka BR (2012) The asymmetric beneﬁts of relational ﬂexibility: Evidence from software development outsourcing. MIS Quart. 36(2):553–576. Greenberg J (1990) Employee theft as a reaction to underpayment inequity: The hidden cost of pay cuts. J. Appl. Psych. 75(5): 561–568. Hart O, Moore J (2008) Contracts as reference points. Quart. J. Econom. 123(1):1–48. Horton JJ (2010) Online labor markets. Saberi A, ed. Internet and Network Economics, Lecture Notes Comput. Sci., Vol. 6484 (Springer-Verlag, Berlin Heidelberg), 515–522.

Chen and Horton: A Field Experiment on the Behavioral Response to Wage Cuts
Information Systems Research 27(2), pp. 403–423, © 2016 INFORMS

423

Downloaded from informs.org by [130.89.97.167] on 17 February 2017, at 08:09 . For personal use only, all rights reserved.

Horton JJ (2011) The condition of the Turking class: Are online employers fair and honest? Econom. Lett. 111(1):10–12. Horton JJ, Chilton LB (2010) The labor economics of paid crowdsourcing. Proc. 11th ACM Conf. Electronic Commerce (ACM, New York), 209–218. Horton JJ, Rand DG, Zeckhauser RJ (2011) The online laboratory: Conducting experiments in a real labor market. Experiment. Econom. 14(3):399–425. Ipeirotis PG (2010) Demographics of Mechanical Turk. Working paper, New York University, New York. Kahneman D, Knetsch JL, Thaler R (1986) Fairness as a constraint on proﬁt seeking: Entitlements in the market. Amer. Econom. Rev. 76(4):728–741. Kittur A, Nickerson JV, Bernstein M, Gerber E, Shaw A, Zimmerman J, Lease M, Horton J (2013) The future of crowd work. Proc. 2013 Conf. Comput. Supported Cooperative Work (ACM, New York), 1301–1318. Kube S, Maréchal M, Puppe C (2013) Do wage cuts damage work morale? Evidence from a natural ﬁeld experiment. J. Eur. Econom. Assoc. 11(4):853–870. Langer E, Blank A, Chanowitz B (1978) The mindlessness of ostensibly thoughtful action: The role of “Placebic” information in interpersonal interaction. J. Personality Soc. Psych. 36(6): 635–642. Lee D, Rupp NG (2007) Retracting a gift: How does employee effort respond to wage reductions? J. Labor Econom. 25(4):725–761. Luttmer EFP (2005) Neighbors as negatives: Relative earnings and well-being. Quart. J. Econom. 120(3):963–1002.

Mas A (2006) Pay, reference points, and police performance. Quart. J. Econom. 121(3):783–821. Moreno A, Terwiesch C (2014) Doing business with strangers: Reputation in online service marketplaces. Inform. Systems Res. 25(4):865–886. Pallais A (2014) Inefﬁcient hiring in entry-level labor markets. Amer. Econom. Rev. 104(11):3565–3599. Pallais A, Sands EG (2016) Why the referential treatment? Evidence from ﬁeld experiments on referrals. J. Political Econom. Forthcoming. Pritchard RD, Dunnette MD, Gorgenson DO (1972) Effects of perceptions of equity and inequity on worker performance and satisfaction. J. Appl. Psych. 56(1):75–94. Rabin M (1993) Incorporating fairness into game theory and economics. Amer. Econom. Rev. 83(5):1281–1302. Rees A (1993) The role of fairness in wage determination. J. Labor Econom. 11(1):243–252. Simon HA (1951) A formal theory of the employment relationship. Econometrica: J. Econometric Soc. 19(3):293–305. Steelman ZR, Hammer BI, Limayem M (2014) Data collection in the digital age: Innovative alternatives to student samples. MIS Quart. 38(2):355–378. Tanriverdi H, Konana P, Ge L (2007) The choice of sourcing mechanisms for business processes. Inform. Systems Res. 18(3): 280–299. Valenzi ER, Andrews IR (1971) Effect of hourly overpay and underpay inequity when tested with a new induction procedure. J. Appl. Psych. 55(1):22–27.

