BIG DATA & ANALYTICS IN NETWORKED BUSINESS

CROWD-SQUARED: AMPLIFYING THE PREDICTIVE POWER OF SEARCH TREND DATA1
Erik Brynjolfsson
Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA 02142 U.S.A. {erikb@mit.edu}

Tomer Geva
School of Management, Tel-Aviv University, Tel Aviv 6997801 ISRAEL {tgeva@tau.ac.il}

Shachar Reichman
School of Management, Tel-Aviv University, Tel Aviv 6997801 ISRAEL and Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA 02142 U.S.A. {shachar@mit.edu}

Big data generated by crowds provides a myriad of opportunities for monitoring and modeling people’s intentions, preferences, and opinions. A crucial step in analyzing such big data is selecting the relevant part of the data that should be provided as input to the modeling process. In this paper, we offer a novel, structured, crowd-based method to address the data selection problem in a widely used and challenging context: selecting search trend data. We label the method “crowd-squared,” as it leverages crowds to identify the most relevant terms in search volume data that were generated by a larger crowd. We empirically test this method in two domains and find that our method yields predictions that are equivalent or superior to those obtained in previous studies (using alternative data selection methods) and to predictions obtained using various benchmark data selection methods. These results emphasize the importance of a structured data selection method in the prediction process, and demonstrate the utility of the crowd-squared approach for addressing this problem in the context of prediction using search trend data. Keywords: Prediction, big data, search trends, data selection, crowdsourcing

Introduction1
Online big data provides a myriad of opportunities to model and forecast economic phenomena that were previously difficult to predict. These data enable customers’ intentions,
1

preferences, and opinions to be modeled on a massive scale, and they have the potential to enhance predictive modeling over a wide set of domains. Yet the abundance of data also creates significant challenges for modeling, data collection, and data processing. One of the most important challenges that has emerged is determining which part of the data should be selected for modeling a phenomenon of interest. Given the vast quantity of available data (for example, all possible search engine queries, numbering in billions), it is essential to select the

Bart Baesens, Ravi Bapna, James R. Marsden, Jan Vanthienen, and J. Leon Zhao served as the senior editors for this paper. The appendices for this paper are located in the “Online Supplements” section of the MIS Quarterly’s website (http://www.misq.org).

MIS Quarterly Vol. 40 No. 4, pp. 941-961/December 2016

941

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

specific subset of data that is relevant for a given modeling or prediction task. Clearly, excluding relevant data will render a modeling process “blind” to important information; in contrast, including irrelevant data can confound the modeling process and can result in undesired outcomes such as overfitting, in addition to creating additional processing and data collection costs. Data selection is a critical stage in the process of producing predictions on the basis of large-scale online crowd-based data.2 As illustrated in Figure 1, this stage (stage 2 in the figure) bridges between the generation of data by crowds and extraction of actual inputs for modeling. In effect, under observational research settings, it is the first stage in which the modeler actively makes decisions. Figure 1 also highlights the distinction between data selection (stage 2) and the well-known feature selection problem (Witten et al. 2011) (stage 5).3 Specifically, the data selection task addresses the question of which data, out of all possible data that potentially could be collected, should actually be collected by the researcher. In contrast, the feature selection process (if applied) takes place at a later stage, typically after the selected data have undergone further processing. In particular, it deals with the selection of a subset of informative variables out of a larger set, generated from the collected data. Therefore, the feature selection process is totally dependent on the data selected in stage 2 and is effectively blind to data that were not selected in that stage. In the literature review and methodology sections, we discuss in detail the relationship between data and feature selection and how feature selection procedures could be applied as a subsequent and complementary step to our methodology. A particularly interesting setting for the data selection problem is when modeling involves usage of online data generated by crowds. Such data are massive in scale and usually evolve continuously over time, in accordance with the crowd’s most recent activities and intentions. The usage of “online digital footprints,” generated by a large number of individuals, to model and predict large-scale outcomes is commonplace both in practice and in academic research and has been reported in numerous previous studies.
2

Interestingly, the important step of data selection when using online crowd-based data has received only limited attention in previous literature. In effect, this step was commonly carried out using intuition and prior knowledge of the researcher or the use of a comprehensive scan of proprietary data. While these approaches have been shown to provide good results in various domains, they also suffer from various downsides, including spurious correlation, being domainspecific, having strong dependence on the researcher’s prior knowledge, high computational costs, requiring extensive effort for periodic updates, and a need for the researcher to rely on proprietary data. In this study, we focus on addressing the search trend data selection problem, which is a particular, well-known form of the online data selection problem. To the best of our knowledge, previous literature does not offer any structured and practical4 methodology to address this problem. We offer a structured, crowd-based approach that bridges this methodological gap in the data selection process. Specifically, we propose a method that leverages the crowd to select a relevant subset of the data that should be included in the predictive analysis. We label this method crowd-squared, as it utilizes one crowd’s knowledge and perceptions to select relevant data from large-scale data previously generated by a larger crowd. Developing a search trend data selection method requires addressing several major challenges. First, search trend databased prediction involves selecting and fitting search trend data, derived from big data (tens of millions of data items, or more), whereas modeling is always conducted using several hundred longitudinal observations.5 Lazer et al. (2014) relate to this problem as “a particularly problematic marriage of big data and small data” (p. 1203). This problem is expected to result in overfitting the small number of cases and harming predictive accuracy.

Large-scale data refers to data that is too large to be processed or stored in a standard computer memory using standard software tools. Crowd-based data refers to the aggregation of user-generated data (i.e., aggregating the content generated by multiple users’ activity). We note that regularization (see, for instance, Bishop 1995), which “downplays” features rather than completely eliminating them, is an alternative to feature selection. Yet, like feature selection, this process is conducted at a later stage than the data selection stage and is effectively blind to data that were not selected during the data selection phase.

4

That is, it can be used by most researchers, who do not have access to proprietary search engine logs. Due to search trend data resolution (provided at a weekly or monthly level of aggregation), search trend data are commonly used to predict phenomena at a weekly (or longer) time resolution. Since search trend data are available only from 2004, most studies use only several hundred longitudinal observations.

3

5

942

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Figure 1. A Typical Prediction Process Using Online Crowd-Based Data

Second, the search trend data selection problem is made even more challenging by the fact that Google Trends6 (which is, by far, the most popular source for search trend data) limits users’ access to search trend data to several hundred search trends a day. This limitation creates a challenging data selection problem in which the researcher needs to specify the relevant data without having access to all of the existing data. The crowd-squared method applies a different perspective on the search trend data selection problem. Instead of looking inside the data to select the best predictors, our method begins by looking outside the data by reaching out and leveraging the wisdom of the crowd. In doing so, our method generates a relatively small set of search trends. The small number of the resultant search trends enables researchers to avoid the problem of the marriage of big data and small data as well as to easily handle and operate under the strict access limitations of Google Trends. The crowd-squared method is based on prompting a large number of individuals, via an online task, to produce word associations relating to a focal term (reflecting the phenomenon for which predictions are being made). This approach taps into people’s lexical knowledge (Nelson et al. 2004) and provides a relative index of the accessibility of related words in an individual’s memory. A key element of the association task is the fact that it provides a power law distribution of term associations (Steyvers and Tenenbaum 2005)—most associations relate to terms that are proximal to the focal term,
6 Google Trends (http://www.google.com/trends/) is a publicly available product that aggregates billions of search queries and provides information about the relative volume of different search terms.

yet the tail of the distribution of the associations contains more distant terms. Thus, this technique is expected to provide ample coverage of relevant search terms. Subsequently, we complement the crowd-based term selection with a straightforward statistical selection procedure to balance data coverage with accuracy. We present a simple practical implementation that demonstrates the predictive capacity of the crowd-squared approach in comparison to data selection methods used in previous studies. Specifically, our demonstration of the crowd-squared approach uses Google Trends and an online task designated for a crowdsourcing environment. We use this implementation to identify relevant terms, collect the corresponding search trend data, and generate predictions in two different domains: influenza epidemics and unemployment claims. We then compare our results with those of challenging and well-known benchmark models in each domain as well as with predictions obtained using alternative data selection methods. This is performed while maintaining strict compatibility with the settings used in previous studies including using the same model specifications, prediction algorithms, time periods, and performance measures. We find the use of the crowd-squared method to be highly effective. Our results suggest that our method performs as well as these benchmarks or even outperforms them. Moreover, the method is practical and feasible also in terms of cost (see Appendix F for a detailed estimation of the costs). Additional advantages of our methodology include simple implementation with low cost of updates, improved understandability, robustness across long periods of time, and finergrained analysis capabilities compared with several benchmark methods.

MIS Quarterly Vol. 40 No. 4/December 2016

943

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

This work contributes to several important streams of information systems (IS) and management literature. Specifically, our research adds to the literature on predictive research in information systems (Shmueli and Koppius, 2011), and business intelligence and analytics (Chen et al. 2012). We focus on the data selection problem, which is a fundamental problem in both domains, and address it in the widely used context of search trend data, offering a practical and efficient solution. We suggest a structured approach that is feasible under existing search trend access limitations, and that also offers transparency and ease of replication. These are important traits that, according to Lazer et al. (2014), do not typically receive sufficient attention in studies analyzing big data. Additionally, our work adds to the two broad streams of literature of online search applications (e.g., Ginsberg et al. 2009) and collective intelligence (Malone et al. 2009; Woolley et al. 2010). In particular, our work proposes novel use of the crowd in the context of predictive research: using the crowd to select relevant data. This use of the crowd may have potential applications that go beyond selecting search trend data (e.g., selecting data in other online and non-online data sources).

tions of social and economic outcomes in a wide range of domains, including movie box office sales and Billboard music rankings (Goel et al. 2010), automotive sales (Choi and Varian 2012; Du and Kamakura 2012; Geva et al. 2017), home sales (Choi and Varian 2012; Wu and Brynjolfsson 2009), unemployment claims (Choi and Varian 2012), and private consumption (Vosen and Schmidt 2011). Table 1 presents an overview of selected work in this domain.

Search Trend Data Selection
Selection of search trend data is a widely used, although challenging, data selection task. In effect, the search trend data selection process often entails indicating a term, word combination, or phrase (referred to hereafter as “search terms”), or a general category, and obtaining its corresponding search query volume over time. Yet, people may search for a specific item or phenomenon of interest using several search terms (e.g., influenza may also be referred to as “flu” or “cold”). In straightforward cases, the search terms associated with an item of interest may include subitems from known ontologies (e.g., online searches for various Chevrolet models such as Aveo or Camaro are likely to be indicative of interest in the Chevrolet brand). Additionally, search terms indicative of or correlated with a certain item of interest may not include a direct reference to the item of interest or its subitems. For example, online searches about “inexpensive cars” may also contain valuable predictive information regarding consumer interest in certain brands such as Chevrolet. Furthermore, it is also possible that the relevance of a search term to a phenomenon of interest may be indirect. For instance, Wu and Brynjolfsson (2009) showed that searches related to home purchases are indicative of future home appliance sales. While hand picking search trends is a challenging task, applying automated search trend data selection methods is also very challenging, even under optimal conditions when all search trend data are available. A key challenge is the problem of the marriage of big data and small data, discussed earlier, which requires selecting and then fitting data from tens of millions or more search trends, whereas modeling is conducted using only several hundred longitudinal observations. A second challenge is data availability. Thus far, the overwhelming majority of studies using search trend data have used publicly available data from the Google Trends website,7 which poses strict limitations on the number of search trends that can be retrieved. Currently, only a few hundred search trends can be retrieved per day, out of a much
Two known exceptions are Ginsberg et al. (2009) and Goel et al. (2010), who used proprietary search trend data sources.
7

Related Literature
Current literature in the fields of IS and management demonstrated that online search has become an integral part of people’s lives (Sparrow et al. 2011), and that online search behavior is tied to individual decision-making and to the consumption process in general (Du et al. 2015; Ghose et al. 2014; Lockwood et al. 2006; Moe and Fader 2004). These observations suggest that online search patterns reflect underlying consumer interests, intentions, and activities. Accordingly, the availability of data on consumers’ online search behavior and web activity, along with developments in analytic tools, have dramatically increased our ability to obtain accurate data on millions of economic decisions, as well as on individuals’ intentions to carry out transactions (McAfee and Brynjolfsson 2012). Indeed, in recent years, the use of aggregated search engine log data has become commonplace in scientific research and changed the way we predict events. Wagner et al. (2001) were the first to show how searches associated with influenza-related symptoms—specifically, on WebMD.com—can be used to improve early detection of influenza outbreaks. Ginsberg et al. (2009) broadened this approach and showed that aggregation of large quantities of search query data on Google’s search engine can also be leveraged to generate useful predictions of influenza epidemic outbreaks. These breakthrough papers opened the door to a stream of studies using online activity data to generate predic-

944

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Table 1. Overview of Selected Search Trend Literature
Paper Bordino et al. (2012) Breyer et al. (2011) Data Selection Method (description of terms) Intuition and prior knowledge (stock ticker symbols) Intuition and prior knowledge (defined and tested a small set of terms related to nephrolithiasis and compared to terms that are not related to this topic) Intuition and prior knowledge & algorithmic classification methods (Several Google Trends categories: Local/Jobs and Society/Social Services/Welfare & Unemployment) Intuition and prior knowledge (stock ticker symbols) Intuition and prior knowledge (one search term: jobs) Intuition and prior knowledge (car brands and features) Intuition and prior knowledge & Algorithmic classification methods (car brands and GoogleAds terms in the Google Trends automotive category) Intuition and prior knowledge (search queries related to tourism in Hong Kong) A comprehensive scan of available data (50 million popular searches) A comprehensive scan of available data (All search queries based on the search results. Queries were categorized as movie-related if IMDB link appeared in the results, as games if links to one of three major gaming websites appear in the results, and as songs if queries on Yahoo! Music contained a song’s title.) Intuition and prior knowledge (three search terms associated with suicidal behavior) Intuition and prior knowledge & Algorithmic classification methods (Names of 21 popular models in the U.S. and terms related to these terms generated by GoogleAds. All trends were of searches in the Google Trends automotive category.) Intuition and prior knowledge (one term: BitCoin) Intuition and prior knowledge (terms the authors thought would reflect searches of S&P500 companies) Intuition and prior knowledge (ballot questions and topics relating to ballot questions) Intuition and prior knowledge (Name of one professor, “Paul Kirchhof,” following his controversial statements on several topics.) Algorithmic classification methods (56 consumptionrelevant Google Trends categories) Intuition and prior knowledge (researcher chose specific flu symptoms) Algorithmic classification methods (“Real Estate,” “Real Estate Agencies,” and “Real Estate Listings” categories) Intuition and prior knowledge (one keyword: depression) Prediction Domain Stock market volume admissions for nephrolithiasis (kidney stones) Claims for unemployment

Choi and Varian (2012)

Da et al. (2011) D’Amuri and Marcucci (2012) Du et al (2015) Du and Kamakura (2012)

Stock prices Claims for unemployment Automotive sales Automotive sales

Gawlik et al. (2011) Ginsberg et al. (2009) Goel et al. (2010)

Tourism activities (in Hong Kong) Influenza epidemics Movie box office sales, Video game sales and Billboard music rankings

Gunn and Lester (2013) Hu et al (2014)

Suicide Rates Automotive sales

Kristoufek (2013) Preis et al. (2010) Reilly et al (2012) Scharkow and Vogelgesang (2011) Vosen and Schmidt (2011) Wagner et al. (2001) Wu and Brynjolfsson (2009) Yang et al. (2010)

BitCoin value Financial market fluctuations Voting behavior Public Agenda

Private consumption Influenza epidemics Housing market sales and prices Epidemiology of seasonal depression

MIS Quarterly Vol. 40 No. 4/December 2016

945

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

larger universe that may contain billions of different search trends. Given these challenging characteristics of the search trend data selection problem, it is notable that previous literature does not offer a practical and structured search trend data selection method. Rather, as discussed above, search trend data selection has typically been performed using three main approaches: (1) a comprehensive scan of large-scale data, (2) reliance on intuition and prior knowledge, and (3) algorithmic classification methods (Table 1 presents an overview of data selection methods used in previous search trends research). As detailed below, the first approach commonly requires using proprietary data and may suffer from the marriage of big data and small data problem; the second and third approaches are unstructured and require applying personal knowledge and intuition. The first approach to search trend data selection is to use a comprehensive scan of available data to select the terms that are most strongly correlated with the focal phenomenon. When applying this method, the researcher first heuristically selects a large part of the data to focus on (possibly even selecting all of the available data) and then applies computationally intensive feature selection methods to that data. Ginsberg et al. (2009), for example, who used internal data from Google, first selected search trend data corresponding to the 50 million most popular search terms. The researchers then scanned all of these search trends to select the trends that correlated most strongly with actual influenza data. The benefit of using such a large set of search terms is obtaining high coverage (Geva et al. 2017). However, as mentioned above, the main problem of this approach is a high risk of spurious correlation and overfitting. Additionally, this data selection approach commonly necessitates the use of proprietary data. Thus, it is impossible to reproduce this methodology using the limited data that are publicly available. Using the second approach, the researcher applies human intuition and prior knowledge to identify search trend data pertaining to a certain item (e.g., flu). This task involves choosing specific search terms that are likely to be associated with that item (e.g., flu, influenza). This approach has been employed in various studies, mainly using straightforward keywords. For instance, Preis et al. (2010) selected keywords that the researchers subjectively believed would best reflect search trends pertaining to S&P500 companies. In contrast, Drake et al. (2012) assumed that keywords that are strictly based on a company ticker symbol (e.g., MSFT for Microsoft) best reflect search interest in such companies. Other studies, such as that of D’Amuri and Marcucci (2012), which sought to predict unemployment rates, also used straightforward keywords selected by the authors. This approach is clearly

easy to implement and inexpensive. However, naturally, the modeling performance using this approach may vary across researchers and domains, and it is difficult to use formal methodology to represent researchers’ individual knowledge. The third approach uses automated methods to classify data into predefined categories. For instance, Google Trends offers an internal black box category classifier, which has been used in various studies such as Choi and Varian (2012), Vosen and Schmidt (2011), and Wu and Brynjolfsson (2009). These classifiers commonly focus on detecting search data that pertain to a predetermined category (e.g., search queries that relate to the automotive industry). These classifiers clearly provide ease of use to the end user. However, they do not take into account the context of the prediction task and therefore might be unsuitable for detecting data with indirect relevance for a given prediction task (as in the above mentioned example of Wu and Brynjolfsson, who found that search trends related to new home purchases were indicative of future home appliance sales). Additionally, these classifiers are not transparent to the typical end user (i.e., a user who is not affiliated with the developers of the classifier), who might consequently find it difficult to gauge their accuracy or coverage. We also note that the third approach may be considered as a variant of the second approach. This is because the researcher still needs to apply her own knowledge and intuition to select the category, and it is possible that a classifier’s rules are determined (or that examples are provided for a supervised learning-based method) on the basis of the developer’s prior knowledge and intuition. Finally, it is important to note, in the context of search trend data, the important differences as well as complementarity between data selection and feature selection. As detailed in the introduction, the search trend data selection task addresses the question of which data, out of all possible data that potentially could be collected (i.e., search trends corresponding to any possible combination of terms), should actually be collected by the researcher. In contrast, the feature selection process, (optionally) applied at a later stage, deals with the selection of a subset of informative variables out of a larger set, generated from the selected data. Clearly, given the fact that data selection phase precedes the feature selection procedure, the impact of data selection on the modeling outcome is considerable. Even an ideal feature selection procedure cannot remedy a data selection procedure that has omitted important data. Additionally, a data selection process that is overly inclusive (e.g., bringing into the modeling process a very large number of search trend data) would render feature selection a very difficult task and might ultimately lead to the problem of marriage of big data and small data, mentioned above.

946

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Nevertheless, the two phases, data selection and feature selection, clearly complement each other. In fact, the entire modeling process may be designed to take into account the complementarity between the two phases, with the data selection method driving the selection of the subsequent feature selection procedure. For instance, the comprehensive scan approach detailed above includes an initial data selection process that chooses vast quantities of data items. This warrants applying a subsequent step involving a computationally intensive feature selection procedure.8 In this study, we make use of this complementarity. As detailed in the “Methodology” section, we intentionally use a two-phase approach in which a crowd-based data selection procedure is complemented by a simple feature selection procedure in order to address the tradeoff of accuracy versus coverage in search trends data selection (Geva et al. 2017).

nonexpert individuals can generate results at the same level as those created by experts. Overall, the benefits of crowdsourcing stem from its scale and from the diversity of user backgrounds, levels of expertise, and other demographics, coupled with its low costs (Howe 2006; Schenk and Guittard 2011). We follow the stream of research on crowdsourcing and leverage the crowd to generate relevant terms that can be used to collect search volume data for the purpose of prediction and early detection of events. One of the challenges of crowdsourcing is identifying means of engaging the crowd in a meaningful and productive manner (Boudreau et al. 2013). As noted by Von Ahn (2006), an online game environment is an effective setting for capturing crowd knowledge and may be used to elicit reliable information without any supplementary verification of users’ answers. In the following sections, we elaborate and demonstrate a new crowd-based approach that uses an online word association game task that captures people’s ideas regarding terms that relate to focal phrases. We subsequently show that the resulting terms can be used to identify search trends that are relevant for prediction tasks.

Crowdsourcing
In this paper, we use a crowdsourcing technique to identify relevant search terms. The fundamental idea underlying prediction based on search trend data is that it reflects cumulative actions performed by people over time and, as a result, captures longitudinal changes in behavior. We propose using the crowd to better understand how individuals choose the terminology they use in their online search activity. As online search behavior can be used to reveal consumers’ intentions (Moe and Fader 2004), improved understanding of the term generation process could improve the ability to gather knowledge on search patterns associated with different consumption activities. Crowdsourcing is the act of harnessing a distributed network of individuals to solve a problem or perform a function that was once performed by employees (Brabham 2008; Howe 2006). In recent years, crowdsourcing has become increasingly prevalent in many fields, and is used in a variety of tasks such as capturing new product ideas and innovations (Bayus 2013), generating accurate image tags (Von Ahn 2006), improving image search (Yan et al. 2010), healthcare applications (Hill et al. 2003), processing social media data (Archak et al. 2011), and even solving scientific problems (Lakhani et al. 2007). Furthermore, as shown by Snow et al. (2008), aggregating results for a single task from multiple
8

Methodology
As detailed above, data selection based on choosing potentially relevant search terms is a common practice in studies using search trend data. A key characteristic of such termbased data selection is an inherent tradeoff between search terms coverage and accuracy (Geva et al. 2017).9 For instance, the use of a large set of search terms may provide ample coverage of all relevant search trend data (i.e., it might yield a data set comprising all search trends that are relevant for modeling a certain phenomenon). However, using such a large set of search terms might come at a cost of including some irrelevant terms, which, in turn, may confound the prediction model or result in overfitting the data. The data selection process we propose (stage 2 in Figure 1) aims primarily to provide high coverage of relevant data with a relatively small number of search terms. To efficiently balance this coverage with accuracy, we supplement the data selection stage with a secondary step: a feature selection process (stage 5 in Figure 1).
9 Let K denote a set of search terms, and P denote a phenomenon of interest. “Accuracy” denotes the ratio between the number of search queries that specify (any word in) K and that actually relate to P, and the total number of search queries specifying any word in K. “Coverage” denotes the ratio between the number of search queries using any word in K, and the hypothetical, full number of search queries referring to P (using any search term).

An additional (trivial) example of the fact that the data selection method strongly influences the nature of the subsequent feature selection procedure is found in studies using the second search trend data selection approach discussed above. These studies commonly use the researcher’s prior knowledge and intuition to select only a small number of search trends. This effectively renders the feature selection stage irrelevant. Indeed, most of the studies using this data selection approach do not employ any feature selection procedures.

MIS Quarterly Vol. 40 No. 4/December 2016

947

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Crowdsourced Search Term Selection
We introduce a technique to use crowdsourced human workers to help us identify relevant search terms in an online platform. Specifically, we implemented a word association task (also known as free association) in which workers were asked to provide related phrases or terms. Word association is a task that requires the participant to spontaneously provide a word or a phrase that is related to a presented word (known as the cue). Word association taps into one’s lexical knowledge, which is based on real-world experience (Nelson et al. 2004) and has been shown to be important in predicting cued recall (Nelson et al. 1998). Individuals rely on associations in everyday activities as a means of collecting thoughts (Nelson et al. 2000). When an individual associates a word with a cue term, this constitutes an indication that the word is related to the cue term. Notably, individuals’ word associations have been found to be consistent across different people in the same recall culture (Nelson et al. 1998). In the context of online search behavior, as people use search engines as a form of external or transactive memory (Sparrow 2011), the words that an individual associates with a given topic of interest may correspond to search queries that he or she would use when seeking information on that topic. Because associative terms used to search for a given item are likely to be consistent across individuals, the terms that a set of individuals associates with a topic of interest may reflect broader search patterns and therefore assist in measuring current events and predicting future activities. A key benefit of the word association technique is that it provides a power law distribution of term associations (Steyvers and Tenenbaum 2005). While most associations relate to terms that are proximal to the cue, the “tail” of the power law distribution of associations connects to more distant terms. Thus, this technique allows us to capture many potentially relevant terms that are not necessarily highly correlated with one another, thereby providing wide coverage of the data that may pertain to the phenomenon of interest.

participants were not informed of the purpose of the task or how their terms would be used after completing the task. We used the Amazon Mechanical Turk platform, an online marketplace for tasks that require human intelligence (or tasks that are easily achieved by a human but require large computational costs to be solved algorithmically). Workers (known as Turkers) are paid small amounts of money to complete small tasks (called HITs, human intelligence tasks). The platform allows task assignments to be randomly assigned to multiple Turkers and provides control over task completion. Amazon Mechanical Turk was used as a convenient means of reaching a large number of people. However, other platforms could also be used in a similar way to collect terms or keywords from the crowd. In total, 1,100 Turkers participated in our experiments (550 for each domain).10 Each Turker was given a single cue phrase and was asked to provide five terms or phrases that came to mind when seeing the cue phrase. Each Turker was paid 5 to 8 cents ($0.05 to $0.08) for completing the task. The average duration of a task was 53 seconds (including completion of 4 demographic items). The resulting set of different associated phrases for each domain was very large. Nevertheless, it is important to account for the fact that the use of any single phrase may not represent a common form of thinking, reflective of multiple individuals’ search patterns, but rather might be indicative of only one individual’s unique thinking. As shown by Snow et al. (2008), an aggregation of results from multiple individuals can generate results of high quality. We therefore restricted the analysis to include only terms reported by at least 1% of the users.11 We then collected the search query volume for each term over time and subsequently used these data in the modeling process.12 We note that, in order to avoid the influence of the
10

We eliminated from the analysis 20 Turkers, who did not provide 5 associated terms or did not answer all demographic questions.

Word Association Task Design
We designed and developed an online word association website specifically for this study. The website contains a single page with brief instructions and one phrase (the cue term). Participants enter their associated terms in five text boxes displayed on the screen, as elaborated below (an illustration of this task is presented in Figure 2). The appearance of the website was planned to simulate a common task environment;

11 Google Trends has a policy intended to preserve privacy and therefore does not return results for certain search terms with relatively low search volume. It also rounds numbers to integer value; thus, it effectively rounds down low search query volume to zero. We therefore exclude from our set of search terms those terms for which Google Trends did not return results or that included zero in more than 90% of the instances in the training set. The full list of valid terms is available in Appendix A.

As detailed in Choi and Varian (2012), Google Trends data are computed by a sampling method and therefore may contain some noise. To reduce the noise, we use a similar procedure as in Preis et al. (2013) of averaging the value of multiple draws from Google Trends.

12

948

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Please write five terms (one word or more) that come to your mind when you see the word

Figure 2. Illustration of the Online Word Association Task

researchers’ own judgment (as in the second and third data selection methods described in the literature review section), the search trends that we collected correspond to the raw terms provided by the crowd. That is, we intentionally avoided subjecting the crowd-generated terms to postprocessing procedures, such as stemming (Porter 1980). Nevertheless, since Google Trends returns different search volumes for different variations of the same word (e.g., plural or singular terms, common typos), these variations may actually contain additional predictive information. Finally, we note that our crowdsourcing procedure method includes several inherent properties that can substantially reduce the problem of SPAM outputs that hinder online marketplaces (Ipeirotis 2010). First, the fact that the procedure involves a free-form question in which workers are required to write down their response instead of selecting from predefined answers reduces the likelihood of SPAM outputs (Harris 2011). Second, the introduction of the 1% threshold mentioned above, which means that multiple individuals must repeat the same term in order for it to be considered in subsequent analysis, reduces the likelihood of workers effectively “gaming” the system. Last, the fact that the word association task is very short and easy to perform substantially reduces the incentive to game the system in order to save time and effort.

racy (i.e., with low relevance to the phenomenon of interest). Therefore, as noted above, we implemented a feature selection procedure to choose the most useful variables and thereby to balance the inherent tradeoff between coverage and accuracy within our data. Feature selection procedures have been widely reported in the literature and are generally known for their ability to improve predictive accuracy, reduce overfitting, and decrease model complexity (Witten et al. 2011). Our implementation of feature selection involves a standard sequential forward selection procedure with a nested holdout sample (see, for instance, Provost and Fawcett 2013). Specifically, we train prediction models over the first two-thirds of a training set and repeatedly add the next most useful feature (search query volume for a given term) to the prediction model, based on its contribution to improving model performance over a nested holdout sample that consists of the last one-third of the training data instances. We repeat this procedure for up to 20 features and then select the set of features that optimizes the relevant performance criteria over the nested holdout sample. After selection of the relevant features, the model is then retrained over the entire training set data and performance is evaluated over an external, independent, validation set, which consists of data from a subsequent time period.13 By following common practice in predictive research, and using outof-sample data, our measurements inherently account for, and penalize, excessive model complexity, if it exists.

Feature Selection
As detailed above, the crowd-squared method is designed to use the power-law distribution characterizing word association tasks in order to select data with high coverage. Nevertheless, this method of data selection might introduce into the resultant data set some search trends with low accu-

13

As detailed in the evaluation section, we constrain ourselves and use the same time periods for training the model as in Ginsberg et al. (2009) and Choi and Varian (2012). The nested holdout sample is a subset of these training periods. Overall predictive capacity in the evaluation section is measured using the same, external validation set reported in these studies.

MIS Quarterly Vol. 40 No. 4/December 2016

949

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

We note that our main goal is to evaluate the efficacy of our primary stage of crowdsourcing the data selection process; we use the feature selection only as a complementary step to eliminate search terms with low accuracy. Therefore, while the literature offers a host of advanced feature selection methods, we intentionally apply a straightforward method. Thus, our implementation of the crowd-squared methodology in conjunction with a straightforward feature selection method is designated to provide a rather conservative assessment of the performance of the overall data selection methodology. Clearly, if this method is found to be successful, then applying more advanced feature selection methods could yield even better results.

their search volume with an input time series. Google Correlate also ranks the importance of search terms, displaying the correlation values with the input time series.14 The second method uses lexical terms that are related to a focal term. Specifically, we used the well-known WordNet lexical database (Fellbaum 1998; Miller 1995) and collected all synonyms, hyponyms, direct and inherited hypernyms, as well as sister terms for the focal terms. We then collected the relevant search trend data for each suggested term and applied the same sequential forward selection procedure we used for the crowd-squared process. As the ease of use of Google Correlate and the WordNet lexicon is comparable to that of our methodology, we believe these approaches serve as additional valid benchmarks for our method.

Evaluation
To validate the effectiveness of the crowd-squared approach, we applied our proposed methodology to prediction tasks in two different domains. Specifically, we replicated tasks reported in two well-known related studies. Ginsberg et al. (2009) in the detection of influenza outbreaks, and Choi and Varian (2012) in predictions of unemployment levels. We note that the predictive modeling literature offers numerous models and algorithms that can potentially improve predictive performance. In addition, different studies rely on different performance measures. The choice of performance measure may influence the evaluation of a given set of predictions—that is, a given set of predictions may compare more or less favorably to another under different performance measures. Therefore, to ensure an impartial comparison, and to accurately gauge the effectiveness of our data selection method, we intentionally constrained our analysis to the precise prediction model specifications, training data, time periods, and validation methodologies specified in each of these studies. We also used the exact performance measures reflecting the goal of each study. The only difference was the data selection methodology. We compared our prediction results with the prediction results reported in each paper. Furthermore, in the domain of unemployment levels, we also compared our results with those of a baseline model evaluated by Choi and Varian. In the domain of influenza outbreaks, we further compared the predictive accuracy of our method with that obtained in a related, recent study (Lazer et al. 2014). We further suggest two additional, external, benchmarks for our methodology that rely on alternative, simple-to-use data selection methods. The first method uses Google Correlate (see https://www.google.com/trends/correlate), an online tool that retrieves up to 100 terms according to the correlation of

Influenza Epidemics
The first data set that we used to validate our methodology comprises flu outbreak data from the U.S. Centers for Disease Control (CDC). Ginsberg et al. (2009) used this type of data to construct an early detection system for influenza epidemics (named “Google Flu Trends”). Specifically, the dependent variable in their study was the weekly Influenza-Like Illness (ILI) factor reported by the CDC. To select the search term data to be included in the prediction model, the researchers used Google’s internal data. They began by selecting the 50 million most popular search terms, from which they selected the “top n” terms by calculating individual term correlation with the dependent variable. Subsequently, they used the selected terms to fit a linear model used to generate predictions. Ginsberg et al. reported that their method was highly successful for this application, achieving an out-of-sample mean correlation of 0.97 across U.S. regions. Nevertheless, it is impossible to use similar methodology without access to Google’s proprietary data, since Google does not allow external access to search trend data for more than several hundred search terms a day.

14 After obtaining the search trend data recommended by Google Correlate, in order to determine the “optimal” number of Google Correlate benchmarkbased features to be included in the model, we sequentially added the top-20 ranked Google Correlate-based recommended search trends, and evaluated their predictive performance using a similar linear prediction model over the same “nested validation” data reported above. We then selected the bestperforming feature combination. We note that while the search terms retrieved by Google Correlate are already ranked according to their correlation with the dependent variable; for robustness, we also tested whether applying the same feature selection procedure we used for the crowd-squared process would improve the Google Correlate-data-based prediction results. However, applying this feature selection procedure did not improve the Google Correlate benchmark performance, and therefore these results are not reported.

950

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

In this study, we used U.S. national-level data from the period between January 2004 and the week commencing on March 11, 2007.15 We validated our modeling using out-of-sample data from March 18, 2007, to May 11, 2008; this is the same out-of-sample validation period used by Ginsberg et al. Using the word association setting described above, we asked 535 Turkers (40% female, average age 29.1) to perform an the online task, where the task description was “Please write 5 terms that come to mind when seeing the word flu.” For each phrase reported by at least 1% of the Turkers, we collected the weekly search index from Google Trends (overall, 73 valid terms were identified). This search index is the share of searches at time t (typically week or month) relative to the total search volume across the time period. We limited our results to queries in the United States to match the predicted variable: flu outbreak in the United States. For the sake of fair comparison, similarly to Ginsberg et al., we also used a linear prediction model. Specifically, we used the following prediction model:

535 online users; following our suggested secondary feature selection procedure, our model ultimately used only 18 terms. While our data selection method achieved comparable results to those of Ginsberg et al. over the same validation period used in that study (as presented in Table 2 and Figure 3), a recent study by Butler (2013) suggests that Google’s Flu Trends prediction system (which is based on Ginsberg et al.) has been modified over recent years to account for new data, yet its performance deteriorated, especially during the major flu outbreak of 2012–2013. Therefore, it is interesting to evaluate the robustness of predictions based on our data selection methodology during this more recent period. This setting is especially challenging due to the fact that our model was trained on much earlier data, from 2004–2007. Specifically, we compared our results with the results provided by the Google Flu Trends website.17 Here, our results show higher correlation levels compared with the Google Flu Trends system for the 2012–2013 flu season (following CDC convention, the 2012–2013 flu season is between October 2012 and September 2013). Specifically, our method obtained a correlation of 0.97, whereas the Google Flu Trends prediction obtained a correlation of 0.955. It is also interesting to note that, unlike the results of the Google Flu Trends system, prediction results based on our data selection method remained stable over time and did not deteriorate. Furthermore, Figure 3 shows a comparison of our model’s predictions with actual reported ILI data from the CDC over the two time periods described above. Looking at the 2012– 2013 flu season, and specifically December 2012 to February 2013, we observe that our model generated predictions that better matched the actual influenza outbreak duration than did the predictions of the Google Flu Trends model. Last, we compared our results to the results obtained using three other benchmark data selection methods: choosing search trends based on terms provided by Google Correlate,18 search trends based on lexical terms obtained from WordNet, and a simple benchmark discussed by Goel et al. (2010).19 This simple benchmark uses as explanatory variables two-

flut = α + i βi Termit + ε

(1)

Where flut is the unweighted ILI factor at time t reported by the CDC; Termit is the search trend value at time t for search term i suggested by the crowd-squared method; α is the model intercept; and ε is the error term.16 We note that the process of adding trend data to equation (1) involves rerunning the linear regression model each time a new variable is incorporated, thus adjusting the current coefficients. Table 2 summarizes the results for the different data selection methods. (To maintain compatibility with Ginsberg et al., we report results using correlation as the performance measure.) As shown in this table, our prediction results achieved a very similar level of out-of-sample correlation in predicting ILI (0.966 compared to 0.97 in Ginsberg et al.). Given that the two data sets yielded comparable results, it is important to point out the huge difference in the volume of data that was included in each model. First, Ginsberg et al. used 50 million different search terms and 450 million different models to generate the final model, which included 45 search term queries. The computation involved in this process employed hundreds of machines using a distributed computing framework. Our method is based on 73 valid terms suggested by
15

17

http://www.google.org/flutrends/us/data.txt

As input for the Google Correlate tool, we provided the data corresponding to the variable of interest (ILI CDC reports) during the same training set period.
19 For robustness, we also tested prediction based on search terms selected by 15 experts (medical doctors with specialty in family medicine or internal medicine or public health doctors). The crowd-squared method obtained similar or better results in comparison with predictions based on experts’ terms.

18

We excluded data from 2003 since Google Trends provides data only from 2004.

16

In Google Trend data, each individual search trend variable is already normalized (scaled according to its maximum value).

MIS Quarterly Vol. 40 No. 4/December 2016

951

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Table 2. Correlation Results with CDC-Reported ILI for Prediction Models Using Different Data Selection Methods
Validation Period March 2007 – May 2008 (similar to Ginsberg et al.) Oct 2012-Sep 2013 (2012–2013 Flu Season) CrowdSquared 0.966 0.97 Google Flu Trends 0.97 0.955 Google Correlate 0.965 0.956 WordNet Lexicon 0.947 0.932 Simple Benchmark 0.917 0.912

Note: We normalized the data to allow for a consistent comparison since the data available on Google Flu Trends are on a different scale. All values were normalized on the time series average and standard deviation.

Figure 3. Comparison of the Crowd-Squared Model Predictions with Actual Reported ILI and Ginzberg et al./Google Flu Trends, Over Two Separate Periods: The 2007–2008 Flu Season (Top) and the 2012– 2013 Flu Season (Bottom)

952

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

and three-week-old lags of the dependent variable.20 The results for these models are also reported in Table 2. Overall, we found that, for the validation period reported in Ginsberg et al., predictions based on our data selection methodology slightly outperformed those based on Google Correlate data selection. For the 2012–2013 season, our methodology outperformed predictions using Google Correlate-based data selection. Further, in comparison to the WordNet terms and simple benchmark model, the crowd-based method obtained better results over the two validation periods. Flu Prediction Sensitivity Analysis: Number of Search Trends The results reported above for the crowd-squared method used a final set of search trends that yielded the best performance over a nested validation set (last third of the training set), as determined by the feature selection procedure detailed in the methodology section. For robustness, we provide in Figure 4 a detailed analysis of predictive capacity when constraining this feature selection process to select a fixed number of search trends (from a single feature and up to 20 features). This figure also presents the results of imposing a similar constraint on the number of search terms obtained from Google Correlate and WordNet. This figure shows that when six or more search trends are used, the predictive capacity of the crowd-squared data selection method for the 2007–2008 validation period is comparable to that of Google Flu Trends and to that of the Google Correlate-based data selection method. Notably, the crowd-squared method considerably outperforms these methods in the more challenging 2012–2013 flu outbreak season. Additional Analysis: An Alternative Flu Outbreak Prediction Model In a recent paper, Lazer et al. (2014) raised criticisms regarding the methodology and performance of the Google Flu Trends system. In addition, they showed that a model they developed, one that makes use of additional data as explanatory variables, outperforms the Google Flu Trends model. Specifically, Lazer et al.’s model includes the following variables. Google Flu Trends predictions, lagged CDC data, lagged differences between Google Flu Trends predictions and CDC data, as well as binary variables indi-

cating the week within the year to account for seasonality.21 Their model is formalized as follows:

flut = α + β1 gflut + β2 flut − 2 + β3 ( gflut − 2 − flut − 2 ) + β4 ( gflut − 3 − flut − 3 ) +  γ j week jt + ε
j =1 52

(2)

where flut is the ILI factor at week t reported by the CDC; gflut is the Google Flu Trends system estimate for week t; weekjt is a binary (weekly) seasonality variable; α is the model intercept; and ε is the error term. Lazer et al. showed that during the time period between September 2009 and August 2013, their model outperformed the Google Flu Trends system and obtained an MAE of 0.232, whereas the Google Flu Trends system obtained an MAE of 0.486. Lazer et al.’s model (equation 2) relies on a rich set of information, based on predictors (explanatory variables) not employed by Ginsberg et al. The model utilizes this additional information effectively, obtaining predictions that are more accurate than those of the Google Flu Trends system. This raises the question of whether incorporating information obtained from the crowd-squared method could further enrich the (already rich) model developed by Lazer et al., or whether the information already contained in that model would render the crowd-squared-based predictors redundant. To evaluate this, we applied the crowd-squared method using the linear prediction model described in equation 3, basically adding to Lazer et al.’s model additional predictors based on search volume for terms suggested by the crowd-squared method (Termit in equation 3), and then compared the predictive accuracy of the two models.

flut = α + β1 gflut + β2 flut − 2 + β3 ( gflut − 2 ) + β4 ( gflut − 3 − flut − 3 ) +  j =1 γ j week jt (3)
52

+ i δi Termit + ε
For consistency we use the same performance measure (MAE) and validation period reported by Lazer et al. In addition, like Lazer et al., we used dynamic recalibration of the model (a one-step-ahead, expanding window approach) to ensure impartial evaluation. Overall, we found that inclusion of the crowd squared-based predictors improved the MAE to
21 In their supplementary materials, Lazer et al. also test several other variants of this model. This model, which was reported in the main body of the paper, obtained the best MAE results.

20 Most related studies avoid using one-week-old lags of CDC data since this information may not be available in time for predicting the current week CDC ILI values.

MIS Quarterly Vol. 40 No. 4/December 2016

953

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Figure 4. Correlation Results When Using a Fixed Number of Search Terms 2007–2008 Validation Period (Top) and 2012–2013 Flu Season (Bottom)

0.209 over the validation period compared with 0.232 in Lazer et al. For robustness, we provide in Figure 5 a detailed analysis of the case in which the crowd-squared procedure is constrained to select a fixed number of search trends, from a single feature up to 20 features. This figure shows the consistency of the improvement when adding crowd-squared-based features to the prediction model. Last, we note that beyond demonstrating the performance of the crowd-squared method, more accurate prediction of influenza outbreaks has considerable practical value. More

accurate prediction models that detect influenza outbreaks in time have the potential to increase public awareness of the necessity for timely vaccination, to provide medical institutions more time to prepare for medical crisis situations, and ultimately to save human lives. Economic benefits of more accurate flu predictions include decreasing the quantity of sick leave attributable to influenza; enabling pharmaceutical companies to better align their manufacturing and supply chains; and allowing financial traders to make more informed decisions with regard to investing in pharmaceutical and healthcare companies.

954

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Figure 5. MAE Results When Using a Fixed Number of Search Terms (Flu Outbreak Prediction)

Initial Claims for Unemployment Benefits
The second set of data that we used to evaluate the capacity of the crowd-squared method involves the task of early assessment of the volume of initial claims for unemployment benefits in the United States. This economic index is published by the U.S. Department of Labor each Thursday, for the previous (Sunday–Saturday) week, and is considered an important measure of the state of the U.S. economy.22 Choi and Varian (2012) carried out a study that used search trend data to produce early assessments of initial claims for unemployment. They developed a model that incorporates both baseline information (seasonally adjusted initial claims for the previous week) as well as (seasonally adjusted) search trends for the current week based on Google’s predefined categories of “Jobs” and “Welfare...Unemployment,” identified by Google’s automated category classifier. They evaluated this model out-of-sample using a one-week-ahead expanding window prediction (that is, using the data up until week (t-1) to train the model and measure its performance over week (t)), using training and validation data from January 2004 to July 2011. Their model was able to generate accurate predictions of economic turning points, and their overall performance outcome, measured by MAE, was 3.68%. However, Choi and
These historical data are available at http://www.ows.doleta.gov/ unemploy/claims.asp.
22

Varian also reported that a simple baseline model that did not incorporate search trend data generally outperformed the model they developed, yielding an MAE of 3.37% over the measured time period. This autoregressive model with one lag (AR1) is presented in equation (4).

UIC(t ) = α + δ1UIC j (t − 1) + ε

(4)

where UIC(t) is the logarithm of the seasonally adjusted volume of initial claims for unemployment for week t; α is the model intercept; and ε is the error term. Choi and Varian’s result suggests that the search trend data, based on the predefined categories that they used, might have contained information that overlapped with the information contained in the data on the previous week’s claims, in addition to some noise that may have reduced out-of-sample predictive accuracy. To generate our data set, we asked 545 Turkers (58.5% female, average age 33) to perform the word association task described above. In this case, the task description was “Please write five terms that come to mind when seeing the phrase unemployment.” For each term reported by at least 1% of the Turkers, we collected the weekly search index from Google Trends (overall, 91 valid terms were identified, prior to applying the feature selection procedure). We replicated the exact processing and modeling steps reported in Choi and Varian, including the same process of

MIS Quarterly Vol. 40 No. 4/December 2016

955

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

adjusting for seasonality as well as using an expanding window, one-step-ahead prediction methodology.23 Specifically, we used the list of search trends derived from our data selection procedure and reran a simple linear regression model as detailed in equation (5) for each one-stepahead prediction:

WordNet.24 We found that predictions based on our data selection methodology (obtaining an MAE of 3.24%) also outperformed Google Correlate-based search trend selection which obtained an MAE of 3.45% and that of WordNet-based terms which obtained an MAE of 3.69%. Unemployment Prediction Sensitivity Analysis: Number of Search Trends For robustness, we provide in Figure 7 a detailed analysis of the case in which the crowd-squared procedure is constrained to select a fixed number of search trends, from a single feature up to 20 features. This figure also presents the outcome of imposing a similar constraint on the number of search trends based on data selected by Google Correlate and WordNet. This figure shows that when four or more search trends are used, the crowd-squared data selection method consistently obtains better results compared with any of the four benchmarks. It is also interesting to note that the Google Correlate-based model and WordNet-based model show consistent performance degradation when additional search trend-based data are added to the model. Such a degradation can occur when search trend data based on additional terms suggested by these sources does not carry additional predictive information, beyond the (clearly potent) autoregressive component of each model. Therefore, adding such variables to the model would add noise and would cause performance degradation on outof-sample measurements. In contrast, using the crowdsquared approach and adding search trend data (up to 20 search trends) generally improves out-of-sample predictive performance. This indicates that the crowd-squared methodology can consistently select search trends that carry additional predictive information beyond the information carried by the potent autoregressive component of the model. Last, we note that beyond demonstrating the performance of the crowd-squared method, the capacity to obtain accurate predictions of unemployment claims has considerable practical value. Specifically, given that initial claims for unemployment are a leading economic indicator, the ability to more accurately predict this measure could enable financial traders to make better investment decisions, and could perhaps even support the decisions of policy makers.

UIC(t ) = α + δ1UIC(t − 1) + i βi Termi (t ) + ε

(5)

where UIC(t) is the logarithm of the seasonally adjusted volume of initial claims for unemployment for week t, and Termi(t) is the search trend value for the crowd-squared-based term i at week t. We applied this model to the same period used in Choi and Varian (see Figure 6 for a comparison of the prediction model and actual unemployment claims data). As detailed in Table 3, our prediction model obtained an outof-sample MAE value of 3.24%. This value is better than the MAE value for the competent AR1 baseline model (3.37%) and is also superior to the MAE value (3.68%) reported by Choi and Varian. (We report results using MAE as the performance measure to maintain compatibility with Choi and Varian’s performance measure.) This demonstrates that the crowd-squared data selection method can perform as well as or better than a competent baseline model even when other methods such as using Google’s automated classifier (combined with experts hand-picking the classifier categories) fail. The results are even more noteworthy in light of the fact that outperforming the AR1 model under the experimental settings of this research is a challenging task. This is because of two main factors: first, as detailed in the original paper by Choi and Varian, the dependent variable exhibits patterns that are very close to random walk behavior. This fact makes it very difficult to outperform an AR1 model. Second, we constrained our analysis to use the same setting as in Choi and Varian. This entailed using a linear model specification that already included a dominant AR1 component, providing only a relatively minor margin for improvement. For consistency with our flu outbreak analysis, we compared our results to the results of two additional, external benchmarks: choosing search trends based on terms provided by the Google Correlate tool and lexical terms obtained from

To maintain consistency with Choi and Varian, we also seasonally adjusted the associated terms using the STL function in R. Additionally, since Choi and Varian used an expanding window methodology, our feature selection procedure was repeated for each expanding window iteration.

23

For robustness, we also tested prediction based on search terms selected by 15 experts (economics and business professors or Ph.D.s). The crowdsquared method obtained similar or better results in comparison with predictions based on experts’ terms.

24

956

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Table 3. MAE Results for Predicting Initial Claims for Unemployment Benefits Using Different Data Selection Methods
Crowd-Squared 3.24% AR (1) Model 3.37% Choi and Varian 3.68% Google Correlate 3.45% WordNet Lexicon 3.69%

Figure 6. Comparison of the Association-Based Model Predictions with Actual Reported Claims for Unemployment Published by the U.S. Department of Labor

Figure 7. MAE Results Wen Using a Fixed Number of Features

MIS Quarterly Vol. 40 No. 4/December 2016

957

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Conclusions
In the era of big data, accurate monitoring of current events and predictions of future activities are key challenges faced by managers as well as researchers. However, a critical aspect that hinders the use of large-scale crowd-based data for prediction is the lack of an effective method for selecting relevant data associated with the prediction of a specific phenomenon of interest. In this study, we focus on this important problem in largescale data environments and propose a structured and practical approach to address it, in the specific challenging context of search trend data. Specifically, our method offers a novel use of the crowd: crowdsourcing data selection. The method utilizes one crowd to generate a high-coverage list of terms representing the search terms that a larger crowd is expected to use when seeking information about a phenomenon that we wish to predict. We demonstrate this approach and show that it can achieve similar or better prediction accuracy compared with categories hand-picked by expert researchers, high-power big-data technologies applied over large-scale and proprietary search log data, as well as compared to additional benchmarks. We achieved this improved accuracy while intentionally constraining our predictive analysis to use the exact same algorithms, data sets, and performance measures used in previous well-known studies. Our results emphasize the importance of the data selection method in the prediction process, and demonstrate the utility of the crowd-squared concept.

procedures. The importance of recalibrating big data-based prediction models was recently discussed by Lazer et al. (2014). Using experts for repeated data selection would be a costly undertaking. In contrast, the crowd-squared method is inexpensive, given its simplicity and the fact that the entire procedure (including running the Mechanical Turk task) can be done programmatically with forecasts being continuously updated to provide information to support managerial decisions. Another potential benefit of our proposed approach arises in cases where the exact relevant search terms are unknown (for instance, when a new product is launched). Even in cases in which some prior knowledge exists, our proposed method can suggest new related search terms that can potentially improve predictive accuracy.

Limitations and Future Research
While our method has shown its usefulness in two different settings and in comparison with multiple benchmarks, it is expected to suffer from the basic limitations associated with the use of search trend data. For instance, it is important to note that people who perform online searches do not necessarily constitute a representative sample of the population. For example, elderly people or people with low income tend to use the Internet less often, which could lead to inaccurate predictions in some domains. In addition, due to privacy constraints, Google makes search volume data available only when the number of searches of a specific term reaches a threshold that precludes the possibility of using the aggregated data to identify the searchers. As a result, small-scale phenomena, or events that occur in areas with a low population density, will not be made available by these search tools. In a similar manner, the use of crowd-squared search trend selection may also fail to generate a representative sample of the population and may be unsuitable for areas with small populations or areas with a low level of technology adoption. Nevertheless, since crowd demographic properties can be collected as a part of the online task, this enables search terms to be matched to the target group whose behavior one wishes to predict. For example, a crowd of people between the ages of 20 and 25 may be used as the sample for search term selection for sales predictions of a product that is commonly purchased by people of that age group. In future research we plan to analyze this type of custom crowd selection as a possible enhancement of the crowd-squared-based method. We also note that, although our crowd-based approach provides some clear advantages compared to using experts’

Managerial Implications
The successful performance of the crowd-squared method across different domains and in comparison to different benchmarks indicates its robustness. Thus, we expect that this method will have managerial implications that go beyond the specific domains used in this study. Clearly, accurate prediction is crucial in many business applications, including inventory and manufacturing planning, budgeting, marketing activities, and others. Therefore, a structured method to address the data selection problem and amplify the predictive power of search-volume data could be useful for a host of businesses. Moreover, various aspects of the crowd-squared method make it practical and feasible in terms of cost. Direct costs for prompting the crowds to provide terms are very low (approximately $35, see the detailed method cost estimation in the appendix). In addition, this method is particularly efficient for recalibrating modeling

958

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

knowledge (e.g., potential cost savings, ease of updating), in certain settings the use of expert knowledge is likely to outperform our approach. Importantly, the applicability of our method is contingent on the crowd having some level of knowledge regarding the phenomenon of interest. When common knowledge regarding a given task is expected to be limited (e.g., if a company is trying to predict component failures), or if the topic is difficult to understand, the crowd is not expected to be able to generate a relevant subset of data. In this case, we expect that methods applying expert knowledge will produce better results. Moreover, although our method has several inherent properties that are expected to substantially limit the likelihood of SPAM outputs (as discussed in the “Methodology” section), reliance on expert knowledge eliminates this concern altogether. In general, we think that our method has potential applicability to other domains and applications which constitute avenues for future research. First, while our analysis focused on search trend data, the crowd-squared method could also be implemented in a broader context of online crowd-based data usage, including various forms of online social media such as online discussion forums, user reviews, and blogs. By offering a structured method to identify relevant terms and subsequently identify corresponding data, our method has the potential to improve predictive performance in these domains. Second, our method could be applicable to improving the effectiveness of online ads in sponsored search. In sponsored search, firms bid on specific keywords that consumers are searching for and are considered to be relevant to their products or services. The firm with the winning bid is allocated a “sponsored search” slot above the organic search results, in which a link to its ad appears. Therefore, using the crowdsquared method to capture the set of terms that people consider as highly related to a focal product or service seems to be a useful method that would allow firms to bid on a more accurate set of keywords, increase ad conversion rates, and decrease their costs per click.

References
Archak, N., Ghose, A., and Ipeirotis, P. 2011. “Deriving the Pricing Power of Product Features by Mining Consumer Reviews,” Management Science (57:8), pp. 1485-509. Bayus, B. L. 2013. “Crowdsourcing New Product Ideas over Time: An Analysis of the Dell Ideastorm Community,” Management Science (59:1), pp 226-44. Bishop, C. M. 1995. Neural Networks for Pattern Recognition, New York: Oxford University Press. Bordino, I., Battiston, S., Caldarelli, G., Cristelli, M., Ukkonen, A., and Weber, I. 2012. “Web Search Queries Can Predict Stock Market Volumes,” PLoS ONE 7(7), e40014. Boudreau, K. J., and Lakhani, K. R. 2013. “Using the Crowd as an Innovation Partner,” Harvard Business Review (91:4), pp. 60-69. Brabham, D. C. 2008. “Crowdsourcing as a Model for Problem Solving an Introduction and Cases,” Convergence: The International Journal of Research into New Media Technologies (14:1), pp. 75-90. Breyer, B. N., Sen, S., Aaronson, D. S., Stoller, M. L., Erickson, B. A., and Eisenberg, M. L. 2011. “Use of Google Insights for Search to Track Seasonal and Geographic Kidney Stone Incidence in the United States,” Urology (78:2), pp. 267-271. Butler, D. 2013. “When Google Got Flu Wrong,” Nature (494:7436), pp. 155-156. Chen, H., Chiang, R. H., and Storey, V. C. 2012. “Business Intelligence and Analytics: From Big Data to Big Impact, “ MIS Quarterly (36:4), pp. 1165-1188. Choi, H., and Varian, H. 2012. “Predicting the Present with Google Trends,” Economic Record (88:s1), pp. 2-9. Da, Z., Engelberg, J., and Gao, P. 2011. “In Search of Sttention,” The Journal of Finance (66:5), pp. 1461-1499. D’Amuri, F., and Marcucci, J. 2012. “The Predictive Power of Google Searches in Forecasting Unemployment,” Bank of Italy Temi di Discussione (Working Paper No. 891). Drake, M. S., Roulstone, D. T., and Thornock, J. R. 2012. “Investor Information Demand: Evidence from Google Searches around Earnings Announcements,” Journal of Accounting Research (50:4), pp. 1001-1040. Du, R. Y., Hu, Y., and Damangir, S. 2015. “Leveraging Trends in Online Searches for Product Features in Market Response Modeling,” Journal of Marketing, (79:1), pp. 29-43. Du, R. Y., and Kamakura, W. A. 2012. “Quantitative Trendspotting,” Journal of Marketing Research (49:4), pp. 514-36. Fellbaum, C. 1998. WordNet, Oxford, UK: Blackwell Publishing Ltd. Gawlik, E., Kabaria, H., and Kaur, S. 2011. “Predicting Tourism Trends with Google Insights,” unpublished paper , retrieved from http://cs229.stanford.edu/proj2011/GawlikKaurKabariaPredictingTourismTrendsWithGoogleInsights.pdf Geva, T., Oestreicher-Singer, G., Efron, N., and Shimshoni, Y. 2017. “Using Forum and Search Data for Sales Prediction of High-Involvement Products,” MIS Quarterly, forthcoming.

Acknowledgments
We would like to thank the senior editors, the associate editor, and two reviewers for their constructive and insightful comments and suggestions. We also thank participants of WISE 2013, SCECR 2014, CIST 2014, ICIS 2014, ZEW-EICT 2015, and the MISQ workshop at KU Leuven 2015 for their valuable comments and feedback. This study benefitted from support of MIT IDE, Israel Science Foundation (ISF grant #1443/14) and the Henry Crown Business Research Center.

MIS Quarterly Vol. 40 No. 4/December 2016

959

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Ghose, A., Ipeirotis, P. G., and Li, B. 2014. “Examining the Impact of Ranking on Consumer Behavior and Search Engine Revenue,” Management Science (60:7), pp. 1632-1654. Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., and Brilliant, L. 2009. “Detecting Influenza Epidemics Using Search Engine Query Data,” Nature (457:7232), pp. 1012-1014. Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., and Watts, D. J. 2010. “Predicting Consumer Behavior with Web Search,” Proceedings of the National Academy of Sciences (107:41), pp. 17486-17490. Gunn III, J. F., and Lester, D. 2013. “Using Google Searches on the Internet to Monitor Suicidal Behavior,” Journal of Affective Disorders (148:2), pp. 411-412. Harris, C. 2011. “You’re Hired! An Examination of Crowdsourcing Incentive Models in Human Resource Tasks,” in Proceedings of the Workshop on Crowdsourcing for Search and Data Mining (CSDM) at the Fourth ACM International Conference on Web Search and Data Mining (WSDM), M. Lease, V. Carvalho, and E. Yilmaz (eds.), Hong Kong, China, pp. 15-18. Howe, J. 2006. “The Rise of Crowdsourcing,” Wired Magazine (14:6), pp. 1-4. Hill, S., Merchant, R., and Ungar, L. 2013. “Lessons Learned About Public Health from Online Crowd Surveillance,” Big Data (1:3), pp. 160-167. Hu, Y., Du, R. Y., and Damangir, S. 2014. “Decomposing the Impact of Advertising. Augmenting Sales with Online Search Data,” Journal of Marketing Research (51:3), pp. 300-319. Ipeirotis, P. 2010. “A Computer Scientist in a Business School,” online blog (http://www.behind-the-enemy-lines.com/search?q= spam). Kristoufek, L. 2013. “BitCoin Meets Google Trends and Wikipedia: Quantifying the Relationship Between Phenomena of the Internet Era,” Scientific Reports (3:3415). Lakhani, K. R., Jeppesen, L. B., Lohse, P. A., and Panetta, J. A. 2007. “The Value of Openness in Scientific Problem Solving,” Working Paper, Harvard Business School, Boston. Lazer , D., Kennedy, R., King, G., and Vespignani, A. 2014. “The Parable of Google Flu: Traps in Big Data Analysis,” Science (343:6176), pp. 1203-1205 Lockwood, A., Jones, J., and Zhang, J. 2006. “Does Search Matter? Using Clickstream Data to Examine the Relationship Between Online Search and Purchase Behavior,” in Proceedings of the 27th International Conference on Information Systems, Milwaukee, WI. Malone, T. W., Laubacher, R., and Dellarocas, C. 2009. “Harnessing Crowds: Mapping the Genome of Collective Intelligence,” Sloan School Working Paper 4732-09, Massachusetts Institute of Technology, Cambridge, MA. McAfee, A., and Brynjolfsson, E. 2012. “Big Data: The Management Revolution,” Harvard Business Review (October), pp. 2-9. Miller, G. A. 1995. “WordNet: A Lexical Database for English,” Communications of the ACM (38:11), pp. 39-41. Moe, W. W., and Fader, P. S. 2004. “Dynamic Conversion Behavior at E-Commerce Sites,” Management Science (50:3), pp. 326-335.

Nelson, D. L., McEvoy, C. L., and Dennis, S. 2000. “What Is Free Association and What Does it Measure?,” Memory & Cognition (28:6), pp. 887-99. Nelson, D. L., McEvoy, C. L., and Schreiber, T. A. 2004. “The University of South Florida Free Association, Rhyme, and Word Fragment Norms,” Behavior Research Methods, Instruments, & Computers (36:3), pp. 402-407. Nelson, D. L., McKinney, V. M., Gee, N. R., and Janczura, G. A. 1998. “Interpreting the Influence of Implicitly Activated Memories on Recall and Recognition,” Psychological Review (105:2), pp. 299-324. Porter, M. F. 1980. “An Algorithm for Suffix Stripping,” Program (14:3), pp. 130-137. Preis, T., Moat, H. S., and Stanley, H. E. 2013. “Quantifying Trading Behavior in Financial Markets Using Google Trends,” Scientific Reports (3:1684). Preis, T., Reith, D., and Stanley. H. E. 2010. “Complex Dynamics of Our Economic Life on Different Scales: Insights from Search Engine Query Data,” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences (368:1933), pp. 5707-5719. Provost, F., and Fawcett, T. 2013. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking, Sebastopol, CA: O’Reilly Media, Inc. Reilly, S., Richey, S., and Taylor, J. B. 2012. “Using Google Search Data for State Politics Research: An Empirical Validity Test Using Roll-Off Data,” State Politics & Policy Quarterly (12:2), pp. 146-159. Schenk, E., and Guittard, C. 2011. “Towards a Characterization of Crowdsourcing Practices,” Journal of Innovation Economics & Management (7:1), pp. 93-107. Scharkow, M., and Vogelgesang, J. 2011. “Measuring the Public Agenda Using Search Engine Queries,” International Journal of Public Opinion Research (23:1), pp. 104-113. Shmueli, G., and Koppius, O. 2011. “Predictive Analytics in Information Systems Research,” MIS Quarterly (35:3), pp. 553-572. Snow, R., O’Connor, B., Jurafsky, D., and Ng, A. Y. 2008. “Cheap and Fast—But Is It Good? Evaluating Non-Expert Annotations for Natural Language Tasks,” in Proceedings of the Conference on Empirical Methods in Natural Language Processing, Stroudsburg, PA: Association for Computational Linguistics, pp. 254-263. Sparrow, B., Liu, J., and Wegner, D. M. 2011. “Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips,” Science (333:6043), pp. 776-778. Steyvers, M., and Tenenbaum, J. B. 2005. “The Large Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth,” Cognitive Science (29:1), pp. 41-78. Von Ahn, L. 2006. “Games with a Purpose,” Computer (39:6), pp. 92-94. Vosen, S., and Schmidt, T. 2011. “Forecasting Private Consumption: Survey Based Indicators vs. Google Trends,” Journal of Forecasting (30:6), pp. 565-78. Wagner, M. M., Tsui, F. C., Espino, J. U., Dato, V. M., Sitting, D. F., Caruana, R. A., and Fridsma, D. B. 2001. “The Emerging

960

MIS Quarterly Vol. 40 No. 4/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Science of Very Early Detection of Disease Outbreaks,” Journal of Public Health Management and Practice (7:6), pp. 51-59 Witten, I. H., Frank, E., and Hall, M. A. 2011. Data Mining: Practical Machine Learning Tools and Techniques, San Francisco: Morgan Kaufmann Publishers. Woolley, A. W., Chabris, C. F., Pentland, A., Hashmi, N., and Malone, T. W. 2010. “Evidence for a Collective Intelligence Factor in the Performance of Human Groups,” Science (330:6004), pp. 686-688. Wu, L., and Brynjolfsson, E. 2009. “The Future of Prediction: How Google Searches Foreshadow Housing Prices and Quantities,” in Proceedings of the 30th International Conference on Information Systems, Phoenix, AZ. Yan, T., Kumar, V., and Ganesan, D. 2010. “CrowdSearch: Exploiting Crowds for Accurate Real-Time Image Search on Mobile Phones,” in Proceedings of the 8th International Conference on Mobile Systems, Applications, and Services, New York: ACM, pp. 77-90. Yang, A. C., Huang, N. E., Peng, C. K., and Tsai, S. J. 2010. “Do Seasons Have an Influence on the Incidence of Depression? The Use of an Internet Search Engine Query Data as a Proxy of Human Affect,” PloS one (5:10), e13728.

author or coauthor of several books, including The Second Machine Age. He holds Bachelor’s and Master’s degrees from Harvard University in Applied Mathematics and Decision Sciences and a Ph.D. from MIT in Managerial Economics. Tomer Geva is an assistant professor at Tel Aviv University School of Management. His research interests include using large-scale data for business decision making, and the effective use of crowdbased information for predictive modeling. Previously he was a visiting scholar at New York University’s Stern School of Business and a post-doctoral research scientist at Google. Tomer holds a Ph.D. and an MBA (cum laude) from Tel-Aviv University and a B.Sc. (cum laude) in Industrial Engineering from the Technion — Israel Institute of Technology. Prior to his Ph.D. studies, he held various engineering and management positions in the high-tech industry. His research has been published in Information Systems Research and Decision Support Systems. Tomer teaches data mining and business analytics courses at Tel Aviv University where he heads the Business Analytics Program. Shachar Reichman is an assistant professor at Tel Aviv University School of Management and research affiliate at MIT Sloan School of Management. His research focuses on utilizing unique and interesting structures found in online data environments to improve consumers’ experiences and businesses’ performances. Previously, he was a post-doctoral associate at the Sloan School of Management at the Massachusetts Institute of Technology. His prior research has been published in Journal of Marketing Research, Operations Research, and proceedings of the International Conference on Information Systems. He received his Ph.D. from Tel-Aviv University’s School of Management. He holds B.Sc. and M.Sc. degrees in industrial engineering from Ben-Gurion University.

About the Authors
Erik Brynjolfsson is the Schussel Family Professor at the MIT Sloan School, the director of the MIT Initiative on the Digital Economy, a research associate at the NBER, and chairman of the MIT Sloan Management Review. His research examines the effects of information technologies on business strategy, productivity and performance, Internet commerce and intangible assets. He is the

MIS Quarterly Vol. 40 No. 4/December 2016

961

962

MIS Quarterly Vol. 40 No. 4/December 2016

BIG DATA & ANALYTICS IN NETWORKED BUSINESS

CROWD-SQUARED: AMPLIFYING THE PREDICTIVE POWER OF SEARCH TREND DATA
Erik Brynjolfsson
Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA 02142 U.S.A. {erikb@mit.edu}

Tomer Geva
School of Management, Tel-Aviv University, Tel Aviv 6997801 ISRAEL {tgeva@tau.ac.il}

Shachar Reichman
School of Management, Tel-Aviv University, Tel Aviv 6997801 ISRAEL and Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA 02142 U.S.A. {shachar@mit.edu}

Appendix A
Crowd-Squared Search Terms Distribution
Table A1. Search Terms Generated by the Crowd
(a) Influenza – Crowd-Squared Search Terms and % of Turkers Mentioning Each Term Search % Search % Search % Term Mention Term Mention Term Mention sick 68% influenza 4% achy 2% fever 39% coughing 4% weak 2% cough 21% germs 4% bad 2% cold 16% headache 4% shots 2% shot 13% ache 4% body aches 2% vomit 11% runny nose 4% flu shot 2% ill sneeze vaccine virus medicine contagious tired 10% 10% 10% 9% 8% 8% 8% pain aches nausea bird miserable rest sore 4% 4% 4% 3% 3% 3% 3% sore throat soup tissue vomiting bug diarrhea sweat 2% 2% 2% 2% 2% 2% 2% Search Term fatigue home hospital nyquil season tea under the weather % Mention 1% 1% 1% 1% 1% 1% 1%

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

A1

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Table A1. Search Terms Generated by the Crowd (Continued)
(a) Influenza – Crowd-Squared Search Terms and % of Turkers Mentioning Each Term Search % Search % Search % Search % Term Mention Term Mention Term Mention Term Mention sickness 8% mucus 3% nose 1% illness 7% puke 3% throwing up 1% doctor 7% sneezing 3% vaccination 1% bed 6% swine 3% nasty 1% snot 6% congestion 3% stuffy 1% chills 5% death 3% tissues 1% disease 5% hot 2% green 1% sleep 5% stomach 2% sad 1% gross 5% winter 2% achey 1% (b) Initial Claims for Unemployment Benefits – Crowd-Squared Search Terms and % of Turkers Mentioning Each Term Search % Search % Search % Search % Term Mention Term Mention Term Mention Term Mention poor 23% economy 3% sadness 2% stressful 1% broke 20% hungry 3% search 2% struggling 1% jobless 15% desperate 3% unfortunate 2% angry 1% lazy 10% hunger 3% depressing 2% applications 1% money 10% loss 3% difficult 2% bad 1% sad 10% out of work 3% fear 2% compensation 1% no money 8% work 3% looking for 2% destitute 1% work poverty 8% boredom 3% rent 2% job loss 1% welfare 8% family 3% anger 1% searching 1% depression 7% food 2% check 1% uncertainty 1% job 7% insurance 2% helpless 1% uneducated 1% stress 6% anxiety 2% no work 1% worry 1% homeless 6% failure 2% scared 1% assistance 1% job search 5% food stamps 2% unlucky 1% bad economy 1% bills 5% hardship 2% boring 1% despair 1% no job 5% help 2% foreclosure 1% frustrated 1% laid off 4% hopeless 2% free time 1% loser 1% struggle 4% job hunting 2% frustration 1% no insurance 1% resume 4% jobs 2% hard times 1% panic 1% fired 4% scary 2% interviews 1% rate 1% depressed 3% debt 2% needy 1% sucks 1% bored 3% government 2% not working 1% worthless 1% benefits 3% obama 2% recession 1%

A2

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Appendix B
Significance Values for Crowd-Squared Improvement Over Benchmark1
Our goal in the empirical evaluation section of this paper was to test whether our method, which is structured and transparent yet simple, achieves performance that is at least equivalent to the performance of existing benchmarks. We find that, in most cases, the crowd-squared method not only performs at the same level of existing benchmarks but actually obtains better results. In Table B1 below we report the results of significance tests regarding this comparison.

Table B1. Significance Test of Performance Improvement
Significance for Crowd Squared Performance Improvement over Benchmark Model

Domain

Replicated Research and Evaluation Time Period Ginsberg et al. (2009), March 2007 – May 2008

Influenza Epidemics

Ginsberg et al. (2009), October 2012 – September 2013 (Period as in Butler 2013) Lazer et al. (2014)

Initial Claims for Unemployment Benefits *p value < 0.1
a

Choi and Varian (2012)

Benchmark Model Ginsberg et al.’s Model Google Correlate WordNet Lexicon Simple AR Benchmark Ginsberg et al.’s Model Google Correlate WordNet Lexicon Simple AR Benchmark Google Flu Trends Lazer et al.’s Model AR Model Choi and Varian’s Model Google Correlate WordNet Lexicon

*** *** * * *** *** *** * n/aa * ***

**p value < 0.05

***p value < 0.01

We do not include results pertaining to Choi and Varian’s (2012) model. This is due to recent changes in Google Trends categories that prohibit using the same categories and reconstructing the weekly level predictions and prediction errors of Choi and Varian’s model. Nevertheless, we note that our results were significantly better than the Google Correlate model’s results over the same data, which, in turn, outperforms Choi and Varian’s reported results

The different studies that we replicated used different performance measures and required different methods for significance value calculations. We used bootstrap p-values for significance testing when replicating the study by Ginsberg et al. (2009), which used correlation with CDC-reported ILI as a performance measure. We used the Diebold-Mariano test, which was used in Lazer et al. (2014), to calculate significance values for the MAE performance measure reported in Lazer et al. and in Choi and Varian (2012).

1

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

A3

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Appendix C
Participant Demographics
Table C1. Reported Demographics for Crowd Squared Participants and Data Collection Costs
Influenza Epidemics (N = 535) Age Gender Education Level • Bachelor degree • Master degree • Some College • Professional degree • High School • Doctorate degree Number of Participants’ States Mechanical Turk Cost Avg. 29.1 (std. 9.12) 40% female 210 39 212 10 53 11 48 $35.31 Initial Claims for Unemployment Benefits (N = 545) Avg. 33 (std. 11) 58.5% female 206 56 209 12 56 6 47 $32.11

Appendix D
Correlation as a Performance Measure
As detailed in the main body of the paper, we used an experimental design geared to provide a fair comparison with previous studies. This requires that when comparing the crowd-squared method to an alternative methodology used in a prior study, we use the same goal and performance measures adopted in that study. Specifically, Ginsberg et al. (2009) aimed to obtain the best correlation, whereas Lazer et al. (2014) and Choi and Varian (2012) sought to minimize MAE. Therefore, in different comparisons, our method was evaluated on the basis of different performance measures. Nevertheless, for robustness we present in Tables D1 and D2 correlation results for models originally set to optimize MAE. As shown in these tables, our method obtains comparable or superior results, in terms of correlation, compared to the benchmark studies and models, even though the goal we set was to improve MAE. We note, however, that performance improvement using correlation values for the models set to improve MAE was (expectedly) smaller in scale.

Table D1. Correlation Results with CDC-Reported ILI for Prediction Models Using Different Data Selection Methods (Comparison to Lazer et al. 2014)
Crowd-Squared 0.96722 Lazer et al. 0.95608 Google Flu Trends 0.86779

A4

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Table D2. Correlation Results with Initial Claims for Unemployment Benefits Using Different Data Selection Methods
Crowd-Squared 0.98137 AR (1) Model 0.98132 Choi and Varian n/aa Google Correlate 0.98074 WordNet Lexicon 0.98035

a We do not include results pertaining to Choi and Varian's model. This is due to recent changes in Google Trends categories that prohibit using the same categories and reconstructing the weekly-level predictions and prediction errors of Choi and Varian's model. Nevertheless, we note that our results were significantly better than the Google Correlate model's results over the same data, which, in turn, outperforms Choi and Varian's reported results

Appendix E
Additional Analysis 2014–2015
In the main body of the paper, we provide a comparison of our data selection method performance with data selection methods used in previous studies while using the same time periods reported in these studies. For robustness, we evaluate whether the crowd-squared method could provide comparable or better results to the above benchmarks studies and models on recent data. For this purpose we evaluate our method performance over a full year of out-of-sample data (May 1, 2014–April 30, 2015) immediately following the month in which we ran the online word-association task (April 2014). The results are detailed in Tables E1 and E2 10 and show that in accordance with the previous findings, our method obtain comparable or better results to the benchmarks.

Table E1. Correlation Results with CDC-Reported ILI for Prediction Models Using Different Data Selection Methodsa
Crowd-Squared 0.982
a

Google Flu Trends 0.985

Google Correlate 0.976

WordNet Lexicon 0.972

Simple Benchmark 0.937

Recently Google reported about changes to their flu trend system which now also uses lagged CDC data as predictors. Unfortunately, at the current time Google has not yet provided details about the specifics of their method. For the sake of comparison with the Google flu trend system we added to our model only a single lag of CDC data (lag t-2) and weekly dummy variables (as in the Lazer et al. reference that Google report provided). See also http://googleresearch.blogspot.co.il/2014/10/google-flu-trends-gets-brand-new-engine.html, accessed July 2015.

Table E2. MAE Results for Predicting Initial Claims for Unemployment Benefits Using Different Data Selection Methods
Crowd-Squared 3.37%
a

AR (1) Model 3.44%

Choi and Varian (2012) n/aa

Google Correlate 3.75%

WordNet Lexicon 3.47%

We do not include results pertaining to Choi and Varian's model. This is due to recent changes in Google Trends categories that prohibit using the same categories and reconstructing the weekly-level predictions and prediction errors of Choi and Varian's model.

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

A5

Brynjolfsson et al./Amplifying the Predictive Power of Search Trend Data

Appendix F
Cost and Time Estimation
Comprehensive Scana 1 billion queries Data download (computer hours) Data storage Model analysis and predictions(computer hours) Payments to participants (through AMT) Data Scientists
a b

Crowd- Squared 0.007 computer hours (~100 queries) ~ 2 MB < 1 hour 500 participants × $0.06 per task = $30 + 10% AMT platform cost = $33 2days

Prior Knowledge and Intuitionb 0.007 computer hours (up to 100 queries) ~ 2MB < 1 hour

50 million queries ~2,800 ~ 1TB ~20 hours

~55,000 ~ 20 TB ~20 hours

— 2days

— 2 days

— 2 days

Based on the analysis process reported in Ginsberg et al. (2009). Based on the analysis process reported in Choi and Varian et al. (2012).

Cost Estimation Assumptions
• Comprehensive scan includes two options: – Downloading data for 1 billion queries and then finding the most popular 50 million search queries, assuming there is no prior information about the most popular search terms. – Downloading data on the most popular 50 million queries, assuming that the search engine publishes information on the popularity of search terms. Average download time for query trend data: 0.2 second. Average file size for data for a single query trend: 20 KB. 500 participants in the crowd-squared tasks. Average payment to participant: $0.06. Expert (data scientists) time to perform the complete analysis in any method: 2 days.

• • • • •

References
Choi, H., and Varian, H. 2012. “Predicting the Present with Google Trends,” Economic Record (88:s1), pp. 2-9. Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., and Brilliant, L. 2009. “Detecting Influenza Epidemics Using Search Engine Query Data,” Nature (457:7232), pp. 1012-1014. Lazer , D., Kennedy, R., King, G., and Vespignani, A. 2014. “The Parable of Google Flu: Traps in Big Data Analysis,” Science (343:6176), pp. 1203-1205

A6

MIS Quarterly Vol. 40 No. 4—Appendices/December 2016

Copyright of MIS Quarterly is the property of MIS Quarterly and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.

