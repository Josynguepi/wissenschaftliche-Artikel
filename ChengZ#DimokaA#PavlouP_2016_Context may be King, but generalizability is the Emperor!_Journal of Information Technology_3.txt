Journal of Information Technology (2016) 31, 257–264
ª 2016 Association for Information Technology Trust All rights reserved 0268-3962/16 www.palgrave.com/journals

Commentary

Context may be King, but generalizability is the Emperor!
Zhi (Aaron) Cheng, Angelika Dimoka, Paul A. Pavlou
Fox School of Business, Temple University, Philadelphia, USA Correspondence: Zhi (Aaron) Cheng, Fox School of Business, Temple University, Philadelphia, USA E-mail: acheng@temple.edu

Abstract The relative importance of context and generalizability (or particularism and universalism) has long been debated in scientiﬁc research. Recently, Davison and Martinsons raised valid concerns about the possibility of false universalism in IS research, discussed its negative consequences, and made a call for explicitly including particularism in research design and reporting. In this commentary, we generally agree with the notion that context should matter more in IS research; yet, the importance of generalizability in research should not be downplayed. Speciﬁcally, we posit that generalizability should be given higher position in the scientiﬁc process and be the ultimate goal for researchers. Still, researchers need to fully understand the research context, which, in combination and replication, can help to cautiously make generalizable knowledge claims. Therefore, we characterize the relationship between context and generalizability as that of a ‘‘King’’ (as an analogy of the local role of context) versus the ‘‘Emperor’’ (as an analogy of the global role of generalizability). Journal of Information Technology (2016) 31, 257–264. doi:10.1057/s41265-016-0005-7; published online 21 September 2016

he relative importance of context and generalizability (or particularism and universalism) has been debated at great lengths in scientiﬁc research in multiple disciplines for a long time (e.g., McGrath and Brinberg, 1983; Deaton, 2010; Lee and Baskerville, 2012). While generalizability is typically deemed to be the ‘‘gold standard’’ of scientiﬁc research by proposing universal knowledge claims, skeptics of generalizability have questioned the ability to generalize empirical ﬁndings from a speciﬁc narrow context (such as from a controlled lab experiment or an industry case study) to virtually all situations (often also termed external validity). The motivation of Davison and Martinsons’s work stems from a basic question, how far can research validity reasonably be expected in terms of generalizability and universalism, given the limited context in which most empirical research is conducted in practice? Their basic premise is reasonable and justiﬁable; that is, researchers are better off conducting context-speciﬁc research to avoid getting into the trap of universalism and trying to prove the generalizability of research ﬁndings in a futile fashion from their relatively narrow research context to virtually all contexts.

T

Davison and Martinsons (p. 1) observed ‘‘… the acceptance of papers, that falsely imply universalism, rely on convenient samples, or ignore indigenous constructs.’’ driving them to make a call for explicitly and thoughtfully considering context in scientiﬁc research. Their call is not the only one. For example, Seddon and Scheepers (2012, p. 10) examined two leading IS journals in 2007 and 2008 showed that ‘‘many quantitative studies need clearer deﬁnition of populations and more discussion of the extent to which ‘signiﬁcant’ statistics and use of nonprobability sampling affect support for their knowledge claims. Many qualitative studies need more discussion of boundary conditions for their sample-based general knowledge claims.’’ Another investigation by Li et al. (2014) and his colleagues showed that in their sample, 63 out of 115 studies, which were conducted in China and published in the eight ‘‘senior IS scholars’ Basket of Journals’’ from 2000 to 2013, tend to be under-contextualized, meaning that most studies did not provide a rich discussion of the cultural, institutional, and national aspects of the focal Chinese context. Therefore, there is strong basis for Davison and Martinsons to make a call for focusing on context to better understand the dynamics of a particular research problem.

Context may be King, but generalizability is the Emperor! 258

A. Cheng et al.

In this commentary, we generally agree with the notion that context should matter more in IS research; nonetheless, the importance of generalizability in research cannot be downplayed. Speciﬁcally, we posit that generalizability should be given higher position in the scientiﬁc progress and be the ultimate goal for researchers. Although there have been debates on how easy generalizability can be achieved, it is possible to mitigate the difﬁculties toward pursuing generalizability. Especially for IS research, we have a tradition on replication, triangulation, meta-analyses, and interdisciplinary studies, which requires a deep understanding of the research context. By focusing on context, we can cautiously make generalizable knowledge claims. Hence, in this paper, we characterize the relationship between context and generalizability as that of a ‘‘King’’ (as an analogy of the local or relatively narrow importance of context) versus the ‘‘Emperor’’ (as an analogy of the global or universal importance of generalizability). The subsequent sections discuss the (1) importance of context in IS research and related disciplines; (2) the importance of generalizability, and why we need to think beyond context-speciﬁc studies from a historical and philosophical view; (3) recommend solutions for promoting generalizability to facilitate scientiﬁc progress in the IS literature, and 4) promote generalizability while raising a cautionary tale on the credibility and applicability of generalizations from empirical ﬁndings. The importance of context in scientiﬁc research To better understand and model the real world, researchers must ﬁrst understand and properly describe their research context. Contextualization makes research models more accurate and the interpretation of results more robust. In Rousseau and Fried (2001)’s paper titled ‘‘Location, location, location,’’ contextualization was given a prominent role in organizational research. Contextualization studies can be divided into two categories: one category focuses on testing and reﬁning theories to a new context by adding contextspeciﬁc factors and the second one focuses on building new theory by grounding contextual variables and relationships within a new setting (e.g., Barney and Zhang, 2009; Tsui, 2006). Either way can be effective depending on the phenomenon the researcher observes and the research questions that are studied by the empirical research (Barney and Zhang, 2009). In this sense, research turns out to be a location strategy with the metaphor, know thy context before thinking how to win. IS scholars have also discussed the signiﬁcance of context. Avgerou (2001) pointed out that most IS research tends to develop general knowledge around IT but ‘‘lacks systematic consideration of variations of organization and broader context within which the innovation is embedded’’ (p. 103). Hong et al. (2013) also highlighted the importance of research context and provided a framework and guidelines for context-speciﬁc theorizing in IS research. In addition to papers that stressed the importance of context, many IS studies explicitly incorporated context into their studies, such as the context of government (e.g., Pang et al., 2014), auction design (e.g., Adomavicius et al., 2005; Hong et al., 2015), country-speciﬁc studies (e.g., Atasoy et al., 2016), speciﬁc online markets, such as eBay and Amazon (e.g., Pavlou and

Dimoka, 2006; Dimoka, 2010; Dimoka et al., 2012; Gefen and Pavlou, 2012; Hong and Pavlou, 2014), and privacy (e.g., Pavlou, 2011; Smith et al., 2011). Other studies have discussed the importance of industry, such as the logistics industry (e.g., Rai et al., 2012), new product development (e.g., Ettlie and Pavlou, 2006, Pavlou and El Sawy, 2006), and electronic commerce (e.g., Pavlou and Fygenson, 2006), among others. Furthermore, it is reasonable and highly justiﬁable for cross-cultural scholars to raise the awareness of context. This is because diverse cultures create variations in management practice at multiple levels, including national, cultural, and organizational (Leidner and Kayworth, 2006). Cultural differences motivate researchers to consider whether their knowledge claims formed in one context can hold elsewhere, requiring more thorough understanding of the relevant context (e.g., Avgerou, 2001). What Davison and Martinsons added to the conversation about context lies in two aspects: (a) contextual inadequacies could lead to misinterpretation of the scope of validity for research ﬁndings and conclusions and (b) prescriptions regarding ‘‘where to play’’ and ‘‘how to win’’ are provided to avoid false universalism and to some degree embrace particularism. But we think, more importantly, that they made an implicit point about what motivates scholars not to explicitly consider context in their empirical research. First, the implicit requirement (or delusion) for western norm-based theories in top journals is proposed by Davison and Martinson to be one of the reasons that scholars are reluctant to explicitly explain their context in their research. Because authors are concerned that readers show little interest to research conducted in nonwestern countries, since such empirical studies or theories may not readily generalize to western countries, they prefer not to emphasize the research context. Second, some scholars may simply not realize the problem of contextual inadequacies. In other words, scholars may simply neglect the role of context in pursuit of generalization. Also, while most such studies are indeed context-dependent, some scholars may naively believe that contextual factors do not restrict the generalizability of their study’s ﬁndings and liberally claim the generalizability of their ﬁndings. However, most academic papers, including those in IS research, do (and should) discuss the extent of generalization with a cautious and conservative tone. The importance of generalizability in scientiﬁc research We start our conversation on the importance of generalizability by starting from a critique written by Angus Deaton (2010), the 2015 Nobel laureate in economics, on instruments and randomization in development economics. Deaton reminded economic scholars of the distinction of ‘‘hunting cause’’ and ‘‘using’’ them when applying results of Randomized Controlled Trials (RCT) and experiment-like tools to inform public policy. He raised the awareness of generalizability by calling (p. 447): ‘‘We need to know when we can use local results, from instrumental variables, from RCTs, or from nonexperimental analyses, in contexts other than those in which they were obtained.’’ Deaton (p. 424) argued: ‘‘As with IV (Instrumental Variable) methods, RCTbased evaluation of projects, without guidance from an

Context may be King, but generalizability is the Emperor!

A. Cheng et al.

259

understanding of underlying mechanisms, is unlikely to lead to scientiﬁc progress in the understanding of economic development.’’ Deaton’s commentary was a sharp response to the increasing use of experimental techniques to evaluate development economics, irrespective of the underlying mechanism. Similarly, James Heckman, another well-known Nobel laureate, suggested using IV techniques (or matching or regression discontinuity designs) with caution. In consistence with Deaton, Heckman and his colleagues demonstrated that, in most cases, randomization methods cannot produce more insights than a structural model to understand a given economic phenomenon (Heckman and Urzua, 2010, p. 27). Proponents of causal modeling using either experimental or quasi-experimental methods have responded to these critiques by emphasizing the importance of internal validity versus external validity (or generalizability) (Imbens, 2010): ‘‘with the view that a credible estimate of the average effect for a subpopulation is preferred to an estimate of the average for the overall population with little credibility’’ (p. 5). They also justiﬁed their preference to internal validity by citing other disciplines, for instance, biomedical sciences, in order to show that emphasizing internal validity over external validity is common in the traditional sciences. This shows a common pain in scientiﬁc research, that is, peer inﬂuence among disciplines has stuck disciplines in context-bounded internal validity, while only few disciplines have taken serious initiatives to deal with external validity or generalizability. Since social psychologists Campbell and Stanley (1967) introduced the concept of external validity: ‘‘external validity asks the question of generalizability: To what populations, settings, treatment variables, and measurement variables can this effect be generalized?’’ (p. 5) the debate around generalizability has recurred without cease. In his American Psychologist paper, titled ‘‘In Defense of External Validity,’’ Mook (1983) argued that it is meaningless to justify external validity since such generalizability is not intended in most papers, which, however, are often accused of (p. 379): ‘‘failure to generalize to the real world.’’ Mook maintained that research lacking external validity should not be criticized but should be encouraged if its ﬁnding is interesting even for a lab experiment. Facing such controversy over years, Campbell (1986), the well-known methodologist, admitted the imperfect nature of the deﬁnition of external validity, which he coined himself, and suggested an approach to generalizability called the proximal similarity model, which he thought was a more suitable term than external validity. But the concept of external validity has already left an imprint into the minds of psychologists. Academia commonly accepts a similar deﬁnition as original one as external validity, the extent to which the results of a study can be generalized to other situations and to other people (Aronson et al., 2013). Whether external validity is useful is also a hotly debated topic in consumer research. Calder et al. (1982) argued that external validity should not be emphasized in theory testing because this could affect the rigor of internal validity. ‘‘Controlling, rather than varying and examining, background factors is encouraged.’’ (p. 240). Since confounding background factors can be controlled through randomized controlled trials, these background factors are viewed as black box. This was the exact point attacked by the advocates of generalizability (e.g., Lynch, 1982, 1983, 1999) who

maintained that scholars should bridge lab experiments with the real world. Lynch (1982, p. 225) believed that the: ‘‘usefulness of experimental results is affected by researchers’ treatment of unmanipulated background factors in designing and analyzing the experiment.’’ More in-depth understanding of the interaction between treatment and background (or contextual) factors could help create a lab environment that simulates the real world. Nonetheless, the advocates of external validity did not provide a direct guidance how researchers could prove the external validity or generalizability of their lab studies. In the ﬁeld of management science, since Rosenzweig’s (1994, p. 28) asked: ‘‘When can management science research be generalized internationally?’’ management scholars started to evaluate their research by incorporating considerations of external validity or generalizability (e.g., Miller and Dess 1993; Lukka and Kasanen 1995; Hubbard et al., 1998; Scandura and Williams, 2000; Short et al., 2002; Boyd et al., 2005). Approaches, such as replication (Hubbard et al., 1998), triangulation (Scandura and Williams, 2000), and meta-analysis (Joshi and Roh, 2009), were demanded for a cumulative intellectual tradition in the ﬁeld of management science. Taken together, many disciplines have debated the notions of generalizability (or external validity) and internal validity, and several approaches have been proposed to improve the validity of empirical research in terms of internal validity and external validity (or generalizability). These disciplines have also informed the discussions of generalizability in the IS ﬁeld, as discussed next. Generalizability in information systems research The issues regarding external validity or generalizability in economics and psychology are more or less the same issues for IS research because IS research has beneﬁtted from the importation of theories and methodologies from economics and psychology. However, compared to economics and psychology, the discussion of generalizability in the IS literature is mild. Perhaps IS scholars are still sticking and dealing with internal validity (and perhaps construct validity and reliability (Boudreau et al., 2001), that is, how well a study is conducted to establish causality by eliminating confounding effects (Shadish et al., 2002). As Campbell and Stanley (1967, p. 5) suggested: ‘‘both types of criteria (internal and external validity) are obviously important… and … the selection of designs strong in both validity is obviously our ideal,’’ so too we IS scholars should ﬁgure out how to better justify both generalizability and internal validity. In the IS literature, only a few papers that call for generalizability provide tangible recommendations. Seddon and Scheepers (2012) provided guidance with several alternative logical pathways to justify generalizability. Three key concepts underpinning the framework are (p. 6): ‘‘the need for researcher judgment when making any claim about the likely truth of sample-based knowledge claims in other settings; the importance of sample representativeness and its assessment in terms of the knowledge claim of interest; and the desirability of integrating a study’s general knowledge claims with those from prior research.’’ The framework is based on two generalization logics, statistical and analytical,

Context may be King, but generalizability is the Emperor! 260

A. Cheng et al.

for quantitative and qualitative IS studies, respectively. However, quantitative and qualitative labels constitute a rough classiﬁcation used in the IS literature. According to Gregor (2006), IS studies can be classiﬁed by ﬁve distinct types of theories, including analysis, explanation, prediction, explanation and prediction, and design and action. Gregor argued the level of generality can be chosen depending on the type of theory used in the scientiﬁc inquiry. Also, to establish a meta-theory or a mid-range theory requires different levels of generality and abstraction, for instance, structuration theory is at a high level of generalizability and can be viewed as context-free instead of context-bounded. Therefore, the degree of generalizability depends on the type of theory one wants to develop. Polit and Beck (2010) provided a more general, hands-on list of strategies for researchers to enhance their ability to derive generalized inferences, including replication in sampling, replication in studies (multiple setting, varied times, and different types of people), integration of evidence (metasynthesis), thinking conceptually and reﬂexively, ‘‘know thy data,’’ thick description, mixed methods research, and pragmatic trials. Next, we elaborate some approaches suggested by Polit and Beck in order to justify generalizability in IS research, namely replication, triangulation, and interdisciplinary studies. First, in terms of replication, IS research at its best has a good tradition of replication. We have seen numerous studies around the ‘‘Technology Acceptance Model (TAM)’’ to not only replicate its basic hypotheses across different contexts but also to incorporate contextual factors to test the extent of its external validity and generalizability. As Polit and Beck (2010) suggested, when replicating prior work, one should think conceptually and reﬂexively and try to challenge existing theory and methodology to come up with new contributions. Second, in terms of triangulation, the IS discipline embraces pluralist methodology and triangulation (Mingers, 2001). Since each methodology has limitations, which may hamper the justiﬁcation of generalizability, we need to integrate appropriate methodologies. For instance, some researchers believe that ﬁeld experimentation helps to increase external validity. Nonetheless, Dipboye and Flanagan (1979) showed that even though the context of a ﬁeld experiment is in a given real-world setting, since the settings may differ dramatically, ﬁndings from one real-world setting may or may not generalize to another (real-world) setting. They suggested the best way is to adopt coordinated strategies of research in both laboratory and ﬁeld settings. For example, Ba and Pavlou (2002) studied how feedback systems build trust and lead to higher auction prices using both an online ﬁeld experiment and also archival ﬁeld data. The reason for the latter method is to enhance the external validity by examining the real-world auction market. Such triangulations of methods for generalization have been observed more and more in IS studies, and this is hopefully a promising trend moving forward. Finally, IS, as an interdisciplinary research ﬁeld, has its own advantage to enhance theories and methods by borrowing from related ﬁelds. For instance, a new research stream, which is termed ‘‘NeuroIS’’ (Dimoka et al., 2010, 2012), recently commenced to apply neuroscience theories and neurophysiological tools (e.g., functional magnetic resonance

imaging (fMRI) and eye tracking) into IS research. In the study of ‘‘What Does the Brain Tell Us About Trust and Distrust?’’ (Dimoka, 2010), fMRI was used to complement traditional psychometric measures of trust and distrust by observing the location, timing, and level of brain activity that underlies trust and distrust. The results showed that trust and distrust activate different brain areas and have different effects, implying that consumers may show both trust and distrust simultaneously when making decisions in online markets under uncertainty. The uncovering hidden processes cannot be easily captured with traditional self-reported tools, and neuroscience methods, which are particularly strong in terms of internal validity by looking directly into the brain, helped to enhance the validity of existing methods. Another recent study (Venkatraman et al., 2015) used novel measures through eight methods (i.e., self-reports, implicit measures, eye tracking, skin conductance, heart rate, breathing, electroencephalography, and fMRI) to predict advertising success. The authors showed that fMRI can explain the most variance in advertising elasticities beyond traditional measures, thereby offering another strong integration of internal validity (by looking directly into the brain) with external validity (by showing that the brain activations predict realworld outcomes). It is not difﬁcult to see that the integration of IS with the hard sciences, such as neuroscience, is a promising step in pursuit of both internal validity and external validity (generalizability). A note of caution on generalizability This paper generally agrees with the call that Davison and Martinsons made that context should matter more in IS research given the liberal assertions of generalizability of context-speciﬁc research ﬁndings. However, if all IS scholars emphasize their own particular and idiosyncratic context, how can we derive knowledge claims that can generalize to other contexts? That is, would the speciﬁc ﬁndings from context-bounded studies be useful, helpful, and insightful (or generalizable) to other contexts? Hence, we stress that the ultimate goal of scientiﬁc research should be to create generalizable knowledge claims, which cannot be reached solely by idiosyncratic context-speciﬁc studies, and while context is ‘‘King,’’ ultimately we should strive, as IS discipline, toward generalizability (which we term ‘‘The Emperor’’). And despite our bold statement that generalizability is the ‘‘Emperor,’’ we want to conclude with a note of caution based on a historical and philosophical perspective on generalizability. We would like to stress the concerns of credibility and applicability when making claims about the generalizability of empirical ﬁndings. As a matter of fact, scientiﬁc research in general is suffering an upheaval. An article by Lehrer (2010), titled ‘‘The truth wears off’’ in the New Yorker, shows a ‘‘decline effect’’ that many results that are rigorously proved and accepted start shrinking in later studies. Lehrer believes that the decline effect is a decline of illusion, the illusion of ‘‘reliable’’ large dataset, the illusion of ‘‘rigorous’’ methods, and the illusion of a ‘‘solid’’ theoretical background. The decline effect is along with ‘‘replication crisis’’ identiﬁed subsequently in most main ﬁelds in social science. In the psychology ﬁeld, a study from an Open Science Collaboration project, published in Science (2015), conducted replications of 100 experimental and correlational studies

Context may be King, but generalizability is the Emperor!

A. Cheng et al.

261

published in three psychology journals using high-powered designs and original materials when available. The results are as follows (pp. aac4716-1, Vol 349, Issue 6251): Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically signiﬁcant results. Thirty-six percent of replications had statistically signiﬁcant results; 47% of original effect sizes were in the 95% conﬁdence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically signiﬁcant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. In the economics ﬁeld, a study (Chang and Li, 2015), titled ‘‘Is Economic Research Replicable,’’ attempted to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication ﬁles that included both the data and code. The results are shown below (p. 1). Aside from 6 papers that use conﬁdential data, we obtain data and code replication ﬁles for 29 of 35 papers (83%) that are required to provide such ﬁles as a condition of publication, compared to 11 of 26 papers (42%) that are not required to provide data and code replication ﬁles. We successfully replicate the key qualitative result of 22 of 67 papers (33%) without contacting the authors. Excluding the 6 papers that use conﬁdential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49%) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. In the sociology ﬁeld, a PhD candidate of Princeton University, Christobal Young (now an Assistant Professor at Stanford University), published a paper (2009) in the American Sociological Review to challenge a highly cited research by well-known Harvard scholars – Barro and McCleary (2003) – because of their nonreplicable ﬁndings. And he raised a concern on the common phenomena of model uncertainty and selective reporting. The abstract of his paper is quoted below (p. 380). Model uncertainty is pervasive in quantitative research. Classical statistical theory assumes that only one (true) model is applied to a sample of data. In practice, however, researchers do not know which exact model speciﬁcation is best. Modern computing power allows researchers to estimate a huge number of plausible models, yet only a few of these estimates are published. The result is a severe asymmetry of information between analyst and reader. The applied modeling process produces a much wider range of estimates than is suggested by the usual standard errors or conﬁdence intervals. I demonstrate this using the work of Barro and McCleary on religion and economic growth. Small, sensible changes in their model speciﬁcation produce large changes in the results: the results are

inconsistent across time, and the instrumental variables strategy suffers from a weak instrument set. Also, the observed relationship between religiosity and economic growth does not hold in the West; it is largely a feature of Asian and African countries and of countries whose data is poor quality. In short, empirical ﬁndings should be evaluated not just by their signiﬁcance but also by their robustness to model speciﬁcation. I conclude with suggestions for incorporating model uncertainty into practice and improving the transparency of social science research. In response to the increasing concern of fallible research ﬁndings, Ioannidis (2005) published a paper with a striking title, ‘‘Why most published research ﬁndings are false?’’ He showed that the probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientiﬁc ﬁeld. His main conclusion was (p. 40): A research ﬁnding is less likely to be true when the studies conducted in a ﬁeld are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater ﬂexibility in designs, deﬁnitions, outcomes, and analytical modes; when there is greater ﬁnancial and other interest and prejudice; and when more teams are involved in a scientiﬁc ﬁeld in chase of statistical signiﬁcance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientiﬁc ﬁelds, claimed research ﬁndings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research. In the strategy ﬁeld, most management strategy scholars are familiar with Clayton M. Christensen and his theory of disruptive innovation (1997). Few other theories can have so much inﬂuence in the business world as disruptive innovation theory. Nonetheless, an article (King and Baatartogtokh, 2015) published recently in the MIT Sloan Management Review asked a question: ‘‘How well does this theory describe what actually happens in business’’ (http://sloanreview.mit. edu/article/how-useful-is-the-theory-of-disruptive-innovation/ ), concluding with unsatisfactory results. King and Baatartogtokh spent two years digging into disruption, interviewing experts, trying to test whether 77 of Christensen’s own examples conformed to his theory. This investigation involved large companies, such as Ford, McDonald’s, and Google, as well as lesser-known makers of blood-glucose meters and blended plastics. Only a small portion, 9%, ﬁt Christensen’s criteria. Their conclusion of this study is also an alarming problem for management scholars (p. 77). 1) The theory’s essential validity and generalizability have seldom been tested in the academic literature. (2) Many of the theory’s exemplary cases did not ﬁt all its conditions and predictions well; 3) Theories can provide warnings of what may happen, but they are no substitute for thoughtful analysis.

Context may be King, but generalizability is the Emperor! 262

A. Cheng et al.

Illustrating these examples from several related disciplines helps us understand that other disciplines struggle with the issues noted by Davison and Martinsons. As scientists, we ought to know our responsibility for the cumulative scientiﬁc enterprises. The core responsibility is to use our unique ability to push forward scientiﬁc progress. Here the unique ability is the ability to connect evidence and theory to establish generalizable knowledge claims about the world we live in. Contextualization does matter as to the rigor of research, but generalizability is what probably only scientists could do best. Therefore, we posit the ultimate goal of scientiﬁc research is to generalize knowledge claims, and to achieve this goal, evidence and theory should be collected, formulated, accumulated, and integrated from multiple contexts to cautiously derive generalizable knowledge claims. We would like to conclude our premise by arguing that contextualization (or particularism) and generalization (or universalism) are not two extreme, mutually exclusive, ends of a continuum, but rather contextual studies, in combination and replication, can help us to cautiously make generalizable knowledge claims. That is, to fully understand the context is the basis to eventually derive generalizability. Along these lines, we would like to conclude by citing Rousseau and Fried (2001) who tried to explain the relationship between contextualization and generalization (p. 1): The term ‘context’ comes from a Latin root meaning ‘to knit together’ or ‘to make a connection.’ Contextualizing entails linking observations to a set of relevant facts, events, or points of view that make possible research and theory that form part of a larger whole. And while context may be the King, making the connection across contexts is what scientiﬁc research is about to form the larger whole, what we posit here as ‘‘The Emperor.’’ Notes
1 Although the concepts of external validity and generalizability are frequently exchangeable (e.g., Deaton, 2010; Imbens, 2010; Seddon and Scheepers, 2012), they are different notions (Mook, 1983). External validity is an evaluation criterion for the rigor of a study, but generalizability comes from the term ‘‘generalize,’’ described as ‘‘to form general notions by abstraction from particular instances’’ (Lee and Baskerville, 2003, p. 221). 2 In the IS discipline, most positivists view generalization as statistical, sampling-based (Williams and Tsang, 2012), whereas interpretivists view generalization as analytical (Yin, 1994). Lee and Baskerville (2012) extended the notion of generalizability by integrating both notions from the positivism and the interpretivism perspectives. For more rich discussion about the meaning of generalization, please refer to Seddon and Scheepers (2015).

References
Adomavicius, G., Sankaranarayanan, R., Sen, S. and Tuzhilin, A. (2005). Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach, ACM Transactions on Information Systems (TOIS) 23(1): 103–145. Aronson, E., Wilson, T.D. and Sommers, S.R. (2013). Social Psychology, Boston: Pearson.

Atasoy, H., Banker, R. and Pavlou, P.A. (2016). Firm-Level Evidence of the Longitudinal Effects of IT Use on Employment, Information Systems Research 27(1): 6–26. Avgerou, C. (2001). The Signiﬁcance of Context in Information Systems and Organizational Change, Information Systems Journal 11(1): 43–63. Ba, S. and Pavlou, P.A. (2002). Evidence of the Effect of Trust Building Technology in Electronic Markets: Price Premiums and Buyer Behavior, MIS Quarterly 26(3): 243–268. Barney, J.B. and Zhang, S. (2009). The Future of Chinese Management Research: A Theory of Chinese Management Versus a Chinese Theory of Management, Management and Organization Review 5(1): 15–28. Barro, R.J. and McCleary, R.M. (2003). Religion and Economic Growth Across Countries, American Sociological Review 68(5): 760–781. Boudreau, M.-C., Gefen, D. and Straub, D.W. (2001). Validation in Information Systems Research: A State-of-the-art Assessment, MIS Quarterly 25(1): 1–16. Boyd, B.K., Gove, S. and Hitt, M.A. (2005). Construct Measurement in Strategic Management Research: Illusion or Reality? Strategic Management Journal 26(3): 239–257. Calder, B.J., Phillips, L.W. and Tybout, A.M. (1982). The Concept of External Validity, Journal of Consumer Research 9: 240–244. Campbell, D.T. (1986). Relabeling Internal and External Validity for Applied Social Scientists, New Directions for Program Evaluation 31: 67–77. Campbell, D.T. and Stanley, J.C. (1967). Experimental and Quasi Experimental Designs for Research. Chicago: Rand Mc Nally and Co Inc. Chang, A.C. and Li, P. (2015). Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say ‘Usually Not’, SSRN. Retrieved from http:// www.federalreserve.gov/econresdata/feds/2015/ﬁles/2015083pap.pdf. Christenson, C. (1997). The Innovator’s Dilemma. Cambridge, MA: Harvard Business School Press. Deaton, A. (2010). Instruments, Randomization, and Learning About Development, Journal of Economic Literature 48: 424–455. Dimoka, A. (2010). What Does the Brain Tell Us About Trust and Distrust? Evidence from a Functional Neuroimaging Study, MIS Quarterly 34(2): 373–396. Dimoka, A., Banker, R.D., Benbasat, I., Davis, F.D., Dennis, A.R., Gefen, D., ¨ ller-Putz, G., Riedl, Gupta, A., Ischebeck, A., Kenning, P.H., Pavlou, P.A., Mu R., vom Brocke, J. and Weber, B. (2012). On the Use of Neurophysiological Tools in is Research: Developing a Research Agenda for NeuroIS, MIS Quarterly 36(3): 679–719. Dimoka, A., Hong, Y. and Pavlou, P.A. (2012b). On Product Uncertainty in Online Markets: Theory and Evidence, MIS Quarterly 36(2): 395–426. Dimoka, A., Pavlou, P.A. and Davis, F.D. (2010). Research Commentary – NeuroIS: The Potential of Cognitive Neuroscience for Information Systems Research, Information Systems Research 22(4): 687–702. Dipboye, R.L. and Flanagan, M.F. (1979). Research Settings in Industrial and Organizational Psychology: Are Findings in the Field More Generalizable than in the Laboratory? American Psychologist 34(2): 141. Ettlie, J. and Pavlou, P.A. (2006). Technology-Based New Product Development Partnerships, Decision Sciences 37(2): 117–148. Gefen, D. and Pavlou, P.A. (2012). The Boundaries of Trust and Risk: The Quadratic Moderating Role of Institutional Structures, Information Systems Research 23(3): 940–959. Gregor, S. (2006). The Nature of Theory in Information Systems, MIS Quarterly 30: 611–642. Heckman, J.J. and Urzua, S. (2010). Comparing IV with Structural Models: What Simple IV Can and Cannot Identify, Journal of Econometrics 156(1): 27–37. Hong, W., Chan, F.K., Thong, J.Y., Chasalow, L.C. and Dhillon, G. (2013). A Framework and Guidelines for Context-Speciﬁc Theorizing in Information Systems Research, Information Systems Research 25(1): 111–136. Hong, Y. and Pavlou, P.A. (2014). Product Fit Uncertainty: Nature, Effects, and Antecedents, Information Systems Research 25(2): 328–344. Hong, Y., Wang, A. and Pavlou, P.A. (2015). Comparing Open and Sealed Bid Auctions: Evidence from Online Labor Markets, Information Systems Research (forthcoming). Hubbard, R., Vetter, D.E. and Little, E.L. (1998). Replication in Strategic Management: Scientiﬁc Testing for Validity, Generalizability, and Usefulness, Strategic Management Journal 19(3): 243–254. Imbens, G.W. (2010). Better LATE Than Nothing: Some Comments on Deaton (2009) and Heckman and Urzua (2009). Journal of the Economic Literature 48(2): 399–423.

Context may be King, but generalizability is the Emperor!

A. Cheng et al.

263

Ioannidis, J.P. (2005). Why Most Published Research Findings are False, Chance 18(4): 40–47. Joshi, A. and Roh, H. (2009). The Role of Context in Work Team Diversity Research: A Meta-Analytic Review, Academy of Management Journal 52(3): 599–627. King, A.A. and Baatartogtokh, B. (2015). How Useful is the Theory of Disruptive Innovation? MIT Sloan Management Review 57(1): 77. Lee, A.S. and Baskerville, R.L. (2003). Generalizing Generalizability in Information Systems Research, Information Systems Research 14(3): 221–243. Lee, A.S. and Baskerville, R.L. (2012). Conceptualizing Generalizability: New Contributions and a Reply, MIS Quarterly 36(3): 749–761. Lehrer, J. (2010). The Truth Wears Off, The New Yorker 13: 52. Leidner, D.E. and Kayworth, T. (2006). Review: A Review of Culture in Information Systems Research: Toward a Theory of Information Technology Culture Conﬂict, MIS Quarterly 30(2): 357–399. Li, L., Gao, P. and Mao, J. (2014). Research on IT in China: A Call for Greater Contextualization, Journal of Information Technology 29(3): 208–222. Lukka, K. and Kasanen, E. (1995). The Problem of Generalizability: Anecdotes and Evidence in Accounting Research, Accounting, Auditing & Accountability Journal 8(5): 71–90. Lynch, J.G., Jr. (1982). On the External Validity of Experiments in Consumer Research, Journal of Consumer Research 9: 225–239. Lynch, J.G. (1983). The Role of External Validity in Theoretical Research, Journal of Consumer Research 10: 109–111. Lynch, J.G. (1999). Theory and external validity, Journal of the Academy of Marketing Science 27(3): 367–376. McGrath, J.E. and Brinberg, D. (1983). External Validity and the Research Process: A Comment on the Calder/Lynch Dialogue, Journal of Consumer Research 10(1): 115–124. Miller, A. and Dess, G.G. (1993). Assessing Porter’s (1980) Model in Terms of Its Generalizability, Accuracy and Simplicity, Journal of Management Studies 30(4): 553–585. Mingers, J. (2001). Combining IS Research Methods: Towards a Pluralist Methodology, Information Systems Research 12(3): 240–259. Mook, D.G. (1983). In Defense of External Invalidity, American Psychologist 38(4): 379. Open Science Collaboration. (2015). Estimating the Reproducibility of Psychological Science, Science 349(6251): aac4716. Pang, M.-S., Tafti, A. and Krishnan, M.S. (2014). Information Technology and Administrative Efﬁciency in US State Governments: A Stochastic Frontier Approach, Management Information Systems Quarterly 38(4): 1079–1101. Pavlou, P.A. (2011). State of the Information Privacy Literature: Where are We Now and Where Should We Go? MIS Quarterly 35(4): 977–988. Pavlou, P.A. and Dimoka, A. (2006). The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation, Information Systems Research 17(4): 391–412. Pavlou, P.A. and El Sawy, O.A. (2006). From IT Leveraging Competence to Competitive Advantage in Turbulent Environments: The Case of New Product Development, Information Systems Research 17(3): 198–227. Pavlou, P.A. and Fygenson, M. (2006). Understanding and Predicting Electronic Commerce Adoption: An Extension of the Theory of Planned Behavior, MIS Quarterly 30(1): 115–143. Polit, D.F. and Beck, C.T. (2010). Generalization in Quantitative and Qualitative Research: Myths and Strategies, International Journal of Nursing Studies 47(11): 1451–1458. Rai, A., Pavlou, P.A., Im, G. and Steve, D. (2012). Inter-ﬁrm IT Capability Proﬁles and Communications for Co-Creating Relational Value: Evidence from the Logistics Industry, MIS Quarterly 36(1): 233–262. Rosenzweig, P.M. (1994). When can Management Science Research be Generalized Internationally? Management Science 40(1): 28–39. Rousseau, D.M. and Fried, Y. (2001). Location, Location, Location: Contextualizing Organizational Research, Journal of organizational behavior 22(1): 1–13. Scandura, T.A. and Williams, E.A. (2000). Research Methodology in Management: Current Practices, Trends, and Implications for Future Research, Academy of Management Journal 43(6): 1248–1264. Seddon, P.B. and Scheepers, R. (2012). Towards the Improved Treatment of Generalization of Knowledge Claims in IS Research: Drawing general Conclusions from Samples, European Journal of Information Systems 21(1): 6–21.

Seddon, P.B. and Scheepers, R. (2015). Generalization in IS Research: A Critique of the Conﬂicting Positions of Lee & Baskerville and Tsang & Williams, Journal of Information Technology 30(1): 30–43. Shadish, W.R., Cook, T.D. and Campbell, D.T. (2002), Experimental and QuasiExperimental Designs for Generalized Causal Inference. Belmont, CA: Wadsworth Cengage Learning. Short, J.C., Ketchen, D.J. and Palmer, T.B. (2002). The Role of Sampling in Strategic Management Research on Performance: A Two-Study Analysis, Journal of Management 28(3): 363–385. Smith, H.J., Dinev, T. and Xu, H. (2011). Information Privacy Research: An Interdisciplinary Review, MIS Quarterly 35(4): 989–1016. Tsui, A.S. (2006). Contextualization in Chinese Management Research, Management and Organization Review 2(1): 1–13. Venkatraman, V., Dimoka, A., Pavlou, P.A., Vo, K., Hampton, W., Bollinger, B., Hershﬁeld, H.E., Ishihara, M. and Winer, R.S. (2014). Predicting Advertising Success Beyond Traditional Measures: New Insights from Neurophysiological Methods and Market Response Modeling, Journal of Marketing Research 52(4): 436–452. Williams, J. and Tsang, E. (2012). Generalization and Hume’s Problem of Induction: Misconceptions and Clariﬁcations, Management Information Systems Quarterly 36(3): 729–748. Yin, R. (1994). Case Study Research: Design and Methods. Beverly Hills, CA: Sage. Young, C. (2009). Model Uncertainty in Sociological Research: An Application to Religion and Economic Growth, American Sociological Review 74(3): 380–397.

About the Authors Zhi (Aaron) Cheng is a doctoral student on the concentration of Management Information Systems (MIS) at Fox School of Business, Temple University. His research interests lie in economics of information technology (IT), IT and public policy, societal impact of IT (on public health, education, transportation, and etc.), as well as various quantitative methods (econometrics, machine learning and randomized ﬁeld experiment). He has his research accepted and presented at prestigious conferences including the International Conference on Information Systems (ICIS), American Conference on Information Systems (AMCIS), INFORMS Annual Meeting, Conference on Information System & Technology (CIST), Statistical Challenges in e-Commerce Research (SCECR), among others. Angelika Dimoka has been appointed as an Associate Professor of Marketing and the Marvin Wachman Senior Research Fellow. She also holds a secondary appointment in Management Information Systems and the College of Engineering. Dr. Dimoka earned her PhD degree in Biomedical Engineering with emphasis in Neuroscience from the University of Southern California and engages in cuttingedge research on Decision Neuroscience - the study of how the brain functions when humans make decisions. She is the Director of the Center for Neural Decision Making, which uses brain imaging and physiological measures in combination with traditional behavioral measures to develop models on how humans make decisions. Her research appeared in the top journals in the ﬁeld (Journal of Marketing Research, Information Systems Research, Management of Information Systems Quarterly, NeuroImage, Annals of Biomedical Engineering, and the IEEE Transactions on Biomedical Engineering) as well as in popular press such as Newsweek, Forbes, NPR, CBS, and other international media outlets.

Context may be King, but generalizability is the Emperor! 264

A. Cheng et al.

Paul A. Pavlou is Associate Dean of Research, Doctoral Programs, and Strategic Initiatives at the Fox School of Business at Temple University. He is also the Milton F. Stauffer Professor of Information Technology and Strategy. He also serves as the Co-Director of the university-wide Big Data Institute at Temple University. As the Fox School’s Chief Research Ofﬁcer, Paul oversees all research activities and research-oriented centers. He also administers over $5M in research grants. Paul is responsible for the development and mentoring of faculty and PhD students. During his tenure, ﬁve departments in the Fox School’s ranked among the Top 10 in the nation in terms of research productivity. Paul developed and implemented the FOX RESEARCH brand to enhance the Fox School’s research proﬁle, including leading the development of the ‘Research Impact Report’ to demonstrate the broader impact of the Fox School’s research for academia, industry, practice, and society. Paul is also responsible for all doctoral programs at the Fox School, such as the PhD in Business Administration, PhD in Statistics, the Inter-Disciplinary PhD program in Decision Neuroscience, and the Executive Doctorate in Business Administration (EDBA), a newly-created doctoral program catered to senior executives. Paul also leads several strategic initiatives at the department, school, and university levels, such as Data Science. As Co-Director of the Big Data Institute, Paul oversees the business analytics programs at the Fox School, including Masters in Business Analytics, Masters in Statistical Science, and Bachelors in Statistical Science and Data Science. He also facilitates grant proposals across schools and promotes the Institute’s fundraising efforts. Paul received his Ph.D. from the University of Southern California. He was ranked ﬁrst in the world in publications in the two top MIS

journals (MISQ and ISR) for 2010–2014. His work has been cited over 20,000 times by Google Scholar. Paul was also recognized among the ‘‘World’s Most Inﬂuential Scientiﬁc Minds’’ by Thomson Reuters based on analysis of ‘‘Highly Cited’’ authors during the 2002–2012 period. His research appeared in MIS Quarterly (MISQ), Information Systems Research (ISR), Journal of Marketing, Journal of Marketing Research, Journal of Management Information Systems, Journal of the Association of Information Systems, Journal of the Academy of Marketing Science, and Decision Sciences, among others. His research spans multiple disciplines—information systems, marketing, and strategic management—and it focuses on data analytics, e-commerce strategy, and the development of research methods. Paul won several Best Paper recognitions for his research, including the Maynard Award nomination for the ‘‘Most Signiﬁcant Contribution to Marketing’’ in the Journal of Marketing in 2015, the ISR Best Paper award in 2007, the 2006 IS Publication of the Year award, the Top 5 Papers award in Decision Sciences in 2006, the Runner-Up to the Best Paper award of the 2005 Academy of Management Conference, the Best Doctoral Dissertation award of the 2004 International Conference on Information Systems (ICIS), the Best Interactive Paper award of the 2002 Academy of Management Conference, and the Best Student Paper award and the Best Paper Award of the Academy of Management Conference in 2001 and 2012, respectively. Finally, Paul also won several Reviewer awards, including the 2009 Management Science Meritorious service award, the ‘Best Reviewer’ award of the 2005 Academy of Management Conference (OCIS Division), and the 2003 MISQ ‘Reviewer of the Year’ award. Paul has been a Senior Editor at MISQ and ISR.

